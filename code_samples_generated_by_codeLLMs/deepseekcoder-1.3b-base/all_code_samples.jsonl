{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport time\n\tfrom datetime import datetime\n\n\tseconds = int(nanoseconds / 1e9)\n\tnow = datetime.now()\n\n\ttry:\n\t\tdt = datetime.utcfromtimestamp(seconds)\n\texcept ValueError:\n\t\tprint(\"Could not convert nanoseconds to datetime.\")\n\t\treturn\n\n\tif tz:\n\t\tdt = dt.replace(tzinfo=tz)\n\n\treturn dt.strftime(\"%H:%M:%S\")", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\ttry:\n\t\tfrom datetime import datetime\n\texcept ImportError:\n\t\tfrom time import strptime\n\n\tdef _to_str(time):\n\t\tif tz:\n\t\t\ttime = time.replace(tzinfo=tz)\n\t\treturn time.strftime('%d/%m/%Y %H:%M:%S')\n\n\tif isinstance(nanoseconds, datetime):\n\t\treturn _to_str(nanoseconds)\n\telif isinstance(nanoseconds, str):\n\t\treturn _to_str(datetime.fromtimestamp(float(nanoseconds)))\n\telif isinstance(nanoseconds, int):\n\t\treturn _to_str(datetime.fromtimestamp(float(nanoseconds)))\n\telif isinstance(nanoseconds, float):\n\t\treturn _to_str(datetime.fromtimestamp(float(nanoseconds)))\n\telif isinstance(nanoseconds, basestring):\n\t\ttry:\n\t\t\treturn _to_str(datetime.strptime(nanoseconds, '%Y-%m-%dT%H:%M:%S'))\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\treturn _to_str(datetime.strptime(nanoseconds, '%Y", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tdelta = timedelta(nanoseconds=nanoseconds)\n\tif tz is None:\n\t\ttz = datetime.utcfromtimestamp(0)\n\tdt = tz + delta\n\treturn dt.strftime(\"%H:%M:%S\")\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\ttry:\n\t\tfrom datetime import datetime\n\texcept ImportError:\n\t\tfrom datetime import datetime as datetime\n\ttz = datetime.now().astimezone(tz).tzinfo\n\tdt = datetime.utcfromtimestamp(nanoseconds/1000000000)\n\treturn dt.strftime(\"%H:%M:%S\")", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\timport time\n\t\n\ttry:\n\t\tseconds = nanoseconds / 1e9\n\t\tdt = time.gmtime(seconds)\n\t\tif (tz):\n\t\t\tdt = time.tzlocal().localize(dt)\n\texcept:\n\t\traise ValueError(\"Invalid nanoseconds\")\n\t\n\treturn dt\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tsecs = int(nanoseconds / 1000000000)\n\tnanos = nanoseconds % 1000000000\n\treturn \"%02d:%02d:%02d.%06d\" % (secs / 3600, secs % 3600 / 60, secs % 60, nanos)", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport time\n\ttry:\n\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds / 1e9))\n\texcept ValueError:\n\t\traise ValueError(\"Invalid time, use a number of nanoseconds\")\n\n#src/main.py\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Jul 13 22:16:46 2019\n\n@author: yulia\n\"\"\"\n\nimport sys\nfrom datetime import datetime\nfrom time import time\nfrom utils import hydrate_time\nfrom utils import parse_line\nfrom utils import get_dur\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport time\n\treturn time.asctime(time.gmtime(nanoseconds / 1e9))\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tseconds = nanoseconds / 1e9\n\ttime = datetime.fromtimestamp(seconds)\n\ttime = time.astimezone(tz) if tz else time\n\ttime = time.strftime(\"%I:%M %p\")\n\treturn time\n\n", "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom time import strftime, gmtime\n\n\tdef to_string(t):\n\t\t\"\"\"\n\t\tReturn a string representing the time in fixed format.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn strftime(\"%H:%M\", gmtime(t))\n\t\texcept ValueError:\n\t\t\treturn \"00:00\"\n\n\treturn to_string(nanoseconds / 1000000000)\n\n#src/main/python/sensor_handler.py\nfrom datetime import datetime\nimport os\nimport json\nimport logging\nfrom time import time\n\nfrom . import utils\n\nlogging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\nlog = logging.getLogger(\"sensor_handler\")\nlog.setLevel(logging.INFO)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport time\n\tfrom datetime import datetime\n\n\tseconds = int(nanoseconds / 1e9)\n\tnow = datetime.now()\n\n\ttry:\n\t\tdt = datetime.utcfromtimestamp(seconds)\n\texcept ValueError:\n\t\tprint(\"Could not convert nanoseconds to datetime.\")\n\t\treturn\n\n\tif tz:\n\t\tdt = dt.replace(tzinfo=tz)\n\n\treturn dt.strftime(\"%H:%M:%S\")", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\ttry:\n\t\tfrom datetime import datetime\n\texcept ImportError:\n\t\tfrom time import strptime\n\n\tdef _to_str(time):\n\t\tif tz:\n\t\t\ttime = time.replace(tzinfo=tz)\n\t\treturn time.strftime('%d/%m/%Y %H:%M:%S')\n\n\tif isinstance(nanoseconds, datetime):\n\t\treturn _to_str(nanoseconds)\n\telif isinstance(nanoseconds, str):\n\t\treturn _to_str(datetime.fromtimestamp(float(nanoseconds)))\n\telif isinstance(nanoseconds, int):\n\t\treturn _to_str(datetime.fromtimestamp(float(nanoseconds)))\n\telif isinstance(nanoseconds, float):\n\t\treturn _to_str(datetime.fromtimestamp(float(nanoseconds)))\n\telif isinstance(nanoseconds, basestring):\n\t\ttry:\n\t\t\treturn _to_str(datetime.strptime(nanoseconds, '%Y-%m-%dT%H:%M:%S'))\n\t\texcept ValueError:\n\t\t\ttry:\n\t\t\t\treturn _to_str(datetime.strptime(nanoseconds, '%Y", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tdelta = timedelta(nanoseconds=nanoseconds)\n\tif tz is None:\n\t\ttz = datetime.utcfromtimestamp(0)\n\tdt = tz + delta\n\treturn dt.strftime(\"%H:%M:%S\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\ttry:\n\t\tfrom datetime import datetime\n\texcept ImportError:\n\t\tfrom datetime import datetime as datetime\n\ttz = datetime.now().astimezone(tz).tzinfo\n\tdt = datetime.utcfromtimestamp(nanoseconds/1000000000)\n\treturn dt.strftime(\"%H:%M:%S\")", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\t\n\timport time\n\t\n\ttry:\n\t\tseconds = nanoseconds / 1e9\n\t\tdt = time.gmtime(seconds)\n\t\tif (tz):\n\t\t\tdt = time.tzlocal().localize(dt)\n\texcept:\n\t\traise ValueError(\"Invalid nanoseconds\")\n\t\n\treturn dt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tsecs = int(nanoseconds / 1000000000)\n\tnanos = nanoseconds % 1000000000\n\treturn \"%02d:%02d:%02d.%06d\" % (secs / 3600, secs % 3600 / 60, secs % 60, nanos)", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport time\n\ttry:\n\t\treturn time.strftime(\"%H:%M:%S\", time.localtime(nanoseconds / 1e9))\n\texcept ValueError:\n\t\traise ValueError(\"Invalid time, use a number of nanoseconds\")\n\n#src/main.py\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Jul 13 22:16:46 2019\n\n@author: yulia\n\"\"\"\n\nimport sys\nfrom datetime import datetime\nfrom time import time\nfrom utils import hydrate_time\nfrom utils import parse_line\nfrom utils import get_dur\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\timport time\n\treturn time.asctime(time.gmtime(nanoseconds / 1e9))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tseconds = nanoseconds / 1e9\n\ttime = datetime.fromtimestamp(seconds)\n\ttime = time.astimezone(tz) if tz else time\n\ttime = time.strftime(\"%I:%M %p\")\n\treturn time\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\t\"\"\"\n\tConvert nanoseconds to a time in fixed format.\n\t\"\"\"\n\tfrom time import strftime, gmtime\n\n\tdef to_string(t):\n\t\t\"\"\"\n\t\tReturn a string representing the time in fixed format.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn strftime(\"%H:%M\", gmtime(t))\n\t\texcept ValueError:\n\t\t\treturn \"00:00\"\n\n\treturn to_string(nanoseconds / 1000000000)\n\n#src/main/python/sensor_handler.py\nfrom datetime import datetime\nimport os\nimport json\nimport logging\nfrom time import time\n\nfrom . import utils\n\nlogging.basicConfig(format=\"[%(asctime)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\nlog = logging.getLogger(\"sensor_handler\")\nlog.setLevel(logging.INFO)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tparts = []\n\tif value.days:\n\t\tparts.append(gettext(\"%(number)d days\") % {\"number\": value.days})\n\tif value.seconds:\n\t\tparts.append(gettext(\"%(number)d seconds\") % {\"number\": value.seconds})\n\treturn \" \".join(parts) if parts else None", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\ttime_delta = value.days * 86400 + value.seconds\n\t\treturn time_delta\n\texcept AttributeError:\n\t\traise ValueError('Value must be a timedelta')\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\n\treturn time_delta_to_structure(value)", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\tvalue = int(value.total_seconds())\n\texcept AttributeError:\n\t\traise TypeError(\"Timedelta can not be converted to int\")\n\n\treturn Structure(value=value)", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn Timedelta(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n\texcept ValueError:\n\t\treturn None\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom django.db.models.fields import FloatField\n\tfrom django.db.models.fields.related_descriptors import ForwardManyToOneDescriptor\n\tfrom django.utils import timezone\n\tfrom django.utils.formats import get_format\n\n\tclass TimedeltaField(FloatField):\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tkwargs['max_digits'] = 24\n\t\t\tkwargs['decimal_places'] = 8\n\t\t\tsuper(TimedeltaField, self).__init__(*args, **kwargs)\n\n\t\tdef to_python(self, value):\n\t\t\tif value is None:\n\t\t\t\treturn None\n\t\t\treturn timezone.timedelta(seconds=value)\n\n\t\tdef get_prep_value(self, value):\n\t\t\tif value is None:\n\t\t\t\treturn None\n\t\t\treturn value.total_seconds()\n\n\t\tdef value_to_string(self, obj):\n\t\t\tvalue = self.value_from_object(obj)\n\t\t\tif value is None:\n\t\t\t\treturn None\n\t\t\treturn get_format(self.get_internal_type()).format(value)\n\n\tfield = TimedeltaField()\n\tsetattr(field, 'dehydrate', dehydrate_timedelta)\n\tset", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tparts = value.split(':')\n\tif len(parts) == 3:\n\t\treturn '{day}:{hour}:{min}:{sec}'.format(day=parts[0], hour=parts[1], min=parts[2], sec=parts[3])\n\telse:\n\t\treturn '{hour}:{min}:{sec}'.format(hour=parts[0], min=parts[1], sec=parts[2])\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tvalue = int(value.total_seconds())\n\treturn value\n\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tif isinstance(value, timedelta):\n\t\treturn Structure(value.total_seconds())\n\telse:\n\t\treturn value\n", "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tvalue = int(value.total_seconds())\n\n\tif value < 0:\n\t\treturn -1\n\n\tif value > 2**31:\n\t\traise ValueError('The value must be between -2**31 and 2**31')\n\n\treturn value\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tparts = []\n\tif value.days:\n\t\tparts.append(gettext(\"%(number)d days\") % {\"number\": value.days})\n\tif value.seconds:\n\t\tparts.append(gettext(\"%(number)d seconds\") % {\"number\": value.seconds})\n\treturn \" \".join(parts) if parts else None", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\ttime_delta = value.days * 86400 + value.seconds\n\t\treturn time_delta\n\texcept AttributeError:\n\t\traise ValueError('Value must be a timedelta')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\t\n\treturn time_delta_to_structure(value)", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\tvalue = int(value.total_seconds())\n\texcept AttributeError:\n\t\traise TypeError(\"Timedelta can not be converted to int\")\n\n\treturn Structure(value=value)", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn Timedelta(days=value.days, seconds=value.seconds, microseconds=value.microseconds)\n\texcept ValueError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom django.db.models.fields import FloatField\n\tfrom django.db.models.fields.related_descriptors import ForwardManyToOneDescriptor\n\tfrom django.utils import timezone\n\tfrom django.utils.formats import get_format\n\n\tclass TimedeltaField(FloatField):\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tkwargs['max_digits'] = 24\n\t\t\tkwargs['decimal_places'] = 8\n\t\t\tsuper(TimedeltaField, self).__init__(*args, **kwargs)\n\n\t\tdef to_python(self, value):\n\t\t\tif value is None:\n\t\t\t\treturn None\n\t\t\treturn timezone.timedelta(seconds=value)\n\n\t\tdef get_prep_value(self, value):\n\t\t\tif value is None:\n\t\t\t\treturn None\n\t\t\treturn value.total_seconds()\n\n\t\tdef value_to_string(self, obj):\n\t\t\tvalue = self.value_from_object(obj)\n\t\t\tif value is None:\n\t\t\t\treturn None\n\t\t\treturn get_format(self.get_internal_type()).format(value)\n\n\tfield = TimedeltaField()\n\tsetattr(field, 'dehydrate', dehydrate_timedelta)\n\tset", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tparts = value.split(':')\n\tif len(parts) == 3:\n\t\treturn '{day}:{hour}:{min}:{sec}'.format(day=parts[0], hour=parts[1], min=parts[2], sec=parts[3])\n\telse:\n\t\treturn '{hour}:{min}:{sec}'.format(hour=parts[0], min=parts[1], sec=parts[2])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tvalue = int(value.total_seconds())\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tfrom datetime import timedelta\n\tif isinstance(value, timedelta):\n\t\treturn Structure(value.total_seconds())\n\telse:\n\t\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\t\"\"\"\n\tUse the value in timedelta to generate the Structure class.\n\t\"\"\"\n\tvalue = int(value.total_seconds())\n\n\tif value < 0:\n\t\treturn -1\n\n\tif value > 2**31:\n\t\traise ValueError('The value must be between -2**31 and 2**31')\n\n\treturn value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\tfrom datetime import timedelta\n\t\n\ttry:\n\t\tif isinstance(value, timedelta):\n\t\t\treturn value\n\t\t\n\t\tif isinstance(value, int):\n\t\t\treturn timedelta(seconds=value)\n\t\t\n\t\tif hasattr(value, 'seconds'):\n\t\t\treturn timedelta(seconds=value.seconds)\n\texcept:\n\t\tpass\n\t\n\treturn None\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn datetime.fromtimestamp(value)\n\texcept AttributeError:\n\t\treturn value\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\treturn Structure(value.seconds)\n\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\tfrom time import Time\n\texcept ImportError:\n\t\tfrom datetime import Time\n\t\n\ttry:\n\t\tvalue = Time(value)\n\texcept (ValueError, TypeError):\n\t\tpass\n\t\n\treturn value.to_tuple()", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttime = time.Time()\n\ttime.from_ticks(value)\n\treturn time\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\timport time\n\timport datetime\n\timport timezone\n\t\n\tif value is None:\n\t\treturn None\n\telif isinstance(value, time.struct_time):\n\t\tvalue = time.mktime(value)\n\telif isinstance(value, datetime.datetime):\n\t\tvalue = value.strftime('%s')\n\telif isinstance(value, timezone.Timezone):\n\t\tvalue = value.utc_offset()\n\t\n\treturn value\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn time.mktime(value.timetuple()) * 1000\n\texcept AttributeError:\n\t\treturn 0\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn time.mktime(time.strptime(value, '%m-%d-%Y %H:%M:%S'))\n\texcept Exception:\n\t\treturn None\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import time\n\treturn time()\n", "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\ttime = Time(value)\n\t\treturn time.ticks\n\texcept ValueError:\n\t\treturn value\n\n#\n# 2. Conversion functions\n#\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\tfrom datetime import timedelta\n\t\n\ttry:\n\t\tif isinstance(value, timedelta):\n\t\t\treturn value\n\t\t\n\t\tif isinstance(value, int):\n\t\t\treturn timedelta(seconds=value)\n\t\t\n\t\tif hasattr(value, 'seconds'):\n\t\t\treturn timedelta(seconds=value.seconds)\n\texcept:\n\t\tpass\n\t\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn datetime.fromtimestamp(value)\n\texcept AttributeError:\n\t\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\t\n\treturn Structure(value.seconds)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\tfrom time import Time\n\texcept ImportError:\n\t\tfrom datetime import Time\n\t\n\ttry:\n\t\tvalue = Time(value)\n\texcept (ValueError, TypeError):\n\t\tpass\n\t\n\treturn value.to_tuple()", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttime = time.Time()\n\ttime.from_ticks(value)\n\treturn time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\timport time\n\timport datetime\n\timport timezone\n\t\n\tif value is None:\n\t\treturn None\n\telif isinstance(value, time.struct_time):\n\t\tvalue = time.mktime(value)\n\telif isinstance(value, datetime.datetime):\n\t\tvalue = value.strftime('%s')\n\telif isinstance(value, timezone.Timezone):\n\t\tvalue = value.utc_offset()\n\t\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn time.mktime(value.timetuple()) * 1000\n\texcept AttributeError:\n\t\treturn 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\treturn time.mktime(time.strptime(value, '%m-%d-%Y %H:%M:%S'))\n\texcept Exception:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\tfrom time import time\n\treturn time()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\t\"\"\"\n\tUse ticks in the Time class to generate the Structure class.\n\t\"\"\"\n\ttry:\n\t\ttime = Time(value)\n\t\treturn time.ticks\n\texcept ValueError:\n\t\treturn value\n\n#\n# 2. Conversion functions\n#\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tSTRUCTURE_CLASS = {\n\t\t1: int,\n\t\t2: int,\n\t\t3: int,\n\t\t4: int,\n\t\t5: int,\n\t\t6: int,\n\t\t7: int,\n\t\t8: int\n\t}\n\n\treturn STRUCTURE_CLASS.get(len(value), str)(value)", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tclass_name = 'Point'\n\tif value < 1000:\n\t\tclass_name = 'IntPoint'\n\telif value < 1000000:\n\t\tclass_name = 'LongPoint'\n\telif value < 1000000000:\n\t\tclass_name = 'FloatPoint'\n\telif value < 1000000000000:\n\t\tclass_name = 'DoublePoint'\n\t\n\treturn getattr(sys.modules[__name__], class_name)(value)\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tstructure = [\n\t\t(2, 'int'),\n\t\t(4, 'int'),\n\t\t(8, 'int'),\n\t\t(0, 'char')\n\t]\n\t\n\tl = len(value)\n\t\n\tif l < 2:\n\t\treturn value\n\telif l < 4:\n\t\treturn (value[0], value[1])\n\telif l < 8:\n\t\treturn (value[0], value[1], value[2], value[3])\n\telse:\n\t\treturn (value[0], value[1], value[2], value[3], value[4], value[5], value[6], value[7])", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue_size = len(value)\n\tif value_size == 4:\n\t\treturn value[0] + value[1] + value[2] + value[3]\n\tif value_size == 3:\n\t\treturn value[0] + value[1] + value[2]\n\tif value_size == 2:\n\t\treturn value[0] + value[1]\n\tif value_size == 1:\n\t\treturn value[0]\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\timport math\n\t\n\tvalue = str(value)\n\t\n\tvalue_len = len(value)\n\t\n\tif value_len == 0:\n\t\treturn 0\n\telif value_len == 1:\n\t\treturn int(value)\n\telse:\n\t\tvalue_len = int(math.log(int(value), 10)) + 1\n\t\tround_value = float(value)/float(10**value_len)\n\t\t\n\t\tif round_value <= 1:\n\t\t\treturn int(round_value)\n\t\telif round_value > 10:\n\t\t\tround_value = float(round_value)\n\t\t\tround_value = round_value * 10\n\t\t\treturn int(round_value)\n\t\telse:\n\t\t\tround_value = float(round_value)\n\t\t\tround_value = round_value * 10\n\t\t\treturn int(round_value)\n\t\t\t", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\timport math\n\n\tvalue = value.replace('-', '').replace('+', '')\n\n\tif len(value) == 1:\n\t\treturn '0' + value\n\telif len(value) == 2:\n\t\treturn value\n\telif len(value) == 3:\n\t\tif value[0] == '0' and value[1] == '0':\n\t\t\treturn value\n\t\telse:\n\t\t\treturn '0' + value\n\telif len(value) == 4:\n\t\tif value[0] == '0' and value[1] == '0' and value[2] == '0':\n\t\t\treturn value\n\t\telse:\n\t\t\treturn '0' * 3\n\telif len(value) == 5:\n\t\tif value[0] == '0' and value[1] == '0' and value[2] == '0' and value[3] == '0':\n\t\t\treturn value\n\t\telse:\n\t\t\treturn '0' * 4\n\telse:\n\t\traise ValueError('The value length must be in 1 to 5')\n\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue = float(value)\n\tprint(value)\n\tif value > 0:\n\t\tvalue = \"+\" + str(value)\n\telif value == 0:\n\t\tvalue = \"\"\n\telse:\n\t\tvalue = str(value)\n\treturn value\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tstructure = {\n\t\t'value': value,\n\t\t'name': 'point',\n\t\t'type': 'float'\n\t}\n\t\n\treturn structure\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tdef __init__(self, value):\n\t\tself.value = value\n\n\tdef __str__(self):\n\t\tif self.value < 10:\n\t\t\treturn \"0\" + str(self.value)\n\t\telse:\n\t\t\treturn str(self.value)\n", "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue = float(value)\n\td = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10}\n\treturn d[value]\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tSTRUCTURE_CLASS = {\n\t\t1: int,\n\t\t2: int,\n\t\t3: int,\n\t\t4: int,\n\t\t5: int,\n\t\t6: int,\n\t\t7: int,\n\t\t8: int\n\t}\n\n\treturn STRUCTURE_CLASS.get(len(value), str)(value)", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tclass_name = 'Point'\n\tif value < 1000:\n\t\tclass_name = 'IntPoint'\n\telif value < 1000000:\n\t\tclass_name = 'LongPoint'\n\telif value < 1000000000:\n\t\tclass_name = 'FloatPoint'\n\telif value < 1000000000000:\n\t\tclass_name = 'DoublePoint'\n\t\n\treturn getattr(sys.modules[__name__], class_name)(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tstructure = [\n\t\t(2, 'int'),\n\t\t(4, 'int'),\n\t\t(8, 'int'),\n\t\t(0, 'char')\n\t]\n\t\n\tl = len(value)\n\t\n\tif l < 2:\n\t\treturn value\n\telif l < 4:\n\t\treturn (value[0], value[1])\n\telif l < 8:\n\t\treturn (value[0], value[1], value[2], value[3])\n\telse:\n\t\treturn (value[0], value[1], value[2], value[3], value[4], value[5], value[6], value[7])", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue_size = len(value)\n\tif value_size == 4:\n\t\treturn value[0] + value[1] + value[2] + value[3]\n\tif value_size == 3:\n\t\treturn value[0] + value[1] + value[2]\n\tif value_size == 2:\n\t\treturn value[0] + value[1]\n\tif value_size == 1:\n\t\treturn value[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\timport math\n\t\n\tvalue = str(value)\n\t\n\tvalue_len = len(value)\n\t\n\tif value_len == 0:\n\t\treturn 0\n\telif value_len == 1:\n\t\treturn int(value)\n\telse:\n\t\tvalue_len = int(math.log(int(value), 10)) + 1\n\t\tround_value = float(value)/float(10**value_len)\n\t\t\n\t\tif round_value <= 1:\n\t\t\treturn int(round_value)\n\t\telif round_value > 10:\n\t\t\tround_value = float(round_value)\n\t\t\tround_value = round_value * 10\n\t\t\treturn int(round_value)\n\t\telse:\n\t\t\tround_value = float(round_value)\n\t\t\tround_value = round_value * 10\n\t\t\treturn int(round_value)\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\timport math\n\n\tvalue = value.replace('-', '').replace('+', '')\n\n\tif len(value) == 1:\n\t\treturn '0' + value\n\telif len(value) == 2:\n\t\treturn value\n\telif len(value) == 3:\n\t\tif value[0] == '0' and value[1] == '0':\n\t\t\treturn value\n\t\telse:\n\t\t\treturn '0' + value\n\telif len(value) == 4:\n\t\tif value[0] == '0' and value[1] == '0' and value[2] == '0':\n\t\t\treturn value\n\t\telse:\n\t\t\treturn '0' * 3\n\telif len(value) == 5:\n\t\tif value[0] == '0' and value[1] == '0' and value[2] == '0' and value[3] == '0':\n\t\t\treturn value\n\t\telse:\n\t\t\treturn '0' * 4\n\telse:\n\t\traise ValueError('The value length must be in 1 to 5')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue = float(value)\n\tprint(value)\n\tif value > 0:\n\t\tvalue = \"+\" + str(value)\n\telif value == 0:\n\t\tvalue = \"\"\n\telse:\n\t\tvalue = str(value)\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tstructure = {\n\t\t'value': value,\n\t\t'name': 'point',\n\t\t'type': 'float'\n\t}\n\t\n\treturn structure\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\t\n\tdef __init__(self, value):\n\t\tself.value = value\n\n\tdef __str__(self):\n\t\tif self.value < 10:\n\t\t\treturn \"0\" + str(self.value)\n\t\telse:\n\t\t\treturn str(self.value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\t\"\"\"\n\tThe structure class is generated based on the value length.\n\t\"\"\"\n\tvalue = float(value)\n\td = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 9, 9: 10}\n\treturn d[value]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tclass_keys = [key for key in self.__dict__.keys()]\n\tclass_keys.remove('__dict__')\n\tclass_keys.remove('__weakref__')\n\treturn class_keys\n\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\n\treturn self.__keys", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = self.__keys\n\treturn keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\tfor key in self.__dict__:\n\t\tif key[0] != '_' and key != '__weakref__':\n\t\t\tkeys.append(key)\n\treturn keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\tfor key in self.__dict__:\n\t\tif key[0] != '_':\n\t\t\tkeys.append(key)\n\treturn keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys_list = list()\n\tfor key in self.__dict__.keys():\n\t\tif key[:1] != \"_\":\n\t\t\tkeys_list.append(key)\n\treturn keys_list\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = list()\n\tfor key in self.__dict__:\n\t\tif key[0] != '_':\n\t\t\tkeys.append(key)\n\treturn keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = self.__keys\n\treturn keys\n", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tfrom lib.utilities import is_list\n\t\n\tif is_list(self.__keys):\n\t\treturn self.__keys\n\telse:\n\t\tkeys = []\n\t\tfor key in self.__dict__:\n\t\t\tif key[0] == \"_\" and key[1:] not in [\"__\", \"__\"] and key != \"_keys\":\n\t\t\t\tkeys.append(key)\n\t\t\t\n\t\tself.__keys = keys\n\t\treturn self.__keys\n\t\t\n\t", "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\t\n\tk = self\n\t\n\twhile k:\n\t\tkeys.append(k.key)\n\t\tk = k._parent\n\t\n\treturn keys"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tclass_keys = [key for key in self.__dict__.keys()]\n\tclass_keys.remove('__dict__')\n\tclass_keys.remove('__weakref__')\n\treturn class_keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\t\n\treturn self.__keys", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = self.__keys\n\treturn keys\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\tfor key in self.__dict__:\n\t\tif key[0] != '_' and key != '__weakref__':\n\t\t\tkeys.append(key)\n\treturn keys\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\tfor key in self.__dict__:\n\t\tif key[0] != '_':\n\t\t\tkeys.append(key)\n\treturn keys\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys_list = list()\n\tfor key in self.__dict__.keys():\n\t\tif key[:1] != \"_\":\n\t\t\tkeys_list.append(key)\n\treturn keys_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = list()\n\tfor key in self.__dict__:\n\t\tif key[0] != '_':\n\t\t\tkeys.append(key)\n\treturn keys\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = self.__keys\n\treturn keys\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tfrom lib.utilities import is_list\n\t\n\tif is_list(self.__keys):\n\t\treturn self.__keys\n\telse:\n\t\tkeys = []\n\t\tfor key in self.__dict__:\n\t\t\tif key[0] == \"_\" and key[1:] not in [\"__\", \"__\"] and key != \"_keys\":\n\t\t\t\tkeys.append(key)\n\t\t\t\n\t\tself.__keys = keys\n\t\treturn self.__keys\n\t\t\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\tkeys = []\n\t\n\tk = self\n\t\n\twhile k:\n\t\tkeys.append(k.key)\n\t\tk = k._parent\n\t\n\treturn keys", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef _bolt_protocol_handlers(protocol_version):\n\t\tif protocol_version == \"1.0\":\n\t\t\treturn v1_0\n\t\tif protocol_version == \"1.1\":\n\t\t\treturn v1_1\n\t\tif protocol_version == \"1.2\":\n\t\t\treturn v1_2\n\t\treturn cls\n\treturn _bolt_protocol_handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandler_cls = protocol_handlers.get(protocol_version, protocol_handlers[None])\n\tprotocol_handler = handler_cls(cls)\n\n\treturn protocol_handler\n\n\n#: protocol handlers for Bolt 3.2+\nprotocol_handlers = {\n\t#: Bolt 3.2+\n\tNone: Bolt32ProtocolHandler,\n\t#: Bolt 3.0+\n\t3: Bolt30ProtocolHandler,\n\t#: Bolt 2.0+\n\t2: Bolt20ProtocolHandler\n}", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tversion = protocol_version if protocol_version else cls.DEFAULT_PROTOCOL_VERSION\n\tif version == \"3.1\":\n\t\treturn cls.v3_1_protocol_handlers\n\telse:\n\t\treturn cls.v3_protocol_handlers\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\n\tif protocol_version is None:\n\t\tprotocol_version = cls.PROTOCOL_VERSION\n\n\tif protocol_version == cls.PROTOCOL_VERSION:\n\t\treturn cls.version_1\n\telse:\n\t\treturn cls.version_2\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\n\tif protocol_version == 4:\n\t\treturn [cls.v4_handler]\n\telif protocol_version == 5:\n\t\treturn [cls.v5_handler]\n\telif protocol_version == 6:\n\t\treturn [cls.v6_handler]\n\telse:\n\t\treturn [cls.v7_handler]\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tversion_handlers = {\n\t\t\"1\": \"handlers.v1\",\n\t\t\"2\": \"handlers.v2\",\n\t\t\"3\": \"handlers.v3\",\n\t\t\"4\": \"handlers.v4\",\n\t\t\"5\": \"handlers.v5\",\n\t\t\"6\": \"handlers.v6\"\n\t}\n\t\n\thandler_name = version_handlers.get(protocol_version, \"handlers.v1\")\n\t\n\ttry:\n\t\tmodule = __import__(handler_name, globals(), locals(), [handler_name])\n\texcept ImportError:\n\t\traise ImportError(\"Protocol handler for version %s not found.\" % protocol_version)\n\t\n\tprotocol_handler_class = getattr(module, \"ProtocolHandler\")\n\t\n\treturn protocol_handler_class(cls)\n\n#src/bolt/handlers/v1.py\n# Copyright 2012-2016 The Iris Project Developers. See the COPYRIGHT\n# file at the top-level directory of this distribution and at\n# http://iris.di.uminho.pt/~vbrazdil/iris/ .\n\nimport logging\n\nfrom bolt.protocol import BoltProtocol\nfrom bolt.handlers import BoltHandler\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tfrom bolt.protocol.version_0 import Version0\n\tfrom bolt.protocol.version_1 import Version1\n\tfrom bolt.protocol.version_2 import Version2\n\tfrom bolt.protocol.version_3 import Version3\n\n\tif protocol_version is None:\n\t\tprotocol_version = Version0\n\n\tif protocol_version == Version1:\n\t\treturn Version1\n\telif protocol_version == Version2:\n\t\treturn Version2\n\telif protocol_version == Version3:\n\t\treturn Version3\n\telse:\n\t\treturn Version0", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef _protocol_handlers_default(self):\n\t\treturn {\n\t\t\tself.PROTOCOL_VERSION_1: protocol_version_1,\n\t\t}\n\n\tdef _protocol_handlers_v0(self):\n\t\t# TODO: Implement V0 protocol handlers here\n\t\treturn {}\n\n\tdef _protocol_handlers_v1(self):\n\t\treturn {\n\t\t\tself.PROTOCOL_VERSION_1: protocol_version_1,\n\t\t}\n\n\treturn getattr(cls, 'PROTOCOL_VERSION_' + protocol_version or 'DEFAULT', _protocol_handlers_default)()\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandler = None\n\tif protocol_version is not None:\n\t\thandler = cls.get(protocol_version)\n\telse:\n\t\thandler = cls.get('latest')\n\treturn handler\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef handler_1_0():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_1_1():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_1_2():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_2_0():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_2_1():\n\t\treturn cls(protocol_version=protocol_version)\n\n\treturn {\n\t\t1.0: handler_1_0,\n\t\t1.1: handler_1_1,\n\t\t1.2: handler_1_2,\n\t\t2.0: handler_2_0,\n\t\t2.1: handler_2_1,\n\t}[protocol_version]()\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef _bolt_protocol_handlers(protocol_version):\n\t\tif protocol_version == \"1.0\":\n\t\t\treturn v1_0\n\t\tif protocol_version == \"1.1\":\n\t\t\treturn v1_1\n\t\tif protocol_version == \"1.2\":\n\t\t\treturn v1_2\n\t\treturn cls\n\treturn _bolt_protocol_handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandler_cls = protocol_handlers.get(protocol_version, protocol_handlers[None])\n\tprotocol_handler = handler_cls(cls)\n\n\treturn protocol_handler\n\n\n#: protocol handlers for Bolt 3.2+\nprotocol_handlers = {\n\t#: Bolt 3.2+\n\tNone: Bolt32ProtocolHandler,\n\t#: Bolt 3.0+\n\t3: Bolt30ProtocolHandler,\n\t#: Bolt 2.0+\n\t2: Bolt20ProtocolHandler\n}", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tversion = protocol_version if protocol_version else cls.DEFAULT_PROTOCOL_VERSION\n\tif version == \"3.1\":\n\t\treturn cls.v3_1_protocol_handlers\n\telse:\n\t\treturn cls.v3_protocol_handlers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\n\tif protocol_version is None:\n\t\tprotocol_version = cls.PROTOCOL_VERSION\n\n\tif protocol_version == cls.PROTOCOL_VERSION:\n\t\treturn cls.version_1\n\telse:\n\t\treturn cls.version_2\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\t\n\tif protocol_version == 4:\n\t\treturn [cls.v4_handler]\n\telif protocol_version == 5:\n\t\treturn [cls.v5_handler]\n\telif protocol_version == 6:\n\t\treturn [cls.v6_handler]\n\telse:\n\t\treturn [cls.v7_handler]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tversion_handlers = {\n\t\t\"1\": \"handlers.v1\",\n\t\t\"2\": \"handlers.v2\",\n\t\t\"3\": \"handlers.v3\",\n\t\t\"4\": \"handlers.v4\",\n\t\t\"5\": \"handlers.v5\",\n\t\t\"6\": \"handlers.v6\"\n\t}\n\t\n\thandler_name = version_handlers.get(protocol_version, \"handlers.v1\")\n\t\n\ttry:\n\t\tmodule = __import__(handler_name, globals(), locals(), [handler_name])\n\texcept ImportError:\n\t\traise ImportError(\"Protocol handler for version %s not found.\" % protocol_version)\n\t\n\tprotocol_handler_class = getattr(module, \"ProtocolHandler\")\n\t\n\treturn protocol_handler_class(cls)\n\n#src/bolt/handlers/v1.py\n# Copyright 2012-2016 The Iris Project Developers. See the COPYRIGHT\n# file at the top-level directory of this distribution and at\n# http://iris.di.uminho.pt/~vbrazdil/iris/ .\n\nimport logging\n\nfrom bolt.protocol import BoltProtocol\nfrom bolt.handlers import BoltHandler\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tfrom bolt.protocol.version_0 import Version0\n\tfrom bolt.protocol.version_1 import Version1\n\tfrom bolt.protocol.version_2 import Version2\n\tfrom bolt.protocol.version_3 import Version3\n\n\tif protocol_version is None:\n\t\tprotocol_version = Version0\n\n\tif protocol_version == Version1:\n\t\treturn Version1\n\telif protocol_version == Version2:\n\t\treturn Version2\n\telif protocol_version == Version3:\n\t\treturn Version3\n\telse:\n\t\treturn Version0", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef _protocol_handlers_default(self):\n\t\treturn {\n\t\t\tself.PROTOCOL_VERSION_1: protocol_version_1,\n\t\t}\n\n\tdef _protocol_handlers_v0(self):\n\t\t# TODO: Implement V0 protocol handlers here\n\t\treturn {}\n\n\tdef _protocol_handlers_v1(self):\n\t\treturn {\n\t\t\tself.PROTOCOL_VERSION_1: protocol_version_1,\n\t\t}\n\n\treturn getattr(cls, 'PROTOCOL_VERSION_' + protocol_version or 'DEFAULT', _protocol_handlers_default)()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\thandler = None\n\tif protocol_version is not None:\n\t\thandler = cls.get(protocol_version)\n\telse:\n\t\thandler = cls.get('latest')\n\treturn handler\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tDifferent forms of Bolt protocol handlers are returned based on the value of protocol_version.\n\t\"\"\"\n\tdef handler_1_0():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_1_1():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_1_2():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_2_0():\n\t\treturn cls(protocol_version=protocol_version)\n\n\tdef handler_2_1():\n\t\treturn cls(protocol_version=protocol_version)\n\n\treturn {\n\t\t1.0: handler_1_0,\n\t\t1.1: handler_1_1,\n\t\t1.2: handler_1_2,\n\t\t2.0: handler_2_0,\n\t\t2.1: handler_2_1,\n\t}[protocol_version]()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tunit_of_work_session = None\n\t\t\ttry:\n\t\t\t\tunit_of_work_session = DBSession.begin(subtransactions=True)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tunit_of_work_session.commit()\n\t\t\t\treturn result\n\t\t\texcept:\n\t\t\t\t# If an exception occurs, rollback any changes and raise it\n\t\t\t\tunit_of_work_session.rollback()\n\t\t\t\traise\n\t\t\tfinally:\n\t\t\t\tif unit_of_work_session:\n\t\t\t\t\tunit_of_work_session.close()\n\t\treturn wrapper\n\treturn decorator\n\n\n@unit_of_work(timeout=300)", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n#src/mongo_connector.py\nimport time\n\nfrom pymongo import MongoClient\n\nfrom mongo_decorators import unit_of_work\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n#tasks.py\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nfrom celery import shared_task\nfrom celery.utils.log import get_task_logger\n\nfrom .utils import unit_of_work\nfrom . import models\n\nlogger = get_task_logger(__name__)\n\n\n@shared_task(bind=True, name='tasks.tasks.create_task')\n@unit_of_work()", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\twith Session(metadata=metadata) as session:\n\t\t\t\tdialect = session.dialect\n\t\t\t\twith timeout:\n\t\t\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tunit_of_work_session = None\n\t\t\ttry:\n\t\t\t\tunit_of_work_session = DBSession.begin(subtransactions=True)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tunit_of_work_session.commit()\n\t\t\t\treturn result\n\t\t\texcept:\n\t\t\t\t# If an exception occurs, rollback any changes and raise it\n\t\t\t\tunit_of_work_session.rollback()\n\t\t\t\traise\n\t\t\tfinally:\n\t\t\t\tif unit_of_work_session:\n\t\t\t\t\tunit_of_work_session.close()\n\t\treturn wrapper\n\treturn decorator\n\n\n@unit_of_work(timeout=300)", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n#src/mongo_connector.py\nimport time\n\nfrom pymongo import MongoClient\n\nfrom mongo_decorators import unit_of_work\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n#tasks.py\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nfrom celery import shared_task\nfrom celery.utils.log import get_task_logger\n\nfrom .utils import unit_of_work\nfrom . import models\n\nlogger = get_task_logger(__name__)\n\n\n@shared_task(bind=True, name='tasks.tasks.create_task')\n@unit_of_work()", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef inner(*args, **kwargs):\n\t\t\twith Session(metadata=metadata) as session:\n\t\t\t\tdialect = session.dialect\n\t\t\t\twith timeout:\n\t\t\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\t\"\"\"\n\tReturns a decorator with metadata and timeout attributes.\n\t\"\"\"\n\tdef decorator(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid key\")\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn None\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tkey = int(key)\n\tif key < 0:\n\t\tkey = -key\n\treturn key\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\tval = int(key)\n\texcept ValueError:\n\t\tval = self._key_to_int(key)\n\treturn val\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn 0", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tvalue = self.get(key)\n\tif not value:\n\t\treturn key\n\treturn int(value)\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn self.__getitem__(key)\n\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept:\n\t\treturn key", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept:\n\t\treturn None\n", "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn None\n\n#def index(self, key):\n#\treturn key\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid key\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tkey = int(key)\n\tif key < 0:\n\t\tkey = -key\n\treturn key\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\tval = int(key)\n\texcept ValueError:\n\t\tval = self._key_to_int(key)\n\treturn val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn 0", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\tvalue = self.get(key)\n\tif not value:\n\t\treturn key\n\treturn int(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn self.__getitem__(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept:\n\t\treturn key", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\ttry:\n\t\treturn int(key)\n\texcept ValueError:\n\t\treturn None\n\n#def index(self, key):\n#\treturn key\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tresult = []\n\tfor key in keys:\n\t\tif key in self.index:\n\t\t\tresult.append(self.index[key])\n\treturn result\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\n\tdef get(self, key):\n\t\t\"\"\"\n\t\tReturns the value of the key.\n\t\t\"\"\"\n\t\treturn self[key]\n\t\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturns the value of the key.\n\t\t\"\"\"\n\t\treturn self[key]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tresult = []\n\tkeys = list(keys)\n\tfor key in keys:\n\t\tif key in self.index:\n\t\t\tresult.append(self.index[key])\n\treturn result", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tkeys = self.index(keys)\n\treturn self.filter(keys)\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = list()\n\tfor key in keys:\n\t\tvalues.append(self.index[key])\n\treturn values\n\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tr = []\n\tfor key in keys:\n\t\tr.append(self.values(key))\n\treturn r\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tdef _(key):\n\t\ttry:\n\t\t\treturn self.index[key]\n\t\texcept KeyError:\n\t\t\treturn None\n\t\n\tkeys = [_(key) for key in keys]\n\treturn [key for key in keys if key]\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\n\tv = []\n\t\n\tfor k in keys:\n\t\tv.append(self.index[k])\n\t\n\treturn v\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = []\n\tfor k in keys:\n\t\tvalues.append(self.data.get(k, \"\"))\n\treturn values\n", "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = self.index[list(keys)]\n\tif values.size() == 0:\n\t\treturn None\n\treturn values\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tresult = []\n\tfor key in keys:\n\t\tif key in self.index:\n\t\t\tresult.append(self.index[key])\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\n\tdef get(self, key):\n\t\t\"\"\"\n\t\tReturns the value of the key.\n\t\t\"\"\"\n\t\treturn self[key]\n\t\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturns the value of the key.\n\t\t\"\"\"\n\t\treturn self[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tresult = []\n\tkeys = list(keys)\n\tfor key in keys:\n\t\tif key in self.index:\n\t\t\tresult.append(self.index[key])\n\treturn result", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tkeys = self.index(keys)\n\treturn self.filter(keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = list()\n\tfor key in keys:\n\t\tvalues.append(self.index[key])\n\treturn values\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tr = []\n\tfor key in keys:\n\t\tr.append(self.values(key))\n\treturn r\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tdef _(key):\n\t\ttry:\n\t\t\treturn self.index[key]\n\t\texcept KeyError:\n\t\t\treturn None\n\t\n\tkeys = [_(key) for key in keys]\n\treturn [key for key in keys if key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\t\n\tv = []\n\t\n\tfor k in keys:\n\t\tv.append(self.index[k])\n\t\n\treturn v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = []\n\tfor k in keys:\n\t\tvalues.append(self.data.get(k, \"\"))\n\treturn values\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\"\"\"\n\tReturns the key filtered by self.index in the form of a list.\n\t\"\"\"\n\tvalues = self.index[list(keys)]\n\tif values.size() == 0:\n\t\treturn None\n\treturn values\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = self.sanitize_keys(keys)\n\tif keys == ('*'):\n\t\tkeys = self.keys\n\treturn keys\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [key.strip() for key in keys]\n\tkeys = [key.lower() for key in keys]\n\tkeys = [key for key in keys if key]\n\treturn keys\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [x.strip() for x in keys]\n\tkeys = [x for x in keys if x]\n\tkeys = [x.strip() for x in keys]\n\tkeys = [x for x in keys if x]\n\treturn keys\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [key for key in keys if key != 'content']\n\treturn {\n\t\t'content': self.content,\n\t\t'record': self.record,\n\t\t'record_id': self.record_id,\n\t\t'record_type': self.record_type,\n\t\t'record_uuid': self.record_uuid,\n\t}\n\nRecordExportTransform.data = data\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\n\tif keys:\n\t\tkeys = [\"%s\" % k for k in keys]\n\t\telse:\n\t\tkeys = None\n\n\treturn self.exporter.export(keys)\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tself.check_mode()\n\tdata = {}\n\tfor key in keys:\n\t\tdata[key] = self.data[key]\n\treturn data\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tresult = []\n\tfor key in keys:\n\t\tresult.append(self.data[key])\n\treturn result\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [self.transform.process_key(key) for key in keys]\n\treturn self.data.get_record(keys)\n\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = tuple(keys)\n\ttransformed_data = self.transform(*keys)\n\tif transformed_data is None:\n\t\treturn {}\n\treturn transformed_data.copy()\n", "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\ttry:\n\t\tkeys = list(keys)\n\t\tif keys[0] is None:\n\t\t\tkeys = keys[1:]\n\texcept IndexError:\n\t\tkeys = []\n\treturn keys\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = self.sanitize_keys(keys)\n\tif keys == ('*'):\n\t\tkeys = self.keys\n\treturn keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [key.strip() for key in keys]\n\tkeys = [key.lower() for key in keys]\n\tkeys = [key for key in keys if key]\n\treturn keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [x.strip() for x in keys]\n\tkeys = [x for x in keys if x]\n\tkeys = [x.strip() for x in keys]\n\tkeys = [x for x in keys if x]\n\treturn keys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [key for key in keys if key != 'content']\n\treturn {\n\t\t'content': self.content,\n\t\t'record': self.record,\n\t\t'record_id': self.record_id,\n\t\t'record_type': self.record_type,\n\t\t'record_uuid': self.record_uuid,\n\t}\n\nRecordExportTransform.data = data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\t\n\tif keys:\n\t\tkeys = [\"%s\" % k for k in keys]\n\t\telse:\n\t\tkeys = None\n\n\treturn self.exporter.export(keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tself.check_mode()\n\tdata = {}\n\tfor key in keys:\n\t\tdata[key] = self.data[key]\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tresult = []\n\tfor key in keys:\n\t\tresult.append(self.data[key])\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = [self.transform.process_key(key) for key in keys]\n\treturn self.data.get_record(keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\tkeys = tuple(keys)\n\ttransformed_data = self.transform(*keys)\n\tif transformed_data is None:\n\t\treturn {}\n\treturn transformed_data.copy()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\"\"\"\n\tReturns the keys processed by the transform method of the RecordExporter class.\n\t\"\"\"\n\ttry:\n\t\tkeys = list(keys)\n\t\tif keys[0] is None:\n\t\t\tkeys = keys[1:]\n\texcept IndexError:\n\t\tkeys = []\n\treturn keys\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tval = self.data[-2]\n\tval |= (self.data[-1] << 8)\n\tself.data = self.data[:-2]\n\treturn val\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tresult = self.data[-2:]\n\tdel self.data[-2:]\n\treturn result\n\n#Python/Lectures/02_IntroductionToPython/06_Packages/01_PackageIntro/06_PackageIntro/01_Intro/__init__.py\n\"\"\"\nThis file is used to import functions from the __main__ file\n\"\"\"\n\n#Python/Lectures/02_IntroductionToPython/06_Packages/01_PackageIntro/06_PackageIntro/01_Intro/main.py\n\"\"\"\nThis file will be used to test out the functions from the __main__ file\n\"\"\"\n\nimport __main__\n\n__main__.hello()\n__main__.bye()\n\n#Python/Lectures/02_IntroductionToPython/06_Packages/01_PackageIntro/06_PackageIntro/02_Basic/main.py\n\"\"\"\nThis file will be used to test out the functions from the __main__ file\n\"\"\"\n\nimport __main__\n\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tres = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn res\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tret = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn ret\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tr = self.data.pop()\n\tr |= self.data.pop() << 8\n\treturn r\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tx = (self.data[-2] << 8) | self.data[-1]\n\tself.data = self.data[:-2]\n\treturn x\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tretval = self.data[-2]\n\tretval |= self.data[-1] << 8\n\tdel self.data[-1]\n\tdel self.data[-1]\n\treturn retval\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tdata = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn int.from_bytes(data, \"big\")\n\n\nwith open(\"16bpp\", \"rb\") as file:\n\tdata = file.read()\n\t\n\twhile len(data) > 0:\n\t\twidth = pop_u16(data)\n\t\theight = pop_u16(data)\n\t\tprint(\"width\", width)\n\t\tprint(\"height\", height)\n\t\tprint(\"data\", data[:width*height])\n\t\tdata = data[width*height:]\n\n\n\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n\t#print(\"pop_u16: \", self)\n\t\n\tval = self.data.pop()\n\tval += self.data.pop() << 8\n\t\n\treturn val\n\n", "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tresult = self.data[-1] << 8\n\tresult += self.data[-2]\n\tself.data = self.data[:-2]\n\treturn result\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tval = self.data[-2]\n\tval |= (self.data[-1] << 8)\n\tself.data = self.data[:-2]\n\treturn val\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tresult = self.data[-2:]\n\tdel self.data[-2:]\n\treturn result\n\n#Python/Lectures/02_IntroductionToPython/06_Packages/01_PackageIntro/06_PackageIntro/01_Intro/__init__.py\n\"\"\"\nThis file is used to import functions from the __main__ file\n\"\"\"\n\n#Python/Lectures/02_IntroductionToPython/06_Packages/01_PackageIntro/06_PackageIntro/01_Intro/main.py\n\"\"\"\nThis file will be used to test out the functions from the __main__ file\n\"\"\"\n\nimport __main__\n\n__main__.hello()\n__main__.bye()\n\n#Python/Lectures/02_IntroductionToPython/06_Packages/01_PackageIntro/06_PackageIntro/02_Basic/main.py\n\"\"\"\nThis file will be used to test out the functions from the __main__ file\n\"\"\"\n\nimport __main__\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tres = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tret = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tr = self.data.pop()\n\tr |= self.data.pop() << 8\n\treturn r\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tx = (self.data[-2] << 8) | self.data[-1]\n\tself.data = self.data[:-2]\n\treturn x\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tretval = self.data[-2]\n\tretval |= self.data[-1] << 8\n\tdel self.data[-1]\n\tdel self.data[-1]\n\treturn retval\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tdata = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn int.from_bytes(data, \"big\")\n\n\nwith open(\"16bpp\", \"rb\") as file:\n\tdata = file.read()\n\t\n\twhile len(data) > 0:\n\t\twidth = pop_u16(data)\n\t\theight = pop_u16(data)\n\t\tprint(\"width\", width)\n\t\tprint(\"height\", height)\n\t\tprint(\"data\", data[:width*height])\n\t\tdata = data[width*height:]\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\t\n\t#print(\"pop_u16: \", self)\n\t\n\tval = self.data.pop()\n\tval += self.data.pop() << 8\n\t\n\treturn val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\t\"\"\"\n\tRemove the last two elements in self.data and return\n\t\"\"\"\n\tresult = self.data[-1] << 8\n\tresult += self.data[-2]\n\tself.data = self.data[:-2]\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.append_message(self.build_message('discard', {\n\t\t'n': n,\n\t\t'qid': qid,\n\t\t'dehydration_hooks': dehydration_hooks,\n\t\t'hydration_hooks': hydration_hooks,\n\t\t'handlers': handlers\n\t}))\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tfrom .response import Response\n\tfrom .message import DISCARD\n\tfrom .exceptions import PackstreamException\n\n\tif not isinstance(n, int):\n\t\traise PackstreamException('n must be an integer')\n\n\tif not isinstance(qid, int):\n\t\traise PackstreamException('qid must be an integer')\n\n\tif not isinstance(dehydration_hooks, dict):\n\t\traise PackstreamException('dehydration_hooks must be a dict')\n\n\tif not isinstance(hydration_hooks, dict):\n\t\traise PackstreamException('hydration_hooks must be a dict')\n\n\tif not isinstance(handlers, dict):\n\t\traise PackstreamException('handlers must be a dict')\n\n\tif not isinstance(self.outq, Queue):\n\t\traise PackstreamException('No output queue')\n\n\t# Create the DISCARD message\n\tdiscard_message = DISCARD(n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n\t                          hydration_hooks=hydration_hooks, **handlers)\n\n\t# Add the message to the queue\n\tself.outq.put(discard_message)\n\n\treturn Response()", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.write(packstream.DISCARD, n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n\t            hydration_hooks=hydration_hooks, **handlers)\n\n#packstream/connection.py\nimport socket\nimport threading\nimport logging\nimport select\nimport sys\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdata = {\n\t\t'op': 'DISCARD',\n\t\t'n': n,\n\t\t'qid': qid,\n\t}\n\tself.send(data, dehydration_hooks, hydration_hooks, **handlers)\n\n#packstream/queue.py\n\nimport threading\nimport time\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.append(self._build_discard(n, qid, dehydration_hooks,\n\t\thydration_hooks, **handlers))\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = discard_message(self, n, qid, dehydration_hooks, hydration_hooks)\n\n\twith self.connection.get_connection() as conn:\n\t\tres = conn.send_message(msg, **handlers)\n\n\t\treturn res\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tqid = self.qid if qid == -1 else qid\n\tself._add_message(Message.DISCARD, qid=qid, n=n,\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._send_message(\n\t\tMessage(\n\t\t\tMessage.DISCARD,\n\t\t\tn=n,\n\t\t\tqid=qid,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks,\n\t\t\thandlers=handlers\n\t\t)\n\t)\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tqid = self.query_id(qid)\n\tqid.add_dehydration_hooks(dehydration_hooks)\n\tqid.add_hydration_hooks(hydration_hooks)\n\tqid.add_handlers(handlers)\n\n\tself.output_queue.append(\n\t\tdiscard_message(\n\t\t\tself,\n\t\t\tn,\n\t\t\tqid.id,\n\t\t\tdehydration_hooks,\n\t\t\thydration_hooks,\n\t\t\thandlers,\n\t\t)\n\t)\n\n#packstream/packstream/message/drop_message.py\nfrom .message import Message\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\thandlers['type'] = 'DISCARD'\n\thandlers['n'] = n\n\thandlers['qid'] = qid\n\thandlers['dehydration_hooks'] = dehydration_hooks\n\thandlers['hydration_hooks'] = hydration_hooks\n\tself.append(handlers)\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.append_message(self.build_message('discard', {\n\t\t'n': n,\n\t\t'qid': qid,\n\t\t'dehydration_hooks': dehydration_hooks,\n\t\t'hydration_hooks': hydration_hooks,\n\t\t'handlers': handlers\n\t}))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tfrom .response import Response\n\tfrom .message import DISCARD\n\tfrom .exceptions import PackstreamException\n\n\tif not isinstance(n, int):\n\t\traise PackstreamException('n must be an integer')\n\n\tif not isinstance(qid, int):\n\t\traise PackstreamException('qid must be an integer')\n\n\tif not isinstance(dehydration_hooks, dict):\n\t\traise PackstreamException('dehydration_hooks must be a dict')\n\n\tif not isinstance(hydration_hooks, dict):\n\t\traise PackstreamException('hydration_hooks must be a dict')\n\n\tif not isinstance(handlers, dict):\n\t\traise PackstreamException('handlers must be a dict')\n\n\tif not isinstance(self.outq, Queue):\n\t\traise PackstreamException('No output queue')\n\n\t# Create the DISCARD message\n\tdiscard_message = DISCARD(n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n\t                          hydration_hooks=hydration_hooks, **handlers)\n\n\t# Add the message to the queue\n\tself.outq.put(discard_message)\n\n\treturn Response()", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.write(packstream.DISCARD, n=n, qid=qid, dehydration_hooks=dehydration_hooks,\n\t            hydration_hooks=hydration_hooks, **handlers)\n\n#packstream/connection.py\nimport socket\nimport threading\nimport logging\nimport select\nimport sys\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tdata = {\n\t\t'op': 'DISCARD',\n\t\t'n': n,\n\t\t'qid': qid,\n\t}\n\tself.send(data, dehydration_hooks, hydration_hooks, **handlers)\n\n#packstream/queue.py\n\nimport threading\nimport time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself.append(self._build_discard(n, qid, dehydration_hooks,\n\t\thydration_hooks, **handlers))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tmsg = discard_message(self, n, qid, dehydration_hooks, hydration_hooks)\n\n\twith self.connection.get_connection() as conn:\n\t\tres = conn.send_message(msg, **handlers)\n\n\t\treturn res\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tqid = self.qid if qid == -1 else qid\n\tself._add_message(Message.DISCARD, qid=qid, n=n,\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tself._send_message(\n\t\tMessage(\n\t\t\tMessage.DISCARD,\n\t\t\tn=n,\n\t\t\tqid=qid,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks,\n\t\t\thandlers=handlers\n\t\t)\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\tqid = self.query_id(qid)\n\tqid.add_dehydration_hooks(dehydration_hooks)\n\tqid.add_hydration_hooks(hydration_hooks)\n\tqid.add_handlers(handlers)\n\n\tself.output_queue.append(\n\t\tdiscard_message(\n\t\t\tself,\n\t\t\tn,\n\t\t\tqid.id,\n\t\t\tdehydration_hooks,\n\t\t\thydration_hooks,\n\t\t\thandlers,\n\t\t)\n\t)\n\n#packstream/packstream/message/drop_message.py\nfrom .message import Message\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n\t\"\"\"\n\thandlers['type'] = 'DISCARD'\n\thandlers['n'] = n\n\thandlers['qid'] = qid\n\thandlers['dehydration_hooks'] = dehydration_hooks\n\thandlers['hydration_hooks'] = hydration_hooks\n\tself.append(handlers)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t...\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself.begin(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout,\n\t           db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n\t           hydration_hooks=hydration_hooks, **handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\n\tdef _get_bookmarks(bookmarks):\n\t\tif bookmarks is None:\n\t\t\treturn None\n\t\tif isinstance(bookmarks, list):\n\t\t\treturn bookmarks\n\t\telif isinstance(bookmarks, dict):\n\t\t\treturn [bookmarks]\n\t\telse:\n\t\t\traise TypeError('bookmarks must be a list or dict')\n\n\tdef _get_metadata(metadata):\n\t\tif metadata is None:\n\t\t\treturn None\n\t\tif isinstance(metadata, dict):\n\t\t\treturn metadata\n\t\telse:\n\t\t\traise TypeError('metadata must be a dict')\n\n\tdef _get_timeout(timeout):\n\t\tif timeout is None:\n\t\t\treturn None\n\t\tif isinstance(timeout, int):\n\t\t\treturn timeout\n\t\telse:\n\t\t\traise TypeError('timeout must be an int')\n\n\treturn self._begin(mode=mode, bookmarks=_get_bookmarks(bookmarks), metadata=_get_metadata(metadata), timeout=_get_timeout(timeout), db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, **handlers)\n\n# Bolt 4.0+", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmsg = {\n\t\t\"type\": \"BEGIN\",\n\t\t\"mode\": mode,\n\t\t\"bookmarks\": bookmarks,\n\t\t\"timeout\": timeout,\n\t\t\"metadata\": metadata,\n\t\t\"db\": db,\n\t\t\"imp_user\": imp_user,\n\t\t\"dehydration_hooks\": dehydration_hooks,\n\t\t\"hydration_hooks\": hydration_hooks,\n\t}\n\tresponse = self.write(msg, **handlers)\n\treturn response\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself._ensure_initialized()\n\tself._check_state(\"BEGIN\")\n\tself._check_validity(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks,\n\t                      hydration_hooks)\n\tdata = self._pack_begin(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks,\n\t                        hydration_hooks)\n\treturn self._process_response(data, handlers)\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tpass\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tpass\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself.add_begin(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\tresponse = self.response(handlers)\n\tresponse.add_begin(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\treturn response\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\n\treturn self.write(\n\t\tbegin={'mode': mode, 'bookmarks': bookmarks,\n\t\t\t'metadata': metadata, 'timeout': timeout,\n\t\t\t'db': db, 'imp_user': imp_user,\n\t\t\t'dehydration_hooks': dehydration_hooks,\n\t\t\t'hydration_hooks': hydration_hooks,\n\t\t\t'**': handlers})\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\n\tself.begin(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, **handlers)\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself.begin(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout,\n\t           db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n\t           hydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\n\tdef _get_bookmarks(bookmarks):\n\t\tif bookmarks is None:\n\t\t\treturn None\n\t\tif isinstance(bookmarks, list):\n\t\t\treturn bookmarks\n\t\telif isinstance(bookmarks, dict):\n\t\t\treturn [bookmarks]\n\t\telse:\n\t\t\traise TypeError('bookmarks must be a list or dict')\n\n\tdef _get_metadata(metadata):\n\t\tif metadata is None:\n\t\t\treturn None\n\t\tif isinstance(metadata, dict):\n\t\t\treturn metadata\n\t\telse:\n\t\t\traise TypeError('metadata must be a dict')\n\n\tdef _get_timeout(timeout):\n\t\tif timeout is None:\n\t\t\treturn None\n\t\tif isinstance(timeout, int):\n\t\t\treturn timeout\n\t\telse:\n\t\t\traise TypeError('timeout must be an int')\n\n\treturn self._begin(mode=mode, bookmarks=_get_bookmarks(bookmarks), metadata=_get_metadata(metadata), timeout=_get_timeout(timeout), db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, **handlers)\n\n# Bolt 4.0+", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tmsg = {\n\t\t\"type\": \"BEGIN\",\n\t\t\"mode\": mode,\n\t\t\"bookmarks\": bookmarks,\n\t\t\"timeout\": timeout,\n\t\t\"metadata\": metadata,\n\t\t\"db\": db,\n\t\t\"imp_user\": imp_user,\n\t\t\"dehydration_hooks\": dehydration_hooks,\n\t\t\"hydration_hooks\": hydration_hooks,\n\t}\n\tresponse = self.write(msg, **handlers)\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself._ensure_initialized()\n\tself._check_state(\"BEGIN\")\n\tself._check_validity(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks,\n\t                      hydration_hooks)\n\tdata = self._pack_begin(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks,\n\t                        hydration_hooks)\n\treturn self._process_response(data, handlers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\tself.add_begin(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\tresponse = self.response(handlers)\n\tresponse.add_begin(mode, bookmarks, metadata, timeout, db, imp_user, dehydration_hooks, hydration_hooks)\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\n\treturn self.write(\n\t\tbegin={'mode': mode, 'bookmarks': bookmarks,\n\t\t\t'metadata': metadata, 'timeout': timeout,\n\t\t\t'db': db, 'imp_user': imp_user,\n\t\t\t'dehydration_hooks': dehydration_hooks,\n\t\t\t'hydration_hooks': hydration_hooks,\n\t\t\t'**': handlers})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object\n\t\"\"\"\n\t\n\tself.begin(mode=mode, bookmarks=bookmarks, metadata=metadata, timeout=timeout, db=db, imp_user=imp_user, dehydration_hooks=dehydration_hooks, hydration_hooks=hydration_hooks, **handlers)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tround_to = 0.5\n\treturn round(n/round_to)*round_to\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiple = 10\n\tvalue = n * multiple\n\tvalue = int(value)\n\tif (value % 10) >=5:\n\t\tvalue = value - (value % 10)\n\telse:\n\t\tvalue = value - (value % 10) + 5\n\t\n\tvalue = value / multiple\n\treturn value", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiple = 10\n\treturn round(n * multiple) / multiple\n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiplier = 1\n\tif n < 0:\n\t\tmultiplier = -1\n\tn = round(n * multiplier)\n\treturn n / multiplier\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = float('%.2f' % n)\n\treturn n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiple = 10\n\td = n * multiple\n\td = int(d)\n\td = d / multiple\n\treturn d\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiplier = 10\n\tdigit, decimals = divmod(n*multiplier, 1)\n\tif decimals >= 5:\n\t\tdigit += 1\n\treturn digit / multiplier\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiple = 2\n\twhile n % multiple != 0:\n\t\tmultiple += 2\n\treturn n\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tret = math.floor(n + 0.5)\n\treturn ret\n", "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiplier = 1\n\tif n % 2 != 0:\n\t\tmultiplier = 0.5\n\treturn int((n + multiplier) / 1)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tround_to = 0.5\n\treturn round(n/round_to)*round_to\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiple = 10\n\tvalue = n * multiple\n\tvalue = int(value)\n\tif (value % 10) >=5:\n\t\tvalue = value - (value % 10)\n\telse:\n\t\tvalue = value - (value % 10) + 5\n\t\n\tvalue = value / multiple\n\treturn value", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiple = 10\n\treturn round(n * multiple) / multiple\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiplier = 1\n\tif n < 0:\n\t\tmultiplier = -1\n\tn = round(n * multiplier)\n\treturn n / multiplier\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tn = float('%.2f' % n)\n\treturn n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiple = 10\n\td = n * multiple\n\td = int(d)\n\td = d / multiple\n\treturn d\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiplier = 10\n\tdigit, decimals = divmod(n*multiplier, 1)\n\tif decimals >= 5:\n\t\tdigit += 1\n\treturn digit / multiplier\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tret = math.floor(n + 0.5)\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\t\"\"\"\n\tRound a floating-point number\n\t\"\"\"\n\tmultiplier = 1\n\tif n % 2 != 0:\n\t\tmultiplier = 0.5\n\treturn int((n + multiplier) / 1)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\tdef __init__(self, **kwargs):\n\t\t\tself.__dict__.update(kwargs)\n\t\t\tfor field in fields:\n\t\t\t\tif field not in self.__dict__:\n\t\t\t\t\traise AttributeError(\"Missing required field: %s\" % field)\n\t\tdef __repr__(self):\n\t\t\treturn \"<%s %s>\" % (self.__class__.__name__, dict([(f, self.__dict__[f]) for f in self.__slots__]))\n\t\tdef __eq__(self, other):\n\t\t\tif other is None:\n\t\t\t\treturn False\n\t\t\tif type(self) != type(other):\n\t\t\t\treturn False\n\t\t\treturn dict([(f, self.__dict__[f]) for f in self.__slots__]).items() == dict([(f, other.__dict__[f]) for f in self.__slots__]).items()\n\t\tdef __ne__(self, other):\n\t\t\treturn not self == other\n\t\tdef __hash__(self):\n\t\t\treturn hash(repr(self))\n\t\tdef geometry_type(self):\n\t\t\treturn \"Point\"\n\t\tdef to_dict(self):\n\t\t\treturn dict([(f, self.__dict__[f]) for f in", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tfields_str = ','.join(fields)\n\tfields_str = '(' + fields_str + ')'\n\n\tclass_name = 'Point'\n\tclass_str = 'class ' + class_name + '(' + fields_str + '):\\n\\tdef __init__(self, ' + fields_str + '):\\n\\t\\tself.' + fields_str + ' = ' + fields_str + '\\n'\n\n\tif name in srid_map:\n\t\tsrid = srid_map[name]\n\t\tclass_str += '\\t\\tself.srid = ' + str(srid) + '\\n'\n\n\tclass_str += '\\tdef __repr__(self):\\n\\t\\treturn ' + '\\'\\' + self.srid + ' ' + fields_str + '\\'\\'\\n'\n\n\tclass_str += '\\n\\n\\tdef __eq__(self, other):\\n\\t\\treturn self.' + fields_str + ' == other.' + fields_str + '\\n'\n\n\tclass_str += '\\n\\tdef __ne__(self, other):\\n\\t\\treturn not self.__eq__(other)\\n'\n\n\treturn class_str\n\t", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tself.x = args[0]\n\t\t\tself.y = args[1]\n\t\t\tself.srid = srid_map.get(kwargs.get('srid', 0))\n\t\t\tif not self.srid:\n\t\t\t\traise ValueError(\"Invalid SRID\")\n\t\tdef __repr__(self):\n\t\t\treturn \"<Point: ({}, {})>\".format(self.x, self.y)\n\t\tdef __str__(self):\n\t\t\treturn \"({}, {})\".format(self.x, self.y)\n\t\tdef __eq__(self, other):\n\t\t\treturn self.x == other.x and self.y == other.y\n\t\tdef __ne__(self, other):\n\t\t\treturn not self.__eq__(other)\n\t\tdef __hash__(self):\n\t\t\treturn hash(str(self))\n\t\treturn Point\n\tpoint_class = Point(name, fields, srid_map)\n\treturn point_class\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point:\n\t\tdef __init__(self, *args):\n\t\t\tif len(args) != len(fields):\n\t\t\t\traise ValueError(\"Point class constructor needs \" + str(len(fields)) + \" arguments, but got \" + str(len(args)))\n\t\t\telse:\n\t\t\t\tfor i in range(len(fields)):\n\t\t\t\t\tif fields[i] == \"x\":\n\t\t\t\t\t\tself.x = float(args[i])\n\t\t\t\t\telif fields[i] == \"y\":\n\t\t\t\t\t\tself.y = float(args[i])\n\t\t\t\t\telif fields[i] == \"z\":\n\t\t\t\t\t\tself.z = float(args[i])\n\t\t\t\t\telif fields[i] == \"srid\":\n\t\t\t\t\t\tself.srid = int(args[i])\n\t\t\t\t\telif fields[i] == \"name\":\n\t\t\t\t\t\tself.name = args[i]\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ValueError(\"Unsupported field: \" + fields[i])\n\t\t\t\tif self.srid not in srid_map:\n\t\t\t\t\traise ValueError(\"Spatial Reference ID \" + str(self.srid) + \" is not defined by the system.\")\n\tdef __str__(self):\n\t\treturn \"Point (\" + str(self.x) + \",\" + str(self.y", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\tclass Point(object):\n\t\t\"\"\"\n\t\tThis is a Point class.\n\t\t\"\"\"\n\t\t__slots__ = fields\n\t\t\n\t\t# constructor\n\t\tdef __init__(self):\n\t\t\tfor field in fields:\n\t\t\t\tsetattr(self, field, None)\n\t\t\n\t\tdef __str__(self):\n\t\t\tstr_list = []\n\t\t\tfor field in fields:\n\t\t\t\tstr_list.append(\"%s: %s\" % (field, getattr(self, field)))\n\t\t\treturn \",\".join(str_list)\n\t\t\t\n\tdef point_factory(point_name, fields):\n\t\t\"\"\"\n\t\tThis is a factory function for Point class.\n\t\t\"\"\"\n\t\treturn type(point_name, (Point,), dict(__slots__=fields))\n\t\t\n\tpoint_name = point_type(name, fields, srid_map)\n\tsrid_map[name] = point_name\n\treturn point_name\n\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point:\n\t\tdef __init__(self, **kwargs):\n\t\t\tfor field in fields:\n\t\t\t\tsetattr(self, field, kwargs[field])\n\tdef get_srid(self):\n\t\treturn srid_map[self.srid]\n\tdef __repr__(self):\n\t\treturn \"<{0} {1}>\".format(self.__class__.__name__, self.srid)\n\tdef __str__(self):\n\t\treturn \"{0} {1}\".format(self.__class__.__name__, self.srid)\n\tdef __unicode__(self):\n\t\treturn \"{0} {1}\".format(self.__class__.__name__, self.srid)\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Point):\n\t\t\treturn self.srid == other.srid\n\t\telse:\n\t\t\treturn False\n\tdef __ne__(self, other):\n\t\tif isinstance(other, Point):\n\t\t\treturn not self.srid == other.srid\n\t\telse:\n\t\t\treturn True\n\tdef __hash__(self):\n\t\treturn hash(self.srid)\n\treturn Point\n", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass_name = name.capitalize()\n\tclass_fields = [f.capitalize() for f in fields]\n\tclass_srid_map = ['{}={}'.format(k.capitalize(), v) for k, v in srid_map.items()]\n\tclass_srid_map = ','.join(class_srid_map)\n\tclass_str = 'class {}({}):'.format(class_name, ', '.join(class_fields))\n\tclass_str += '\\n    def __init__(self, {}, **kwargs):\\n'.format(class_srid_map)\n\tclass_str += '        super(self.__class__, self).__init__(**kwargs)\\n'\n\tclass_str += '        self.srid = self.srid or self.srid_map.get(self.srid, None)\\n'\n\tclass_str += '        if self.srid is None:\\n'\n\tclass_str += '            raise ValueError(\"srid must be specified in srid_map or explicitly\")\\n'\n\tclass_str += '\\n'\n\tclass_str += '    def __str__(self):\\n'\n\tclass_str += '        return \"{} {} {}\".format(self.__class__.__name", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass_name = name.title()\n\tfields = [field.lower() for field in fields]\n\tfield_list = []\n\n\tfor field in fields:\n\t\tfield_list.append(\"{0} = models.FloatField(null=True, blank=True)\".format(field))\n\n\tclass_string = \"class {0}(models.Model):\\n{1}\\n\\tdef __init__(self):\\n\\t\\tsuper(Point, self).__init__()\".format(class_name, '\\n\\t\\t'.join(field_list))\n\texec(class_string)\n\n\tdef __str__(self):\n\t\treturn self.name\n\n\tfor field in fields:\n\t\texec(\"self.{0} = models.FloatField(null=True, blank=True)\".format(field))\n\n\tdef save(self):\n\t\tsuper(Point, self).save()\n\n\tdef get_srid(self):\n\t\treturn self.srid\n\n\tdef get_name(self):\n\t\treturn self.name\n\n\tdef get_fields(self):\n\t\treturn self.fields\n\n\tdef get_fields_list(self):\n\t\treturn self.fields_list\n\n\tdef get_srid_map(self):\n\t\treturn self.s", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\tdef __init__(self, *args):\n\t\t\tself.name = name\n\t\t\tself.fields = fields\n\t\t\tself.srid_map = srid_map\n\t\t\tself.coordinates = args\n\n\t\tdef __str__(self):\n\t\t\treturn \"{name}({coordinates})\".format(name = self.name, coordinates = self.coordinates)\n\n\t\tdef srid(self):\n\t\t\treturn self.srid_map[self.name]\n\n\t\tdef as_dict(self):\n\t\t\t\"\"\"\n\t\t\tReturns a dictionary representation of self, except for the srid field.\n\t\t\t\"\"\"\n\t\t\treturn dict(zip(self.fields[1:], self.coordinates))\n\n\t\tdef as_dict_with_srid(self):\n\t\t\t\"\"\"\n\t\t\tReturns a dictionary representation of self, with the srid field.\n\t\t\t\"\"\"\n\t\t\treturn dict(zip(self.fields, self.coordinates + [self.srid]))\n\n\tdef make_point(name, fields, srid_map):\n\t\treturn Point(name, fields, srid_map)\n\n\treturn make_point", "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\tdef __init__(self, type):\n\t\t\tfor field in fields:\n\t\t\t\tif field[0] in name:\n\t\t\t\t\tsetattr(self, field[0], field[1])\n\t\t\t\telse:\n\t\t\t\t\traise AttributeError\n\n\tif name in srid_map:\n\t\traise AttributeError\n\telse:\n\t\tPoint.srid = srid_map[name]\n\t\tsrid_map[name] = Point.srid\n\n\treturn Point\n\n\n# WGS84 Point\nwgs84_point = point_type(\n\t'WGS84_Point',\n\t[\n\t\t('id', 'int'),\n\t\t('type', 'str'),\n\t\t('lat', 'float'),\n\t\t('lon', 'float'),\n\t\t('height', 'float'),\n\t\t('altitude_mode', 'str'),\n\t\t('elevation', 'float'),\n\t\t('accuracy', 'float'),\n\t\t('elevation_accuracy', 'float'),\n\t\t('source', 'str'),\n\t\t('source_id', 'str'),\n\t\t('citation', 'str'),\n\t\t('citation_id', 'str'),\n\t\t('citation_url', 'str'),\n\t\t('citation_doi"], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\tdef __init__(self, **kwargs):\n\t\t\tself.__dict__.update(kwargs)\n\t\t\tfor field in fields:\n\t\t\t\tif field not in self.__dict__:\n\t\t\t\t\traise AttributeError(\"Missing required field: %s\" % field)\n\t\tdef __repr__(self):\n\t\t\treturn \"<%s %s>\" % (self.__class__.__name__, dict([(f, self.__dict__[f]) for f in self.__slots__]))\n\t\tdef __eq__(self, other):\n\t\t\tif other is None:\n\t\t\t\treturn False\n\t\t\tif type(self) != type(other):\n\t\t\t\treturn False\n\t\t\treturn dict([(f, self.__dict__[f]) for f in self.__slots__]).items() == dict([(f, other.__dict__[f]) for f in self.__slots__]).items()\n\t\tdef __ne__(self, other):\n\t\t\treturn not self == other\n\t\tdef __hash__(self):\n\t\t\treturn hash(repr(self))\n\t\tdef geometry_type(self):\n\t\t\treturn \"Point\"\n\t\tdef to_dict(self):\n\t\t\treturn dict([(f, self.__dict__[f]) for f in", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tfields_str = ','.join(fields)\n\tfields_str = '(' + fields_str + ')'\n\n\tclass_name = 'Point'\n\tclass_str = 'class ' + class_name + '(' + fields_str + '):\\n\\tdef __init__(self, ' + fields_str + '):\\n\\t\\tself.' + fields_str + ' = ' + fields_str + '\\n'\n\n\tif name in srid_map:\n\t\tsrid = srid_map[name]\n\t\tclass_str += '\\t\\tself.srid = ' + str(srid) + '\\n'\n\n\tclass_str += '\\tdef __repr__(self):\\n\\t\\treturn ' + '\\'\\' + self.srid + ' ' + fields_str + '\\'\\'\\n'\n\n\tclass_str += '\\n\\n\\tdef __eq__(self, other):\\n\\t\\treturn self.' + fields_str + ' == other.' + fields_str + '\\n'\n\n\tclass_str += '\\n\\tdef __ne__(self, other):\\n\\t\\treturn not self.__eq__(other)\\n'\n\n\treturn class_str\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tself.x = args[0]\n\t\t\tself.y = args[1]\n\t\t\tself.srid = srid_map.get(kwargs.get('srid', 0))\n\t\t\tif not self.srid:\n\t\t\t\traise ValueError(\"Invalid SRID\")\n\t\tdef __repr__(self):\n\t\t\treturn \"<Point: ({}, {})>\".format(self.x, self.y)\n\t\tdef __str__(self):\n\t\t\treturn \"({}, {})\".format(self.x, self.y)\n\t\tdef __eq__(self, other):\n\t\t\treturn self.x == other.x and self.y == other.y\n\t\tdef __ne__(self, other):\n\t\t\treturn not self.__eq__(other)\n\t\tdef __hash__(self):\n\t\t\treturn hash(str(self))\n\t\treturn Point\n\tpoint_class = Point(name, fields, srid_map)\n\treturn point_class\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point:\n\t\tdef __init__(self, *args):\n\t\t\tif len(args) != len(fields):\n\t\t\t\traise ValueError(\"Point class constructor needs \" + str(len(fields)) + \" arguments, but got \" + str(len(args)))\n\t\t\telse:\n\t\t\t\tfor i in range(len(fields)):\n\t\t\t\t\tif fields[i] == \"x\":\n\t\t\t\t\t\tself.x = float(args[i])\n\t\t\t\t\telif fields[i] == \"y\":\n\t\t\t\t\t\tself.y = float(args[i])\n\t\t\t\t\telif fields[i] == \"z\":\n\t\t\t\t\t\tself.z = float(args[i])\n\t\t\t\t\telif fields[i] == \"srid\":\n\t\t\t\t\t\tself.srid = int(args[i])\n\t\t\t\t\telif fields[i] == \"name\":\n\t\t\t\t\t\tself.name = args[i]\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ValueError(\"Unsupported field: \" + fields[i])\n\t\t\t\tif self.srid not in srid_map:\n\t\t\t\t\traise ValueError(\"Spatial Reference ID \" + str(self.srid) + \" is not defined by the system.\")\n\tdef __str__(self):\n\t\treturn \"Point (\" + str(self.x) + \",\" + str(self.y", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\t\n\tclass Point(object):\n\t\t\"\"\"\n\t\tThis is a Point class.\n\t\t\"\"\"\n\t\t__slots__ = fields\n\t\t\n\t\t# constructor\n\t\tdef __init__(self):\n\t\t\tfor field in fields:\n\t\t\t\tsetattr(self, field, None)\n\t\t\n\t\tdef __str__(self):\n\t\t\tstr_list = []\n\t\t\tfor field in fields:\n\t\t\t\tstr_list.append(\"%s: %s\" % (field, getattr(self, field)))\n\t\t\treturn \",\".join(str_list)\n\t\t\t\n\tdef point_factory(point_name, fields):\n\t\t\"\"\"\n\t\tThis is a factory function for Point class.\n\t\t\"\"\"\n\t\treturn type(point_name, (Point,), dict(__slots__=fields))\n\t\t\n\tpoint_name = point_type(name, fields, srid_map)\n\tsrid_map[name] = point_name\n\treturn point_name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point:\n\t\tdef __init__(self, **kwargs):\n\t\t\tfor field in fields:\n\t\t\t\tsetattr(self, field, kwargs[field])\n\tdef get_srid(self):\n\t\treturn srid_map[self.srid]\n\tdef __repr__(self):\n\t\treturn \"<{0} {1}>\".format(self.__class__.__name__, self.srid)\n\tdef __str__(self):\n\t\treturn \"{0} {1}\".format(self.__class__.__name__, self.srid)\n\tdef __unicode__(self):\n\t\treturn \"{0} {1}\".format(self.__class__.__name__, self.srid)\n\tdef __eq__(self, other):\n\t\tif isinstance(other, Point):\n\t\t\treturn self.srid == other.srid\n\t\telse:\n\t\t\treturn False\n\tdef __ne__(self, other):\n\t\tif isinstance(other, Point):\n\t\t\treturn not self.srid == other.srid\n\t\telse:\n\t\t\treturn True\n\tdef __hash__(self):\n\t\treturn hash(self.srid)\n\treturn Point\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass_name = name.capitalize()\n\tclass_fields = [f.capitalize() for f in fields]\n\tclass_srid_map = ['{}={}'.format(k.capitalize(), v) for k, v in srid_map.items()]\n\tclass_srid_map = ','.join(class_srid_map)\n\tclass_str = 'class {}({}):'.format(class_name, ', '.join(class_fields))\n\tclass_str += '\\n    def __init__(self, {}, **kwargs):\\n'.format(class_srid_map)\n\tclass_str += '        super(self.__class__, self).__init__(**kwargs)\\n'\n\tclass_str += '        self.srid = self.srid or self.srid_map.get(self.srid, None)\\n'\n\tclass_str += '        if self.srid is None:\\n'\n\tclass_str += '            raise ValueError(\"srid must be specified in srid_map or explicitly\")\\n'\n\tclass_str += '\\n'\n\tclass_str += '    def __str__(self):\\n'\n\tclass_str += '        return \"{} {} {}\".format(self.__class__.__name", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass_name = name.title()\n\tfields = [field.lower() for field in fields]\n\tfield_list = []\n\n\tfor field in fields:\n\t\tfield_list.append(\"{0} = models.FloatField(null=True, blank=True)\".format(field))\n\n\tclass_string = \"class {0}(models.Model):\\n{1}\\n\\tdef __init__(self):\\n\\t\\tsuper(Point, self).__init__()\".format(class_name, '\\n\\t\\t'.join(field_list))\n\texec(class_string)\n\n\tdef __str__(self):\n\t\treturn self.name\n\n\tfor field in fields:\n\t\texec(\"self.{0} = models.FloatField(null=True, blank=True)\".format(field))\n\n\tdef save(self):\n\t\tsuper(Point, self).save()\n\n\tdef get_srid(self):\n\t\treturn self.srid\n\n\tdef get_name(self):\n\t\treturn self.name\n\n\tdef get_fields(self):\n\t\treturn self.fields\n\n\tdef get_fields_list(self):\n\t\treturn self.fields_list\n\n\tdef get_srid_map(self):\n\t\treturn self.s", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\tdef __init__(self, *args):\n\t\t\tself.name = name\n\t\t\tself.fields = fields\n\t\t\tself.srid_map = srid_map\n\t\t\tself.coordinates = args\n\n\t\tdef __str__(self):\n\t\t\treturn \"{name}({coordinates})\".format(name = self.name, coordinates = self.coordinates)\n\n\t\tdef srid(self):\n\t\t\treturn self.srid_map[self.name]\n\n\t\tdef as_dict(self):\n\t\t\t\"\"\"\n\t\t\tReturns a dictionary representation of self, except for the srid field.\n\t\t\t\"\"\"\n\t\t\treturn dict(zip(self.fields[1:], self.coordinates))\n\n\t\tdef as_dict_with_srid(self):\n\t\t\t\"\"\"\n\t\t\tReturns a dictionary representation of self, with the srid field.\n\t\t\t\"\"\"\n\t\t\treturn dict(zip(self.fields, self.coordinates + [self.srid]))\n\n\tdef make_point(name, fields, srid_map):\n\t\treturn Point(name, fields, srid_map)\n\n\treturn make_point", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\"\"\"\n\tDynamically Generating Point Class\n\t\"\"\"\n\tclass Point(object):\n\t\tdef __init__(self, type):\n\t\t\tfor field in fields:\n\t\t\t\tif field[0] in name:\n\t\t\t\t\tsetattr(self, field[0], field[1])\n\t\t\t\telse:\n\t\t\t\t\traise AttributeError\n\n\tif name in srid_map:\n\t\traise AttributeError\n\telse:\n\t\tPoint.srid = srid_map[name]\n\t\tsrid_map[name] = Point.srid\n\n\treturn Point\n\n\n# WGS84 Point\nwgs84_point = point_type(\n\t'WGS84_Point',\n\t[\n\t\t('id', 'int'),\n\t\t('type', 'str'),\n\t\t('lat', 'float'),\n\t\t('lon', 'float'),\n\t\t('height', 'float'),\n\t\t('altitude_mode', 'str'),\n\t\t('elevation', 'float'),\n\t\t('accuracy', 'float'),\n\t\t('elevation_accuracy', 'float'),\n\t\t('source', 'str'),\n\t\t('source_id', 'str'),\n\t\t('citation', 'str'),\n\t\t('citation_id', 'str'),\n\t\t('citation_url', 'str'),\n\t\t('citation_doi", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef newfunc(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn newfunc\n\treturn decorator\n\n\n@deprecated(\"old\")", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(\n\t\t\t\t\"Function %s is deprecated, please use %s instead\" % (\n\t\t\t\t\tfunc.__name__,\n\t\t\t\t\tmessage\n\t\t\t\t)\n\t\t\t)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorator", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(\"WARNING: %s is deprecated\" % func.__name__)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorator\n\n\n@deprecated", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\timport warnings\n\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\n\treturn decorator\n\nfrom . import deprecated", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(\"This function or method is deprecated and will be removed in a future version. \" + message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treturn deprecated_func\n\n\treturn decorator\n\n#src/pynode.py\nimport time\nimport json\nimport datetime\nimport logging\nimport sys\n\nfrom pytz import UTC\nfrom .utils import deprecated\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef _deprecated(func):\n\t\t\tdef deprecated_func(*args, **kwargs):\n\t\t\t\timport warnings\n\t\t\t\twarnings.warn(message, category=FutureWarning, \n\t\t\t\t\tstacklevel=2)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn deprecated_func\n\n\t\tif hasattr(func, '__name__'):\n\t\t\treturn _deprecated(func)\n\t\telse:\n\t\t\treturn _deprecated\n\treturn decorator\n\n", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorate(func):\n\t\t\"\"\"\n\t\tThis is a decorator function for deprecating functions and methods.\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThis is a wrapper function for deprecating functions and methods.\n\t\t\t\"\"\"\n\t\t\tprint(\"Calling \" + func.__name__ + \" is deprecated. \" + message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorate\n\n# Deprecation decorator for functions\n@deprecated(\"Function is deprecated. Use my_function_v2 instead\")", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorate(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorate\n\n\n@deprecated(\"deprecated method\")", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treturn deprecated_func\n\n\treturn decorator\n\n#examples/deprecated_example.py\nimport deprecated\n\n@deprecated.deprecated(\"Use my function\")", "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(function):\n\t\tdef deprecated_wrapped(*args, **kwargs):\n\t\t\tprint(\"DEPRECATED: \" + function.__name__ + \" is deprecated. \" + message)\n\t\t\treturn function(*args, **kwargs)\n\t\treturn deprecated_wrapped\n\treturn deprecated_decorator\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef newfunc(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn newfunc\n\treturn decorator\n\n\n@deprecated(\"old\")", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(\n\t\t\t\t\"Function %s is deprecated, please use %s instead\" % (\n\t\t\t\t\tfunc.__name__,\n\t\t\t\t\tmessage\n\t\t\t\t)\n\t\t\t)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorator", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(\"WARNING: %s is deprecated\" % func.__name__)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorator\n\n\n@deprecated", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\timport warnings\n\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\n\treturn decorator\n\nfrom . import deprecated", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(\"This function or method is deprecated and will be removed in a future version. \" + message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treturn deprecated_func\n\n\treturn decorator\n\n#src/pynode.py\nimport time\nimport json\nimport datetime\nimport logging\nimport sys\n\nfrom pytz import UTC\nfrom .utils import deprecated\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef _deprecated(func):\n\t\t\tdef deprecated_func(*args, **kwargs):\n\t\t\t\timport warnings\n\t\t\t\twarnings.warn(message, category=FutureWarning, \n\t\t\t\t\tstacklevel=2)\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn deprecated_func\n\n\t\tif hasattr(func, '__name__'):\n\t\t\treturn _deprecated(func)\n\t\telse:\n\t\t\treturn _deprecated\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorate(func):\n\t\t\"\"\"\n\t\tThis is a decorator function for deprecating functions and methods.\n\t\t\"\"\"\n\t\t@wraps(func)\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThis is a wrapper function for deprecating functions and methods.\n\t\t\t\"\"\"\n\t\t\tprint(\"Calling \" + func.__name__ + \" is deprecated. \" + message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorate\n\n# Deprecation decorator for functions\n@deprecated(\"Function is deprecated. Use my_function_v2 instead\")", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorate(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn decorate\n\n\n@deprecated(\"deprecated method\")", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treturn deprecated_func\n\n\treturn decorator\n\n#examples/deprecated_example.py\nimport deprecated\n\n@deprecated.deprecated(\"Use my function\")", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\t\"\"\"\n\tReturn a decorator function for deprecating functions and methods.\n\t\"\"\"\n\tdef deprecated_decorator(function):\n\t\tdef deprecated_wrapped(*args, **kwargs):\n\t\t\tprint(\"DEPRECATED: \" + function.__name__ + \" is deprecated. \" + message)\n\t\t\treturn function(*args, **kwargs)\n\t\treturn deprecated_wrapped\n\treturn deprecated_decorator\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\timport os\n\timport rpy2\n\t\n\t# The following two lines are needed to avoid an error in R\n\t# when using Rpy2's rpy.embed.embed() call below.\n\t# TODO: Why does this need to be here?\n\tos.environ[\"R_LIBS\"] = os.path.abspath(\"rlib\")\n\tos.environ[\"R_LIBS_USER\"] = os.path.abspath(\"rlib\")\n\n\tos.environ[\"R_LIBS_USER\"] = os.path.join(os.path.dirname(rpy2.__file__), \"rlib\")\n\n\tcode = code.replace(\"rpy2.rinterface.embed.embed()\", \"rpy2.rinterface.embed.embed(local_ns=locals())\")\n\n\treturn code\n\n#tests/test_r_setup.py\nimport pytest\n\nfrom rpy2.rinterface import _inline_r_setup\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"R_LIBS_USER=\\\"\", \"R_LIBS_USER=/rlib/user/\")\n\tcode = code.replace(\"R_LIBS_USER=\\\"\", \"R_LIBS_USER=/rlib/user/\")\n\treturn code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\n\"\n\tcode += \"library(Rmisc)\\n\"\n\tcode += \"rmisc::RmiscOptions()\\n\"\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\tcode = code.replace(\"R_LIBS=\\\"$R_LIBS\\\"\", \"\")\n\t\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"R_HOME\", \"\")\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace('R_LIBS_USER=', '')\n\tcode = code.replace('R_LIBS_USER=\"', '')\n\tcode = code.replace('\"', '')\n\tcode = code.replace('R_LIBS_SITE=', '')\n\tcode = code.replace('R_LIBS_SITE=\"', '')\n\tcode = code.replace('\"', '')\n\treturn code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace('\\n', ' ')\n\tcode = code.replace('\\t', ' ').strip()\n\treturn code\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\tcode = code.replace(\"--no-save\", \"\")\n\tcode = code.replace(\"--quiet\", \"\")\n\tcode = code.replace(\"--no-restore\", \"\")\n\tcode = code.replace(\"--no-restore.data\", \"\")\n\tcode = code.replace(\"--no-restore.site\", \"\")\n\tcode = code.replace(\"--no-site-file\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site-file\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tr_setup = f\"\"\"\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\t\"\"\"\n\treturn r_setup + code\n\n", "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"Rscript\", \"R\")\n\tcode = code.replace(\"Rscript -e\", \"R\")\n\tcode = code.replace('\"', \"\\\\\\\"\")\n\treturn code\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\timport os\n\timport rpy2\n\t\n\t# The following two lines are needed to avoid an error in R\n\t# when using Rpy2's rpy.embed.embed() call below.\n\t# TODO: Why does this need to be here?\n\tos.environ[\"R_LIBS\"] = os.path.abspath(\"rlib\")\n\tos.environ[\"R_LIBS_USER\"] = os.path.abspath(\"rlib\")\n\n\tos.environ[\"R_LIBS_USER\"] = os.path.join(os.path.dirname(rpy2.__file__), \"rlib\")\n\n\tcode = code.replace(\"rpy2.rinterface.embed.embed()\", \"rpy2.rinterface.embed.embed(local_ns=locals())\")\n\n\treturn code\n\n#tests/test_r_setup.py\nimport pytest\n\nfrom rpy2.rinterface import _inline_r_setup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"R_LIBS_USER=\\\"\", \"R_LIBS_USER=/rlib/user/\")\n\tcode = code.replace(\"R_LIBS_USER=\\\"\", \"R_LIBS_USER=/rlib/user/\")\n\treturn code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode += \"\\n\"\n\tcode += \"library(Rmisc)\\n\"\n\tcode += \"rmisc::RmiscOptions()\\n\"\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\tcode = code.replace(\"R_LIBS=\\\"$R_LIBS\\\"\", \"\")\n\t\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"R_HOME\", \"\")\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace('R_LIBS_USER=', '')\n\tcode = code.replace('R_LIBS_USER=\"', '')\n\tcode = code.replace('\"', '')\n\tcode = code.replace('R_LIBS_SITE=', '')\n\tcode = code.replace('R_LIBS_SITE=\"', '')\n\tcode = code.replace('\"', '')\n\treturn code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace('\\n', ' ')\n\tcode = code.replace('\\t', ' ').strip()\n\treturn code\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\t\n\tcode = code.replace(\"--no-save\", \"\")\n\tcode = code.replace(\"--quiet\", \"\")\n\tcode = code.replace(\"--no-restore\", \"\")\n\tcode = code.replace(\"--no-restore.data\", \"\")\n\tcode = code.replace(\"--no-restore.site\", \"\")\n\tcode = code.replace(\"--no-site-file\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site-file\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--no-site\", \"\")\n\tcode = code.replace(\"--no-site-directory\", \"\")\n\tcode = code.replace(\"--", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tr_setup = f\"\"\"\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\toptions(\"repos\" = \"{os.getenv('R_REPOS_URL')}\")\n\t\"\"\"\n\treturn r_setup + code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t\"\"\"\n\tSome behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.\n\t\"\"\"\n\tcode = code.replace(\"Rscript\", \"R\")\n\tcode = code.replace(\"Rscript -e\", \"R\")\n\tcode = code.replace('\"', \"\\\\\\\"\")\n\treturn code\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\ttarget_concurrency = min(len(varargs), target_concurrency)\n\n\t# Make sure we don't exceed the maximum length of the current platform\n\tvarargs = [v for v in varargs if len(v) < _max_length]\n\n\t# Generate the final command\n\tcommand = [cmd[0]]\n\tfor v, c in zip(varargs, cmd[1:]):\n\t\tcommand.append(v)\n\t\tcommand.append(c)\n\n\toutput = b\"\"\n\n\tprocess = Popen(command, stdout=PIPE)\n\n\twhile True:\n\t\tline = process.stdout.readline()\n\t\toutput += line\n\t\tif b\"\\n\" in line:\n\t\t\tprocess.stdout.close()\n\t\t\tbreak\n\n\tlines = output.split(b\"\\n\")\n\n\t# Check for an error\n\tif b\"error\" in lines:\n\t\traise RuntimeError(f\"xargs: {' '.join(command)} failed\")\n\n\tline_count = len(lines)\n\n\t# Check for the proper amount of arguments\n\tif line_count != target_concurrency + 1:\n\t\tif target_concurrency == 1:\n\t\t\traise RuntimeError(f\"xargs: {' '.join(command)} failed with {line_count} lines", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tdef _format_str(string: str, target_concurrency: int) -> str:\n\t\tres = string[:]\n\t\tif target_concurrency > 1:\n\t\t\tres += \" &\"\n\t\treturn res\n\n\tif len(cmd) > 1 or len(varargs) > 1 or color:\n\t\tprint(f\"\\n{_format_str(f'{cmd[0]} {\" \".join(varargs)}', target_concurrency)}\")\n\n\tfor i in range(1, len(varargs) + 1):\n\t\tprint(f\"{_format_str(f'{cmd[0]} {\" \".join(varargs[:i])}', target_concurrency)}\")\n\n\nif __name__ == \"__main__\":\n\txargs(\n\t\tcmd=(\"ls\", \"-l\", \"-a\", \"-h\"),\n\t\tvarargs=(\"-l\", \"-a\", \"-h\"),\n\t\tcolor=True,\n\t\ttarget_concurrency=4,\n\t)", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcmd = cmd + (str(i) for i in varargs)\n\tprocesses: list[str] = (xargs.run(cmd, color, target_concurrency)\n\t\tfor xargs in cmd)\n\tprocesses = [p for p in processes if p]\n\tif processes:\n\t\tprint(f'{\" \".join(processes)}', **kwargs)\n\n#xargs.py\nfrom typing import Sequence\nfrom xargs import xargs\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\targs = [str(arg) for arg in cmd]\n\targs.extend(varargs)\n\tfor arg in kwargs:\n\t\targs.extend([str(arg), str(kwargs[arg])])\n\targs.extend(map(str, args[_max_length:]))\n\tif target_concurrency == 1:\n\t\tsubprocess.call(args)\n\telse:\n\t\targs_per_process = math.ceil(len(args) / target_concurrency)\n\t\targs = [args[i:i + args_per_process] for i in range(0, len(args), args_per_process)]\n\t\tprocesses = []\n\t\tfor arg_list in args:\n\t\t\tprocesses.append(subprocess.Popen(arg_list))\n\t\tfor process in processes:\n\t\t\tprocess.wait()\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcolor_map = {\n\t\t\"red\": \"\\033[31m\",\n\t\t\"green\": \"\\033[32m\",\n\t\t\"yellow\": \"\\033[33m\",\n\t\t\"blue\": \"\\033[34m\",\n\t\t\"magenta\": \"\\033[35m\",\n\t\t\"cyan\": \"\\033[36m\",\n\t\t\"white\": \"\\033[37m\",\n\t\t\"reset\": \"\\033[0m\",\n\t}\n\n\tcmd_list = []\n\tfor i in varargs:\n\t\tcmd_list.append(i.split(\" \"))\n\n\tcmd_list.extend(cmd)\n\n\tprint(f\"{' '*_max_length}\", \" \".join(cmd), end=\" \")\n\tprint(f\"{' '*_max_length}\", \" \".join(varargs), end=\" \")\n\tprint(f\"{' '*_max_length}\", end=\" \")\n\n\tfor i in range(target_concurrency):\n\t\tprint(f\"{' '*_max_length}\", end=\" \")\n\n\tprint(\"\")\n\n\tproc = multiprocessing.Process(target=_process, args=(cmd_list, _max_length, color", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\targs = [cmd[0]]\n\targs.extend(cmd[1:])\n\tfor v in varargs:\n\t\targs.append(v)\n\n\targs = [str(v) for v in args]\n\targs_string = ' '.join(args)\n\tif color:\n\t\targs_string = colored(args_string, 'green')\n\t\n\tprint(f'{args_string}')\n\n\twith threading.ThreadPoolExecutor(target_concurrency) as executor:\n\t\texecutor.submit(run, args_string)\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tpass\n\n#tests/test_xargs.py\nimport pytest\nfrom xargs import xargs\n\n#TODO: Test using `xargs`", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\timport os\n\tfrom multiprocessing import Process, Queue\n\timport subprocess\n\n\t# Create a Queue for the process to write to\n\tcmd_q = Queue()\n\n\t# Create a Queue for the process to read from\n\tresult_q = Queue()\n\n\t# Start the child process and pass it the commands\n\tchild_process = Process(target=_run_xargs, args=(cmd_q, result_q, color, target_concurrency))\n\tchild_process.start()\n\n\t# Write the arguments to the Queue\n\tcmd_q.put(cmd)\n\n\t# Create a list to store the results\n\tresults = []\n\n\t# Read from the Queue until it is empty\n\twhile not result_q.empty():\n\t\tresults.append(result_q.get())\n\n\t# Wait for the child process to finish\n\tchild_process.join()\n\n\t# Create a list to store the results\n\tresults = []\n\n\t# Read from the Queue until it is empty\n\twhile not result_q.empty():\n\t\tresults.append(result_q.get())\n\n\t# Return the results\n\treturn results\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\targ_list = []\n\tfor i in range(len(varargs)):\n\t\targ_list.append(varargs[i])\n\tfor i in range(len(cmd)):\n\t\targ_list.append(cmd[i])\n\targ_list = list(map(str, arg_list))\n\tpool = multiprocessing.Pool(processes=target_concurrency)\n\tpool.map(pooled_xargs, arg_list)\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcmd_string = \" \".join(cmd)\n\tcmd_string += \" \" + \" \".join(varargs)\n\tcmd_string += f\" --color={color}\"\n\tcmd_string += f\" --max-length={_max_length}\"\n\tcmd_string += \" \" + \" \".join(kwargs)\n\tcmd_string += \" --target-concurrency=\" + str(target_concurrency)\n\tos.system(cmd_string)"], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\ttarget_concurrency = min(len(varargs), target_concurrency)\n\n\t# Make sure we don't exceed the maximum length of the current platform\n\tvarargs = [v for v in varargs if len(v) < _max_length]\n\n\t# Generate the final command\n\tcommand = [cmd[0]]\n\tfor v, c in zip(varargs, cmd[1:]):\n\t\tcommand.append(v)\n\t\tcommand.append(c)\n\n\toutput = b\"\"\n\n\tprocess = Popen(command, stdout=PIPE)\n\n\twhile True:\n\t\tline = process.stdout.readline()\n\t\toutput += line\n\t\tif b\"\\n\" in line:\n\t\t\tprocess.stdout.close()\n\t\t\tbreak\n\n\tlines = output.split(b\"\\n\")\n\n\t# Check for an error\n\tif b\"error\" in lines:\n\t\traise RuntimeError(f\"xargs: {' '.join(command)} failed\")\n\n\tline_count = len(lines)\n\n\t# Check for the proper amount of arguments\n\tif line_count != target_concurrency + 1:\n\t\tif target_concurrency == 1:\n\t\t\traise RuntimeError(f\"xargs: {' '.join(command)} failed with {line_count} lines", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tdef _format_str(string: str, target_concurrency: int) -> str:\n\t\tres = string[:]\n\t\tif target_concurrency > 1:\n\t\t\tres += \" &\"\n\t\treturn res\n\n\tif len(cmd) > 1 or len(varargs) > 1 or color:\n\t\tprint(f\"\\n{_format_str(f'{cmd[0]} {\" \".join(varargs)}', target_concurrency)}\")\n\n\tfor i in range(1, len(varargs) + 1):\n\t\tprint(f\"{_format_str(f'{cmd[0]} {\" \".join(varargs[:i])}', target_concurrency)}\")\n\n\nif __name__ == \"__main__\":\n\txargs(\n\t\tcmd=(\"ls\", \"-l\", \"-a\", \"-h\"),\n\t\tvarargs=(\"-l\", \"-a\", \"-h\"),\n\t\tcolor=True,\n\t\ttarget_concurrency=4,\n\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcmd = cmd + (str(i) for i in varargs)\n\tprocesses: list[str] = (xargs.run(cmd, color, target_concurrency)\n\t\tfor xargs in cmd)\n\tprocesses = [p for p in processes if p]\n\tif processes:\n\t\tprint(f'{\" \".join(processes)}', **kwargs)\n\n#xargs.py\nfrom typing import Sequence\nfrom xargs import xargs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\targs = [str(arg) for arg in cmd]\n\targs.extend(varargs)\n\tfor arg in kwargs:\n\t\targs.extend([str(arg), str(kwargs[arg])])\n\targs.extend(map(str, args[_max_length:]))\n\tif target_concurrency == 1:\n\t\tsubprocess.call(args)\n\telse:\n\t\targs_per_process = math.ceil(len(args) / target_concurrency)\n\t\targs = [args[i:i + args_per_process] for i in range(0, len(args), args_per_process)]\n\t\tprocesses = []\n\t\tfor arg_list in args:\n\t\t\tprocesses.append(subprocess.Popen(arg_list))\n\t\tfor process in processes:\n\t\t\tprocess.wait()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcolor_map = {\n\t\t\"red\": \"\\033[31m\",\n\t\t\"green\": \"\\033[32m\",\n\t\t\"yellow\": \"\\033[33m\",\n\t\t\"blue\": \"\\033[34m\",\n\t\t\"magenta\": \"\\033[35m\",\n\t\t\"cyan\": \"\\033[36m\",\n\t\t\"white\": \"\\033[37m\",\n\t\t\"reset\": \"\\033[0m\",\n\t}\n\n\tcmd_list = []\n\tfor i in varargs:\n\t\tcmd_list.append(i.split(\" \"))\n\n\tcmd_list.extend(cmd)\n\n\tprint(f\"{' '*_max_length}\", \" \".join(cmd), end=\" \")\n\tprint(f\"{' '*_max_length}\", \" \".join(varargs), end=\" \")\n\tprint(f\"{' '*_max_length}\", end=\" \")\n\n\tfor i in range(target_concurrency):\n\t\tprint(f\"{' '*_max_length}\", end=\" \")\n\n\tprint(\"\")\n\n\tproc = multiprocessing.Process(target=_process, args=(cmd_list, _max_length, color", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\targs = [cmd[0]]\n\targs.extend(cmd[1:])\n\tfor v in varargs:\n\t\targs.append(v)\n\n\targs = [str(v) for v in args]\n\targs_string = ' '.join(args)\n\tif color:\n\t\targs_string = colored(args_string, 'green')\n\t\n\tprint(f'{args_string}')\n\n\twith threading.ThreadPoolExecutor(target_concurrency) as executor:\n\t\texecutor.submit(run, args_string)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tpass\n\n#tests/test_xargs.py\nimport pytest\nfrom xargs import xargs\n\n#TODO: Test using `xargs`", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\timport os\n\tfrom multiprocessing import Process, Queue\n\timport subprocess\n\n\t# Create a Queue for the process to write to\n\tcmd_q = Queue()\n\n\t# Create a Queue for the process to read from\n\tresult_q = Queue()\n\n\t# Start the child process and pass it the commands\n\tchild_process = Process(target=_run_xargs, args=(cmd_q, result_q, color, target_concurrency))\n\tchild_process.start()\n\n\t# Write the arguments to the Queue\n\tcmd_q.put(cmd)\n\n\t# Create a list to store the results\n\tresults = []\n\n\t# Read from the Queue until it is empty\n\twhile not result_q.empty():\n\t\tresults.append(result_q.get())\n\n\t# Wait for the child process to finish\n\tchild_process.join()\n\n\t# Create a list to store the results\n\tresults = []\n\n\t# Read from the Queue until it is empty\n\twhile not result_q.empty():\n\t\tresults.append(result_q.get())\n\n\t# Return the results\n\treturn results\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\targ_list = []\n\tfor i in range(len(varargs)):\n\t\targ_list.append(varargs[i])\n\tfor i in range(len(cmd)):\n\t\targ_list.append(cmd[i])\n\targ_list = list(map(str, arg_list))\n\tpool = multiprocessing.Pool(processes=target_concurrency)\n\tpool.map(pooled_xargs, arg_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\"\"\"\n\tSimplified Implementation of Xargs in Linux\n\t\"\"\"\n\tcmd_string = \" \".join(cmd)\n\tcmd_string += \" \" + \" \".join(varargs)\n\tcmd_string += f\" --color={color}\"\n\tcmd_string += f\" --max-length={_max_length}\"\n\tcmd_string += \" \" + \" \".join(kwargs)\n\tcmd_string += \" --target-concurrency=\" + str(target_concurrency)\n\tos.system(cmd_string)", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(list(seq), len(seq))\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.choices(seq, k=len(seq))\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\tseq = list(seq)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\timport copy\n\n\trandom.seed(42)\n\n\treturn copy.deepcopy(seq)\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\tl = list(seq)\n\trandom.shuffle(l)\n\treturn l\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(list(seq), len(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.choices(seq, k=len(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\tseq = list(seq)\n\trandom.shuffle(seq)\n\treturn seq\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\timport copy\n\n\trandom.seed(42)\n\n\treturn copy.deepcopy(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\timport random\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\t\"\"\"\n\tShuffle a given seq with the given FIXED_RANDOM_SEED\n\t\"\"\"\n\trandom.seed(FIXED_RANDOM_SEED)\n\tl = list(seq)\n\trandom.shuffle(l)\n\treturn l\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tversion = []\n\tfor part in s.split('.'):\n\t\tif part.isdigit():\n\t\t\tversion.append(int(part))\n\t\telse:\n\t\t\traise ValueError(f\"Invalid version part: {part}\")\n\treturn tuple(version)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(int(x) for x in s.split(\".\"))\n\texcept ValueError:\n\t\traise ValueError(\"Invalid version: \" + s)\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\tparts.reverse()\n\tresult = []\n\tfor part in parts:\n\t\tresult.append(int(part))\n\treturn tuple(result)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tres = []\n\tfor part in s.split(\".\"):\n\t\tif part and all(c in \"0123456789\" for c in part):\n\t\t\tres.append(int(part))\n\t\telse:\n\t\t\traise ValueError(\"invalid version string\")\n\treturn tuple(res)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\tparts = [int(p) for p in parts]\n\treturn tuple(parts)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\treturn tuple(map(int, parts))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n\tparts = s.split('.')\n\t\n\treturn tuple(int(p) for p in parts)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tcomponents: list[int] = []\n\tcurr: int = 0\n\tpos: int = 0\n\n\tfor c in s:\n\t\tif c == \".\":\n\t\t\tcomponents.append(curr)\n\t\t\tcurr = 0\n\t\t\tpos += 1\n\t\telse:\n\t\t\tcurr = curr * 10 + int(c)\n\n\tif curr != 0:\n\t\tcomponents.append(curr)\n\n\treturn tuple(components[:pos + 1])\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split(\".\")\n\treturn tuple(int(part) for part in parts)\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n\ts = s.strip()\n\t\n\tparts = s.split('.')\n\t\n\tresult = list()\n\t\n\tfor part in parts:\n\t\tresult.append(int(part))\n\t\n\treturn tuple(result)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tversion = []\n\tfor part in s.split('.'):\n\t\tif part.isdigit():\n\t\t\tversion.append(int(part))\n\t\telse:\n\t\t\traise ValueError(f\"Invalid version part: {part}\")\n\treturn tuple(version)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\ttry:\n\t\treturn tuple(int(x) for x in s.split(\".\"))\n\texcept ValueError:\n\t\traise ValueError(\"Invalid version: \" + s)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\tparts.reverse()\n\tresult = []\n\tfor part in parts:\n\t\tresult.append(int(part))\n\treturn tuple(result)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tres = []\n\tfor part in s.split(\".\"):\n\t\tif part and all(c in \"0123456789\" for c in part):\n\t\t\tres.append(int(part))\n\t\telse:\n\t\t\traise ValueError(\"invalid version string\")\n\treturn tuple(res)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\tparts = [int(p) for p in parts]\n\treturn tuple(parts)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split('.')\n\treturn tuple(map(int, parts))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n\tparts = s.split('.')\n\t\n\treturn tuple(int(p) for p in parts)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tcomponents: list[int] = []\n\tcurr: int = 0\n\tpos: int = 0\n\n\tfor c in s:\n\t\tif c == \".\":\n\t\t\tcomponents.append(curr)\n\t\t\tcurr = 0\n\t\t\tpos += 1\n\t\telse:\n\t\t\tcurr = curr * 10 + int(c)\n\n\tif curr != 0:\n\t\tcomponents.append(curr)\n\n\treturn tuple(components[:pos + 1])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\tparts = s.split(\".\")\n\treturn tuple(int(part) for part in parts)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\t\"\"\"\n\tConverts a string concatenated by dot to a tuple consisting of integers.\n\t\"\"\"\n\t\n\ts = s.strip()\n\t\n\tparts = s.split('.')\n\t\n\tresult = list()\n\t\n\tfor part in parts:\n\t\tresult.append(int(part))\n\t\n\treturn tuple(result)\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tnew_cmd = []\n\tfor arg in cmd:\n\t\targ = shlex.quote(arg)\n\t\tnew_cmd.append(arg)\n\treturn tuple(new_cmd)\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = cmd[:]\n\tif cmd[0].startswith('\"'):\n\t\tcmd[0] = cmd[0][1:]\n\tif cmd[0].endswith('\"'):\n\t\tcmd[0] = cmd[0][:-1]\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tif exe == '':\n\t\treturn ('', *cmd)\n\n\texe = sys.executable if exe == 'python' else exe\n\tif exe == 'python':\n\t\texe = os.path.abspath(sys.executable)\n\t\texe = exe if exe[-1] == '\\\\' else os.path.dirname(exe)\n\telse:\n\t\texe = os.path.abspath(exe)\n\t\texe = exe if exe[-1] == '\\\\' else os.path.dirname(exe)\n\n\treturn (exe, *cmd[1:])\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd[0] = cmd[0].replace(\"\\\\\", \"/\")\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = list(cmd)\n\tcmd[0] = cmd[0].strip()\n\tif cmd[0].startswith(\"'\"):\n\t\tcmd[0] = cmd[0][1:]\n\tif cmd[0].endswith(\"'\"):\n\t\tcmd[0] = cmd[0][:-1]\n\treturn tuple(cmd)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd_copy = cmd[:]\n\tif cmd[0] == 'python':\n\t\tcmd_copy[0] = '/usr/bin/env'\n\treturn cmd_copy\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = [item for item in cmd if item != '']\n\texe = cmd[0]\n\treturn exe, *cmd[1:]\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tfor i in range(0, len(cmd)):\n\t\tif cmd[i][0] == '~':\n\t\t\tcmd[i] = os.path.expanduser(cmd[i][1:])\n\t\tif not os.path.exists(cmd[i]):\n\t\t\tcmd[i] = os.path.join(os.path.dirname(exe), cmd[i])\n\treturn tuple(cmd)\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\targs = []\n\tfor arg in cmd[1:]:\n\t\tif arg.startswith('\"'):\n\t\t\targ = arg[1:]\n\t\telse:\n\t\t\targ = arg\n\t\targs.append(arg)\n\treturn exe, args\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tret = [cmd[0]]\n\tfor arg in cmd[1:]:\n\t\tif arg.startswith(\"'\"):\n\t\t\targ = arg.replace(\"'\", \"\")\n\t\t\tret.append(arg)\n\t\telse:\n\t\t\tret.append(arg)\n\treturn tuple(ret)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tnew_cmd = []\n\tfor arg in cmd:\n\t\targ = shlex.quote(arg)\n\t\tnew_cmd.append(arg)\n\treturn tuple(new_cmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = cmd[:]\n\tif cmd[0].startswith('\"'):\n\t\tcmd[0] = cmd[0][1:]\n\tif cmd[0].endswith('\"'):\n\t\tcmd[0] = cmd[0][:-1]\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tif exe == '':\n\t\treturn ('', *cmd)\n\n\texe = sys.executable if exe == 'python' else exe\n\tif exe == 'python':\n\t\texe = os.path.abspath(sys.executable)\n\t\texe = exe if exe[-1] == '\\\\' else os.path.dirname(exe)\n\telse:\n\t\texe = os.path.abspath(exe)\n\t\texe = exe if exe[-1] == '\\\\' else os.path.dirname(exe)\n\n\treturn (exe, *cmd[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd[0] = cmd[0].replace(\"\\\\\", \"/\")\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = list(cmd)\n\tcmd[0] = cmd[0].strip()\n\tif cmd[0].startswith(\"'\"):\n\t\tcmd[0] = cmd[0][1:]\n\tif cmd[0].endswith(\"'\"):\n\t\tcmd[0] = cmd[0][:-1]\n\treturn tuple(cmd)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd_copy = cmd[:]\n\tif cmd[0] == 'python':\n\t\tcmd_copy[0] = '/usr/bin/env'\n\treturn cmd_copy\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tcmd = [item for item in cmd if item != '']\n\texe = cmd[0]\n\treturn exe, *cmd[1:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\tfor i in range(0, len(cmd)):\n\t\tif cmd[i][0] == '~':\n\t\t\tcmd[i] = os.path.expanduser(cmd[i][1:])\n\t\tif not os.path.exists(cmd[i]):\n\t\t\tcmd[i] = os.path.join(os.path.dirname(exe), cmd[i])\n\treturn tuple(cmd)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\texe = cmd[0]\n\targs = []\n\tfor arg in cmd[1:]:\n\t\tif arg.startswith('\"'):\n\t\t\targ = arg[1:]\n\t\telse:\n\t\t\targ = arg\n\t\targs.append(arg)\n\treturn exe, args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t\"\"\"\n\tComplement the full path to exe and return it in its original form\n\t\"\"\"\n\tret = [cmd[0]]\n\tfor arg in cmd[1:]:\n\t\tif arg.startswith(\"'\"):\n\t\t\targ = arg.replace(\"'\", \"\")\n\t\t\tret.append(arg)\n\t\telse:\n\t\t\tret.append(arg)\n\treturn tuple(ret)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef fn(*args, **kw):\n\t\t\ttry:\n\t\t\t\tv = cache[key(*args, **kw)]\n\t\t\t\tprint('hit')\n\t\t\t\treturn v\n\t\t\texcept KeyError:\n\t\t\t\tv = f(*args, **kw)\n\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\tprint('miss')\n\t\t\t\treturn v\n\t\t\texcept AttributeError:\n\t\t\t\tv = f(*args, **kw)\n\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\tprint('miss')\n\t\t\t\treturn v\n\t\t\texcept TypeError:\n\t\t\t\tv = f(*args, **kw)\n\t\t\t\tif lock is not None:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\telse:\n\t\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\tprint('miss')\n\t\t\t\treturn v\n\t\treturn fn\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcache_key = key(*args, **kwargs)\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\tresult = cache.get(cache_key)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(cache_key, result)\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tv = cache.get(key(*args, **kwargs))\n\t\t\tif v is None:\n\t\t\t\tv = func(*args, **kwargs)\n\t\t\t\tcache.set(key(*args, **kwargs), v)\n\t\t\treturn v\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(function):\n\t\tdef inner(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = function(*args, **kwargs)\n\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\treturn result\n\t\treturn inner\n\treturn decorator", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef fn(*args, **kwargs):\n\t\t\tcache_key = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\treturn cache[cache_key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = f(*args, **kwargs)\n\t\t\t\tcache[cache_key] = value\n\t\t\t\treturn value\n\t\treturn fn\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcacheKey = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tresult = cache[cacheKey]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tif result is not None:\n\t\t\t\t\tcache[cacheKey] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcachekey = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\treturn cache.get(cachekey)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\t# if the cache is locked, we can't use it\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\t# if the cache is locked, we can't use it\n\t\t\t\t\treturn cache.get(cachekey)\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache.set(cachekey, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\n\t\tdef decorator_f(*args, **kwargs):\n\t\t\thash_key = key(*args, **kwargs)\n\t\t\tlock_key = 'lock_%s' % hash_key\n\t\t\tlock_obj = cache.get(lock_key)\n\t\t\tlock = lock_obj if lock_obj else Lock(lock_key)\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\tcache_key = 'result_%s' % hash_key\n\t\t\t\t\tresult = cache.get(cache_key)\n\t\t\t\t\tif result is not None:\n\t\t\t\t\t\treturn result\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache.set(cache_key, result)\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\n\t\treturn decorator_f\n\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef decorated(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key](*args, **kwargs)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\treturn cached(cache, key=key, lock=lock)(decorated)\n\treturn decorator\n\n#main.py\n#!/usr/bin/env python3\n\nimport re\nfrom collections import deque\nfrom cached import cached\nfrom cache import memoize\n\n", "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kw):\n\t\t\tcache_key = key(*args, **kw)\n\t\t\tif cache.has(cache_key):\n\t\t\t\treturn cache.get(cache_key)\n\t\t\telse:\n\t\t\t\tresult = func(*args, **kw)\n\t\t\t\tcache.add(cache_key, result)\n\t\t\t\treturn result\n\t\tif lock:\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\treturn wrapper\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\telse:\n\t\t\treturn wrapper\n\treturn decorator\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef fn(*args, **kw):\n\t\t\ttry:\n\t\t\t\tv = cache[key(*args, **kw)]\n\t\t\t\tprint('hit')\n\t\t\t\treturn v\n\t\t\texcept KeyError:\n\t\t\t\tv = f(*args, **kw)\n\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\tprint('miss')\n\t\t\t\treturn v\n\t\t\texcept AttributeError:\n\t\t\t\tv = f(*args, **kw)\n\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\tprint('miss')\n\t\t\t\treturn v\n\t\t\texcept TypeError:\n\t\t\t\tv = f(*args, **kw)\n\t\t\t\tif lock is not None:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\telse:\n\t\t\t\t\tcache[key(*args, **kw)] = v\n\t\t\t\tprint('miss')\n\t\t\t\treturn v\n\t\treturn fn\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcache_key = key(*args, **kwargs)\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\tresult = cache.get(cache_key)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache.set(cache_key, result)\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tv = cache.get(key(*args, **kwargs))\n\t\t\tif v is None:\n\t\t\t\tv = func(*args, **kwargs)\n\t\t\t\tcache.set(key(*args, **kwargs), v)\n\t\t\treturn v\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(function):\n\t\tdef inner(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = function(*args, **kwargs)\n\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\treturn result\n\t\treturn inner\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef fn(*args, **kwargs):\n\t\t\tcache_key = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\treturn cache[cache_key]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = f(*args, **kwargs)\n\t\t\t\tcache[cache_key] = value\n\t\t\t\treturn value\n\t\treturn fn\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcacheKey = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\tresult = cache[cacheKey]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tif result is not None:\n\t\t\t\t\tcache[cacheKey] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcachekey = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\treturn cache.get(cachekey)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\t# if the cache is locked, we can't use it\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\t# if the cache is locked, we can't use it\n\t\t\t\t\treturn cache.get(cachekey)\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache.set(cachekey, result)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\n\t\tdef decorator_f(*args, **kwargs):\n\t\t\thash_key = key(*args, **kwargs)\n\t\t\tlock_key = 'lock_%s' % hash_key\n\t\t\tlock_obj = cache.get(lock_key)\n\t\t\tlock = lock_obj if lock_obj else Lock(lock_key)\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\tcache_key = 'result_%s' % hash_key\n\t\t\t\t\tresult = cache.get(cache_key)\n\t\t\t\t\tif result is not None:\n\t\t\t\t\t\treturn result\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache.set(cache_key, result)\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\treturn result\n\n\t\treturn decorator_f\n\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef decorated(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key](*args, **kwargs)\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\treturn cached(cache, key=key, lock=lock)(decorated)\n\treturn decorator\n\n#main.py\n#!/usr/bin/env python3\n\nimport re\nfrom collections import deque\nfrom cached import cached\nfrom cache import memoize\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that saves the results in the cache\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kw):\n\t\t\tcache_key = key(*args, **kw)\n\t\t\tif cache.has(cache_key):\n\t\t\t\treturn cache.get(cache_key)\n\t\t\telse:\n\t\t\t\tresult = func(*args, **kw)\n\t\t\t\tcache.add(cache_key, result)\n\t\t\t\treturn result\n\t\tif lock:\n\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\treturn wrapper\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\telse:\n\t\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tlru = LRU(maxsize, timer)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tif (typed and\n\t\t\t\t(isinstance(args[0], type) and\n\t\t\t\tissubclass(args[0], tuple))):\n\t\t\t\targs = tuple(args)\n\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\tif key in lru:\n\t\t\t\treturn lru[key]\n\t\t\telse:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tlru[key] = value\n\t\t\t\treturn value\n\t\treturn wrapped\n\treturn wrapper\n\n\n# Example\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tclass LRUdict(dict):\n\t\tdef __init__(self, maxsize, ttl, timer, typed):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.ttl = ttl\n\t\t\tself.timer = timer\n\t\t\tself.typed = typed\n\t\tdef __setitem__(self, key, value):\n\t\t\tdict.__setitem__(self, key, value)\n\t\t\tself.move_to_end(key)\n\t\tdef __getitem__(self, key):\n\t\t\treturn dict.__getitem__(self, key)\n\t\tdef __delitem__(self, key):\n\t\t\tdict.__delitem__(self, key)\n\t\tdef move_to_end(self, key):\n\t\t\tif key not in self:\n\t\t\t\treturn\n\t\t\ttry:\n\t\t\t\tvalue = self.pop(key)\n\t\t\t\tdict.setdefault(self, key, value)\n\t\t\t\tself[key] = value\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\tdef pop(self, key, default=None):\n\t\t\ttry:\n\t\t\t\tvalue = dict.pop(self, key)\n\t\t\t\treturn value\n\t\t\texcept KeyError:\n\t\t\t\tif default is None:\n\t\t\t\t\traise\n\t\t\t\telse:\n\t\t\t\t\treturn default\n\t\tdef __contains__(self, key):", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = LRUCache(maxsize, ttl, timer, typed)\n\tdef wrapper(*args, **kwargs):\n\t\tresult = cache.get(*args, **kwargs)\n\t\tif not result:\n\t\t\tresult = cache.set(*args, **kwargs)\n\t\treturn result\n\treturn wrapper\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kw):\n\t\t\tkey = (\n\t\t\t\tfunc.__name__,\n\t\t\t\targs,\n\t\t\t\tkw,\n\t\t\t\ttyped and type(func(*args, **kw))\n\t\t\t)\n\t\t\ttry:\n\t\t\t\titem = cache[key]\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\treturn item[0]\n\t\t\texcept KeyError:\n\t\t\t\titem = (func(*args, **kw), timer())\n\t\t\t\tcache[key] = item\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tdel cache[list(cache.keys())[0]]\n\t\t\t\treturn item[0]\n\t\treturn wrapper\n\tcache = {}\n\treturn decorator\n\n\nif __name__ == '__main__':\n\tfrom time import sleep\n\t\n\t@ttl_cache()\n\tdef fib(n):\n\t\treturn n if n<2 else fib(n-1)+fib(n-2)\n\t\n\tdef rfib(n):\n\t\tif n<2:\n\t\t\treturn n\n\t\tr, w = multiprocessing.Pipe()\n\t\tp1 = multiprocessing.Process(target=fib, args=(n-", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = lru_cache(maxsize=maxsize, timer=timer, typed=typed)\n\n\tdef wrapper(fn, *args, **kwargs):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tkey = args + tuple(sorted(kwargs.items()))\n\t\t\tvalue = cache[key]\n\t\t\tif value is None:\n\t\t\t\tvalue = fn(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\telse:\n\t\t\t\tcache.touch(key)\n\t\t\treturn value\n\t\treturn wrapped\n\treturn wrapper\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = LRUCache(maxsize)\n\tdef ttl_cache_wrap(func):\n\t\tdef ttl_cache_wrap(*args, **kwargs):\n\t\t\tkey = (func.__name__, args, kwargs)\n\t\t\tif key in cache:\n\t\t\t\tvalue = cache[key]\n\t\t\t\tif timer() - value['time'] > ttl:\n\t\t\t\t\tdel cache[key]\n\t\t\telse:\n\t\t\t\tvalue = {'value': func(*args, **kwargs), 'time': timer()}\n\t\t\t\tcache[key] = value\n\t\t\treturn value['value']\n\t\treturn ttl_cache_wrap\n\treturn ttl_cache_wrap\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = LRUCache(maxsize, ttl, timer, typed)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args if not typed else tuple(args)\n\t\t\ttry:\n\t\t\t\treturn cache.get(key)\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache.set(key, value)\n\t\t\t\treturn value\n\t\twrapper.cache = cache\n\t\treturn wrapper\n\treturn decorator\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(f):\n\t\tcache = LRUCache(maxsize, ttl, timer, typed)\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args + tuple(sorted(kwargs.items()))\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\tresult = f(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorator", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef inner(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tself = args[0]\n\t\t\tkey = (func, args[1:], kwargs)\n\t\t\tcache_key = cache_key_from_args(key)\n\t\t\ttry:\n\t\t\t\t_, value = self.__cache.get(cache_key)\n\t\t\t\tif value is None:\n\t\t\t\t\traise KeyError\n\t\t\t\tself.__cache[cache_key] = (timer(), value)\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tself.__cache[cache_key] = (timer(), value)\n\t\t\t\tself.__cache = self.__cache.lru_cache(maxsize)\n\t\t\t\tif timer() - self.__cache.time > self.__ttl:\n\t\t\t\t\tself.__cache.clear()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn inner\n\n#src/utils/cache.py\nfrom collections import defaultdict\nfrom .decorators import ttl_cache\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef _ttl_cache(func):\n\t\tdef _cache_func(*args, **kwargs):\n\t\t\tcache = ttl_cache._ttl_cache_cache\n\t\t\tkey = _ttl_cache._make_key(func, args, kwargs)\n\t\t\tinfo = cache.get(key)\n\t\t\tif info is None:\n\t\t\t\tinfo = cache.put(key, func(*args, **kwargs), ttl)\n\t\t\treturn info\n\t\t_cache_func.__name__ = func.__name__\n\t\t_cache_func.__doc__ = func.__doc__\n\t\treturn _cache_func\n\n\t_ttl_cache._ttl_cache_cache = LRUCache(maxsize)\n\t_ttl_cache._ttl_cache_timer = timer\n\t_ttl_cache._ttl_cache_typed = typed\n\n\tdef _make_key(func, args, kwargs):\n\t\tkey = _ttl_cache._ttl_cache_cache._make_key(func, args, kwargs)\n\t\tif _ttl_cache._ttl_cache_typed:\n\t\t\tkey = (\"ttl_cache\", key)\n\t\treturn key\n\n\treturn _ttl_cache\n\n\n@ttl_cache(maxsize=128, ttl="], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef wrapper(func):\n\t\tlru = LRU(maxsize, timer)\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tif (typed and\n\t\t\t\t(isinstance(args[0], type) and\n\t\t\t\tissubclass(args[0], tuple))):\n\t\t\t\targs = tuple(args)\n\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\tif key in lru:\n\t\t\t\treturn lru[key]\n\t\t\telse:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tlru[key] = value\n\t\t\t\treturn value\n\t\treturn wrapped\n\treturn wrapper\n\n\n# Example\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tclass LRUdict(dict):\n\t\tdef __init__(self, maxsize, ttl, timer, typed):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.ttl = ttl\n\t\t\tself.timer = timer\n\t\t\tself.typed = typed\n\t\tdef __setitem__(self, key, value):\n\t\t\tdict.__setitem__(self, key, value)\n\t\t\tself.move_to_end(key)\n\t\tdef __getitem__(self, key):\n\t\t\treturn dict.__getitem__(self, key)\n\t\tdef __delitem__(self, key):\n\t\t\tdict.__delitem__(self, key)\n\t\tdef move_to_end(self, key):\n\t\t\tif key not in self:\n\t\t\t\treturn\n\t\t\ttry:\n\t\t\t\tvalue = self.pop(key)\n\t\t\t\tdict.setdefault(self, key, value)\n\t\t\t\tself[key] = value\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\tdef pop(self, key, default=None):\n\t\t\ttry:\n\t\t\t\tvalue = dict.pop(self, key)\n\t\t\t\treturn value\n\t\t\texcept KeyError:\n\t\t\t\tif default is None:\n\t\t\t\t\traise\n\t\t\t\telse:\n\t\t\t\t\treturn default\n\t\tdef __contains__(self, key):", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = LRUCache(maxsize, ttl, timer, typed)\n\tdef wrapper(*args, **kwargs):\n\t\tresult = cache.get(*args, **kwargs)\n\t\tif not result:\n\t\t\tresult = cache.set(*args, **kwargs)\n\t\treturn result\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kw):\n\t\t\tkey = (\n\t\t\t\tfunc.__name__,\n\t\t\t\targs,\n\t\t\t\tkw,\n\t\t\t\ttyped and type(func(*args, **kw))\n\t\t\t)\n\t\t\ttry:\n\t\t\t\titem = cache[key]\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\treturn item[0]\n\t\t\texcept KeyError:\n\t\t\t\titem = (func(*args, **kw), timer())\n\t\t\t\tcache[key] = item\n\t\t\t\tcache.move_to_end(key)\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tdel cache[list(cache.keys())[0]]\n\t\t\t\treturn item[0]\n\t\treturn wrapper\n\tcache = {}\n\treturn decorator\n\n\nif __name__ == '__main__':\n\tfrom time import sleep\n\t\n\t@ttl_cache()\n\tdef fib(n):\n\t\treturn n if n<2 else fib(n-1)+fib(n-2)\n\t\n\tdef rfib(n):\n\t\tif n<2:\n\t\t\treturn n\n\t\tr, w = multiprocessing.Pipe()\n\t\tp1 = multiprocessing.Process(target=fib, args=(n-", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = lru_cache(maxsize=maxsize, timer=timer, typed=typed)\n\n\tdef wrapper(fn, *args, **kwargs):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\tkey = args + tuple(sorted(kwargs.items()))\n\t\t\tvalue = cache[key]\n\t\t\tif value is None:\n\t\t\t\tvalue = fn(*args, **kwargs)\n\t\t\t\tcache[key] = value\n\t\t\telse:\n\t\t\t\tcache.touch(key)\n\t\t\treturn value\n\t\treturn wrapped\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tcache = LRUCache(maxsize)\n\tdef ttl_cache_wrap(func):\n\t\tdef ttl_cache_wrap(*args, **kwargs):\n\t\t\tkey = (func.__name__, args, kwargs)\n\t\t\tif key in cache:\n\t\t\t\tvalue = cache[key]\n\t\t\t\tif timer() - value['time'] > ttl:\n\t\t\t\t\tdel cache[key]\n\t\t\telse:\n\t\t\t\tvalue = {'value': func(*args, **kwargs), 'time': timer()}\n\t\t\t\tcache[key] = value\n\t\t\treturn value['value']\n\t\treturn ttl_cache_wrap\n\treturn ttl_cache_wrap\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(func):\n\t\tcache = LRUCache(maxsize, ttl, timer, typed)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args if not typed else tuple(args)\n\t\t\ttry:\n\t\t\t\treturn cache.get(key)\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tcache.set(key, value)\n\t\t\t\treturn value\n\t\twrapper.cache = cache\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef decorator(f):\n\t\tcache = LRUCache(maxsize, ttl, timer, typed)\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = args + tuple(sorted(kwargs.items()))\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\tresult = f(*args, **kwargs)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef inner(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tself = args[0]\n\t\t\tkey = (func, args[1:], kwargs)\n\t\t\tcache_key = cache_key_from_args(key)\n\t\t\ttry:\n\t\t\t\t_, value = self.__cache.get(cache_key)\n\t\t\t\tif value is None:\n\t\t\t\t\traise KeyError\n\t\t\t\tself.__cache[cache_key] = (timer(), value)\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tself.__cache[cache_key] = (timer(), value)\n\t\t\t\tself.__cache = self.__cache.lru_cache(maxsize)\n\t\t\t\tif timer() - self.__cache.time > self.__ttl:\n\t\t\t\t\tself.__cache.clear()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn inner\n\n#src/utils/cache.py\nfrom collections import defaultdict\nfrom .decorators import ttl_cache\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.\n\t\"\"\"\n\tdef _ttl_cache(func):\n\t\tdef _cache_func(*args, **kwargs):\n\t\t\tcache = ttl_cache._ttl_cache_cache\n\t\t\tkey = _ttl_cache._make_key(func, args, kwargs)\n\t\t\tinfo = cache.get(key)\n\t\t\tif info is None:\n\t\t\t\tinfo = cache.put(key, func(*args, **kwargs), ttl)\n\t\t\treturn info\n\t\t_cache_func.__name__ = func.__name__\n\t\t_cache_func.__doc__ = func.__doc__\n\t\treturn _cache_func\n\n\t_ttl_cache._ttl_cache_cache = LRUCache(maxsize)\n\t_ttl_cache._ttl_cache_timer = timer\n\t_ttl_cache._ttl_cache_typed = typed\n\n\tdef _make_key(func, args, kwargs):\n\t\tkey = _ttl_cache._ttl_cache_cache._make_key(func, args, kwargs)\n\t\tif _ttl_cache._ttl_cache_typed:\n\t\t\tkey = (\"ttl_cache\", key)\n\t\treturn key\n\n\treturn _ttl_cache\n\n\n@ttl_cache(maxsize=128, ttl=", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef decorator(func):\n\t\tdef memoizer(*args, **kwargs):\n\t\t\tkey = str(args) + str(kwargs)\n\t\t\tif key in cache:\n\t\t\t\tprint(\"returned cached value for %s\" % key)\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tprint('calculating %s' % key)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\treturn memoizer\n\treturn decorator\n\n@mru_cache()", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef decorator(f):\n\t\tdef memoized(*args):\n\t\t\tkey = (f, args)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args)\n\t\t\treturn cache[key]\n\t\treturn memoized\n\treturn decorator\n\n#test.py\nimport time\nimport unittest\nfrom memoize import mru_cache\n\n@mru_cache(maxsize=20)", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\t\n\tdef memoize(f):\n\t\tdef inner(*args, **kwargs):\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += tuple(sorted(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn inner\n\treturn memoize\n\n# This is a slightly modified version of the following code from the\n# Python documentation:\n# http://docs.python.org/library/collections.html#collections.OrderedDict\n#\n# The original copyright notice, Copyright (C) 2001-2010 Python\n# Software Foundation, with modifications by the Python Software\n# Foundation and the PEP 8 coding style.\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\timport weakref\n\t\n\tcache = weakref.WeakValueDictionary()\n\t\n\tdef _mru_cache(func):\n\t\t\n\t\t@functools.wraps(func)\n\t\tdef _mru_cache_wrapper(*args, **kwargs):\n\t\t\tkey = (func, args, tuple(kwargs.items()))\n\t\t\tif typed:\n\t\t\t\tkey = (func,) + args + tuple(kwargs.items())\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\t\n\t\treturn _mru_cache_wrapper\n\t\n\treturn _mru_cache", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = lru_cache(maxsize=maxsize, typed=typed)\n\tdef decorator(f):\n\t\tdef helper(*args):\n\t\t\treturn cache.get(*args) or (cache[args] = f(*args))\n\t\treturn helper\n\treturn decorator\n\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\tclass Memoizer(object):\n\t\n\t\tdef __init__(self, func):\n\t\t\tself.func = func\n\t\t\tself.cache = {}\n\t\t\n\t\tdef __call__(self, *args):\n\t\t\t# Check if we have a hit for args\n\t\t\tkey = self.get_key(*args) if typed else str(args)\n\t\t\tif key in self.cache:\n\t\t\t\treturn self.cache[key]\n\t\t\t# Otherwise, save the result\n\t\t\tresult = self.func(*args)\n\t\t\tif len(self.cache) >= maxsize:\n\t\t\t\tself.cache.popitem()\n\t\t\tself.cache[key] = result\n\t\t\treturn result\n\t\t\t\n\t\tdef get_key(self, *args):\n\t\t\t\"\"\"\n\t\t\tReturns a cache key for the given arguments.\n\t\t\t\"\"\"\n\t\t\treturn args\n\t\t\t\n\tdef memoize(func):\n\t\treturn Memoizer(func)\n\t\t\n\treturn memoize", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef memoize(f):\n\t\tdef memoizer(*args, **kwargs):\n\t\t\tkey = (f, args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\n\tif typed:\n\t\tdef typed_memoize(f):\n\t\t\tdef memoizer(*args):\n\t\t\t\tkey = (f, args)\n\t\t\t\tif key not in cache:\n\t\t\t\t\tcache[key] = f(*args)\n\t\t\t\treturn cache[key]\n\t\t\treturn memoizer\n\t\treturn typed_memoize\n\telse:\n\t\treturn memoize\n\n# This is a simple cache decorator that saves up to 128 results based on a\n# MRU (Most Recently Used) algorithm.\n@mru_cache(maxsize=128)", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\tcache = lru_cache(maxsize=maxsize)\n\t\n\tdef memoized_function(f):\n\t\tdef memoizer(*args, **kwargs):\n\t\t\tkey = (f, args, tuple(kwargs.items()))\n\t\t\tif typed:\n\t\t\t\tkey = (f, args, frozenset(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\t\n\t\treturn memoizer\n\t\n\treturn memoized_function\n\n\n@mru_cache(maxsize=1000)", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef decorator(func):\n\t\tdef memoizer(*args):\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey = tuple(args)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = func(*args)\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\treturn decorator\n", "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\t\n\tdef decorate(f):\n\t\tdef memoize(args):\n\t\t\tif not typed or (typed and args in cache):\n\t\t\t\treturn cache[args]\n\t\t\tresult = f(args)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\tmemoize.__name__ = f.__name__\n\t\tmemoize.__doc__ = f.__doc__\n\t\treturn memoize\n\treturn decorate\n\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef decorator(func):\n\t\tdef memoizer(*args, **kwargs):\n\t\t\tkey = str(args) + str(kwargs)\n\t\t\tif key in cache:\n\t\t\t\tprint(\"returned cached value for %s\" % key)\n\t\t\t\treturn cache[key]\n\t\t\telse:\n\t\t\t\tprint('calculating %s' % key)\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\treturn memoizer\n\treturn decorator\n\n@mru_cache()", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef decorator(f):\n\t\tdef memoized(*args):\n\t\t\tkey = (f, args)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args)\n\t\t\treturn cache[key]\n\t\treturn memoized\n\treturn decorator\n\n#test.py\nimport time\nimport unittest\nfrom memoize import mru_cache\n\n@mru_cache(maxsize=20)", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\t\n\tdef memoize(f):\n\t\tdef inner(*args, **kwargs):\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey += tuple(sorted(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn inner\n\treturn memoize\n\n# This is a slightly modified version of the following code from the\n# Python documentation:\n# http://docs.python.org/library/collections.html#collections.OrderedDict\n#\n# The original copyright notice, Copyright (C) 2001-2010 Python\n# Software Foundation, with modifications by the Python Software\n# Foundation and the PEP 8 coding style.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\timport weakref\n\t\n\tcache = weakref.WeakValueDictionary()\n\t\n\tdef _mru_cache(func):\n\t\t\n\t\t@functools.wraps(func)\n\t\tdef _mru_cache_wrapper(*args, **kwargs):\n\t\t\tkey = (func, args, tuple(kwargs.items()))\n\t\t\tif typed:\n\t\t\t\tkey = (func,) + args + tuple(kwargs.items())\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\t\n\t\treturn _mru_cache_wrapper\n\t\n\treturn _mru_cache", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = lru_cache(maxsize=maxsize, typed=typed)\n\tdef decorator(f):\n\t\tdef helper(*args):\n\t\t\treturn cache.get(*args) or (cache[args] = f(*args))\n\t\treturn helper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\tclass Memoizer(object):\n\t\n\t\tdef __init__(self, func):\n\t\t\tself.func = func\n\t\t\tself.cache = {}\n\t\t\n\t\tdef __call__(self, *args):\n\t\t\t# Check if we have a hit for args\n\t\t\tkey = self.get_key(*args) if typed else str(args)\n\t\t\tif key in self.cache:\n\t\t\t\treturn self.cache[key]\n\t\t\t# Otherwise, save the result\n\t\t\tresult = self.func(*args)\n\t\t\tif len(self.cache) >= maxsize:\n\t\t\t\tself.cache.popitem()\n\t\t\tself.cache[key] = result\n\t\t\treturn result\n\t\t\t\n\t\tdef get_key(self, *args):\n\t\t\t\"\"\"\n\t\t\tReturns a cache key for the given arguments.\n\t\t\t\"\"\"\n\t\t\treturn args\n\t\t\t\n\tdef memoize(func):\n\t\treturn Memoizer(func)\n\t\t\n\treturn memoize", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef memoize(f):\n\t\tdef memoizer(*args, **kwargs):\n\t\t\tkey = (f, args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\n\tif typed:\n\t\tdef typed_memoize(f):\n\t\t\tdef memoizer(*args):\n\t\t\t\tkey = (f, args)\n\t\t\t\tif key not in cache:\n\t\t\t\t\tcache[key] = f(*args)\n\t\t\t\treturn cache[key]\n\t\t\treturn memoizer\n\t\treturn typed_memoize\n\telse:\n\t\treturn memoize\n\n# This is a simple cache decorator that saves up to 128 results based on a\n# MRU (Most Recently Used) algorithm.\n@mru_cache(maxsize=128)", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\t\n\tcache = lru_cache(maxsize=maxsize)\n\t\n\tdef memoized_function(f):\n\t\tdef memoizer(*args, **kwargs):\n\t\t\tkey = (f, args, tuple(kwargs.items()))\n\t\t\tif typed:\n\t\t\t\tkey = (f, args, frozenset(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\t\n\t\treturn memoizer\n\t\n\treturn memoized_function\n\n\n@mru_cache(maxsize=1000)", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef decorator(func):\n\t\tdef memoizer(*args):\n\t\t\tkey = args\n\t\t\tif typed:\n\t\t\t\tkey = tuple(args)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = func(*args)\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\t\n\tdef decorate(f):\n\t\tdef memoize(args):\n\t\t\tif not typed or (typed and args in cache):\n\t\t\t\treturn cache[args]\n\t\t\tresult = f(args)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\tmemoize.__name__ = f.__name__\n\t\tmemoize.__doc__ = f.__doc__\n\t\treturn memoize\n\treturn decorate\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef memoizer(func):\n\t\tdef wrapper(x):\n\t\t\tif typed:\n\t\t\t\tif x not in cache:\n\t\t\t\t\tcache[x] = func(x)\n\t\t\t\treturn cache[x]\n\t\t\telse:\n\t\t\t\tif x not in cache:\n\t\t\t\t\tcache[x] = func(x)\n\t\t\t\treturn cache[x]\n\t\treturn wrapper\n\n\treturn memoizer\n\n\n\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\tcache = {}\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tThe actual decorator function.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef memoizer(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe memoizing function.\n\t\t\t\"\"\"\n\t\t\tkey = (func.__name__, args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tdel cache[next(iter(cache))]\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\treturn decorator\n\n\n\n@lru_cache()", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\tcache = {}\n\t\n\tdef decorator(f):\n\t\t\"\"\"\n\t\tDecorator\n\t\t\"\"\"\n\t\t@functools.wraps(f)\n\t\tdef memoizer(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tMemoizer\n\t\t\t\"\"\"\n\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\n\treturn decorator\n\n@lru_cache()", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = cache.get((args, frozenset(kwargs.items())), None)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[(args, frozenset(kwargs.items()))] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = (func.__name__, args, frozenset(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) == maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn decorator", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n\tcache = {}\n\t\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = (func, args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn decorator", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tcache_times = {}\n\n\tdef memoize(fn):\n\t\tdef memoizer(*args, **kw):\n\t\t\tkey = (fn, args, kw)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = fn(*args, **kw)\n\t\t\t\tcache_times[key] = 0\n\t\t\tcache_times[key] += 1\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\treturn memoize\n\n\n@lru_cache(maxsize=128)", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\t\n\tclass Node(object):\n\t\tdef __init__(self, key, value):\n\t\t\tself.key = key\n\t\t\tself.value = value\n\t\t\tself.prev = None\n\t\t\tself.next = None\n\t\n\tclass LRUCache(object):\n\t\tdef __init__(self, maxsize):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.cache = {}\n\t\t\tself.head = Node(None, None)\n\t\t\tself.tail = Node(None, None)\n\t\t\tself.head.next = self.tail\n\t\t\tself.tail.prev = self.head\n\t\n\t\tdef __getitem__(self, key):\n\t\t\t\"\"\"\n\t\t\tRetrieve an item from the cache.\n\t\t\t\"\"\"\n\t\t\tif key in self.cache:\n\t\t\t\tnode = self.cache[key]\n\t\t\t\tself.remove(node)\n\t\t\t\tself.set(node)\n\t\t\t\treturn node.value\n\t\t\telse:\n\t\t\t\treturn None\n\t\t\n\t\tdef __setitem__(self, key, value):\n\t\t\t\"\"\"\n\t\t\tSet an item in the cache.\n\t\t\t\"\"\"\n\t\t\tif key in self.cache:\n\t\t\t\tself.remove(self.cache[key])\n\t\t\tnode = Node(key,", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tclass Memoizer(object):\n\t\t\"\"\"\n\t\tMemoizer caches the return value for any callable that has the same arguments.\n\t\t\"\"\"\n\t\tdef __init__(self):\n\t\t\tself.cache = {}\n\t\t\tself.maxsize = maxsize\n\t\t\n\t\tdef __call__(self, fn):\n\t\t\tdef memoized(*args, **kwargs):\n\t\t\t\tkey = (fn, args, tuple(kwargs.items()))\n\t\t\t\tif key not in self.cache:\n\t\t\t\t\tif len(self.cache) >= self.maxsize:\n\t\t\t\t\t\tself.cache.popitem()\n\t\t\t\t\tself.cache[key] = fn(*args, **kwargs)\n\t\t\t\treturn self.cache[key]\n\t\t\treturn memoized\n\telse:\n\t\tdef memoizer(fn):\n\t\t\treturn Memoizer()(fn)\n\telse:\n\t\treturn memoizer", "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef memoized(func):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (func, tuple(args), frozendict(kwds))\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\tcache[key] = func(*args, **kwds)\n\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn memoized"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef memoizer(func):\n\t\tdef wrapper(x):\n\t\t\tif typed:\n\t\t\t\tif x not in cache:\n\t\t\t\t\tcache[x] = func(x)\n\t\t\t\treturn cache[x]\n\t\t\telse:\n\t\t\t\tif x not in cache:\n\t\t\t\t\tcache[x] = func(x)\n\t\t\t\treturn cache[x]\n\t\treturn wrapper\n\n\treturn memoizer\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\tcache = {}\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tThe actual decorator function.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef memoizer(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tThe memoizing function.\n\t\t\t\"\"\"\n\t\t\tkey = (func.__name__, args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tdel cache[next(iter(cache))]\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\treturn decorator\n\n\n\n@lru_cache()", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\tcache = {}\n\t\n\tdef decorator(f):\n\t\t\"\"\"\n\t\tDecorator\n\t\t\"\"\"\n\t\t@functools.wraps(f)\n\t\tdef memoizer(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tMemoizer\n\t\t\t\"\"\"\n\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = f(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\n\treturn decorator\n\n@lru_cache()", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = cache.get((args, frozenset(kwargs.items())), None)\n\t\t\tif result is None:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[(args, frozenset(kwargs.items()))] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = (func.__name__, args, frozenset(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) == maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\t\n\tcache = {}\n\t\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = (func, args, tuple(kwargs.items()))\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\tcache[key] = func(*args, **kwargs)\n\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn decorator", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\tcache_times = {}\n\n\tdef memoize(fn):\n\t\tdef memoizer(*args, **kw):\n\t\t\tkey = (fn, args, kw)\n\t\t\tif key not in cache:\n\t\t\t\tcache[key] = fn(*args, **kw)\n\t\t\t\tcache_times[key] = 0\n\t\t\tcache_times[key] += 1\n\t\t\treturn cache[key]\n\t\treturn memoizer\n\treturn memoize\n\n\n@lru_cache(maxsize=128)", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\t\n\tclass Node(object):\n\t\tdef __init__(self, key, value):\n\t\t\tself.key = key\n\t\t\tself.value = value\n\t\t\tself.prev = None\n\t\t\tself.next = None\n\t\n\tclass LRUCache(object):\n\t\tdef __init__(self, maxsize):\n\t\t\tself.maxsize = maxsize\n\t\t\tself.cache = {}\n\t\t\tself.head = Node(None, None)\n\t\t\tself.tail = Node(None, None)\n\t\t\tself.head.next = self.tail\n\t\t\tself.tail.prev = self.head\n\t\n\t\tdef __getitem__(self, key):\n\t\t\t\"\"\"\n\t\t\tRetrieve an item from the cache.\n\t\t\t\"\"\"\n\t\t\tif key in self.cache:\n\t\t\t\tnode = self.cache[key]\n\t\t\t\tself.remove(node)\n\t\t\t\tself.set(node)\n\t\t\t\treturn node.value\n\t\t\telse:\n\t\t\t\treturn None\n\t\t\n\t\tdef __setitem__(self, key, value):\n\t\t\t\"\"\"\n\t\t\tSet an item in the cache.\n\t\t\t\"\"\"\n\t\t\tif key in self.cache:\n\t\t\t\tself.remove(self.cache[key])\n\t\t\tnode = Node(key,", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tclass Memoizer(object):\n\t\t\"\"\"\n\t\tMemoizer caches the return value for any callable that has the same arguments.\n\t\t\"\"\"\n\t\tdef __init__(self):\n\t\t\tself.cache = {}\n\t\t\tself.maxsize = maxsize\n\t\t\n\t\tdef __call__(self, fn):\n\t\t\tdef memoized(*args, **kwargs):\n\t\t\t\tkey = (fn, args, tuple(kwargs.items()))\n\t\t\t\tif key not in self.cache:\n\t\t\t\t\tif len(self.cache) >= self.maxsize:\n\t\t\t\t\t\tself.cache.popitem()\n\t\t\t\t\tself.cache[key] = fn(*args, **kwargs)\n\t\t\t\treturn self.cache[key]\n\t\t\treturn memoized\n\telse:\n\t\tdef memoizer(fn):\n\t\t\treturn Memoizer()(fn)\n\telse:\n\t\treturn memoizer", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.\n\t\"\"\"\n\tcache = {}\n\n\tdef memoized(func):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (func, tuple(args), frozendict(kwds))\n\t\t\tif key not in cache:\n\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\tcache.popitem(last=False)\n\t\t\t\tcache[key] = func(*args, **kwds)\n\t\t\treturn cache[key]\n\t\treturn wrapper\n\treturn memoized", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = lfu_cache_typed() if typed else lfu_cache_untyped()\n\tcache.maxsize = maxsize\n\treturn cache\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = lfu_cache_typed(maxsize, typed) if typed else lfu_cache_uncached(maxsize)\n\treturn lru_cache(maxsize=maxsize)(cache)\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcounter = collections.defaultdict(int)\n\tlfu = collections.defaultdict(collections.OrderedDict)\n\t\n\tdef decorate(fn):\n\t\tdef memoized_fn(*args, **kwargs):\n\t\t\tkey = (fn, args, tuple(kwargs.items()))\n\t\t\tif typed and isinstance(args[0], tuple):\n\t\t\t\tkey += (type(args[0]),)\n\t\t\ttry:\n\t\t\t\tcounter[key] += 1\n\t\t\t\tlfu[counter[key]].move_to_end(key)\n\t\t\texcept KeyError:\n\t\t\t\tcounter[key] = 1\n\t\t\t\tlfu[counter[key]][key] = (fn(*args, **kwargs), args, kwargs)\n\t\t\t\tif len(counter) > maxsize:\n\t\t\t\t\toldest_counter = min(counter.keys())\n\t\t\t\t\tdel counter[oldest_counter]\n\t\t\t\t\toldest_lfu = lfu.pop(oldest_counter)\n\t\t\t\t\tfor key in oldest_lfu:\n\t\t\t\t\t\tdel lfu[oldest_counter][key]\n\t\treturn memoized_fn\n\t\n\treturn decorate\n\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator that will memoize the output of the wrapped function.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper that will wrap the call to the decorated function.\n\t\t\t\"\"\"\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tdel cache[next(iter(cache))]\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\timport collections\n\timport weakref\n\t\n\tcache = weakref.WeakKeyDictionary()\n\t\n\tdef lfu_cache_decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef lfu_cache_wrapper(*args, **kwargs):\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\tif typed:\n\t\t\t\tkey = (args[0], frozenset(kwargs.items()))\n\t\t\tif key in cache:\n\t\t\t\tcache[key] += 1\n\t\t\telse:\n\t\t\t\tcache[key] = 1\n\t\t\treturn cache[key]\n\t\treturn lfu_cache_wrapper\n\t\n\treturn lfu_cache_decorator\n\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tclass LFUCache(object):\n\t\tdef __init__(self, maxsize=128, typed=False, default=None):\n\t\t\t# The default value to return if there's no default value.\n\t\t\tself.default = default\n\t\t\tself.maxsize = maxsize\n\t\t\tself.typed = typed\n\t\t\tself.data = {}\n\t\t\tself.freqs = {}\n\t\t\tself.lru = {}\n\t\t\tself.used = 0\n\t\t\tself.count = 0\n\t\t\tself.cold = False\n\t\t\tself.hot = False\n\n\t\tdef __call__(self, key):\n\t\t\tvalue = self.data.get(key)\n\t\t\tif value is not None:\n\t\t\t\tself.lru[key] += 1\n\t\t\t\treturn value\n\t\t\tself.data[key] = self.default\n\t\t\tself.freqs[key] = 1\n\t\t\tself.lru[key] = 0\n\t\t\tself.used += 1\n\t\t\tself.count += 1\n\t\t\tif self.used > self.maxsize:\n\t\t\t\tkey, self.cold = min(self.lru, key=self.lru.get)\n\t\t\t\tself.used -= 1\n\t\t\t\tdel self.lru[key]\n\t\t\t\tdel self", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tclass LFUCache(object):\n\t\t\"\"\"\n\t\tClass to memoize the results of a callable.\n\t\t\"\"\"\n\t\tdef __init__(self, func):\n\t\t\tself.func = func\n\t\t\tself.cache = {}\n\t\t\tself.counts = {}\n\t\t\tself.maxsize = maxsize\n\t\t\tself.typed = typed\n\n\t\tdef __call__(self, *args, **kwargs):\n\t\t\tif self.typed:\n\t\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = args\n\t\t\tif key not in self.cache:\n\t\t\t\tresult = self.func(*args, **kwargs)\n\t\t\t\tif len(self.cache) >= self.maxsize:\n\t\t\t\t\tself.cache.popitem()\n\t\t\t\tself.cache[key] = result\n\t\t\t\tself.counts[key] = 0\n\t\t\telse:\n\t\t\t\tresult = self.cache[key]\n\t\t\tself.counts[key] += 1\n\t\t\treturn result\n\n\t\treturn LFUCache\n\treturn lfu_cache", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = lfucache(maxsize, typed)\n\treturn lru_cache(maxsize=maxsize, typed=typed)(cache)\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator that wraps func with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\t\"\"\"\n\t\tdef memoized(func, **kwargs):\n\t\t\t\"\"\"\n\t\t\tMemoized function that saves up to `maxsize` results based on a\nLeast Frequently Used (LFU) algorithm.\n\t\t\t\"\"\"\n\t\t\tcache = lfu_cache.Cache(maxsize=maxsize, typed=typed)\n\t\t\tdef _memoized(key):\n\t\t\t\t\"\"\"\n\t\t\t\tMemoized function that saves up to `maxsize` results based on a\nLeast Frequently Used (LFU) algorithm.\n\t\t\t\t\"\"\"\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(key)\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\t\treturn _memoized\n\t\treturn memoized(func)\n\treturn decorator\n\n", "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\timport collections\n\timport hashlib\n\n\tcache = collections.defaultdict(collections.OrderedDict)\n\thits = collections.defaultdict(int)\n\t\n\tdef _lfu_cache(f):\n\t\tdef _lfu_cache_inner(key, *args, **kwargs):\n\t\t\tkey = _safe_key(key, typed)\n\t\t\thits[key] += 1\n\t\t\tcache[hits[key]][key] = f(*args, **kwargs)\n\t\t\tif len(cache[hits[key]]) > maxsize:\n\t\t\t\tdel cache[hits[key]][cache[hits[key]].popitem(last=False)[0]]\n\t\t\treturn cache[hits[key]][key]\n\t\treturn _lfu_cache_inner\n\n\treturn _lfu_cache\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = lfu_cache_typed() if typed else lfu_cache_untyped()\n\tcache.maxsize = maxsize\n\treturn cache\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = lfu_cache_typed(maxsize, typed) if typed else lfu_cache_uncached(maxsize)\n\treturn lru_cache(maxsize=maxsize)(cache)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcounter = collections.defaultdict(int)\n\tlfu = collections.defaultdict(collections.OrderedDict)\n\t\n\tdef decorate(fn):\n\t\tdef memoized_fn(*args, **kwargs):\n\t\t\tkey = (fn, args, tuple(kwargs.items()))\n\t\t\tif typed and isinstance(args[0], tuple):\n\t\t\t\tkey += (type(args[0]),)\n\t\t\ttry:\n\t\t\t\tcounter[key] += 1\n\t\t\t\tlfu[counter[key]].move_to_end(key)\n\t\t\texcept KeyError:\n\t\t\t\tcounter[key] = 1\n\t\t\t\tlfu[counter[key]][key] = (fn(*args, **kwargs), args, kwargs)\n\t\t\t\tif len(counter) > maxsize:\n\t\t\t\t\toldest_counter = min(counter.keys())\n\t\t\t\t\tdel counter[oldest_counter]\n\t\t\t\t\toldest_lfu = lfu.pop(oldest_counter)\n\t\t\t\t\tfor key in oldest_lfu:\n\t\t\t\t\t\tdel lfu[oldest_counter][key]\n\t\treturn memoized_fn\n\t\n\treturn decorate\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator that will memoize the output of the wrapped function.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tWrapper that will wrap the call to the decorated function.\n\t\t\t\"\"\"\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\tif len(cache) > maxsize:\n\t\t\t\t\tdel cache[next(iter(cache))]\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\timport functools\n\timport collections\n\timport weakref\n\t\n\tcache = weakref.WeakKeyDictionary()\n\t\n\tdef lfu_cache_decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef lfu_cache_wrapper(*args, **kwargs):\n\t\t\tkey = (args, frozenset(kwargs.items()))\n\t\t\tif typed:\n\t\t\t\tkey = (args[0], frozenset(kwargs.items()))\n\t\t\tif key in cache:\n\t\t\t\tcache[key] += 1\n\t\t\telse:\n\t\t\t\tcache[key] = 1\n\t\t\treturn cache[key]\n\t\treturn lfu_cache_wrapper\n\t\n\treturn lfu_cache_decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tclass LFUCache(object):\n\t\tdef __init__(self, maxsize=128, typed=False, default=None):\n\t\t\t# The default value to return if there's no default value.\n\t\t\tself.default = default\n\t\t\tself.maxsize = maxsize\n\t\t\tself.typed = typed\n\t\t\tself.data = {}\n\t\t\tself.freqs = {}\n\t\t\tself.lru = {}\n\t\t\tself.used = 0\n\t\t\tself.count = 0\n\t\t\tself.cold = False\n\t\t\tself.hot = False\n\n\t\tdef __call__(self, key):\n\t\t\tvalue = self.data.get(key)\n\t\t\tif value is not None:\n\t\t\t\tself.lru[key] += 1\n\t\t\t\treturn value\n\t\t\tself.data[key] = self.default\n\t\t\tself.freqs[key] = 1\n\t\t\tself.lru[key] = 0\n\t\t\tself.used += 1\n\t\t\tself.count += 1\n\t\t\tif self.used > self.maxsize:\n\t\t\t\tkey, self.cold = min(self.lru, key=self.lru.get)\n\t\t\t\tself.used -= 1\n\t\t\t\tdel self.lru[key]\n\t\t\t\tdel self", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tclass LFUCache(object):\n\t\t\"\"\"\n\t\tClass to memoize the results of a callable.\n\t\t\"\"\"\n\t\tdef __init__(self, func):\n\t\t\tself.func = func\n\t\t\tself.cache = {}\n\t\t\tself.counts = {}\n\t\t\tself.maxsize = maxsize\n\t\t\tself.typed = typed\n\n\t\tdef __call__(self, *args, **kwargs):\n\t\t\tif self.typed:\n\t\t\t\tkey = (args, tuple(kwargs.items()))\n\t\t\telse:\n\t\t\t\tkey = args\n\t\t\tif key not in self.cache:\n\t\t\t\tresult = self.func(*args, **kwargs)\n\t\t\t\tif len(self.cache) >= self.maxsize:\n\t\t\t\t\tself.cache.popitem()\n\t\t\t\tself.cache[key] = result\n\t\t\t\tself.counts[key] = 0\n\t\t\telse:\n\t\t\t\tresult = self.cache[key]\n\t\t\tself.counts[key] += 1\n\t\t\treturn result\n\n\t\treturn LFUCache\n\treturn lfu_cache", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tcache = lfucache(maxsize, typed)\n\treturn lru_cache(maxsize=maxsize, typed=typed)(cache)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorator that wraps func with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\t\"\"\"\n\t\tdef memoized(func, **kwargs):\n\t\t\t\"\"\"\n\t\t\tMemoized function that saves up to `maxsize` results based on a\nLeast Frequently Used (LFU) algorithm.\n\t\t\t\"\"\"\n\t\t\tcache = lfu_cache.Cache(maxsize=maxsize, typed=typed)\n\t\t\tdef _memoized(key):\n\t\t\t\t\"\"\"\n\t\t\t\tMemoized function that saves up to `maxsize` results based on a\nLeast Frequently Used (LFU) algorithm.\n\t\t\t\t\"\"\"\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(key)\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\t\treturn _memoized\n\t\treturn memoized(func)\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\t\"\"\"\n\tDecorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.\n\t\"\"\"\n\timport collections\n\timport hashlib\n\n\tcache = collections.defaultdict(collections.OrderedDict)\n\thits = collections.defaultdict(int)\n\t\n\tdef _lfu_cache(f):\n\t\tdef _lfu_cache_inner(key, *args, **kwargs):\n\t\t\tkey = _safe_key(key, typed)\n\t\t\thits[key] += 1\n\t\t\tcache[hits[key]][key] = f(*args, **kwargs)\n\t\t\tif len(cache[hits[key]]) > maxsize:\n\t\t\t\tdel cache[hits[key]][cache[hits[key]].popitem(last=False)[0]]\n\t\t\treturn cache[hits[key]][key]\n\t\treturn _lfu_cache_inner\n\n\treturn _lfu_cache\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\ttry:\n\t\tkey = self.data.popitem()[0]\n\texcept KeyError:\n\t\traise KeyError(\"dictionary is empty\")\n\treturn key, self.data[key]\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tself.pop(self.head.key)\n\tif self.head.next != None:\n\t\tself.head = self.head.next\n\t\treturn self.head.key, self.head.value\n\telse:\n\t\treturn None\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tpass\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\traise NotImplementedError", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tprint(\"Popitem\\n\")\n\n\tkey = self.data[self.top-1]\n\tvalue = self.value[self.top-1]\n\tself.top -= 1\n\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tpop_value = self.items[self.index]\n\tdel self.items[self.index]\n\tself.index -= 1\n\treturn pop_value\n\t\n# 1.1", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tself.pop(next(iter(self)))\n\n# Use the popitem() function to remove the first inserted key and value in the dictionary.\n\npopitem(my_dict)\n\nprint(my_dict)\n\n# Output: {1: 'b', 2: 'c'}\n\n# Return a tuple containing the key and value in tuple format.\n\nnext(iter(my_dict))\n\nprint(next(iter(my_dict)))\n\n# Output: (1, 'b')\n\n# Return the first inserted key and value in the dictionary.\n\nnext(iter(my_dict))\n\nprint(next(iter(my_dict)))\n\n# Output: (1, 'b')\n\n# Return the first inserted key and value in the dictionary.\n\npopitem(my_dict)\n\nprint(my_dict)\n\n# Output: {2: 'c'}\n\n# Return the first inserted key and value in the dictionary.\n\nnext(iter(my_dict))\n\nprint(next(iter(my_dict)))\n\n# Output: (2, 'c')\n\n# Return the first inserted key and value in the dictionary.\n\npopitem(my_dict)\n\nprint(my_dict)\n\n# Output: {}\n\n# Return the first inserted key and value in the dictionary.", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey = self.data[0]\n\tvalue = self.data[1]\n\tself.data.pop(0)\n\treturn key, value\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tself._check()\n\tself.check_popitem()\n\titem = self.stack.pop()\n\tself.check_popitem()\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tpass\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\ttry:\n\t\tkey = self.data.popitem()[0]\n\texcept KeyError:\n\t\traise KeyError(\"dictionary is empty\")\n\treturn key, self.data[key]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tself.pop(self.head.key)\n\tif self.head.next != None:\n\t\tself.head = self.head.next\n\t\treturn self.head.key, self.head.value\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tprint(\"Popitem\\n\")\n\n\tkey = self.data[self.top-1]\n\tvalue = self.value[self.top-1]\n\tself.top -= 1\n\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tpop_value = self.items[self.index]\n\tdel self.items[self.index]\n\tself.index -= 1\n\treturn pop_value\n\t\n# 1.1", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tself.pop(next(iter(self)))\n\n# Use the popitem() function to remove the first inserted key and value in the dictionary.\n\npopitem(my_dict)\n\nprint(my_dict)\n\n# Output: {1: 'b', 2: 'c'}\n\n# Return a tuple containing the key and value in tuple format.\n\nnext(iter(my_dict))\n\nprint(next(iter(my_dict)))\n\n# Output: (1, 'b')\n\n# Return the first inserted key and value in the dictionary.\n\nnext(iter(my_dict))\n\nprint(next(iter(my_dict)))\n\n# Output: (1, 'b')\n\n# Return the first inserted key and value in the dictionary.\n\npopitem(my_dict)\n\nprint(my_dict)\n\n# Output: {2: 'c'}\n\n# Return the first inserted key and value in the dictionary.\n\nnext(iter(my_dict))\n\nprint(next(iter(my_dict)))\n\n# Output: (2, 'c')\n\n# Return the first inserted key and value in the dictionary.\n\npopitem(my_dict)\n\nprint(my_dict)\n\n# Output: {}\n\n# Return the first inserted key and value in the dictionary.", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tkey = self.data[0]\n\tvalue = self.data[1]\n\tself.data.pop(0)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tself._check()\n\tself.check_popitem()\n\titem = self.stack.pop()\n\tself.check_popitem()\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove the value corresponding to the first inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept:\n\t\tself.__dict__[key] = default\n\n# -------------------------------\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__class__(self).__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\texcept AttributeError:\n\t\tself.__dict__[key] = default\n\t\treturn default", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\tsetattr(self, key, default)\n\t\treturn default", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept:\n\t\tself.__dict__[key] = default\n\n# -------------------------------\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__class__(self).__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\texcept AttributeError:\n\t\tself.__dict__[key] = default\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\tsetattr(self, key, default)\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, the value corresponding to the key is set to default.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\tself.__dict__[key] = default", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError, e:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default\n\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattribute__(key)\n\texcept AttributeError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\treturn default", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default\n\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tvalue = self.__dict__.get(key, default)\n\tif value is not None:\n\t\treturn value\n\telse:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tvalue = getattr(self, key, default)\n\treturn value\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\treturn default\n", "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError, e:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__getattribute__(key)\n\texcept AttributeError:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tvalue = self.__dict__.get(key, default)\n\tif value is not None:\n\t\treturn value\n\telse:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\tvalue = getattr(self, key, default)\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn self.__dict__[key]\n\texcept KeyError:\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\t\"\"\"\n\tIf a key exists in the class, the value corresponding to the key is returned. Otherwise, default is returned.\n\t\"\"\"\n\ttry:\n\t\treturn getattr(self, key)\n\texcept AttributeError:\n\t\treturn default\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwds):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwds)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwds)\n\t\t\t\tcache[key(*args, **kwds)] = result\n\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\tlock = lock or func\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\targs_hash = key(*args)\n\t\t\ttry:\n\t\t\t\tresult = cache[args_hash]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[args_hash] = result\n\t\t\treturn result\n\t\treturn cachedmethod(cache, key, lock)\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\t\treturn result\n\t\treturn wrapped\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kw):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache[key(*args, **kw)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kw)\n\t\t\t\tcache[key(*args, **kw)] = result\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef cached(func):\n\t\tdef cached_func(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\ttry:\n\t\t\t\tcache[key(*args, **kwargs)] = result = func(*args, **kwargs)\n\t\t\texcept:\n\t\t\t\traise\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn cached_func\n\treturn cached\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef f(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\ttry:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\tcache[key(*args, **kwargs)] = func(*args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept Exception, e:\n\t\t\t\traise e\n\t\treturn f\n\treturn decorator\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\t\tcache[key(*args, **kwargs)] = value\n\t\t\t\t\tlock.release()\n\t\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(function, *args, **kwargs):\n\t\tdef inner(*args):\n\t\t\tvalue = cache.get(key(*args, **kwargs))\n\t\t\tif value is None:\n\t\t\t\tvalue = function(*args, **kwargs)\n\t\t\t\tcache.set(key(*args, **kwargs), value, lock=lock)\n\t\t\treturn value\n\t\treturn inner\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef cached(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tvalue = cache.get(key)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache[key] = value\n\t\t\t\treturn value\n\t\treturn cached\n\treturn decorator\n\n#-------------------------------------------------------------------------------\n#\t\tImplementation\n#-------------------------------------------------------------------------------\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwds):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwds)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwds)\n\t\t\t\tcache[key(*args, **kwds)] = result\n\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\tlock = lock or func\n\t\t\ttry:\n\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\targs_hash = key(*args)\n\t\t\ttry:\n\t\t\t\tresult = cache[args_hash]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[args_hash] = result\n\t\t\treturn result\n\t\treturn cachedmethod(cache, key, lock)\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(f):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\t\treturn result\n\t\treturn wrapped\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kw):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache[key(*args, **kw)]\n\t\t\texcept KeyError:\n\t\t\t\tresult = func(*args, **kw)\n\t\t\t\tcache[key(*args, **kw)] = result\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef cached(func):\n\t\tdef cached_func(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\ttry:\n\t\t\t\tcache[key(*args, **kwargs)] = result = func(*args, **kwargs)\n\t\t\texcept:\n\t\t\t\traise\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\tfinally:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn cached_func\n\treturn cached\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef f(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\ttry:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\tcache[key(*args, **kwargs)] = func(*args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept Exception, e:\n\t\t\t\traise e\n\t\treturn f\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\t\tcache[key(*args, **kwargs)] = value\n\t\t\t\t\tlock.release()\n\t\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(function, *args, **kwargs):\n\t\tdef inner(*args):\n\t\t\tvalue = cache.get(key(*args, **kwargs))\n\t\t\tif value is None:\n\t\t\t\tvalue = function(*args, **kwargs)\n\t\t\t\tcache.set(key(*args, **kwargs), value, lock=lock)\n\t\t\treturn value\n\t\treturn inner\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\"\"\"\n\tReturns a decorator function that can call a result in the cache.\n\t\"\"\"\n\tdef decorator(func):\n\t\tdef cached(*args, **kwargs):\n\t\t\tkey = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tvalue = cache.get(key)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache[key] = value\n\t\t\t\treturn value\n\t\treturn cached\n\treturn decorator\n\n#-------------------------------------------------------------------------------\n#\t\tImplementation\n#-------------------------------------------------------------------------------\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport traceback\n\t\n\tpath = cls.__module__\n\tif path.startswith('__main__'):\n\t\tpath = path.split('.')[-2]\n\tif path.startswith('<frozen'):\n\t\tpath = path.split('<')[-1].split('>')[0]\n\t\n\tfilename = cls.__module__.split('.')[-1].split('_')[-1]\n\tif filename.endswith('_test'):\n\t\tfilename = filename[:-5]\n\t\n\tpath = path.split('.')[-1]\n\t\n\ttraceback_list = traceback.extract_tb(sys.exc_info()[2])\n\tline = traceback_list[-1][2]\n\t\n\tresult = ''\n\t\n\tresult += filename + ' '\n\tresult += 'at '\n\tresult += path + ':' + str(line)\n\t\n\tresult += ': '\n\t\n\tresult += str(e)\n\t\n\tresult += '\\n'\n\t\n\tfor trace in traceback_list[1:max_path_level]:\n\t\tresult += '\\t'\n\t\tresult += trace[0] + ' '\n\t\tresult += trace[1] + ' '\n\t\tresult += trace", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tstack = cls.format_exception(e)\n\tmsg = '\\n'.join(stack)\n\t\n\ttraceback = \"\".join(traceback.format_tb(e.__traceback__))\n\t\n\tmsg = \"{0}\\n{1}\".format(msg, traceback)\n\tmsg = msg.replace(\"\\n\", \"\\n#\")\n\tmsg = \"{0}#{1}\".format(msg, str(e))\n\treturn msg\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tres = \"\"\n\tlevel = 0\n\twhile e:\n\t\tres += \"    \" * level + e.args[0] + \"\\n\"\n\t\tlevel += 1\n\t\tif level > max_path_level:\n\t\t\tres += \"    \" * (max_path_level - level) + \"...\\n\"\n\t\t\tbreak\n\t\te = e.__cause__\n\tif level > max_level:\n\t\tres += \"    \" * (max_level - level) + \"...\\n\"\n\treturn res", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tpath = e.__class__.__module__\n\tpath = path.split('.')\n\tpath = path[len(path)-1] + '.' + path[len(path)-2]\n\tpath = path.replace('.', '::')\n\tpath = path.replace('cx_Oracle.', '')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('c", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\tif e is None:\n\t\treturn 'No exception'\n\t\n\tmsg = e.message\n\tif msg is None:\n\t\tmsg = str(e)\n\t\n\tlines = [msg]\n\t\n\tprefix = ' ' * 2\n\t\n\t# Recurse\n\twhile e.cause is not None:\n\t\tlines.insert(0, prefix + 'Caused by:')\n\t\te = e.cause\n\t\t\n\t\tprefix = ' ' * 4\n\t\t\n\t\t# Recurse\n\t\twhile e.cause is not None:\n\t\t\tlines.insert(0, prefix + 'Caused by:')\n\t\t\te = e.cause\n\t\t\n\t\tprefix = ' ' * 6\n\t\n\tresult = '\\n'.join(lines)\n\t\n\tif not cls.is_ignorable(e):\n\t\tresult = cls.format_message(e, max_level=max_level, max_path_level=max_path_level) + '\\n' + result\n\t\n\treturn result\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tpath = []\n\twhile e:\n\t\tpath.append(e.__class__.__name__)\n\t\te = e.__cause__\n\t\tif not e:\n\t\t\tbreak\n\tpath.reverse()\n\t\n\tresult = \"<%s\" % path[0]\n\tfor i in path[1:]:\n\t\tresult += \":%s\" % i\n\tresult += \">\"\n\treturn result\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\tfrom traceback import extract_tb\n\n\tif e.__class__.__bases__[0] == BaseException:\n\t\tclassname = e.__class__.__name__\n\t\tmsg = e.args[0]\n\telse:\n\t\tclassname = e.__class__.__name__\n\t\tmsg = e\n\n\tif classname == \"TypeError\":\n\t\tclassname = \"TypeError: \" + msg\n\telif classname == \"ValueError\":\n\t\tclassname = \"ValueError: \" + msg\n\telif classname == \"OverflowError\":\n\t\tclassname = \"OverflowError: \" + msg\n\telif classname == \"IndexError\":\n\t\tclassname = \"IndexError: \" + msg\n\telif classname == \"KeyError\":\n\t\tclassname = \"KeyError: \" + msg\n\telif classname == \"ZeroDivisionError\":\n\t\tclassname = \"ZeroDivisionError: \" + msg\n\telif classname == \"AttributeError\":\n\t\tclassname = \"AttributeError: \" + msg\n\telif classname == \"NameError\":\n\t\tclassname = \"NameError: \" + msg\n\telif classname == \"UnicodeEncodeError\":\n\t\tclassname = \"UnicodeEncodeError: \"", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tmsg = e.message or \"Unknown exception\"\n\tif max_level:\n\t\tmsg = format_exception(e.__class__, e, e.__traceback__, max_level)\n\t\n\tpath = []\n\tparent = e.__traceback__\n\twhile parent:\n\t\tpath.append(parent.tb_frame.f_code.co_filename)\n\t\tparent = parent.tb_next\n\tpath = path[::-1][:max_path_level]\n\t\n\tmsg = \"{0}\\n\\n{1}\".format(msg, \" -> \".join(path))\n\treturn msg\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\tpath = cls.path(e.__traceback__)\n\tpath = ''.join(path[max_path_level:max_path_level+max_level])\n\t\n\treturn '%s: %s\\n\\n%s' % (cls, e, path)\n\n#src/tasks/__init__.py\n# -*- coding: utf-8 -*-\n\nfrom celery import Celery\n\nfrom . import tasks_logger\n\ncelery = Celery('tasks',\n\t\tbroker='amqp://guest:guest@127.0.0.1//',\n\t\tinclude=['tasks.tasks'])\n\n#src/tasks/tasks.py\n# -*- coding: utf-8 -*-\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\nimport traceback\n\nfrom celery import task\n\nfrom tasks import celery\nfrom tasks import tasks_logger\n\nsys.path.append(os.path.join(os.getcwd(), 'src'))\n\nfrom src import exceptions\nfrom src.tasks.utils import get_random_string\n\nlogger = logging.getLogger(__name__)\n\n\n@task(name='tasks.tasks.generate_report', queue='report", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tname = cls.__name__\n\tif max_level < len(e.args) or max_path_level < len(e.args):\n\t\treturn repr(e)\n\t\n\tlevel = len(e.args)\n\tpath = []\n\twhile e and level:\n\t\tpath.append(e.args[0])\n\t\te = e.__cause__\n\t\tlevel -= 1\n\tpath = path[::-1]\n\t\n\tpath_str = ' '.join(path)\n\t\n\treturn '{}: {}'.format(name, path_str)"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\timport traceback\n\t\n\tpath = cls.__module__\n\tif path.startswith('__main__'):\n\t\tpath = path.split('.')[-2]\n\tif path.startswith('<frozen'):\n\t\tpath = path.split('<')[-1].split('>')[0]\n\t\n\tfilename = cls.__module__.split('.')[-1].split('_')[-1]\n\tif filename.endswith('_test'):\n\t\tfilename = filename[:-5]\n\t\n\tpath = path.split('.')[-1]\n\t\n\ttraceback_list = traceback.extract_tb(sys.exc_info()[2])\n\tline = traceback_list[-1][2]\n\t\n\tresult = ''\n\t\n\tresult += filename + ' '\n\tresult += 'at '\n\tresult += path + ':' + str(line)\n\t\n\tresult += ': '\n\t\n\tresult += str(e)\n\t\n\tresult += '\\n'\n\t\n\tfor trace in traceback_list[1:max_path_level]:\n\t\tresult += '\\t'\n\t\tresult += trace[0] + ' '\n\t\tresult += trace[1] + ' '\n\t\tresult += trace", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tstack = cls.format_exception(e)\n\tmsg = '\\n'.join(stack)\n\t\n\ttraceback = \"\".join(traceback.format_tb(e.__traceback__))\n\t\n\tmsg = \"{0}\\n{1}\".format(msg, traceback)\n\tmsg = msg.replace(\"\\n\", \"\\n#\")\n\tmsg = \"{0}#{1}\".format(msg, str(e))\n\treturn msg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tres = \"\"\n\tlevel = 0\n\twhile e:\n\t\tres += \"    \" * level + e.args[0] + \"\\n\"\n\t\tlevel += 1\n\t\tif level > max_path_level:\n\t\t\tres += \"    \" * (max_path_level - level) + \"...\\n\"\n\t\t\tbreak\n\t\te = e.__cause__\n\tif level > max_level:\n\t\tres += \"    \" * (max_level - level) + \"...\\n\"\n\treturn res", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tpath = e.__class__.__module__\n\tpath = path.split('.')\n\tpath = path[len(path)-1] + '.' + path[len(path)-2]\n\tpath = path.replace('.', '::')\n\tpath = path.replace('cx_Oracle.', '')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('cx_Oracle.locations.', ':')\n\tpath = path.replace('c", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\tif e is None:\n\t\treturn 'No exception'\n\t\n\tmsg = e.message\n\tif msg is None:\n\t\tmsg = str(e)\n\t\n\tlines = [msg]\n\t\n\tprefix = ' ' * 2\n\t\n\t# Recurse\n\twhile e.cause is not None:\n\t\tlines.insert(0, prefix + 'Caused by:')\n\t\te = e.cause\n\t\t\n\t\tprefix = ' ' * 4\n\t\t\n\t\t# Recurse\n\t\twhile e.cause is not None:\n\t\t\tlines.insert(0, prefix + 'Caused by:')\n\t\t\te = e.cause\n\t\t\n\t\tprefix = ' ' * 6\n\t\n\tresult = '\\n'.join(lines)\n\t\n\tif not cls.is_ignorable(e):\n\t\tresult = cls.format_message(e, max_level=max_level, max_path_level=max_path_level) + '\\n' + result\n\t\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tpath = []\n\twhile e:\n\t\tpath.append(e.__class__.__name__)\n\t\te = e.__cause__\n\t\tif not e:\n\t\t\tbreak\n\tpath.reverse()\n\t\n\tresult = \"<%s\" % path[0]\n\tfor i in path[1:]:\n\t\tresult += \":%s\" % i\n\tresult += \">\"\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\timport sys\n\tfrom traceback import extract_tb\n\n\tif e.__class__.__bases__[0] == BaseException:\n\t\tclassname = e.__class__.__name__\n\t\tmsg = e.args[0]\n\telse:\n\t\tclassname = e.__class__.__name__\n\t\tmsg = e\n\n\tif classname == \"TypeError\":\n\t\tclassname = \"TypeError: \" + msg\n\telif classname == \"ValueError\":\n\t\tclassname = \"ValueError: \" + msg\n\telif classname == \"OverflowError\":\n\t\tclassname = \"OverflowError: \" + msg\n\telif classname == \"IndexError\":\n\t\tclassname = \"IndexError: \" + msg\n\telif classname == \"KeyError\":\n\t\tclassname = \"KeyError: \" + msg\n\telif classname == \"ZeroDivisionError\":\n\t\tclassname = \"ZeroDivisionError: \" + msg\n\telif classname == \"AttributeError\":\n\t\tclassname = \"AttributeError: \" + msg\n\telif classname == \"NameError\":\n\t\tclassname = \"NameError: \" + msg\n\telif classname == \"UnicodeEncodeError\":\n\t\tclassname = \"UnicodeEncodeError: \"", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tmsg = e.message or \"Unknown exception\"\n\tif max_level:\n\t\tmsg = format_exception(e.__class__, e, e.__traceback__, max_level)\n\t\n\tpath = []\n\tparent = e.__traceback__\n\twhile parent:\n\t\tpath.append(parent.tb_frame.f_code.co_filename)\n\t\tparent = parent.tb_next\n\tpath = path[::-1][:max_path_level]\n\t\n\tmsg = \"{0}\\n\\n{1}\".format(msg, \" -> \".join(path))\n\treturn msg\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\t\n\tpath = cls.path(e.__traceback__)\n\tpath = ''.join(path[max_path_level:max_path_level+max_level])\n\t\n\treturn '%s: %s\\n\\n%s' % (cls, e, path)\n\n#src/tasks/__init__.py\n# -*- coding: utf-8 -*-\n\nfrom celery import Celery\n\nfrom . import tasks_logger\n\ncelery = Celery('tasks',\n\t\tbroker='amqp://guest:guest@127.0.0.1//',\n\t\tinclude=['tasks.tasks'])\n\n#src/tasks/tasks.py\n# -*- coding: utf-8 -*-\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\nimport traceback\n\nfrom celery import task\n\nfrom tasks import celery\nfrom tasks import tasks_logger\n\nsys.path.append(os.path.join(os.getcwd(), 'src'))\n\nfrom src import exceptions\nfrom src.tasks.utils import get_random_string\n\nlogger = logging.getLogger(__name__)\n\n\n@task(name='tasks.tasks.generate_report', queue='report", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t\"\"\"\n\tFormat the exception as a string\n\t\"\"\"\n\tname = cls.__name__\n\tif max_level < len(e.args) or max_path_level < len(e.args):\n\t\treturn repr(e)\n\t\n\tlevel = len(e.args)\n\tpath = []\n\twhile e and level:\n\t\tpath.append(e.args[0])\n\t\te = e.__cause__\n\t\tlevel -= 1\n\tpath = path[::-1]\n\t\n\tpath_str = ' '.join(path)\n\t\n\treturn '{}: {}'.format(name, path_str)", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'a' if overwrite else 'w'\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'a' if overwrite else 'a+', encoding) as file:\n\t\t\tfile.write(text_buffer)\n\texcept Exception as error:\n\t\traise Exception(f'Failed to write to {file_name}')\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\tif not os.path.exists(file_name):\n\t\traise Exception('File does not exist')\n\t\n\tfile_exists = os.path.exists(file_name)\n\t\n\tif file_exists and not overwrite:\n\t\traise Exception('File already exists')\n\t\n\tmode = 'w' if not file_exists else 'a'\n\t\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tfile_name_with_path = Path(file_name)\n\tif not overwrite and file_name_with_path.exists():\n\t\traise FileExistsError(\"File already exists\")\n\twith open(file_name_with_path, \"a\", encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, 'a', encoding=encoding) as text_file:\n\t\ttext_file.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, 'a' if overwrite else 'a+', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, 'a', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\ttry:\n\t\tfile_object = open(file_name, \"a\", encoding=encoding)\n\texcept IOError as e:\n\t\tprint(e)\n\t\treturn False\n\n\t# If the file exists, then we need to check if we need to overwrite it\n\tif(os.path.isfile(file_name)):\n\t\twith open(file_name, 'r') as file:\n\t\t\tcontents = file.read()\n\t\t\tif(overwrite or (contents != '' and not contents.endswith(\"\\n\"))):\n\t\t\t\tfile_object.write(text_buffer)\n\t\t\telse:\n\t\t\t\tprint(\"File exists, but is empty or doesn't end with a newline\")\n\t\t\t\treturn False\n\telse:\n\t\tfile_object.write(text_buffer)\n\n\tfile_object.close()\n\treturn True\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, \"a\", encoding=encoding) as file:\n\t\tif overwrite:\n\t\t\tfile.write(\"\\n\")\n\t\tfile.write(text_buffer)\n\n# Read the contents of a file and return as a list of lines", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tfile_object = open(file_name, \"a\" if overwrite else \"w\", encoding=encoding)\n\tfile_object.write(text_buffer)\n\tfile_object.close()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tmode = 'a' if overwrite else 'w'\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\ttry:\n\t\twith open(file_name, 'a' if overwrite else 'a+', encoding) as file:\n\t\t\tfile.write(text_buffer)\n\texcept Exception as error:\n\t\traise Exception(f'Failed to write to {file_name}')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\t\n\tif not os.path.exists(file_name):\n\t\traise Exception('File does not exist')\n\t\n\tfile_exists = os.path.exists(file_name)\n\t\n\tif file_exists and not overwrite:\n\t\traise Exception('File already exists')\n\t\n\tmode = 'w' if not file_exists else 'a'\n\t\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tfile_name_with_path = Path(file_name)\n\tif not overwrite and file_name_with_path.exists():\n\t\traise FileExistsError(\"File already exists\")\n\twith open(file_name_with_path, \"a\", encoding=encoding) as f:\n\t\tf.write(text_buffer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, 'a', encoding=encoding) as text_file:\n\t\ttext_file.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, 'a' if overwrite else 'a+', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, 'a', encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\ttry:\n\t\tfile_object = open(file_name, \"a\", encoding=encoding)\n\texcept IOError as e:\n\t\tprint(e)\n\t\treturn False\n\n\t# If the file exists, then we need to check if we need to overwrite it\n\tif(os.path.isfile(file_name)):\n\t\twith open(file_name, 'r') as file:\n\t\t\tcontents = file.read()\n\t\t\tif(overwrite or (contents != '' and not contents.endswith(\"\\n\"))):\n\t\t\t\tfile_object.write(text_buffer)\n\t\t\telse:\n\t\t\t\tprint(\"File exists, but is empty or doesn't end with a newline\")\n\t\t\t\treturn False\n\telse:\n\t\tfile_object.write(text_buffer)\n\n\tfile_object.close()\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\twith open(file_name, \"a\", encoding=encoding) as file:\n\t\tif overwrite:\n\t\t\tfile.write(\"\\n\")\n\t\tfile.write(text_buffer)\n\n# Read the contents of a file and return as a list of lines", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\"\"\"\n\tWrites the data in the text buffer to a file\n\t\"\"\"\n\tfile_object = open(file_name, \"a\" if overwrite else \"w\", encoding=encoding)\n\tfile_object.write(text_buffer)\n\tfile_object.close()\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttextbuffer = f.read()\n\t\treturn textbuffer\n\t", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile_buffer = \"\"\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as file:\n\t\t\tfile_buffer = file.read()\n\texcept Exception as e:\n\t\tprint(\"Error: \", e)\n\treturn file_buffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\t# Open the file\n\tfile = open(file_name, \"r\", encoding=encoding)\n\t\n\t# Read the file into a string\n\tfile_text = file.read()\n\t\n\t# Close the file\n\tfile.close()\n\t\n\t# Return the file text\n\treturn file_text\n\n# Load the file\nfile_text = file_to_textbuffer(\"text.txt\", \"utf-8\")\n\n# Print the file text\nprint(file_text)", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as file:\n\t\tcontent = file.read()\n\treturn content\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttext_buffer = open(file_name, 'r', encoding=encoding).read()\n\t\n\treturn text_buffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttext_buffer = open(file_name, 'rb').read()\n\treturn text_buffer.decode(encoding)\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttextbuffer = f.read()\n\treturn textbuffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttextbuffer = []\n\twith open(file_name, encoding=encoding) as file:\n\t\tfor line in file:\n\t\t\ttextbuffer.append(line)\n\treturn textbuffer\n", "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttextbuffer = f.read()\n\t\treturn textbuffer\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\tfile_buffer = \"\"\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as file:\n\t\t\tfile_buffer = file.read()\n\texcept Exception as e:\n\t\tprint(\"Error: \", e)\n\treturn file_buffer\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\t\n\t# Open the file\n\tfile = open(file_name, \"r\", encoding=encoding)\n\t\n\t# Read the file into a string\n\tfile_text = file.read()\n\t\n\t# Close the file\n\tfile.close()\n\t\n\t# Return the file text\n\treturn file_text\n\n# Load the file\nfile_text = file_to_textbuffer(\"text.txt\", \"utf-8\")\n\n# Print the file text\nprint(file_text)", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as file:\n\t\tcontent = file.read()\n\treturn content\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttext_buffer = open(file_name, 'r', encoding=encoding).read()\n\t\n\treturn text_buffer\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttext_buffer = open(file_name, 'rb').read()\n\treturn text_buffer.decode(encoding)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\ttextbuffer = f.read()\n\treturn textbuffer\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\ttextbuffer = []\n\twith open(file_name, encoding=encoding) as file:\n\t\tfor line in file:\n\t\t\ttextbuffer.append(line)\n\treturn textbuffer\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t\"\"\"\n\tLoad a file toward a text buffer\n\t\"\"\"\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name)\n\texcept IOError:\n\t\treturn False\n\treturn True\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name)\n\t\treturn True\n\texcept Exception as e:\n\t\tprint e\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os.path\n\treturn os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name, 'r')\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile = open(file_name, 'r')\n\t\tfile.close()\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name):\n\t\t\treturn True\n\texcept OSError:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name, \"r\")\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile_name = open(file_name).close()\n\texcept IOError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os\n\treturn os.path.isfile(file_name)\n", "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile_name = os.path.expanduser(file_name)\n\t\tos.stat(file_name)\n\t\treturn True\n\texcept:\n\t\treturn False\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name)\n\texcept IOError:\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name)\n\t\treturn True\n\texcept Exception as e:\n\t\tprint e\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os.path\n\treturn os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name, 'r')\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile = open(file_name, 'r')\n\t\tfile.close()\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\twith open(file_name):\n\t\t\treturn True\n\texcept OSError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\topen(file_name, \"r\")\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile_name = open(file_name).close()\n\texcept IOError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\timport os\n\treturn os.path.isfile(file_name)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\t\"\"\"\n\tCheck whether file_name is an existing file.\n\t\"\"\"\n\ttry:\n\t\tfile_name = os.path.expanduser(file_name)\n\t\tos.stat(file_name)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogger = logging.getLogger(cls.__name__)\n\tlogger.setLevel(logging.INFO)\n\t\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.INFO)\n\tstream_handler.setFormatter(formatter)\n\t\n\tfile_handler = logging.FileHandler('{}.log'.format(cls.__name__), 'w')\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\t\n\tlogger.addHandler(stream_handler)\n\tlogger.addHandler(file_handler)\n\n#src/main.py\nimport logging\nimport sys\n\nfrom src.util import _reset_logging\nfrom src.ui.main_ui import MainUI\n\nif __name__ == '__main__':\n\t\n\t_reset_logging(MainUI)\n\t\n\tdef main():\n\t\tapp = MainUI()\n\t\tsys.exit(app.exec_())\n\t\n\tmain()", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger = logging.getLogger()\n\tcls.logger.handlers = []\n\tcls.logger.setLevel(logging.INFO)\n\tcls.logger.addHandler(logging.StreamHandler())\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger = None\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tglobal logger\n\tlogger = logging.getLogger(cls.__module__)\n\n@pytest.fixture(autouse=True)", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger.handlers = []\n\tfrom logging import StreamHandler\n\tfrom logging import Formatter\n\tfrom logging import DEBUG\n\thandler = StreamHandler()\n\thandler.setLevel(DEBUG)\n\tformatter = Formatter('%(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tcls.logger.addHandler(handler)\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tglobal _log_level\n\tglobal _log_handler\n\tglobal _log_format\n\tglobal _log_file\n\tglobal _log_console\n\tglobal _log_stream\n\tglobal _log_lock\n\tglobal _log_file_lock\n\t\n\t_log_level = 0\n\t_log_handler = None\n\t_log_format = ''\n\t_log_file = None\n\t_log_console = False\n\t_log_stream = None\n\t_log_lock = None\n\t_log_file_lock = None\n\t\n\t# The default log handler is the console log handler\n\t_log_handler = _stdout_log_handler\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.basicConfig(level=logging.INFO)\n\tcls.logger = logging.getLogger(cls.__name__)\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogger = logging.getLogger(cls.__name__)\n\tlogger.handlers = []\n\tlogger.setLevel(logging.NOTSET)", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tdel cls._logger.handlers[:]\n\n", "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tfrom logging import NullHandler\n\tcls.logger.handlers = [NullHandler()]\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogger = logging.getLogger(cls.__name__)\n\tlogger.setLevel(logging.INFO)\n\t\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setLevel(logging.INFO)\n\tstream_handler.setFormatter(formatter)\n\t\n\tfile_handler = logging.FileHandler('{}.log'.format(cls.__name__), 'w')\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\t\n\tlogger.addHandler(stream_handler)\n\tlogger.addHandler(file_handler)\n\n#src/main.py\nimport logging\nimport sys\n\nfrom src.util import _reset_logging\nfrom src.ui.main_ui import MainUI\n\nif __name__ == '__main__':\n\t\n\t_reset_logging(MainUI)\n\t\n\tdef main():\n\t\tapp = MainUI()\n\t\tsys.exit(app.exec_())\n\t\n\tmain()", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger = logging.getLogger()\n\tcls.logger.handlers = []\n\tcls.logger.setLevel(logging.INFO)\n\tcls.logger.addHandler(logging.StreamHandler())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger = None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tglobal logger\n\tlogger = logging.getLogger(cls.__module__)\n\n@pytest.fixture(autouse=True)", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tcls.logger.handlers = []\n\tfrom logging import StreamHandler\n\tfrom logging import Formatter\n\tfrom logging import DEBUG\n\thandler = StreamHandler()\n\thandler.setLevel(DEBUG)\n\tformatter = Formatter('%(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tcls.logger.addHandler(handler)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tglobal _log_level\n\tglobal _log_handler\n\tglobal _log_format\n\tglobal _log_file\n\tglobal _log_console\n\tglobal _log_stream\n\tglobal _log_lock\n\tglobal _log_file_lock\n\t\n\t_log_level = 0\n\t_log_handler = None\n\t_log_format = ''\n\t_log_file = None\n\t_log_console = False\n\t_log_stream = None\n\t_log_lock = None\n\t_log_file_lock = None\n\t\n\t# The default log handler is the console log handler\n\t_log_handler = _stdout_log_handler\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\timport logging\n\tlogging.basicConfig(level=logging.INFO)\n\tcls.logger = logging.getLogger(cls.__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tlogger = logging.getLogger(cls.__name__)\n\tlogger.handlers = []\n\tlogger.setLevel(logging.NOTSET)", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tdel cls._logger.handlers[:]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _reset_logging(cls):\n\t\"\"\"\n\tReset the logging system\n\t\"\"\"\n\tfrom logging import NullHandler\n\tcls.logger.handlers = [NullHandler()]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError()\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError(\"_getTargetClass must be overriden by subclasses\")\n\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError, \"Implement this function in your subclass\"\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\timport sys\n\timport inspect\n\tmodule = inspect.getmodule(inspect.stack()[1][0])\n\tcls = module.__dict__[module.__name__]\n\twhile hasattr(cls, '__module__'):\n\t\tcls = cls.__module__\n\t\treturn cls.replace('Fallback', '')\n\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttry:\n\t\treturn self.PyClass\n\texcept AttributeError:\n\t\treturn self.FallbackClass\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n\n# PyPy (PyPy3)", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttarget = self.getTarget()\n\tif target.startswith(\"Py\"):\n\t\treturn target[2:]\n\telif target.startswith(\"Fallback\"):\n\t\treturn target[8:]\n\telse:\n\t\traise RuntimeError(\"The target class is not properly specified: %s\" % target)\n", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttarget = str(self.target)\n\treturn target.replace('Py', '').replace('Fallback', '')", "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttarget = getattr(self, 'target', None)\n\tif target is not None:\n\t\ttarget = target.split('.')[-1]\n\t\treturn target\n\telse:\n\t\treturn None\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError(\"_getTargetClass must be overriden by subclasses\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError, \"Implement this function in your subclass\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\timport sys\n\timport inspect\n\tmodule = inspect.getmodule(inspect.stack()[1][0])\n\tcls = module.__dict__[module.__name__]\n\twhile hasattr(cls, '__module__'):\n\t\tcls = cls.__module__\n\t\treturn cls.replace('Fallback', '')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttry:\n\t\treturn self.PyClass\n\texcept AttributeError:\n\t\treturn self.FallbackClass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\traise NotImplementedError\n\n# PyPy (PyPy3)", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttarget = self.getTarget()\n\tif target.startswith(\"Py\"):\n\t\treturn target[2:]\n\telif target.startswith(\"Fallback\"):\n\t\treturn target[8:]\n\telse:\n\t\traise RuntimeError(\"The target class is not properly specified: %s\" % target)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttarget = str(self.target)\n\treturn target.replace('Py', '').replace('Fallback', '')", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\t\"\"\"\n\tDefine this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.\n\t\"\"\"\n\ttarget = getattr(self, 'target', None)\n\tif target is not None:\n\t\ttarget = target.split('.')[-1]\n\t\treturn target\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerged = []\n\tfor order in orderings:\n\t\tfor item in order:\n\t\t\tif item not in merged:\n\t\t\t\tmerged.append(item)\n\treturn merged\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor o in ordering:\n\t\t\tif o not in merged:\n\t\t\t\tmerged.append(o)\n\t\t\t\n\treturn merged\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerge = []\n\tfor ordering in orderings:\n\t\tfor i in ordering:\n\t\t\tif i not in merge:\n\t\t\t\tmerge.append(i)\n\treturn merge\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\torderings = set(orderings)\n\ti = 0\n\twhile i < len(orderings) - 1:\n\t\tif orderings[i] in orderings[i+1]:\n\t\t\torderings.remove(orderings[i])\n\t\telse:\n\t\t\ti += 1\n\treturn orderings\n\n#py_project/mergemodel.py\nimport csv\nfrom datetime import datetime\n\nfrom matplotlib import pyplot as plt\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult.extend(ordering)\n\tresult = list(set(result))\n\tresult.sort()\n\treturn result\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tout = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in out:\n\t\t\t\tout.append(item)\n\treturn out\n\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tnew_orderings = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in new_orderings:\n\t\t\t\tnew_orderings.append(item)\n\treturn new_orderings\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\t\n\t\n\t\n\treturn orderings\n\t\n\t\n\t\n\t\n\t", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tcombined = []\n\tfor ordering in orderings:\n\t\tfor order in ordering:\n\t\t\tif order not in combined:\n\t\t\t\tcombined.append(order)\n\treturn combined\n", "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\tmerged = []\n\t\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in merged:\n\t\t\t\tmerged.append(item)\n\t\n\treturn merged\n\t\n\t\n#==============================================================================\n# Miscellaneous\n#==============================================================================\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerged = []\n\tfor order in orderings:\n\t\tfor item in order:\n\t\t\tif item not in merged:\n\t\t\t\tmerged.append(item)\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor o in ordering:\n\t\t\tif o not in merged:\n\t\t\t\tmerged.append(o)\n\t\t\t\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tmerge = []\n\tfor ordering in orderings:\n\t\tfor i in ordering:\n\t\t\tif i not in merge:\n\t\t\t\tmerge.append(i)\n\treturn merge\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\torderings = set(orderings)\n\ti = 0\n\twhile i < len(orderings) - 1:\n\t\tif orderings[i] in orderings[i+1]:\n\t\t\torderings.remove(orderings[i])\n\t\telse:\n\t\t\ti += 1\n\treturn orderings\n\n#py_project/mergemodel.py\nimport csv\nfrom datetime import datetime\n\nfrom matplotlib import pyplot as plt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult.extend(ordering)\n\tresult = list(set(result))\n\tresult.sort()\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tout = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in out:\n\t\t\t\tout.append(item)\n\treturn out\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tnew_orderings = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in new_orderings:\n\t\t\t\tnew_orderings.append(item)\n\treturn new_orderings\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\t\n\t\n\t\n\treturn orderings\n\t\n\t\n\t\n\t\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\tcombined = []\n\tfor ordering in orderings:\n\t\tfor order in ordering:\n\t\t\tif order not in combined:\n\t\t\t\tcombined.append(order)\n\treturn combined\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t\"\"\"\n\tCombine multiple lists in the sequence of occurrence into a list with no duplicate elements.\n\t\"\"\"\n\t\n\tmerged = []\n\t\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in merged:\n\t\t\t\tmerged.append(item)\n\t\n\treturn merged\n\t\n\t\n#==============================================================================\n# Miscellaneous\n#==============================================================================\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tinterfaces = getProvidedBy(object)\n\tprovides = []\n\tfor interface in interfaces:\n\t\tprovides.append(interface.provides)\n\treturn provides\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface.interfaces import IDeclarations\n\tfrom zope.interface.interfaces import IInterface\n\tfrom zope.interface.interfaces import IMissing\n\tfrom zope.interface.interfaces import IInterface\n\n\tdef _getProvidedBy(interface, context):\n\t\tproviders = IDeclarations(interface).getProviders()\n\t\tfor provider in providers:\n\t\t\tp = provider(context)\n\t\t\tif p is not None:\n\t\t\t\treturn p\n\t\treturn None\n\n\tdef _getProvidedByRecursive(interface, context):\n\t\tproviders = IDeclarations(interface).getProviders()\n\t\tfor provider in providers:\n\t\t\tp = _getProvidedBy(provider, context)\n\t\t\tif p is not None:\n\t\t\t\treturn p\n\t\treturn None\n\n\tdef _getProvidedByRecursive(interface, context):\n\t\tproviders = IDeclarations(interface).getProviders()\n\t\tfor provider in providers:\n\t\t\tp = _getProvidedBy(provider, context)\n\t\t\tif p is not None:\n\t\t\t\treturn p\n\t\treturn None\n\n\tdef _getProvidedBy(interface, context):\n\t\tif isinstance(interface, IInterface):\n\t\t\treturn _getProvidedByRecursive(interface, context)\n\t\tif", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface import Interface\n\t\n\treturn Interface(object).directlyProvidedBy()", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\ttry:\n\t\treturn interfaces.IDeclaration.__of__(object)\n\texcept AttributeError:\n\t\tpass\n\n\traise NotImplementedError('No interfaces directly provided by %s' % object)", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tinterfaces = []\n\tfor intf in object.interfaces():\n\t\tif intf in object.directlyProvidedBy():\n\t\t\tinterfaces.append(intf)\n\treturn interfaces\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tproviders = []\n\tfor interface in object.providedBy():\n\t\tproviders.append(interface)\n\treturn providers\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t__docformat__ = 'restructuredtext'\n\t__doc__ = InterfaceDeclarations.__doc__\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom ..interfaces import IProvides\n\treturn IProvides(object).providedBy\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tprovidedBy = object.providedBy\n\treturn providedBy\n\n", "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface import Interface\n\tfrom zope.interface.declaration import InterfaceDeclaration\n\tfrom zope.interface.declaration import Implementation\n\tfrom zope.interface.declaration import ImplementedBy\n\tfrom zope.interface.declaration import IImplementedBy\n\tfrom zope.interface.declaration import IProvidedBy\n\tfrom zope.interface.declaration import IProvidedByInterface\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementor\n\tfrom zope.interface.declaration import IProvidedByImplementor\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementorOrInterface\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementorOrInterfaceOrImplementor\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterface\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterfaceOrInterface\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementorOrInterfaceOrImplementorOrInterface\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterfaceOrInterfaceOrInterface\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterfaceOrInterfaceOrInterfaceOr"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tinterfaces = getProvidedBy(object)\n\tprovides = []\n\tfor interface in interfaces:\n\t\tprovides.append(interface.provides)\n\treturn provides\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface.interfaces import IDeclarations\n\tfrom zope.interface.interfaces import IInterface\n\tfrom zope.interface.interfaces import IMissing\n\tfrom zope.interface.interfaces import IInterface\n\n\tdef _getProvidedBy(interface, context):\n\t\tproviders = IDeclarations(interface).getProviders()\n\t\tfor provider in providers:\n\t\t\tp = provider(context)\n\t\t\tif p is not None:\n\t\t\t\treturn p\n\t\treturn None\n\n\tdef _getProvidedByRecursive(interface, context):\n\t\tproviders = IDeclarations(interface).getProviders()\n\t\tfor provider in providers:\n\t\t\tp = _getProvidedBy(provider, context)\n\t\t\tif p is not None:\n\t\t\t\treturn p\n\t\treturn None\n\n\tdef _getProvidedByRecursive(interface, context):\n\t\tproviders = IDeclarations(interface).getProviders()\n\t\tfor provider in providers:\n\t\t\tp = _getProvidedBy(provider, context)\n\t\t\tif p is not None:\n\t\t\t\treturn p\n\t\treturn None\n\n\tdef _getProvidedBy(interface, context):\n\t\tif isinstance(interface, IInterface):\n\t\t\treturn _getProvidedByRecursive(interface, context)\n\t\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface import Interface\n\t\n\treturn Interface(object).directlyProvidedBy()", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\ttry:\n\t\treturn interfaces.IDeclaration.__of__(object)\n\texcept AttributeError:\n\t\tpass\n\n\traise NotImplementedError('No interfaces directly provided by %s' % object)", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tinterfaces = []\n\tfor intf in object.interfaces():\n\t\tif intf in object.directlyProvidedBy():\n\t\t\tinterfaces.append(intf)\n\treturn interfaces\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tproviders = []\n\tfor interface in object.providedBy():\n\t\tproviders.append(interface)\n\treturn providers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\t__docformat__ = 'restructuredtext'\n\t__doc__ = InterfaceDeclarations.__doc__\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom ..interfaces import IProvides\n\treturn IProvides(object).providedBy\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tprovidedBy = object.providedBy\n\treturn providedBy\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\t\"\"\"\n\tReturn the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.\n\t\"\"\"\n\tfrom zope.interface import Interface\n\tfrom zope.interface.declaration import InterfaceDeclaration\n\tfrom zope.interface.declaration import Implementation\n\tfrom zope.interface.declaration import ImplementedBy\n\tfrom zope.interface.declaration import IImplementedBy\n\tfrom zope.interface.declaration import IProvidedBy\n\tfrom zope.interface.declaration import IProvidedByInterface\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementor\n\tfrom zope.interface.declaration import IProvidedByImplementor\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementorOrInterface\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementorOrInterfaceOrImplementor\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterface\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterfaceOrInterface\n\tfrom zope.interface.declaration import IProvidedByInterfaceOrImplementorOrInterfaceOrImplementorOrInterface\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterfaceOrInterfaceOrInterface\n\tfrom zope.interface.declaration import IProvidedByImplementorOrInterfaceOrInterfaceOrInterfaceOr", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tout = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tout.append(c)\n\treturn out\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tclasses = set(classes)\n\tbaseClasses = set()\n\twhile classes:\n\t\tnewClasses = set()\n\t\tfor c in classes:\n\t\t\tnewClasses.update(c.subclasses)\n\t\tclasses = newClasses\n\treturn list(classes)\n\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tall_classes = classes.copy()\n\twhile True:\n\t\tnew_classes = []\n\t\tfor c in all_classes:\n\t\t\tnew_classes += c.getSubclasses()\n\t\tif all_classes == new_classes:\n\t\t\treturn all_classes\n\t\tall_classes = new_classes\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\t\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tresult.append(c)\n\t\n\treturn result\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tminimal.append(c)\n\t\tif c.subclasses:\n\t\t\tminimal += [m for s in c.subclasses for m in minimalBases(s)]\n\treturn minimal\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\tfor class_ in classes:\n\t\tif class_ not in classes:\n\t\t\tresult += [class_]\n\treturn result\n\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tclasses = set(classes)\n\tclasses = list(classes)\n\tclasses.sort()\n\ti = 0\n\twhile i < len(classes):\n\t\tj = i + 1\n\t\twhile j < len(classes):\n\t\t\tif classes[i] < classes[j]:\n\t\t\t\tclasses.remove(classes[j])\n\t\t\telse:\n\t\t\t\tj += 1\n\t\ti += 1\n\treturn classes\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tif c.subclasses is None:\n\t\t\tminimal.append(c)\n\t\telse:\n\t\t\tminimal.extend(minimalBases(c.subclasses))\n\treturn minimal\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\tfor class_ in classes:\n\t\tresult.append(class_['name'])\n\t\tfor subclass in class_['subclasses']:\n\t\t\tresult.append(subclass['name'])\n\treturn result\n", "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tres = []\n\ti = 0\n\twhile i < len(classes):\n\t\tj = i + 1\n\t\twhile j < len(classes):\n\t\t\tif classes[j] in classes[i]:\n\t\t\t\tclasses.remove(classes[j])\n\t\t\tj += 1\n\t\ti += 1\n\tres = classes[:]\n\treturn res\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tout = []\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tout.append(c)\n\treturn out\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tclasses = set(classes)\n\tbaseClasses = set()\n\twhile classes:\n\t\tnewClasses = set()\n\t\tfor c in classes:\n\t\t\tnewClasses.update(c.subclasses)\n\t\tclasses = newClasses\n\treturn list(classes)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tall_classes = classes.copy()\n\twhile True:\n\t\tnew_classes = []\n\t\tfor c in all_classes:\n\t\t\tnew_classes += c.getSubclasses()\n\t\tif all_classes == new_classes:\n\t\t\treturn all_classes\n\t\tall_classes = new_classes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\t\n\tfor c in classes:\n\t\tif not c.subclasses:\n\t\t\tresult.append(c)\n\t\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tminimal.append(c)\n\t\tif c.subclasses:\n\t\t\tminimal += [m for s in c.subclasses for m in minimalBases(s)]\n\treturn minimal\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\tfor class_ in classes:\n\t\tif class_ not in classes:\n\t\t\tresult += [class_]\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tclasses = set(classes)\n\tclasses = list(classes)\n\tclasses.sort()\n\ti = 0\n\twhile i < len(classes):\n\t\tj = i + 1\n\t\twhile j < len(classes):\n\t\t\tif classes[i] < classes[j]:\n\t\t\t\tclasses.remove(classes[j])\n\t\t\telse:\n\t\t\t\tj += 1\n\t\ti += 1\n\treturn classes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tminimal = []\n\tfor c in classes:\n\t\tif c.subclasses is None:\n\t\t\tminimal.append(c)\n\t\telse:\n\t\t\tminimal.extend(minimalBases(c.subclasses))\n\treturn minimal\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tresult = []\n\tfor class_ in classes:\n\t\tresult.append(class_['name'])\n\t\tfor subclass in class_['subclasses']:\n\t\t\tresult.append(subclass['name'])\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t\"\"\"\n\tReturns all classes without subclasses as a list.\n\t\"\"\"\n\tres = []\n\ti = 0\n\twhile i < len(classes):\n\t\tj = i + 1\n\t\twhile j < len(classes):\n\t\t\tif classes[j] in classes[i]:\n\t\t\t\tclasses.remove(classes[j])\n\t\t\tj += 1\n\t\ti += 1\n\tres = classes[:]\n\treturn res\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef getAttr(attr):\n\t\ttry:\n\t\t\tdata = getattr(self, attr)\n\t\t\tdesc = str(data).strip('\\n').replace('\\t', ' ')\n\t\t\treturn desc\n\t\texcept:\n\t\t\treturn ''\n\timport inspect\n\tallAttrs = inspect.getmembers(self)\n\tallAttrs.sort(key=lambda x: x[0])\n\tallAttrs = [allAttrs[i] for i in range(len(allAttrs)) if not getattr(self, allAttrs[i][0]).__class__.__name__ == 'function']\n\tif all:\n\t\treturn allAttrs\n\telse:\n\t\treturn [allAttrs[i] for i in range(len(allAttrs)) if not getattr(self, allAttrs[i][0]).__class__.__name__ == 'function']", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tfields = self.fields\n\tif all:\n\t\tfields = fields + self.meta.local_fields\n\tnames = []\n\tdescriptions = []\n\tfor field in fields:\n\t\tnames.append(field.name)\n\t\tdescriptions.append(field.description or '')\n\treturn names, descriptions\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tnamesAndDescriptions = [\n\t\tself.name,\n\t\tself.description,\n\t]\n\t\n\tif not all:\n\t\tnamesAndDescriptions = [n for n in namesAndDescriptions if n]\n\t\n\treturn namesAndDescriptions\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tcurrent = self.__class__.current\n\tresult = [self.__class__.current.name, self.__class__.current.description]\n\tself.__class__.current = self.__class__.current.parent\n\twhile self.__class__.current is not None:\n\t\tresult.insert(0, (self.__class__.current.name, self.__class__.current.description))\n\t\tself.__class__.current = self.__class__.current.parent\n\tif not all:\n\t\tself.__class__.current = current\n\treturn result\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef _namesAndDescriptions(cls, all=False):\n\t\t\"\"\"\n\t\tReturns the name and description of the current class attribute.\n\t\t\"\"\"\n\t\tnames = list(cls._fields)\n\t\tdescriptions = [getattr(cls, name) for name in names]\n\n\t\tif all:\n\t\t\treturn [(name, description) for name, description in zip(names, descriptions)]\n\t\telse:\n\t\t\treturn names\n\n\tdef _namesAndDescriptionsAll(all=False):\n\t\treturn _namesAndDescriptions(all=all)\n\n\treturn _namesAndDescriptions if all else _namesAndDescriptionsAll\n\n\n#\n# class NamesAndDescriptionsTest(unittest.TestCase):\n# \t@staticmethod\n# \tdef _test_namesAndDescriptions():\n# \t\tclass TestClass(NamesAndDescriptions):\n# \t\t\tfield1 = \"1\"\n# \t\t\tfield2 = \"2\"\n#\n# \t\ttest_object = TestClass()\n# \t\tself.assertEqual(test_object.namesAndDescriptions()[0], (\"field1\", \"1\"))\n# \t\tself.assertEqual(test_object.namesAndDescriptions()[1], (\"field2\", \"2\"))\n# \t\tself", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef getName(self):\n\t\treturn self.__class__.__name__\n\n\tdef getDescription(self):\n\t\treturn self.__doc__\n\n\tdef getAttributes(self):\n\t\tret = []\n\t\tfor name, value in self.__class__.__dict__.items():\n\t\t\tname = name.replace('__', '')\n\t\t\tif (name.startswith('_')):\n\t\t\t\tcontinue\n\n\t\t\tif (all):\n\t\t\t\tret.append((name, value))\n\t\t\telse:\n\t\t\t\tif (hasattr(value, '__call__')):\n\t\t\t\t\tret.append((name, value))\n\n\t\treturn dict(ret)\n\n\tdef getMethods(self):\n\t\tret = []\n\t\tfor name, value in self.__class__.__dict__.items():\n\t\t\tname = name.replace('__', '')\n\t\t\tif (name.startswith('_')):\n\t\t\t\tcontinue\n\n\t\t\tif (hasattr(value, '__call__')):\n\t\t\t\tret.append((name, value))\n\n\t\treturn dict(ret)\n\n\treturn getAttributes(), getMethods()", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\ta = []\n\tfor name in self.__dict__.keys():\n\t\tif all or name.startswith('_'):\n\t\t\tcontinue\n\t\tvalue = self.__dict__[name]\n\t\tif value is None:\n\t\t\tcontinue\n\t\ta.append((name, value.__doc__))\n\treturn a\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tret = []\n\tself._class_attrs.sort()\n\tfor attr in self._class_attrs:\n\t\tret.append(attr)\n\t\tif not all:\n\t\t\tbreak\n\treturn ret\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef getNameAndDescription(self, class_object):\n\t\tif all:\n\t\t\treturn class_object.name, class_object.description\n\t\telse:\n\t\t\treturn class_object.name\n\n\tresults = list(map(getNameAndDescription, self.__annotations__.values()))\n\treturn results\n\n", "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tself.names = []\n\tself.descriptions = []\n\tself.current = None\n\tself.name = None\n\tself.description = None\n\tif self.name != None:\n\t\tself.names.append(self.name)\n\t\tself.descriptions.append(self.description)\n\t\n\tfor key, val in sorted(self.items()):\n\t\tif not all:\n\t\t\tif val.name == self.name:\n\t\t\t\tcontinue\n\t\tself.current = key\n\t\tself.name = val.name\n\t\tself.description = val.description\n\t\tself.names.append(self.name)\n\t\tself.descriptions.append(self.description)\n\n\tif len(self.names) == 0:\n\t\tself.names.append(\"<None>\")\n\t\tself.descriptions.append(\"<None>\")\n\t\n\treturn self.names, self.descriptions\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef getAttr(attr):\n\t\ttry:\n\t\t\tdata = getattr(self, attr)\n\t\t\tdesc = str(data).strip('\\n').replace('\\t', ' ')\n\t\t\treturn desc\n\t\texcept:\n\t\t\treturn ''\n\timport inspect\n\tallAttrs = inspect.getmembers(self)\n\tallAttrs.sort(key=lambda x: x[0])\n\tallAttrs = [allAttrs[i] for i in range(len(allAttrs)) if not getattr(self, allAttrs[i][0]).__class__.__name__ == 'function']\n\tif all:\n\t\treturn allAttrs\n\telse:\n\t\treturn [allAttrs[i] for i in range(len(allAttrs)) if not getattr(self, allAttrs[i][0]).__class__.__name__ == 'function']", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tfields = self.fields\n\tif all:\n\t\tfields = fields + self.meta.local_fields\n\tnames = []\n\tdescriptions = []\n\tfor field in fields:\n\t\tnames.append(field.name)\n\t\tdescriptions.append(field.description or '')\n\treturn names, descriptions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tnamesAndDescriptions = [\n\t\tself.name,\n\t\tself.description,\n\t]\n\t\n\tif not all:\n\t\tnamesAndDescriptions = [n for n in namesAndDescriptions if n]\n\t\n\treturn namesAndDescriptions\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tcurrent = self.__class__.current\n\tresult = [self.__class__.current.name, self.__class__.current.description]\n\tself.__class__.current = self.__class__.current.parent\n\twhile self.__class__.current is not None:\n\t\tresult.insert(0, (self.__class__.current.name, self.__class__.current.description))\n\t\tself.__class__.current = self.__class__.current.parent\n\tif not all:\n\t\tself.__class__.current = current\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef _namesAndDescriptions(cls, all=False):\n\t\t\"\"\"\n\t\tReturns the name and description of the current class attribute.\n\t\t\"\"\"\n\t\tnames = list(cls._fields)\n\t\tdescriptions = [getattr(cls, name) for name in names]\n\n\t\tif all:\n\t\t\treturn [(name, description) for name, description in zip(names, descriptions)]\n\t\telse:\n\t\t\treturn names\n\n\tdef _namesAndDescriptionsAll(all=False):\n\t\treturn _namesAndDescriptions(all=all)\n\n\treturn _namesAndDescriptions if all else _namesAndDescriptionsAll\n\n\n#\n# class NamesAndDescriptionsTest(unittest.TestCase):\n# \t@staticmethod\n# \tdef _test_namesAndDescriptions():\n# \t\tclass TestClass(NamesAndDescriptions):\n# \t\t\tfield1 = \"1\"\n# \t\t\tfield2 = \"2\"\n#\n# \t\ttest_object = TestClass()\n# \t\tself.assertEqual(test_object.namesAndDescriptions()[0], (\"field1\", \"1\"))\n# \t\tself.assertEqual(test_object.namesAndDescriptions()[1], (\"field2\", \"2\"))\n# \t\tself", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef getName(self):\n\t\treturn self.__class__.__name__\n\n\tdef getDescription(self):\n\t\treturn self.__doc__\n\n\tdef getAttributes(self):\n\t\tret = []\n\t\tfor name, value in self.__class__.__dict__.items():\n\t\t\tname = name.replace('__', '')\n\t\t\tif (name.startswith('_')):\n\t\t\t\tcontinue\n\n\t\t\tif (all):\n\t\t\t\tret.append((name, value))\n\t\t\telse:\n\t\t\t\tif (hasattr(value, '__call__')):\n\t\t\t\t\tret.append((name, value))\n\n\t\treturn dict(ret)\n\n\tdef getMethods(self):\n\t\tret = []\n\t\tfor name, value in self.__class__.__dict__.items():\n\t\t\tname = name.replace('__', '')\n\t\t\tif (name.startswith('_')):\n\t\t\t\tcontinue\n\n\t\t\tif (hasattr(value, '__call__')):\n\t\t\t\tret.append((name, value))\n\n\t\treturn dict(ret)\n\n\treturn getAttributes(), getMethods()", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\ta = []\n\tfor name in self.__dict__.keys():\n\t\tif all or name.startswith('_'):\n\t\t\tcontinue\n\t\tvalue = self.__dict__[name]\n\t\tif value is None:\n\t\t\tcontinue\n\t\ta.append((name, value.__doc__))\n\treturn a\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tret = []\n\tself._class_attrs.sort()\n\tfor attr in self._class_attrs:\n\t\tret.append(attr)\n\t\tif not all:\n\t\t\tbreak\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tdef getNameAndDescription(self, class_object):\n\t\tif all:\n\t\t\treturn class_object.name, class_object.description\n\t\telse:\n\t\t\treturn class_object.name\n\n\tresults = list(map(getNameAndDescription, self.__annotations__.values()))\n\treturn results\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\t\"\"\"\n\tReturns the name and description of the current class attribute.\n\t\"\"\"\n\tself.names = []\n\tself.descriptions = []\n\tself.current = None\n\tself.name = None\n\tself.description = None\n\tif self.name != None:\n\t\tself.names.append(self.name)\n\t\tself.descriptions.append(self.description)\n\t\n\tfor key, val in sorted(self.items()):\n\t\tif not all:\n\t\t\tif val.name == self.name:\n\t\t\t\tcontinue\n\t\tself.current = key\n\t\tself.name = val.name\n\t\tself.description = val.description\n\t\tself.names.append(self.name)\n\t\tself.descriptions.append(self.description)\n\n\tif len(self.names) == 0:\n\t\tself.names.append(\"<None>\")\n\t\tself.descriptions.append(\"<None>\")\n\t\n\treturn self.names, self.descriptions\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\ttry:\n\t\tnames = self.__dict__.keys()\n\texcept AttributeError:\n\t\tnames = self.__class__.__dict__.keys()\n\ttry:\n\t\tnames = names + self.__class__.__bases__[0].__dict__.keys()\n\texcept:\n\t\tpass\n\t\n\tif all:\n\t\treturn names\n\telse:\n\t\treturn [n for n in names if not n.startswith('__')]\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.__dict__.keys()\n\tif all:\n\t\tattrs.extend(dir(self))\n\treturn attrs\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\tattribute = []\n\t\n\tfor attr in self.__dict__:\n\t\tif not all:\n\t\t\tif not attr.startswith(\"__\") and not attr.endswith(\"__\"):\n\t\t\t\tattribute.append(attr)\n\t\telse:\n\t\t\tattribute.append(attr)\n\t\t\t\n\treturn attribute", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\treturn [i for i in self.__dict__.keys() if not i.startswith(\"__\")]", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = []\n\tfor a in self.attrs:\n\t\tif not a.is_private and not a.is_static:\n\t\t\tnames.append(a.name)\n\t\n\tif all:\n\t\treturn names\n\telse:\n\t\treturn names[1:]\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = []\n\tfor attr in self.__dict__:\n\t\tif attr.startswith('__') and attr.endswith('__'):\n\t\t\tcontinue\n\t\tattrs.append(attr)\n\tattrs.sort()\n\tif all:\n\t\treturn attrs\n\telse:\n\t\treturn map(lambda attr: attr[1:-1], attrs)\n\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.get_class_attrs()\n\tif all:\n\t\treturn attrs.keys()\n\telse:\n\t\treturn attrs.values()\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\tif all:\n\t\treturn self.get_attribute_names()\n\telse:\n\t\treturn self.get_attribute_names(False)\n", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattributes = []\n\tfor name in dir(self):\n\t\tif name.startswith(\"_\") or name.startswith(\"__\") or name.isupper():\n\t\t\tcontinue\n\t\tif name == \"names\":\n\t\t\tcontinue\n\t\tif name == \"__class__\":\n\t\t\tcontinue\n\t\tif self.__getattribute__(name):\n\t\t\tif not all:\n\t\t\t\tattributes.append(name)\n\t\t\telse:\n\t\t\t\tattributes.extend(self.names(all=True))\n\n\treturn attributes", "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = []\n\tfor attr_name in self.__dict__:\n\t\tif attr_name[0] != '_':\n\t\t\tattrs.append(attr_name)\n\tif all:\n\t\tattrs.sort()\n\treturn attrs\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\ttry:\n\t\tnames = self.__dict__.keys()\n\texcept AttributeError:\n\t\tnames = self.__class__.__dict__.keys()\n\ttry:\n\t\tnames = names + self.__class__.__bases__[0].__dict__.keys()\n\texcept:\n\t\tpass\n\t\n\tif all:\n\t\treturn names\n\telse:\n\t\treturn [n for n in names if not n.startswith('__')]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.__dict__.keys()\n\tif all:\n\t\tattrs.extend(dir(self))\n\treturn attrs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\tattribute = []\n\t\n\tfor attr in self.__dict__:\n\t\tif not all:\n\t\t\tif not attr.startswith(\"__\") and not attr.endswith(\"__\"):\n\t\t\t\tattribute.append(attr)\n\t\telse:\n\t\t\tattribute.append(attr)\n\t\t\t\n\treturn attribute", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\treturn [i for i in self.__dict__.keys() if not i.startswith(\"__\")]", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tnames = []\n\tfor a in self.attrs:\n\t\tif not a.is_private and not a.is_static:\n\t\t\tnames.append(a.name)\n\t\n\tif all:\n\t\treturn names\n\telse:\n\t\treturn names[1:]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = []\n\tfor attr in self.__dict__:\n\t\tif attr.startswith('__') and attr.endswith('__'):\n\t\t\tcontinue\n\t\tattrs.append(attr)\n\tattrs.sort()\n\tif all:\n\t\treturn attrs\n\telse:\n\t\treturn map(lambda attr: attr[1:-1], attrs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = self.get_class_attrs()\n\tif all:\n\t\treturn attrs.keys()\n\telse:\n\t\treturn attrs.values()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\t\n\tif all:\n\t\treturn self.get_attribute_names()\n\telse:\n\t\treturn self.get_attribute_names(False)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattributes = []\n\tfor name in dir(self):\n\t\tif name.startswith(\"_\") or name.startswith(\"__\") or name.isupper():\n\t\t\tcontinue\n\t\tif name == \"names\":\n\t\t\tcontinue\n\t\tif name == \"__class__\":\n\t\t\tcontinue\n\t\tif self.__getattribute__(name):\n\t\t\tif not all:\n\t\t\t\tattributes.append(name)\n\t\t\telse:\n\t\t\t\tattributes.extend(self.names(all=True))\n\n\treturn attributes", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\t\"\"\"\n\tReturn the attribute names of current class.\n\t\"\"\"\n\tattrs = []\n\tfor attr_name in self.__dict__:\n\t\tif attr_name[0] != '_':\n\t\t\tattrs.append(attr_name)\n\tif all:\n\t\tattrs.sort()\n\treturn attrs\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport re\n\tfrom . import Declarations\n\n\t# Expand arguments\n\tif isinstance(sequence, tuple):\n\t\tsequence = list(sequence)\n\t\toutput = sequence[:]\n\t\tfor i, decl in enumerate(sequence):\n\t\t\tif isinstance(decl, Declarations.Declaration):\n\t\t\t\toutput[i] = _normalizeargs(decl.arguments, decl.arguments)\n\t\t\telif isinstance(decl, tuple):\n\t\t\t\toutput[i] = list(decl)\n\n\t# Expand interfaces\n\telif isinstance(sequence, Declarations.Declaration):\n\t\toutput = _normalizeargs(sequence.arguments, sequence.arguments)\n\telif isinstance(sequence, dict):\n\t\toutput = sequence.copy()\n\t\tfor key, value in sequence.items():\n\t\t\tif isinstance(value, Declarations.Declaration):\n\t\t\t\toutput[key] = _normalizeargs(value.arguments, value.arguments)\n\t\t\telif isinstance(value, tuple):\n\t\t\t\toutput[key] = list(value)\n\n\treturn output\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnewargs = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, (list, tuple)):\n\t\t\tnewargs.extend(arg)\n\t\telif isinstance(arg, str):\n\t\t\tnewargs.append(arg)\n\t\telif isinstance(arg, Declaration):\n\t\t\tnewargs.append(arg.normalize(output=output))\n\t\telse:\n\t\t\tnewargs.append(arg)\n\n\treturn newargs\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\targs = []\n\n\tif isinstance(sequence, (tuple, list)):\n\t\tfor item in sequence:\n\t\t\tif isinstance(item, str):\n\t\t\t\targs.append(item)\n\t\t\telif isinstance(item, (list, tuple)):\n\t\t\t\tif item[0] in ('implements', 'extends'):\n\t\t\t\t\targs.extend(item[1:])\n\t\t\t\telse:\n\t\t\t\t\targs.append(item)\n\t\t\telif isinstance(item, Declaration):\n\t\t\t\targs.extend(item.args)\n\t\t\telif isinstance(item, Interface):\n\t\t\t\targs.extend(item.declarations)\n\t\t\telse:\n\t\t\t\traise TypeError('Unexpected type in args: %s' % item)\n\n\telif sequence in ('implements', 'extends'):\n\t\targs.extend(sequence[1:])\n\telse:\n\t\targs.append(sequence)\n\n\tif output is None:\n\t\treturn args\n\telse:\n\t\treturn [output] + args", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfrom . import declarations, interfaces, implements, tuples\n\n\t# TODO: make this a better parser, it's a bit dumb\n\tfor (interface, spec) in sequence:\n\t\t# if this is an interface, expand\n\t\tif isinstance(interface, interfaces.Interface):\n\t\t\toutput.append(implements.Implements(interface, spec))\n\t\t# if this is a tuple, expand\n\t\telif isinstance(interface, tuples.Tuple):\n\t\t\toutput.append(tuples.Tuple(interface.args, spec))\n\t\t# if this is a declaration, expand\n\t\telif isinstance(interface, declarations.Declaration):\n\t\t\toutput.append(declarations.Declaration(interface.name, spec))\n\t\t# if this is a tuple, expand\n\t\telif isinstance(interface, tuples.Tuple):\n\t\t\toutput.append(tuples.Tuple(interface.args, spec))\n\t\t# if this is not a declaration or a tuple, just add it\n\t\telse:\n\t\t\toutput.append(interface)\n\treturn output\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = output or []\n\tfor s in sequence:\n\t\tif isinstance(s, str):\n\t\t\toutput.append(s)\n\t\telif isinstance(s, DeclarationSpec):\n\t\t\toutput.append(s.normalize())\n\t\telif isinstance(s, TupleType):\n\t\t\toutput.append(s.normalize())\n\t\telif isinstance(s, InterfaceSpec):\n\t\t\toutput.append(s.normalize())\n\t\telif isinstance(s, InterfaceSpec):\n\t\t\toutput.append(s.normalize())\n\t\telse:\n\t\t\traise TypeError(\"Unsupported argument type: %s\" % repr(s))\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tdef _normalize(arg):\n\t\tif isinstance(arg, Declaration):\n\t\t\treturn arg\n\t\tif isinstance(arg, tuple):\n\t\t\targ = [a for a in arg if not isinstance(a, Declaration)]\n\t\t\tif len(arg) == 0:\n\t\t\t\treturn arg\n\t\t\telif len(arg) == 1:\n\t\t\t\treturn arg[0]\n\t\t\telse:\n\t\t\t\treturn tuple(arg)\n\t\tif isinstance(arg, Interface):\n\t\t\treturn arg\n\t\tif isinstance(arg, tuple):\n\t\t\targ = [a for a in arg if isinstance(a, Interface)]\n\t\t\tif len(arg) == 0:\n\t\t\t\treturn arg\n\t\t\telif len(arg) == 1:\n\t\t\t\treturn arg[0]\n\t\t\telse:\n\t\t\t\treturn tuple(arg)\n\t\tif isinstance(arg, str):\n\t\t\treturn arg\n\t\traise TypeError(f\"Invalid argument type: {type(arg)}\")\n\tsequence = [arg for arg in sequence if arg is not None]\n\tif output is None:\n\t\toutput = []\n\toutput.extend([_normalize(arg) for arg in sequence])\n\treturn output\n\n#class_builder.py\n#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nfrom pathlib import Path\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfrom pprint import pprint\n\tif isinstance(sequence, basestring):\n\t\tsequence = (sequence,)\n\tif isinstance(sequence, tuple):\n\t\tsequence = list(sequence)\n\n\tnew_sequence = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, (tuple, list)):\n\t\t\tnew_sequence.append(_normalizeargs(arg))\n\t\telif isinstance(arg, basestring):\n\t\t\tnew_sequence.append(arg)\n\t\telif isinstance(arg, Declaration):\n\t\t\tnew_sequence.append(arg.normalized())\n\t\telse:\n\t\t\tnew_sequence.append(arg)\n\tif new_sequence and not output:\n\t\toutput = new_sequence[0]\n\n\treturn output if output else tuple(new_sequence)\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tresult = []\n\tfor item in sequence:\n\t\tif isinstance(item, list):\n\t\t\tresult.extend(_normalizeargs(item))\n\t\telif isinstance(item, Declaration):\n\t\t\tresult.append(item)\n\t\telif isinstance(item, tuple):\n\t\t\tif len(item) == 2:\n\t\t\t\tresult.append(Interface(*item))\n\t\t\telse:\n\t\t\t\tresult.append(item)\n\t\telse:\n\t\t\tresult.append(item)\n\treturn result\n\n", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport ast\n\n\tif not isinstance(sequence, ast.AST):\n\t\traise TypeError('Expected ast.AST, got %s' % type(sequence))\n\n\tif isinstance(sequence, ast.arguments):\n\t\tif not sequence.args:\n\t\t\treturn ast.Call(\n\t\t\t\tfunc=ast.Name(id='tuple'),\n\t\t\t\targs=sequence.defaults,\n\t\t\t\tstarargs=sequence.starargs,\n\t\t\t\tkwarg=sequence.kwarg,\n\t\t\t\tlineno=sequence.lineno,\n\t\t\t\tcol_offset=sequence.col_offset)\n\t\telse:\n\t\t\tdefs = []\n\t\t\tfor arg in sequence.args:\n\t\t\t\targ = _normalizeargs(arg)\n\t\t\t\tif isinstance(arg, ast.arguments):\n\t\t\t\t\targ.lineno = sequence.lineno\n\t\t\t\t\targ.col_offset = sequence.col_offset\n\t\t\t\t\tdefs.append(arg)\n\t\t\t\telse:\n\t\t\t\t\tdefs.append(arg)\n\t\t\treturn ast.arguments(\n\t\t\t\targs=defs,\n\t\t\t\tstarargs=sequence.starargs,\n\t\t\t\tkwarg=sequence.kwarg,\n\t\t\t\tlineno=sequence.lineno,\n\t\t\t\tcol_offset=sequence.col_offset)\n\n\tif isinstance(sequence, ast.Name", "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport ast\n\n\tif not output:\n\t\toutput = ast.arguments(args=[], varargs=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=None)\n\n\tdef _expand_decl(decl):\n\t\t\"\"\"\n\t\tNormalize a Declaration\n\t\t\"\"\"\n\t\tif isinstance(decl, ast.Name):\n\t\t\toutput.args.append(ast.arg(id=decl.id, annotation=None))\n\t\telif isinstance(decl, tuple):\n\t\t\toutput.args.extend(ast.arg(id=decl[0], annotation=decl[1]) for decl in decl)\n\t\t\toutput.varargs = decl[2]\n\t\telif isinstance(decl, ast.Dict):\n\t\t\toutput.kwarg = ast.arg(id=decl.key, annotation=decl.value, annotation=None)\n\t\t\tfor k in decl.value.keys():\n\t\t\t\toutput.defaults.append(ast.keyword(arg=k, value=decl.value[k]))\n\t\telif isinstance(decl, ast.Interface):\n\t\t\tfor name, args in decl.body.items():\n\t\t\t\toutput.args.append(ast.arg(id=name, annotation=None))\n\t\t\t\toutput.args."], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport re\n\tfrom . import Declarations\n\n\t# Expand arguments\n\tif isinstance(sequence, tuple):\n\t\tsequence = list(sequence)\n\t\toutput = sequence[:]\n\t\tfor i, decl in enumerate(sequence):\n\t\t\tif isinstance(decl, Declarations.Declaration):\n\t\t\t\toutput[i] = _normalizeargs(decl.arguments, decl.arguments)\n\t\t\telif isinstance(decl, tuple):\n\t\t\t\toutput[i] = list(decl)\n\n\t# Expand interfaces\n\telif isinstance(sequence, Declarations.Declaration):\n\t\toutput = _normalizeargs(sequence.arguments, sequence.arguments)\n\telif isinstance(sequence, dict):\n\t\toutput = sequence.copy()\n\t\tfor key, value in sequence.items():\n\t\t\tif isinstance(value, Declarations.Declaration):\n\t\t\t\toutput[key] = _normalizeargs(value.arguments, value.arguments)\n\t\t\telif isinstance(value, tuple):\n\t\t\t\toutput[key] = list(value)\n\n\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tnewargs = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, (list, tuple)):\n\t\t\tnewargs.extend(arg)\n\t\telif isinstance(arg, str):\n\t\t\tnewargs.append(arg)\n\t\telif isinstance(arg, Declaration):\n\t\t\tnewargs.append(arg.normalize(output=output))\n\t\telse:\n\t\t\tnewargs.append(arg)\n\n\treturn newargs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\targs = []\n\n\tif isinstance(sequence, (tuple, list)):\n\t\tfor item in sequence:\n\t\t\tif isinstance(item, str):\n\t\t\t\targs.append(item)\n\t\t\telif isinstance(item, (list, tuple)):\n\t\t\t\tif item[0] in ('implements', 'extends'):\n\t\t\t\t\targs.extend(item[1:])\n\t\t\t\telse:\n\t\t\t\t\targs.append(item)\n\t\t\telif isinstance(item, Declaration):\n\t\t\t\targs.extend(item.args)\n\t\t\telif isinstance(item, Interface):\n\t\t\t\targs.extend(item.declarations)\n\t\t\telse:\n\t\t\t\traise TypeError('Unexpected type in args: %s' % item)\n\n\telif sequence in ('implements', 'extends'):\n\t\targs.extend(sequence[1:])\n\telse:\n\t\targs.append(sequence)\n\n\tif output is None:\n\t\treturn args\n\telse:\n\t\treturn [output] + args", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfrom . import declarations, interfaces, implements, tuples\n\n\t# TODO: make this a better parser, it's a bit dumb\n\tfor (interface, spec) in sequence:\n\t\t# if this is an interface, expand\n\t\tif isinstance(interface, interfaces.Interface):\n\t\t\toutput.append(implements.Implements(interface, spec))\n\t\t# if this is a tuple, expand\n\t\telif isinstance(interface, tuples.Tuple):\n\t\t\toutput.append(tuples.Tuple(interface.args, spec))\n\t\t# if this is a declaration, expand\n\t\telif isinstance(interface, declarations.Declaration):\n\t\t\toutput.append(declarations.Declaration(interface.name, spec))\n\t\t# if this is a tuple, expand\n\t\telif isinstance(interface, tuples.Tuple):\n\t\t\toutput.append(tuples.Tuple(interface.args, spec))\n\t\t# if this is not a declaration or a tuple, just add it\n\t\telse:\n\t\t\toutput.append(interface)\n\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\toutput = output or []\n\tfor s in sequence:\n\t\tif isinstance(s, str):\n\t\t\toutput.append(s)\n\t\telif isinstance(s, DeclarationSpec):\n\t\t\toutput.append(s.normalize())\n\t\telif isinstance(s, TupleType):\n\t\t\toutput.append(s.normalize())\n\t\telif isinstance(s, InterfaceSpec):\n\t\t\toutput.append(s.normalize())\n\t\telif isinstance(s, InterfaceSpec):\n\t\t\toutput.append(s.normalize())\n\t\telse:\n\t\t\traise TypeError(\"Unsupported argument type: %s\" % repr(s))\n\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tdef _normalize(arg):\n\t\tif isinstance(arg, Declaration):\n\t\t\treturn arg\n\t\tif isinstance(arg, tuple):\n\t\t\targ = [a for a in arg if not isinstance(a, Declaration)]\n\t\t\tif len(arg) == 0:\n\t\t\t\treturn arg\n\t\t\telif len(arg) == 1:\n\t\t\t\treturn arg[0]\n\t\t\telse:\n\t\t\t\treturn tuple(arg)\n\t\tif isinstance(arg, Interface):\n\t\t\treturn arg\n\t\tif isinstance(arg, tuple):\n\t\t\targ = [a for a in arg if isinstance(a, Interface)]\n\t\t\tif len(arg) == 0:\n\t\t\t\treturn arg\n\t\t\telif len(arg) == 1:\n\t\t\t\treturn arg[0]\n\t\t\telse:\n\t\t\t\treturn tuple(arg)\n\t\tif isinstance(arg, str):\n\t\t\treturn arg\n\t\traise TypeError(f\"Invalid argument type: {type(arg)}\")\n\tsequence = [arg for arg in sequence if arg is not None]\n\tif output is None:\n\t\toutput = []\n\toutput.extend([_normalize(arg) for arg in sequence])\n\treturn output\n\n#class_builder.py\n#!/usr/bin/env python3\n\nimport os\nimport sys\nimport argparse\nfrom pathlib import Path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tfrom pprint import pprint\n\tif isinstance(sequence, basestring):\n\t\tsequence = (sequence,)\n\tif isinstance(sequence, tuple):\n\t\tsequence = list(sequence)\n\n\tnew_sequence = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, (tuple, list)):\n\t\t\tnew_sequence.append(_normalizeargs(arg))\n\t\telif isinstance(arg, basestring):\n\t\t\tnew_sequence.append(arg)\n\t\telif isinstance(arg, Declaration):\n\t\t\tnew_sequence.append(arg.normalized())\n\t\telse:\n\t\t\tnew_sequence.append(arg)\n\tif new_sequence and not output:\n\t\toutput = new_sequence[0]\n\n\treturn output if output else tuple(new_sequence)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\tresult = []\n\tfor item in sequence:\n\t\tif isinstance(item, list):\n\t\t\tresult.extend(_normalizeargs(item))\n\t\telif isinstance(item, Declaration):\n\t\t\tresult.append(item)\n\t\telif isinstance(item, tuple):\n\t\t\tif len(item) == 2:\n\t\t\t\tresult.append(Interface(*item))\n\t\t\telse:\n\t\t\t\tresult.append(item)\n\t\telse:\n\t\t\tresult.append(item)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport ast\n\n\tif not isinstance(sequence, ast.AST):\n\t\traise TypeError('Expected ast.AST, got %s' % type(sequence))\n\n\tif isinstance(sequence, ast.arguments):\n\t\tif not sequence.args:\n\t\t\treturn ast.Call(\n\t\t\t\tfunc=ast.Name(id='tuple'),\n\t\t\t\targs=sequence.defaults,\n\t\t\t\tstarargs=sequence.starargs,\n\t\t\t\tkwarg=sequence.kwarg,\n\t\t\t\tlineno=sequence.lineno,\n\t\t\t\tcol_offset=sequence.col_offset)\n\t\telse:\n\t\t\tdefs = []\n\t\t\tfor arg in sequence.args:\n\t\t\t\targ = _normalizeargs(arg)\n\t\t\t\tif isinstance(arg, ast.arguments):\n\t\t\t\t\targ.lineno = sequence.lineno\n\t\t\t\t\targ.col_offset = sequence.col_offset\n\t\t\t\t\tdefs.append(arg)\n\t\t\t\telse:\n\t\t\t\t\tdefs.append(arg)\n\t\t\treturn ast.arguments(\n\t\t\t\targs=defs,\n\t\t\t\tstarargs=sequence.starargs,\n\t\t\t\tkwarg=sequence.kwarg,\n\t\t\t\tlineno=sequence.lineno,\n\t\t\t\tcol_offset=sequence.col_offset)\n\n\tif isinstance(sequence, ast.Name", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\t\"\"\"\n\tNormalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.\n\t\"\"\"\n\timport ast\n\n\tif not output:\n\t\toutput = ast.arguments(args=[], varargs=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=None)\n\n\tdef _expand_decl(decl):\n\t\t\"\"\"\n\t\tNormalize a Declaration\n\t\t\"\"\"\n\t\tif isinstance(decl, ast.Name):\n\t\t\toutput.args.append(ast.arg(id=decl.id, annotation=None))\n\t\telif isinstance(decl, tuple):\n\t\t\toutput.args.extend(ast.arg(id=decl[0], annotation=decl[1]) for decl in decl)\n\t\t\toutput.varargs = decl[2]\n\t\telif isinstance(decl, ast.Dict):\n\t\t\toutput.kwarg = ast.arg(id=decl.key, annotation=decl.value, annotation=None)\n\t\t\tfor k in decl.value.keys():\n\t\t\t\toutput.defaults.append(ast.keyword(arg=k, value=decl.value[k]))\n\t\telif isinstance(decl, ast.Interface):\n\t\t\tfor name, args in decl.body.items():\n\t\t\t\toutput.args.append(ast.arg(id=name, annotation=None))\n\t\t\t\toutput.args.", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\timport sys\n\ttry:\n\t\timport copt\n\t\tsys.modules['copt'] = copt\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\t# this import will fail in a non-C environment\n\t\tfrom ._c_optimizations import *\n\t\treturn True\n\texcept:\n\t\treturn False", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport copt\n\t\timport copt.optimize\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\tavailable = True\n\texcept ImportError:\n\t\tavailable = False\n\treturn available\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom c_optimizations import *\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport copt\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\timport sys\n\ttry:\n\t\timport copt\n\t\tsys.modules['copt'] = copt\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\t# this import will fail in a non-C environment\n\t\tfrom ._c_optimizations import *\n\t\treturn True\n\texcept:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport copt\n\t\timport copt.optimize\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\tavailable = True\n\texcept ImportError:\n\t\tavailable = False\n\treturn available\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\tfrom c_optimizations import *\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport c_optimizations\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\"\"\"\n\tIf available, return the C optimization module, otherwise a false value.\n\t\"\"\"\n\ttry:\n\t\timport copt\n\t\treturn True\n\texcept ImportError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport sys\n\treturn sys.platform.startswith('cygwin')\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n\timport sys\n\t\n\tif sys.platform == 'darwin':\n\t\treturn False\n\t\n\tif sys.platform == 'linux':\n\t\treturn True\n\t\n\tif sys.platform == 'win32':\n\t\treturn True\n\t\n\treturn False\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n\timport platform\n\t\n\t# A list of the strings we're looking for in the current python interpreter\n\t# to determine if we should attempt to use the C optimizations.\n\tlooking_for = [\n\t\t\"Darwin\",\n\t\t\"linux\",\n\t\t\"cygwin\",\n\t\t\"mingw\"\n\t]\n\t\n\t# Look for the matching string in the current platform\n\t# and return the value.\n\treturn any(platform.system().startswith(looking_for_str) for looking_for_str in looking_for)\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport os\n\tfrom os.path import isfile\n\n\tdef _c_optimizations():\n\t\t# Trying to detect if we have the C code for the optimizations\n\t\ttry:\n\t\t\ttry:\n\t\t\t\treturn isfile(os.path.join(os.path.dirname(__file__), 'c_optimizations.c'))\n\t\t\texcept OSError:\n\t\t\t\t# We have a POSIX system, but we don't have the .c file.\n\t\t\t\treturn False\n\t\texcept OSError:\n\t\t\t# We have a Windows system, and we don't have the .c file.\n\t\t\treturn False\n\n\t\t# This code is only run once per process.\n\t\tif not hasattr(_should_attempt_c_optimizations, '_c_optimizations_called'):\n\t\t\t_should_attempt_c_optimizations._c_optimizations_called = True\n\t\t\treturn _c_optimizations()\n\n\treturn _c_optimizations()\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\t\n\treturn ctypes.sizeof(ctypes.c_void_p) == 8\n\n# ------------------\n# Python 2.X support\n# ------------------\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\n\ttry:\n\t\tctypes.CDLL(\"libc.so.6\")\n\texcept OSError:\n\t\treturn False\n\treturn True\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport os\n\tif os.environ.get(\"PYTHON_C_OPTIMIZATIONS\") == \"1\":\n\t\treturn True\n\treturn False\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom ctypes import POINTER, c_void_p, c_uint, c_char, c_int\n\texcept ImportError:\n\t\treturn False\n\treturn True\n\n", "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport sys\n\tif sys.platform == \"linux\":\n\t\timport ctypes\n\t\ttry:\n\t\t\tlib = ctypes.CDLL(\"libclang.so\")\n\t\t\tlib.clang_getDiagnosticClientVersion()\n\t\t\treturn True\n\t\texcept Exception:\n\t\t\treturn False\n\telse:\n\t\treturn False\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport sys\n\treturn sys.platform.startswith('cygwin')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n\timport sys\n\t\n\tif sys.platform == 'darwin':\n\t\treturn False\n\t\n\tif sys.platform == 'linux':\n\t\treturn True\n\t\n\tif sys.platform == 'win32':\n\t\treturn True\n\t\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\t\n\timport platform\n\t\n\t# A list of the strings we're looking for in the current python interpreter\n\t# to determine if we should attempt to use the C optimizations.\n\tlooking_for = [\n\t\t\"Darwin\",\n\t\t\"linux\",\n\t\t\"cygwin\",\n\t\t\"mingw\"\n\t]\n\t\n\t# Look for the matching string in the current platform\n\t# and return the value.\n\treturn any(platform.system().startswith(looking_for_str) for looking_for_str in looking_for)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport os\n\tfrom os.path import isfile\n\n\tdef _c_optimizations():\n\t\t# Trying to detect if we have the C code for the optimizations\n\t\ttry:\n\t\t\ttry:\n\t\t\t\treturn isfile(os.path.join(os.path.dirname(__file__), 'c_optimizations.c'))\n\t\t\texcept OSError:\n\t\t\t\t# We have a POSIX system, but we don't have the .c file.\n\t\t\t\treturn False\n\t\texcept OSError:\n\t\t\t# We have a Windows system, and we don't have the .c file.\n\t\t\treturn False\n\n\t\t# This code is only run once per process.\n\t\tif not hasattr(_should_attempt_c_optimizations, '_c_optimizations_called'):\n\t\t\t_should_attempt_c_optimizations._c_optimizations_called = True\n\t\t\treturn _c_optimizations()\n\n\treturn _c_optimizations()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\t\n\treturn ctypes.sizeof(ctypes.c_void_p) == 8\n\n# ------------------\n# Python 2.X support\n# ------------------\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\n\ttry:\n\t\tctypes.CDLL(\"libc.so.6\")\n\texcept OSError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport os\n\tif os.environ.get(\"PYTHON_C_OPTIMIZATIONS\") == \"1\":\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\ttry:\n\t\tfrom ctypes import POINTER, c_void_p, c_uint, c_char, c_int\n\texcept ImportError:\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\t\"\"\"\n\tReturn a true value if we use the C optimizations.\n\t\"\"\"\n\timport sys\n\tif sys.platform == \"linux\":\n\t\timport ctypes\n\t\ttry:\n\t\t\tlib = ctypes.CDLL(\"libclang.so\")\n\t\t\tlib.clang_getDiagnosticClientVersion()\n\t\t\treturn True\n\t\texcept Exception:\n\t\t\treturn False\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\ttry:\n\t\treturn os.environ[\"PURE_PYTHON\"] != \"0\"\n\texcept KeyError:\n\t\treturn False\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport sys\n\treturn sys.getenv(\"PURE_PYTHON\") != None and int(sys.getenv(\"PURE_PYTHON\")) != 0\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get(\"PY_C_OPTIMIZATIONS_IGNORED\", \"0\") != \"1\"\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.getenv(\"PURE_PYTHON\", None) != \"0\"\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom __future__ import print_function\n\timport os\n\treturn os.environ.get(\"PURE_PYTHON\", \"0\") != \"0\"\n\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\treturn environ.get(\"PURE_PYTHON\", None) != \"0\"\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\tif environ.get(\"PURE_PYTHON\") == None or int(environ.get(\"PURE_PYTHON\")) == 0:\n\t\treturn True\n\treturn False\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\tfrom . import env\n\tPURE_PYTHON = os.environ.get(\"PURE_PYTHON\", None)\n\treturn PURE_PYTHON is not None and PURE_PYTHON != \"0\"\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\treturn environ.get(\"PURE_PYTHON\", None) is not None and int(environ.get(\"PURE_PYTHON\", 0)) != 0\n", "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\ttry:\n\t\treturn bool(getattr(sys, \"PURE_PYTHON\"))\n\texcept AttributeError:\n\t\treturn False\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\ttry:\n\t\treturn os.environ[\"PURE_PYTHON\"] != \"0\"\n\texcept KeyError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport sys\n\treturn sys.getenv(\"PURE_PYTHON\") != None and int(sys.getenv(\"PURE_PYTHON\")) != 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.environ.get(\"PY_C_OPTIMIZATIONS_IGNORED\", \"0\") != \"1\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\treturn os.getenv(\"PURE_PYTHON\", None) != \"0\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom __future__ import print_function\n\timport os\n\treturn os.environ.get(\"PURE_PYTHON\", \"0\") != \"0\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\treturn environ.get(\"PURE_PYTHON\", None) != \"0\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\tif environ.get(\"PURE_PYTHON\") == None or int(environ.get(\"PURE_PYTHON\")) == 0:\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\timport os\n\tfrom . import env\n\tPURE_PYTHON = os.environ.get(\"PURE_PYTHON\", None)\n\treturn PURE_PYTHON is not None and PURE_PYTHON != \"0\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\tfrom os import environ\n\treturn environ.get(\"PURE_PYTHON\", None) is not None and int(environ.get(\"PURE_PYTHON\", 0)) != 0\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\t\"\"\"\n\tReturn True if get \"PURE_PYTHON\" environ is not none and not 0, else return False\n\t\"\"\"\n\ttry:\n\t\treturn bool(getattr(sys, \"PURE_PYTHON\"))\n\texcept AttributeError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport distutils.ccompiler\n\tcc = distutils.ccompiler.new_compiler()\n\tcc.get_opt_info()\n\treturn cc.compiler_so_name == 'cl'\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport cProfile\n\t\treturn True\n\texcept ImportError:\n\t\treturn False", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\tfrom cffi import FFI\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn FFI.can_use_c_optimizations()\n\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\timport os\n\t\n\t# Check that the C++ compiler is available\n\tif not os.system(\"g++ -v >/dev/null 2>&1\"):\n\t\treturn True\n\n\tfrom dyninst.runtime.build_info import BuildInfo\n\tbuild_info = BuildInfo()\n\tif build_info.os_name == \"Darwin\":\n\t\t# Check that the C++11 flag is available (and not the C++0x flag)\n\t\tif os.system(\"g++ -std=c++11 -v >/dev/null 2>&1\"):\n\t\t\treturn True\n\n\t# Check that the C++11 flag is available (and not the C++0x flag)\n\tif os.system(\"g++ -std=c++11 -v >/dev/null 2>&1\"):\n\t\treturn True\n\t\n\t# Check that the C++1y flag is available (and not the C++0x flag)\n\tif os.system(\"g++ -std=c++1y -v >/dev/null 2>&1\"):\n\t\treturn True\n\n\t# Check that the C++14 flag is available (and not the C++0x flag)\n\tif os.system(\"g++ -std=c++14 -v >/dev/null", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\ttry:\n\t\tos.environ[\"CC\"]\n\texcept KeyError:\n\t\treturn False\n\n\treturn \"GCC\" in os.environ[\"CC\"]\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys, platform\n\t\n\treturn platform.system() != 'Windows'\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\timport platform\n\tfrom ._c_version import version as c_version\n\tfrom ._c_version import full_version as c_full_version\n\tfrom ._c_version import git_revision as c_git_revision\n\tfrom ._c_version import git_describe as c_git_describe\n\tfrom ._c_version import git_hash as c_git_hash\n\tfrom ._c_version import c_date as c_date\n\tfrom ._c_version import c_time as c_time\n\tfrom ._c_version import compiler as c_compiler\n\tfrom ._c_version import compiler_date as c_compiler_date\n\tfrom ._c_version import compiler_time as c_compiler_time\n\tfrom ._c_version import compiler_hash as c_compiler_hash\n\tfrom . import _py_version\n\tpy_version = _py_version._version\n\t\n\tcompiler_date = c_compiler_date.decode(\"utf-8\")\n\tcompiler_time = c_compiler_time.decode(\"utf-8\")\n\tcompiler_hash = c_compiler_hash.decode(\"utf-8\")\n\t\n\tdate_hash = \"%s%s%s%s%s%s%s\" % (\n\t", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport platform\n\treturn (platform.system() == \"Windows\" and\n\t\tplatform.processor() == \"AMD64\")\n", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\n\treturn ctypes.sizeof(ctypes.c_double) == 8\n\n# Set the value of this variable to True to enable the\n# C optimizations (speedup, etc.).\nc_optimizations_required = _c_optimizations_required()", "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\tfrom distutils.sysconfig import get_config_vars\n\tconfig_vars = get_config_vars()\n\treturn config_vars.get('OPTIMFLAGS') is not None\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport distutils.ccompiler\n\tcc = distutils.ccompiler.new_compiler()\n\tcc.get_opt_info()\n\treturn cc.compiler_so_name == 'cl'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport cProfile\n\t\treturn True\n\texcept ImportError:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\tfrom cffi import FFI\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn FFI.can_use_c_optimizations()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys\n\timport os\n\t\n\t# Check that the C++ compiler is available\n\tif not os.system(\"g++ -v >/dev/null 2>&1\"):\n\t\treturn True\n\n\tfrom dyninst.runtime.build_info import BuildInfo\n\tbuild_info = BuildInfo()\n\tif build_info.os_name == \"Darwin\":\n\t\t# Check that the C++11 flag is available (and not the C++0x flag)\n\t\tif os.system(\"g++ -std=c++11 -v >/dev/null 2>&1\"):\n\t\t\treturn True\n\n\t# Check that the C++11 flag is available (and not the C++0x flag)\n\tif os.system(\"g++ -std=c++11 -v >/dev/null 2>&1\"):\n\t\treturn True\n\t\n\t# Check that the C++1y flag is available (and not the C++0x flag)\n\tif os.system(\"g++ -std=c++1y -v >/dev/null 2>&1\"):\n\t\treturn True\n\n\t# Check that the C++14 flag is available (and not the C++0x flag)\n\tif os.system(\"g++ -std=c++14 -v >/dev/null", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\ttry:\n\t\tos.environ[\"CC\"]\n\texcept KeyError:\n\t\treturn False\n\n\treturn \"GCC\" in os.environ[\"CC\"]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport sys, platform\n\t\n\treturn platform.system() != 'Windows'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport os\n\timport platform\n\tfrom ._c_version import version as c_version\n\tfrom ._c_version import full_version as c_full_version\n\tfrom ._c_version import git_revision as c_git_revision\n\tfrom ._c_version import git_describe as c_git_describe\n\tfrom ._c_version import git_hash as c_git_hash\n\tfrom ._c_version import c_date as c_date\n\tfrom ._c_version import c_time as c_time\n\tfrom ._c_version import compiler as c_compiler\n\tfrom ._c_version import compiler_date as c_compiler_date\n\tfrom ._c_version import compiler_time as c_compiler_time\n\tfrom ._c_version import compiler_hash as c_compiler_hash\n\tfrom . import _py_version\n\tpy_version = _py_version._version\n\t\n\tcompiler_date = c_compiler_date.decode(\"utf-8\")\n\tcompiler_time = c_compiler_time.decode(\"utf-8\")\n\tcompiler_hash = c_compiler_hash.decode(\"utf-8\")\n\t\n\tdate_hash = \"%s%s%s%s%s%s%s\" % (\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\timport platform\n\treturn (platform.system() == \"Windows\" and\n\t\tplatform.processor() == \"AMD64\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\ttry:\n\t\timport ctypes\n\texcept ImportError:\n\t\treturn False\n\n\treturn ctypes.sizeof(ctypes.c_double) == 8\n\n# Set the value of this variable to True to enable the\n# C optimizations (speedup, etc.).\nc_optimizations_required = _c_optimizations_required()", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t\"\"\"\n\tReturn a true value if the C optimizations are required.\n\t\"\"\"\n\tfrom distutils.sysconfig import get_config_vars\n\tconfig_vars = get_config_vars()\n\treturn config_vars.get('OPTIMFLAGS') is not None\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tpass\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.bins = self.make_bins(*self.initial_value)\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset_bins()\n\tself.reset_context()\n\n\tself.reset_values()\n\tself.reset_bins()\n\n\treset_context_from_initial(self)\n\n\treturn self\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset_bins()\n\tself.reset_context()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset()\n\tself.bins = make_bins(self.initial_value)\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = defaultdict(lambda:defaultdict(int))\n\tfor key in self.bins:\n\t\tself.bins[key] = self.initial_value(key)", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.clear()\n\tself.reset_bins()\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClear the current context.\n\t\t\"\"\"\n\t\tself.context = {}\n\n\tdef update_bins(self, key, value):\n\t\t\"\"\"\n\t\tUpdate the bins of the current context.\n\n\t\t:param key: The key of the context.\n\t\t:param value: The value of the context.\n\t\t\"\"\"\n\t\tbins = self.context.get(key, self.initial_value)\n\t\tif bins is None:\n\t\t\treturn\n\n\t\tbins[value] = bins.get(value, 0) + 1\n\n\tdef reset_bins(self):\n\t\t\"\"\"\n\t\tReset all bins of the current context.\n\t\t\"\"\"\n\t\tfor key, value in self.context.items():\n\t\t\tself.update_bins(key, value)", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\tself.save()\n", "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself._context = {}\n\tself._bins = self._make_bins()\n\tself.reset_bins()\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.bins = self.make_bins(*self.initial_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset_bins()\n\tself.reset_context()\n\n\tself.reset_values()\n\tself.reset_bins()\n\n\treset_context_from_initial(self)\n\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset_bins()\n\tself.reset_context()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.reset()\n\tself.bins = make_bins(self.initial_value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = defaultdict(lambda:defaultdict(int))\n\tfor key in self.bins:\n\t\tself.bins[key] = self.initial_value(key)", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.clear()\n\tself.reset_bins()\n\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClear the current context.\n\t\t\"\"\"\n\t\tself.context = {}\n\n\tdef update_bins(self, key, value):\n\t\t\"\"\"\n\t\tUpdate the bins of the current context.\n\n\t\t:param key: The key of the context.\n\t\t:param value: The value of the context.\n\t\t\"\"\"\n\t\tbins = self.context.get(key, self.initial_value)\n\t\tif bins is None:\n\t\t\treturn\n\n\t\tbins[value] = bins.get(value, 0) + 1\n\n\tdef reset_bins(self):\n\t\t\"\"\"\n\t\tReset all bins of the current context.\n\t\t\"\"\"\n\t\tfor key, value in self.context.items():\n\t\t\tself.update_bins(key, value)", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.context = {}\n\tself.bins = self.make_bins()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\tself.save()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\t\"\"\"\n\tCurrent context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.\n\t\"\"\"\n\tself._context = {}\n\tself._bins = self._make_bins()\n\tself.reset_bins()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tdef _to_csv(self, separator):\n\t\t\"\"\"\n\t\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t\t\"\"\"\n\t\tdef _to_csv_part(value):\n\t\t\tif isinstance(value, str):\n\t\t\t\treturn value\n\t\t\tif isinstance(value, (list, tuple)):\n\t\t\t\treturn separator.join(map(str, value))\n\t\t\treturn str(value)\n\n\t\tcsv = []\n\t\tcsv.append(header)\n\t\tcsv.extend(map(_to_csv_part, self.points))\n\t\treturn csv\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t\t\"\"\"\n\t\treturn self.to_csv()\n\n\tdef to_csv(self, separator=\",\", header=None):\n\t\t\"\"\"\n\t\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t\tConvert graph's points to CSV.\n\n\t\t*separator* delimits values, the default is comma.\n\n\t\t*header*, if not ``None``, is the first string of the output\n\t\t(new line is added automatically", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is deprecated, use to_table instead\")\n\treturn self.to_table(separator=separator, header=header)\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is not used. Iterables are converted to tables.\")\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tdel separator, header\n\traise DeprecationWarning(\n\t\t\"to_csv is deprecated in Lena 0.5. \"\n\t\t\"It is not used anymore. Use to_table instead.\")\n\n#lena/output/graph.py\n# -*- coding: utf-8 -*-\n\"\"\"\n.. module:: graph\n   :synopsis: Output a graph as a graphviz dot file.\n\n.. moduleauthor:: Emmanuel Leblond <leblond.emmanuel@gmail.com>\n\n\"\"\"\n\nfrom lena.output import Output\nimport os\nimport subprocess\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\tw = csv.writer(self.output, delimiter=separator)\n\tif header is not None:\n\t\tw.writerow(header)\n\tfor point in self:\n\t\tw.writerow(point)\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\n\tif header is None:\n\t\theader = list(self.points.keys())\n\n\twith open(self.name, \"w\") as f:\n\t\tcsv_f = csv.writer(f, delimiter=separator)\n\t\tcsv_f.writerow(header)\n\t\tfor point in self.points:\n\t\t\tcsv_f.writerow([point] + [str(v) for v in self.points[point]])\n\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise RuntimeError(\"to_csv is deprecated. Use to_table instead.\")\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom lena.util.table import Table\n\tfrom lena.util.iterable import Iterable\n\tif header is None:\n\t\theader = []\n\tif isinstance(header, Iterable):\n\t\theader = tuple(header)\n\telse:\n\t\theader = (header,)\n\titerable = self.to_iterable()\n\ttable = Table(iterable, separator=separator, header=header)\n\ttable.to_csv()\n\n#lena/output/__init__.py\nfrom .csv import to_csv\nfrom .graph import to_graph\nfrom .iterable import to_iterable\nfrom .table import to_table\nfrom .text import to_text\nfrom .graph import to_graph_pairs\n\n#lena/util/__init__.py\nfrom .iterable import *\nfrom .table import *\nfrom .string import *\nfrom .dict import *\nfrom .graph import *\nfrom .math import *\nfrom .array import *\n\n#lena/util/array.py\nfrom .iterable import Iterable\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\tfrom lena.exceptions import LenaError\n\n\ttry:\n\t\tcsv_data = csv.writer(self, delimiter=separator)\n\t\tif header is not None:\n\t\t\tcsv_data.writerow(header)\n\texcept TypeError:\n\t\traise LenaError(\"Invalid separator '%s'.\" % separator)\n\tfor row in self:\n\t\tcsv_data.writerow(row)\n", "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"Lena 0.5 to_csv is not used. Iterables are converted to tables.\")\n\n#lena/output.py\n# -*- coding: utf-8 -*-\n\n\"\"\"\n.. module:: output\n   :synopsis: Output modules.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport subprocess\nimport shutil\nimport itertools\nimport tempfile\nimport math\nimport re\nimport operator\nimport json\nimport warnings\nfrom collections import OrderedDict\n\nfrom . import graph\nfrom . import data\nfrom . import utils\nfrom . import config\nfrom . import graph\nfrom . import utils\n\n__all__ = [\"ToCSV\", \"ToJSON\", \"ToDot\", \"ToGraphML\", \"ToGraphviz\", \"ToTikZ\", \"ToHTML\", \"ToImage\", \"ToFile\", \"ToStream\", \"ToHTML\", \"ToPDF\"]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tdef _to_csv(self, separator):\n\t\t\"\"\"\n\t\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t\t\"\"\"\n\t\tdef _to_csv_part(value):\n\t\t\tif isinstance(value, str):\n\t\t\t\treturn value\n\t\t\tif isinstance(value, (list, tuple)):\n\t\t\t\treturn separator.join(map(str, value))\n\t\t\treturn str(value)\n\n\t\tcsv = []\n\t\tcsv.append(header)\n\t\tcsv.extend(map(_to_csv_part, self.points))\n\t\treturn csv\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t\t\"\"\"\n\t\treturn self.to_csv()\n\n\tdef to_csv(self, separator=\",\", header=None):\n\t\t\"\"\"\n\t\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n\t\tConvert graph's points to CSV.\n\n\t\t*separator* delimits values, the default is comma.\n\n\t\t*header*, if not ``None``, is the first string of the output\n\t\t(new line is added automatically", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is deprecated, use to_table instead\")\n\treturn self.to_table(separator=separator, header=header)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"to_csv is not used. Iterables are converted to tables.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tdel separator, header\n\traise DeprecationWarning(\n\t\t\"to_csv is deprecated in Lena 0.5. \"\n\t\t\"It is not used anymore. Use to_table instead.\")\n\n#lena/output/graph.py\n# -*- coding: utf-8 -*-\n\"\"\"\n.. module:: graph\n   :synopsis: Output a graph as a graphviz dot file.\n\n.. moduleauthor:: Emmanuel Leblond <leblond.emmanuel@gmail.com>\n\n\"\"\"\n\nfrom lena.output import Output\nimport os\nimport subprocess\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\tw = csv.writer(self.output, delimiter=separator)\n\tif header is not None:\n\t\tw.writerow(header)\n\tfor point in self:\n\t\tw.writerow(point)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\n\tif header is None:\n\t\theader = list(self.points.keys())\n\n\twith open(self.name, \"w\") as f:\n\t\tcsv_f = csv.writer(f, delimiter=separator)\n\t\tcsv_f.writerow(header)\n\t\tfor point in self.points:\n\t\t\tcsv_f.writerow([point] + [str(v) for v in self.points[point]])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise RuntimeError(\"to_csv is deprecated. Use to_table instead.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\tfrom lena.util.table import Table\n\tfrom lena.util.iterable import Iterable\n\tif header is None:\n\t\theader = []\n\tif isinstance(header, Iterable):\n\t\theader = tuple(header)\n\telse:\n\t\theader = (header,)\n\titerable = self.to_iterable()\n\ttable = Table(iterable, separator=separator, header=header)\n\ttable.to_csv()\n\n#lena/output/__init__.py\nfrom .csv import to_csv\nfrom .graph import to_graph\nfrom .iterable import to_iterable\nfrom .table import to_table\nfrom .text import to_text\nfrom .graph import to_graph_pairs\n\n#lena/util/__init__.py\nfrom .iterable import *\nfrom .table import *\nfrom .string import *\nfrom .dict import *\nfrom .graph import *\nfrom .math import *\nfrom .array import *\n\n#lena/util/array.py\nfrom .iterable import Iterable\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\timport csv\n\tfrom lena.exceptions import LenaError\n\n\ttry:\n\t\tcsv_data = csv.writer(self, delimiter=separator)\n\t\tif header is not None:\n\t\t\tcsv_data.writerow(header)\n\texcept TypeError:\n\t\traise LenaError(\"Invalid separator '%s'.\" % separator)\n\tfor row in self:\n\t\tcsv_data.writerow(row)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t\"\"\"\n\t.. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.\n\t\"\"\"\n\traise DeprecationWarning(\"Lena 0.5 to_csv is not used. Iterables are converted to tables.\")\n\n#lena/output.py\n# -*- coding: utf-8 -*-\n\n\"\"\"\n.. module:: output\n   :synopsis: Output modules.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport subprocess\nimport shutil\nimport itertools\nimport tempfile\nimport math\nimport re\nimport operator\nimport json\nimport warnings\nfrom collections import OrderedDict\n\nfrom . import graph\nfrom . import data\nfrom . import utils\nfrom . import config\nfrom . import graph\nfrom . import utils\n\n__all__ = [\"ToCSV\", \"ToJSON\", \"ToDot\", \"ToGraphML\", \"ToGraphviz\", \"ToTikZ\", \"ToHTML\", \"ToImage\", \"ToFile\", \"ToStream\", \"ToHTML\", \"ToPDF\"]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tcoord_indexes = self.coord_indexes[coord_name]\n\treturn np.array([self.coord_indexes['err_' + coord_name][i] for i in coord_indexes])\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terror_indices = []\n\tfor idx, (err_type, err_str) in enumerate(self.error_type_strs):\n\t\tif err_type == 'D' and coord_name in err_str:\n\t\t\terror_indices.append(idx)\n\treturn error_indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terror_indices = self.get_error_indices(coord_name)\n\treturn error_indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor e in self.errors:\n\t\tif e.coord_name == coord_name:\n\t\t\tindices.append(e.index)\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor ii in range(len(self.coords)):\n\t\tif self.coords[ii].name == coord_name:\n\t\t\tindices.append(ii)\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\tidx = self.get_index_by_name(coord_name)\n\tcart_idx = self.get_cartesian_index_by_name(coord_name)\n\t\n\tindices = [idx]\n\t\n\tfor dim in range(self.ndim):\n\t\tif dim == cart_idx:\n\t\t\tcontinue\n\t\telif not self.is_coord_index_cartesian(dim):\n\t\t\tindices.append(self.get_cartesian_index_by_name(coord_name, dim))\n\t\telse:\n\t\t\tindices.append(self.get_coord_index_by_name(coord_name, dim))\n\t\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\ttry:\n\t\tindices = list(self.coords_dict[coord_name].indexes)\n\texcept KeyError:\n\t\tindices = []\n\t\n\treturn indices\n\n#-----------------------------------------------------------------------------", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\tindices = []\n\t\n\tfor i, err_type in enumerate(self._errors):\n\t\tif err_type[0] == coord_name:\n\t\t\tindices.append(i)\n\t\n\treturn indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tlst_err_indices = []\n\tfor i in range(len(self.errors)):\n\t\terr_name = self.errors[i]\n\t\tif err_name == coord_name:\n\t\t\tlst_err_indices.append(i)\n\treturn lst_err_indices\n", "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = self._err_index_names.index(coord_name)\n\treturn self._err_index_indices[indices]\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tcoord_indexes = self.coord_indexes[coord_name]\n\treturn np.array([self.coord_indexes['err_' + coord_name][i] for i in coord_indexes])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terror_indices = []\n\tfor idx, (err_type, err_str) in enumerate(self.error_type_strs):\n\t\tif err_type == 'D' and coord_name in err_str:\n\t\t\terror_indices.append(idx)\n\treturn error_indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\terror_indices = self.get_error_indices(coord_name)\n\treturn error_indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor e in self.errors:\n\t\tif e.coord_name == coord_name:\n\t\t\tindices.append(e.index)\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = []\n\tfor ii in range(len(self.coords)):\n\t\tif self.coords[ii].name == coord_name:\n\t\t\tindices.append(ii)\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\tidx = self.get_index_by_name(coord_name)\n\tcart_idx = self.get_cartesian_index_by_name(coord_name)\n\t\n\tindices = [idx]\n\t\n\tfor dim in range(self.ndim):\n\t\tif dim == cart_idx:\n\t\t\tcontinue\n\t\telif not self.is_coord_index_cartesian(dim):\n\t\t\tindices.append(self.get_cartesian_index_by_name(coord_name, dim))\n\t\telse:\n\t\t\tindices.append(self.get_coord_index_by_name(coord_name, dim))\n\t\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\ttry:\n\t\tindices = list(self.coords_dict[coord_name].indexes)\n\texcept KeyError:\n\t\tindices = []\n\t\n\treturn indices\n\n#-----------------------------------------------------------------------------", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\t\n\tindices = []\n\t\n\tfor i, err_type in enumerate(self._errors):\n\t\tif err_type[0] == coord_name:\n\t\t\tindices.append(i)\n\t\n\treturn indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tlst_err_indices = []\n\tfor i in range(len(self.errors)):\n\t\terr_name = self.errors[i]\n\t\tif err_name == coord_name:\n\t\t\tlst_err_indices.append(i)\n\treturn lst_err_indices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t\"\"\"\n\tFind all error indexes corresponding to coord_name.\n\t\"\"\"\n\tindices = self._err_index_names.index(coord_name)\n\treturn self._err_index_indices[indices]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tfor name in self.fields:\n\t\tvalue = self.value[name]\n\t\tif not value:\n\t\t\tcontinue\n\t\telif isinstance(value, dict):\n\t\t\tcontext[\"error\"][name] = value\n\t\telif isinstance(value, list):\n\t\t\tcontext[\"error\"][name] = {\n\t\t\t\t\"index\": len(value),\n\t\t\t\t\"value\": value,\n\t\t\t}\n\t\telse:\n\t\t\traise ValueError(\"Unknown value type %r\" % value)\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself._update_context_common(context)\n\n\tcontext[\"error\"] = {\n\t\tname: {\"index\": index}\n\t\tfor name, index in self.error_names().items()\n\t}\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {\n\t\tn: {\n\t\t\t\"index\": i\n\t\t\tfor i, (n, _) in enumerate(self.field_names)\n\t\t\tif n in self.error_fields\n\t\t}\n\t}\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tfor (key, value) in self._value.items():\n\t\tif not value:\n\t\t\tcontinue\n\t\telif isinstance(value, tuple):\n\t\t\tcontext[key] = value\n\t\telif key in [\"E\", \"t\"]:\n\t\t\tcontext[\"error\"][key] = value\n\t\telse:\n\t\t\tcontext[key] = value\n\n\t\tif isinstance(value, (list, tuple)):\n\t\t\tfor (i, v) in enumerate(value):\n\t\t\t\tif isinstance(v, (dict, Context)):\n\t\t\t\t\t_update_context(self, v)\n\t\t\t\t\tcontext[key][i] = v\n\t\telif isinstance(value, dict):\n\t\t\t_update_context(self, value)\n\t\t\tcontext[key] = value\n\n\t\telif isinstance(value, (int, float)):\n\t\t\tcontext[key] = value\n\n\t\telif isinstance(value, Exception):\n\t\t\tcontext[\"error\"][key] = value\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tsuper(_update_context, self)._update_context(context)\n\tcontext[\"error\"] = {}\n\tcontext[\"error_%s\" % self.name] = {}\n\tfor field in self.fields:\n\t\tif field.error:\n\t\t\tcontext[\"error\"][field.name] = field.error.dict()\n\t\t\tcontext[\"error_%s\" % field.name] = field.error.dict()\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tcontext[\"error\"][\"x_low\"] = {\"index\": -1}\n\tcontext[\"error\"][\"y_low\"] = {\"index\": -1}\n\tcontext[\"error\"][\"z_low\"] = {\"index\": -1}\n\n\tfor i, x in enumerate(self.x):\n\t\tif i < len(self.x) - 1:\n\t\t\tcontext[\"error\"][\"x_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"x_low\"][\"value\"] = x\n\t\telse:\n\t\t\tcontext[\"error\"][\"x_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"x_low\"][\"value\"] = None\n\n\tfor i, y in enumerate(self.y):\n\t\tif i < len(self.y) - 1:\n\t\t\tcontext[\"error\"][\"y_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"y_low\"][\"value\"] = y\n\t\telse:\n\t\t\tcontext[\"error\"][\"y_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"y_low\"][\"value\"] = None\n\n\tfor i, z in enumerate(self.z):\n\t\tif i < len(self.z) - 1:\n\t\t\tcontext[\"error\"][\"z", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {\"x_low\": {\"index\": 0}}\n\n#------------------------------------------------------------------------------#\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tfor key, value in context.items():\n\t\tif key == \"error\":\n\t\t\tcontinue\n\t\tif isinstance(value, dict):\n\t\t\tcontext[\"error\"][key] = value\n\t\telse:\n\t\t\tcontext[\"error\"][key] = {\"index\": value}\n\tcontext[\"error\"] = {k: v for k, v in sorted(context[\"error\"].items(), key=lambda x: x[0])}\n\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {\"x\": self.error_E_low, \"y\": self.error_t_low, \"z\": self.error_E_high}\n", "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\terror_names = self.error_names\n\tvalue = context[\"value\"]\n\tif error_names:\n\t\tvalue[\"error\"] = {}\n\t\tfor name in error_names:\n\t\t\tvalue[\"error\"][name] = {}\n\t\t\tvalue[\"error\"][name][\"index\"] = []\n\tfor key in value.keys():\n\t\tcontext[key] = value[key]\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tfor name in self.fields:\n\t\tvalue = self.value[name]\n\t\tif not value:\n\t\t\tcontinue\n\t\telif isinstance(value, dict):\n\t\t\tcontext[\"error\"][name] = value\n\t\telif isinstance(value, list):\n\t\t\tcontext[\"error\"][name] = {\n\t\t\t\t\"index\": len(value),\n\t\t\t\t\"value\": value,\n\t\t\t}\n\t\telse:\n\t\t\traise ValueError(\"Unknown value type %r\" % value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tself._update_context_common(context)\n\n\tcontext[\"error\"] = {\n\t\tname: {\"index\": index}\n\t\tfor name, index in self.error_names().items()\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {\n\t\tn: {\n\t\t\t\"index\": i\n\t\t\tfor i, (n, _) in enumerate(self.field_names)\n\t\t\tif n in self.error_fields\n\t\t}\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tfor (key, value) in self._value.items():\n\t\tif not value:\n\t\t\tcontinue\n\t\telif isinstance(value, tuple):\n\t\t\tcontext[key] = value\n\t\telif key in [\"E\", \"t\"]:\n\t\t\tcontext[\"error\"][key] = value\n\t\telse:\n\t\t\tcontext[key] = value\n\n\t\tif isinstance(value, (list, tuple)):\n\t\t\tfor (i, v) in enumerate(value):\n\t\t\t\tif isinstance(v, (dict, Context)):\n\t\t\t\t\t_update_context(self, v)\n\t\t\t\t\tcontext[key][i] = v\n\t\telif isinstance(value, dict):\n\t\t\t_update_context(self, value)\n\t\t\tcontext[key] = value\n\n\t\telif isinstance(value, (int, float)):\n\t\t\tcontext[key] = value\n\n\t\telif isinstance(value, Exception):\n\t\t\tcontext[\"error\"][key] = value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tsuper(_update_context, self)._update_context(context)\n\tcontext[\"error\"] = {}\n\tcontext[\"error_%s\" % self.name] = {}\n\tfor field in self.fields:\n\t\tif field.error:\n\t\t\tcontext[\"error\"][field.name] = field.error.dict()\n\t\t\tcontext[\"error_%s\" % field.name] = field.error.dict()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tcontext[\"error\"][\"x_low\"] = {\"index\": -1}\n\tcontext[\"error\"][\"y_low\"] = {\"index\": -1}\n\tcontext[\"error\"][\"z_low\"] = {\"index\": -1}\n\n\tfor i, x in enumerate(self.x):\n\t\tif i < len(self.x) - 1:\n\t\t\tcontext[\"error\"][\"x_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"x_low\"][\"value\"] = x\n\t\telse:\n\t\t\tcontext[\"error\"][\"x_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"x_low\"][\"value\"] = None\n\n\tfor i, y in enumerate(self.y):\n\t\tif i < len(self.y) - 1:\n\t\t\tcontext[\"error\"][\"y_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"y_low\"][\"value\"] = y\n\t\telse:\n\t\t\tcontext[\"error\"][\"y_low\"][\"index\"] = i\n\t\t\tcontext[\"error\"][\"y_low\"][\"value\"] = None\n\n\tfor i, z in enumerate(self.z):\n\t\tif i < len(self.z) - 1:\n\t\t\tcontext[\"error\"][\"z", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {\"x_low\": {\"index\": 0}}\n\n#------------------------------------------------------------------------------#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {}\n\tfor key, value in context.items():\n\t\tif key == \"error\":\n\t\t\tcontinue\n\t\tif isinstance(value, dict):\n\t\t\tcontext[\"error\"][key] = value\n\t\telse:\n\t\t\tcontext[\"error\"][key] = {\"index\": value}\n\tcontext[\"error\"] = {k: v for k, v in sorted(context[\"error\"].items(), key=lambda x: x[0])}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\tcontext[\"error\"] = {\"x\": self.error_E_low, \"y\": self.error_t_low, \"z\": self.error_E_high}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t\"\"\"\n\tUpdate *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.\n\t\"\"\"\n\terror_names = self.error_names\n\tvalue = context[\"value\"]\n\tif error_names:\n\t\tvalue[\"error\"] = {}\n\t\tfor name in error_names:\n\t\t\tvalue[\"error\"][name] = {}\n\t\t\tvalue[\"error\"][name][\"index\"] = []\n\tfor key in value.keys():\n\t\tcontext[key] = value[key]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tbin_area = bins[1:] - bins[:-1]\n\tbin_area = np.append(bin_area, bin_area[-1])\n\tedge_area = edges[1:] - edges[:-1]\n\tedge_area = np.append(edge_area, edge_area[-1])\n\tarea = np.zeros_like(bin_area)\n\tfor i in range(len(bin_area)):\n\t\tarea[i] = bin_area[i] * edge_area[i]\n\treturn area.sum()\n\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\t\n\tarea_left = bins[0]\n\tarea_right = bins[-1]\n\t\n\tedges_left = edges[0]\n\tedges_right = edges[-1]\n\t\n\tfor i in range(len(bins)-1):\n\t\tif bins[i+1] > bins[i]:\n\t\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\t\telse:\n\t\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\t\t\n\treturn area, area_left, area_right, edges_left, edges_right\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\twidth = bins[1] - bins[0]\n\tarea = width * sum([edges[i][1] - edges[i][0] for i in range(len(edges))])\n\treturn area", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\n\tprint('bins:', bins)\n\tprint('edges:', edges)\n\t\n\tarea = 0\n\t\n\tfor i in range(len(bins) - 1):\n\t\t\n\t\t#print('i:', i)\n\t\t\n\t\tarea += (bins[i + 1] - bins[i]) * (edges[i + 1] - edges[i])\n\t\t\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\n\tintegral = 0.0\n\t\n\tfor i in range(len(edges)-1):\n\t\tintegral += bins[i] * (edges[i+1]-edges[i])\n\t\t\n\treturn integral\n\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = bins[0][1]\n\tfor i in range(len(bins) - 1):\n\t\tarea += (bins[i+1][0] - bins[i][1]) * (edges[i+1] - edges[i])\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tprev = bins[0]\n\tfor curr in bins[1:]:\n\t\tarea += (curr - prev) * (edges[curr-1] + edges[curr]) / 2\n\t\tprev = curr\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(edges)-1):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\ttotal = 0\n\tfor i in range(len(bins)):\n\t\ttotal += bins[i] * (edges[i + 1] - edges[i])\n\treturn total\n", "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\t\n\tfor i in range(len(bins)-1):\n\t\tarea += (bins[i] + bins[i+1]) * (edges[i+1] - edges[i]) / 2\n\t\n\treturn area\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tbin_area = bins[1:] - bins[:-1]\n\tbin_area = np.append(bin_area, bin_area[-1])\n\tedge_area = edges[1:] - edges[:-1]\n\tedge_area = np.append(edge_area, edge_area[-1])\n\tarea = np.zeros_like(bin_area)\n\tfor i in range(len(bin_area)):\n\t\tarea[i] = bin_area[i] * edge_area[i]\n\treturn area.sum()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\t\n\tarea_left = bins[0]\n\tarea_right = bins[-1]\n\t\n\tedges_left = edges[0]\n\tedges_right = edges[-1]\n\t\n\tfor i in range(len(bins)-1):\n\t\tif bins[i+1] > bins[i]:\n\t\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\t\telse:\n\t\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\t\t\n\treturn area, area_left, area_right, edges_left, edges_right\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\twidth = bins[1] - bins[0]\n\tarea = width * sum([edges[i][1] - edges[i][0] for i in range(len(edges))])\n\treturn area", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\n\tprint('bins:', bins)\n\tprint('edges:', edges)\n\t\n\tarea = 0\n\t\n\tfor i in range(len(bins) - 1):\n\t\t\n\t\t#print('i:', i)\n\t\t\n\t\tarea += (bins[i + 1] - bins[i]) * (edges[i + 1] - edges[i])\n\t\t\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\t\n\tintegral = 0.0\n\t\n\tfor i in range(len(edges)-1):\n\t\tintegral += bins[i] * (edges[i+1]-edges[i])\n\t\t\n\treturn integral\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = bins[0][1]\n\tfor i in range(len(bins) - 1):\n\t\tarea += (bins[i+1][0] - bins[i][1]) * (edges[i+1] - edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tprev = bins[0]\n\tfor curr in bins[1:]:\n\t\tarea += (curr - prev) * (edges[curr-1] + edges[curr]) / 2\n\t\tprev = curr\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\tfor i in range(len(edges)-1):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\ttotal = 0\n\tfor i in range(len(bins)):\n\t\ttotal += bins[i] * (edges[i + 1] - edges[i])\n\treturn total\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\"\"\"\n\tCalculate the area of the overall graph.\n\t\"\"\"\n\tarea = 0\n\t\n\tfor i in range(len(bins)-1):\n\t\tarea += (bins[i] + bins[i+1]) * (edges[i+1] - edges[i]) / 2\n\t\n\treturn area\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tfill_request_seq = FillRequestSeq(seq)\n\treturn fill_request_seq\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept (ValueError, TypeError):\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tseq = FillRequestSeq(seq)\n\t\treturn True\n\texcept Exception, e:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tfrom .types import FillRequestSeq\n\n\tseq = seq.strip(' ')\n\ttry:\n\t\tseq = FillRequestSeq(seq)\n\texcept ValueError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\tnew_seq = str(seq)\n\tnew_seq = new_seq.replace('[', '')\n\tnew_seq = new_seq.replace(']', '')\n\tnew_seq = new_seq.replace('\\'', '')\n\tnew_seq = new_seq.replace(' ', '')\n\t\n\tif new_seq[0] == '#':\n\t\treturn False\n\t\t\n\ttry:\n\t\tfill_request_seq = FillRequestSeq()\n\texcept:\n\t\treturn False\n\n\tfill_request_seq.from_string(new_seq)\n\t\n\treturn True\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tfill_request_seq = FillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept:\n\t\treturn False\n\treturn True\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tfill_request_seq = FillRequestSeq(seq)\n\treturn fill_request_seq\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept (ValueError, TypeError):\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tseq = FillRequestSeq(seq)\n\t\treturn True\n\texcept Exception, e:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\tfrom .types import FillRequestSeq\n\n\tseq = seq.strip(' ')\n\ttry:\n\t\tseq = FillRequestSeq(seq)\n\texcept ValueError:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\t\n\tnew_seq = str(seq)\n\tnew_seq = new_seq.replace('[', '')\n\tnew_seq = new_seq.replace(']', '')\n\tnew_seq = new_seq.replace('\\'', '')\n\tnew_seq = new_seq.replace(' ', '')\n\t\n\tif new_seq[0] == '#':\n\t\treturn False\n\t\t\n\ttry:\n\t\tfill_request_seq = FillRequestSeq()\n\texcept:\n\t\treturn False\n\n\tfill_request_seq.from_string(new_seq)\n\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tfill_request_seq = FillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\t\"\"\"\n\tCheck whether seq can be converted to FillRequestSeq and bool is returned.\n\t\"\"\"\n\ttry:\n\t\tFillRequestSeq(seq)\n\texcept:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill_request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\texcept AttributeError:\n\t\treturn False\n\ttry:\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tattrs = ['fill', 'request', 'tile', 'tile_id']\n\treturn all(attr in obj.__dict__ for attr in attrs)\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tattrs = ('fill', 'request')\n\treturn all(attr in obj.__dict__ for attr in attrs)\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tel_name = obj.__name__\n\tif el_name.startswith(\"Fill\"):\n\t\treturn False\n\tel_name = el_name.split(\".\")[-1]\n\tif el_name.endswith(\"Request\"):\n\t\treturn False\n\treturn True\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn obj.has_attr('fill') and obj.has_attr('request')\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn (\n\t\tobj.fill\n\t\tand obj.request\n\t)\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tfill_request_attrs = set(obj.fill_request_attrs)\n\texcept AttributeError:\n\t\treturn False\n\treturn fill_request_attrs == set([\"fill\", \"request\"])\n\n", "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tel_list = [\n\t\t'fill',\n\t\t'request',\n\t]\n\n\treturn all(attr in obj.__dict__ for attr in el_list)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill_request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tobj.fill\n\texcept AttributeError:\n\t\treturn False\n\ttry:\n\t\tobj.request\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tattrs = ['fill', 'request', 'tile', 'tile_id']\n\treturn all(attr in obj.__dict__ for attr in attrs)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tattrs = ('fill', 'request')\n\treturn all(attr in obj.__dict__ for attr in attrs)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tel_name = obj.__name__\n\tif el_name.startswith(\"Fill\"):\n\t\treturn False\n\tel_name = el_name.split(\".\")[-1]\n\tif el_name.endswith(\"Request\"):\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn obj.has_attr('fill') and obj.has_attr('request')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\t\n\treturn (\n\t\tobj.fill\n\t\tand obj.request\n\t)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\ttry:\n\t\tfill_request_attrs = set(obj.fill_request_attrs)\n\texcept AttributeError:\n\t\treturn False\n\treturn fill_request_attrs == set([\"fill\", \"request\"])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and request attributes.\n\t\"\"\"\n\tel_list = [\n\t\t'fill',\n\t\t'request',\n\t]\n\n\treturn all(attr in obj.__dict__ for attr in el_list)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\treturn hasattr(obj, 'run')\n\texcept AttributeError:\n\t\treturn False\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry: return obj.run\n\texcept AttributeError: return False\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tmethod_names = [name for name in dir(obj) if callable(getattr(obj, name))]\n\treturn 'run' in method_names\n\n", "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\treturn hasattr(obj, 'run')\n\texcept AttributeError:\n\t\treturn False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\texcept AttributeError:\n\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\treturn hasattr(obj, 'run')\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry: return obj.run\n\texcept AttributeError: return False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\tmethod_names = [name for name in dir(obj) if callable(getattr(obj, name))]\n\treturn 'run' in method_names\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the run method.\n\t\"\"\"\n\ttry:\n\t\treturn hasattr(obj, 'run')\n\texcept AttributeError:\n\t\treturn False\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tdef is_fill(obj):\n\t\t\"\"\"\n\t\tCheck whether the obj class has the fill method.\n\t\t\"\"\"\n\t\treturn hasattr(obj, 'fill')\n\tdef is_compute(obj):\n\t\t\"\"\"\n\t\tCheck whether the obj class has the compute method.\n\t\t\"\"\"\n\t\treturn hasattr(obj, 'compute')\n\treturn is_fill(obj) and is_compute(obj)\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\texcept:\n\t\treturn False\n\ttry:\n\t\tobj.compute()\n\texcept:\n\t\treturn False\n\treturn True\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\tdef compute_el(self, el):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef fill_el(self, el):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef fill_el_with_attr(self, el, attr):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef compute_el_with_attr(self, el, attr):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef fill_el_with_value(self, el, value):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef compute_el_", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\treturn (hasattr(obj, 'fill') and\n\t        hasattr(obj, 'compute'))\n\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tis_fill = hasattr(obj, 'fill')\n\tis_compute = hasattr(obj, 'compute')\n\treturn is_fill and is_compute\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tclass_methods = [ method for method in dir(obj) if callable(getattr(obj, method)) and not method.startswith(\"_\") ]\n\treturn \"fill\" in class_methods and \"compute\" in class_methods\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tmethods = ['fill', 'compute']\n\tif not hasattr(obj, 'fill'):\n\t\treturn False\n\tif not hasattr(obj, 'compute'):\n\t\treturn False\n\tfor m in methods:\n\t\tif not hasattr(obj, m):\n\t\t\treturn False\n\treturn True\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tdef fill(self, value):\n\t\tself.fill = value\n\n\tdef compute(self):\n\t\tpass\n\n\tobj.__setattr__('fill', fill)\n\tobj.__setattr__('compute', compute)", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\thas_fill = obj.has_method('fill')\n\thas_compute = obj.has_method('compute')\n\treturn has_fill and has_compute\n", "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tdef set_fill(self, val):\n\t\t\"\"\"\n\t\tset the fill value of the obj.\n\t\t\"\"\"\n\t\tself.fill = val\n\n\tdef set_compute(self, func):\n\t\t\"\"\"\n\t\tset the compute function of the obj.\n\t\t\"\"\"\n\t\tself.compute = func\n\n\tdef compute(self):\n\t\t\"\"\"\n\t\tcompute the obj value.\n\t\t\"\"\"\n\t\traise NotImplementedError\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tdef is_fill(obj):\n\t\t\"\"\"\n\t\tCheck whether the obj class has the fill method.\n\t\t\"\"\"\n\t\treturn hasattr(obj, 'fill')\n\tdef is_compute(obj):\n\t\t\"\"\"\n\t\tCheck whether the obj class has the compute method.\n\t\t\"\"\"\n\t\treturn hasattr(obj, 'compute')\n\treturn is_fill(obj) and is_compute(obj)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\ttry:\n\t\tobj.fill()\n\texcept:\n\t\treturn False\n\ttry:\n\t\tobj.compute()\n\texcept:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\tdef compute_el(self, el):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef fill_el(self, el):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef fill_el_with_attr(self, el, attr):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef compute_el_with_attr(self, el, attr):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef fill_el_with_value(self, el, value):\n\t\t\"\"\"\n\t\tThis method should be overriden by a class that wants to be a\n\t\tfillable element.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\t\n\tdef compute_el_", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\t\n\treturn (hasattr(obj, 'fill') and\n\t        hasattr(obj, 'compute'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tis_fill = hasattr(obj, 'fill')\n\tis_compute = hasattr(obj, 'compute')\n\treturn is_fill and is_compute\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tclass_methods = [ method for method in dir(obj) if callable(getattr(obj, method)) and not method.startswith(\"_\") ]\n\treturn \"fill\" in class_methods and \"compute\" in class_methods\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tmethods = ['fill', 'compute']\n\tif not hasattr(obj, 'fill'):\n\t\treturn False\n\tif not hasattr(obj, 'compute'):\n\t\treturn False\n\tfor m in methods:\n\t\tif not hasattr(obj, m):\n\t\t\treturn False\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tdef fill(self, value):\n\t\tself.fill = value\n\n\tdef compute(self):\n\t\tpass\n\n\tobj.__setattr__('fill', fill)\n\tobj.__setattr__('compute', compute)", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\thas_fill = obj.has_method('fill')\n\thas_compute = obj.has_method('compute')\n\treturn has_fill and has_compute\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\t\"\"\"\n\tCheck whether the obj class has the fill and compute methods.\n\t\"\"\"\n\tdef set_fill(self, val):\n\t\t\"\"\"\n\t\tset the fill value of the obj.\n\t\t\"\"\"\n\t\tself.fill = val\n\n\tdef set_compute(self, func):\n\t\t\"\"\"\n\t\tset the compute function of the obj.\n\t\t\"\"\"\n\t\tself.compute = func\n\n\tdef compute(self):\n\t\t\"\"\"\n\t\tcompute the obj value.\n\t\t\"\"\"\n\t\traise NotImplementedError\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\n\tif level == -1:\n\t\tlevel = len(d1)\n\t\t\n\tret = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tret[k] = d1[k]\n\t\telse:\n\t\t\tif isinstance(d1[k], dict):\n\t\t\t\tret[k] = difference(d1[k], d2[k], level-1)\n\t\t\telse:\n\t\t\t\tret[k] = d1[k]\n\treturn ret\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k in d1.keys():\n\t\tif k not in d2.keys():\n\t\t\tif level != -1:\n\t\t\t\tif k.startswith('_'):\n\t\t\t\t\tdiff[k[1:]] = d1[k]\n\t\t\t\telse:\n\t\t\t\t\tdiff[k] = d1[k]\n\t\telse:\n\t\t\tif isinstance(d1[k], dict):\n\t\t\t\tdiff[k] = difference(d1[k], d2[k], level=level+1)\n\t\t\telif isinstance(d1[k], list):\n\t\t\t\tdiff[k] = difference(d1[k], d2[k], level=level+1)\n\treturn diff\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td = {}\n\tfor key in d1:\n\t\tif key not in d2:\n\t\t\td[key] = d1[key]\n\treturn d\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tnewDict = {}\n\tfor item, value in d1.items():\n\t\tif item not in d2:\n\t\t\tnewDict[item] = value\n\treturn newDict\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tdiff[k] = d1[k]\n\n\treturn diff\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k, v in d1.items():\n\t\tif k not in d2:\n\t\t\tif level >= 0:\n\t\t\t\tdiff[k] = difference(v, d2[k], level-1)\n\t\t\telse:\n\t\t\t\tdiff[k] = v\n\treturn diff\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key in d1:\n\t\tif d1[key] != d2[key]:\n\t\t\tif level == 0:\n\t\t\t\tdiff[key] = d1[key]\n\t\t\telif level == 1:\n\t\t\t\tdiff[key] = d1[key]\n\t\t\t\tdiff[key][\"children\"] = difference(d1[key], d2[key], level - 1)\n\t\t\telif level > 1:\n\t\t\t\tdiff[key] = d1[key]\n\t\t\t\tdiff[key][\"children\"] = difference(d1[key], d2[key], level - 1)\n\treturn diff\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdifference = []\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\tdifference.append((key, value))\n\treturn difference\n\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tret = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tret[k] = d1[k]\n\treturn ret\n", "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tret = {}\n\tret.update(d1)\n\tfor k in d2:\n\t\tret.pop(k)\n\treturn ret\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\t\n\tif level == -1:\n\t\tlevel = len(d1)\n\t\t\n\tret = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tret[k] = d1[k]\n\t\telse:\n\t\t\tif isinstance(d1[k], dict):\n\t\t\t\tret[k] = difference(d1[k], d2[k], level-1)\n\t\t\telse:\n\t\t\t\tret[k] = d1[k]\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k in d1.keys():\n\t\tif k not in d2.keys():\n\t\t\tif level != -1:\n\t\t\t\tif k.startswith('_'):\n\t\t\t\t\tdiff[k[1:]] = d1[k]\n\t\t\t\telse:\n\t\t\t\t\tdiff[k] = d1[k]\n\t\telse:\n\t\t\tif isinstance(d1[k], dict):\n\t\t\t\tdiff[k] = difference(d1[k], d2[k], level=level+1)\n\t\t\telif isinstance(d1[k], list):\n\t\t\t\tdiff[k] = difference(d1[k], d2[k], level=level+1)\n\treturn diff\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\td = {}\n\tfor key in d1:\n\t\tif key not in d2:\n\t\t\td[key] = d1[key]\n\treturn d\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tnewDict = {}\n\tfor item, value in d1.items():\n\t\tif item not in d2:\n\t\t\tnewDict[item] = value\n\treturn newDict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tdiff[k] = d1[k]\n\n\treturn diff\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor k, v in d1.items():\n\t\tif k not in d2:\n\t\t\tif level >= 0:\n\t\t\t\tdiff[k] = difference(v, d2[k], level-1)\n\t\t\telse:\n\t\t\t\tdiff[k] = v\n\treturn diff\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdiff = {}\n\tfor key in d1:\n\t\tif d1[key] != d2[key]:\n\t\t\tif level == 0:\n\t\t\t\tdiff[key] = d1[key]\n\t\t\telif level == 1:\n\t\t\t\tdiff[key] = d1[key]\n\t\t\t\tdiff[key][\"children\"] = difference(d1[key], d2[key], level - 1)\n\t\t\telif level > 1:\n\t\t\t\tdiff[key] = d1[key]\n\t\t\t\tdiff[key][\"children\"] = difference(d1[key], d2[key], level - 1)\n\treturn diff\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tdifference = []\n\tfor key, value in d1.items():\n\t\tif key not in d2:\n\t\t\tdifference.append((key, value))\n\treturn difference\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tret = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\tret[k] = d1[k]\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\t\"\"\"\n\tReturn a dictionary with items from d1 not contained in d2.\n\t\"\"\"\n\tret = {}\n\tret.update(d1)\n\tfor k in d2:\n\t\tret.pop(k)\n\treturn ret\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.histogram[coord[0]][coord[1]] *= weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tcoord = tuple(map(int, coord))\n\tif coord[0] < 0 or coord[0] >= self.width:\n\t\treturn\n\tif coord[1] < 0 or coord[1] >= self.height:\n\t\treturn\n\tself.histogram[coord[0]][coord[1]] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tsize = self.getSize()\n\tif coord[0] < 0 or coord[0] >= size[0]:\n\t\treturn\n\tif coord[1] < 0 or coord[1] >= size[1]:\n\t\treturn\n\tself.data[coord[0]][coord[1]] += weight\n\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\txmin, xmax, ymin, ymax = self.bounds\n\tif coord[0] < xmin or coord[0] > xmax or coord[1] < ymin or coord[1] > ymax:\n\t\treturn\n\tself.data[coord[0]-xmin][coord[1]-ymin] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfrom numpy import linspace\n\n\ttry:\n\t\tcoordlist = tuple(coord)\n\texcept TypeError:\n\t\tcoordlist = (coord,)\n\n\tfor c in coordlist:\n\t\tself.data[c] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill(coord, weight)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx1, x2 = self.bounds\n\tdx = x2 - x1\n\tif coord < x1:\n\t\treturn\n\tif coord >= x2:\n\t\treturn\n\tcoord = (coord - x1) / dx * self.size\n\tself.data[int(coord)] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.histogram[coord] += weight\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.hist.fill(coord, weight)\n", "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tdef fill(coord, weight=1):\n\t\t\"\"\"\n\t\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_area(coord1, coord2, weight=1):\n\t\t\"\"\"\n\t\tFill histogram from *coord1* to *coord2* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_rect(coord1, coord2, weight=1):\n\t\t\"\"\"\n\t\tFill histogram from *coord1* to *coord2* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_triangle(coord1, coord2, coord3, weight=1):\n\t\t\"\"\"\n\t\tFill histogram from *coord1* to *coord2* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_poly(coords, weight=1):\n\t\t\"\"\"\n\t\tFill histogram with the given coords and weight.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_area(coord1, coord2, weight="], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.histogram[coord[0]][coord[1]] *= weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tcoord = tuple(map(int, coord))\n\tif coord[0] < 0 or coord[0] >= self.width:\n\t\treturn\n\tif coord[1] < 0 or coord[1] >= self.height:\n\t\treturn\n\tself.histogram[coord[0]][coord[1]] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tsize = self.getSize()\n\tif coord[0] < 0 or coord[0] >= size[0]:\n\t\treturn\n\tif coord[1] < 0 or coord[1] >= size[1]:\n\t\treturn\n\tself.data[coord[0]][coord[1]] += weight\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\txmin, xmax, ymin, ymax = self.bounds\n\tif coord[0] < xmin or coord[0] > xmax or coord[1] < ymin or coord[1] > ymax:\n\t\treturn\n\tself.data[coord[0]-xmin][coord[1]-ymin] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tfrom numpy import linspace\n\n\ttry:\n\t\tcoordlist = tuple(coord)\n\texcept TypeError:\n\t\tcoordlist = (coord,)\n\n\tfor c in coordlist:\n\t\tself.data[c] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.fill(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tx1, x2 = self.bounds\n\tdx = x2 - x1\n\tif coord < x1:\n\t\treturn\n\tif coord >= x2:\n\t\treturn\n\tcoord = (coord - x1) / dx * self.size\n\tself.data[int(coord)] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.histogram[coord] += weight\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tself.hist.fill(coord, weight)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\"\"\"\n\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\"\"\"\n\tdef fill(coord, weight=1):\n\t\t\"\"\"\n\t\tFill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_area(coord1, coord2, weight=1):\n\t\t\"\"\"\n\t\tFill histogram from *coord1* to *coord2* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_rect(coord1, coord2, weight=1):\n\t\t\"\"\"\n\t\tFill histogram from *coord1* to *coord2* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_triangle(coord1, coord2, coord3, weight=1):\n\t\t\"\"\"\n\t\tFill histogram from *coord1* to *coord2* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_poly(coords, weight=1):\n\t\t\"\"\"\n\t\tFill histogram with the given coords and weight.\n\nCoordinates outside the histogram edges are ignored.\n\t\t\"\"\"\n\tdef fill_area(coord1, coord2, weight=", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkey_errors = []\n\tvalue_errors = []\n\tfor key, value in labels.iteritems():\n\t\tif not validate_key(key):\n\t\t\tkey_errors.append(key)\n\t\tif not validate_value(value):\n\t\t\tvalue_errors.append(value)\n\n\tif key_errors or value_errors:\n\t\traise InvalidLabels(key_errors=key_errors, value_errors=value_errors)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tdef validate_key(key):\n\t\tif not validate_key(key):\n\t\t\traise ValueError(\"Invalid label key: '%s'.\" % key)\n\n\t\ttry:\n\t\t\tvalidate_value(labels[key])\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Invalid label value for key '%s': '%s'.\" % (key, labels[key]))\n\n\tfor key in labels:\n\t\tvalidate_key(key)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tvalidate_key(\"labels\", labels)\n\t\n\tfor label, value in labels.items():\n\t\tvalidate_value(\"label\", label)\n\t\tvalidate_value(\"value\", value)\n\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tdef _validate_label(label):\n\t\t# Check that key is a string.\n\t\tif not isinstance(label['key'], basestring):\n\t\t\traise TypeError(\"Label key must be a string.\")\n\n\t\t# Check that key is not an empty string.\n\t\tif not label['key']:\n\t\t\traise ValueError(\"Label key must not be empty.\")\n\n\t\t# Check that value is a string or None.\n\t\tif not isinstance(label['value'], (str, type(None))):\n\t\t\traise TypeError(\"Label value must be a string or None.\")\n\n\tdef _validate_labels(labels):\n\t\tfor label in labels:\n\t\t\t_validate_label(label)\n\n\t_validate_labels(labels)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tkeys = []\n\tfor key in labels.keys():\n\t\tkeys.append(key)\n\t\tassert validate_key(key), \"Labels must be valid key-value pairs\"\n\t\n\tfor key in keys:\n\t\tassert validate_value(key, labels[key]), \"Labels must be valid key-value pairs\"\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkeys = set()\n\tvalues = set()\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\t\tkeys.add(key)\n\t\tvalues.add(value)\n\tif len(keys) != len(values):\n\t\traise Exception('Labels are not unique')\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\t# Check that all keys are valid\n\tfor key in labels:\n\t\tvalidate_key(key)\n\n\t# Check that all values are valid\n\tfor key in labels:\n\t\tvalidate_value(labels[key])\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tvalid_labels = set(labels.keys())\n\tvalid_keys = set(labels.values())\n\tassert valid_keys.issubset(valid_labels)\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkey_errors = []\n\tfor label, value in labels.items():\n\t\tkey_errors.append(validate_key(label))\n\t\tkey_errors.append(validate_value(value))\n\treturn key_errors\n", "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tdef _validate_label(label):\n\t\t\"\"\"\n\t\tCheck that the given label is valid.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvalidate_key(label)\n\t\t\tvalidate_value(label)\n\t\texcept:\n\t\t\traise Exception(\"Invalid label: '%s'\" % label)\n\t\n\tfor label in labels:\n\t\t_validate_label(label)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkey_errors = []\n\tvalue_errors = []\n\tfor key, value in labels.iteritems():\n\t\tif not validate_key(key):\n\t\t\tkey_errors.append(key)\n\t\tif not validate_value(value):\n\t\t\tvalue_errors.append(value)\n\n\tif key_errors or value_errors:\n\t\traise InvalidLabels(key_errors=key_errors, value_errors=value_errors)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tdef validate_key(key):\n\t\tif not validate_key(key):\n\t\t\traise ValueError(\"Invalid label key: '%s'.\" % key)\n\n\t\ttry:\n\t\t\tvalidate_value(labels[key])\n\t\texcept ValueError:\n\t\t\traise ValueError(\"Invalid label value for key '%s': '%s'.\" % (key, labels[key]))\n\n\tfor key in labels:\n\t\tvalidate_key(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tvalidate_key(\"labels\", labels)\n\t\n\tfor label, value in labels.items():\n\t\tvalidate_value(\"label\", label)\n\t\tvalidate_value(\"value\", value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tdef _validate_label(label):\n\t\t# Check that key is a string.\n\t\tif not isinstance(label['key'], basestring):\n\t\t\traise TypeError(\"Label key must be a string.\")\n\n\t\t# Check that key is not an empty string.\n\t\tif not label['key']:\n\t\t\traise ValueError(\"Label key must not be empty.\")\n\n\t\t# Check that value is a string or None.\n\t\tif not isinstance(label['value'], (str, type(None))):\n\t\t\traise TypeError(\"Label value must be a string or None.\")\n\n\tdef _validate_labels(labels):\n\t\tfor label in labels:\n\t\t\t_validate_label(label)\n\n\t_validate_labels(labels)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tkeys = []\n\tfor key in labels.keys():\n\t\tkeys.append(key)\n\t\tassert validate_key(key), \"Labels must be valid key-value pairs\"\n\t\n\tfor key in keys:\n\t\tassert validate_value(key, labels[key]), \"Labels must be valid key-value pairs\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkeys = set()\n\tvalues = set()\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\t\tkeys.add(key)\n\t\tvalues.add(value)\n\tif len(keys) != len(values):\n\t\traise Exception('Labels are not unique')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\t# Check that all keys are valid\n\tfor key in labels:\n\t\tvalidate_key(key)\n\n\t# Check that all values are valid\n\tfor key in labels:\n\t\tvalidate_value(labels[key])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tvalid_labels = set(labels.keys())\n\tvalid_keys = set(labels.values())\n\tassert valid_keys.issubset(valid_labels)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\tkey_errors = []\n\tfor label, value in labels.items():\n\t\tkey_errors.append(validate_key(label))\n\t\tkey_errors.append(validate_value(value))\n\treturn key_errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\t\"\"\"\n\tCheck that keys and values in the given labels by validate_key() and validate_value().\n\t\"\"\"\n\t\n\tdef _validate_label(label):\n\t\t\"\"\"\n\t\tCheck that the given label is valid.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvalidate_key(label)\n\t\t\tvalidate_value(label)\n\t\texcept:\n\t\t\traise Exception(\"Invalid label: '%s'\" % label)\n\t\n\tfor label in labels:\n\t\t_validate_label(label)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\tresource_name_regex = re.compile(r'^[a-z0-9-_]+$')\n\treturn resource_name_regex\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = r'^(?!_)(?!-)[a-zA-Z0-9-]+'\n\treturn resource_name_regex\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tname_regex = r\"^[a-zA-Z0-9_-]+\"\n\treturn name_regex\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\tresource_name_regex = r\"^(?P<resource_type>\\w+)-(?P<resource_name>\\w+)\"\n\t\n\treturn resource_name_regex\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = []\n\tfor resource_type in ['aws_krake_application', 'aws_krake_application_version', 'aws_krake_cluster', 'aws_krake_deployment', 'aws_krake_deployment_group', 'aws_krake_namespace', 'aws_krake_pipeline']:\n\t\tresource_name_regex.append(r'^{0}$'.format(resource_type))\n\n\treturn resource_name_regex\n\n#source/krake/resources/aws_krake_application.py\n# Copyright 2019 VMware, Inc.\n# SPDX-License-Identifier: MPL-2.0\n\nimport botocore.exceptions\n\nfrom . import _get_resource_name_regex\nfrom . import _get_resource_type_regex\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tretval = re.compile(r\"^[A-Za-z0-9][A-Za-z0-9-]{0,31}$\")\n\treturn retval\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = [\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9])$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*)$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9])$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*)$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = [\n\t\tr\"^(?P<prefix>\\w+)-(?P<name>\\w+)$\",\n\t\tr\"^(?P<prefix>\\w+)-(?P<name>\\w+)-(?P<suffix>\\w+)$\",\n\t\tr\"^(?P<name>\\w+)$\",\n\t\tr\"^(?P<name>\\w+)-(?P<suffix>\\w+)$\",\n\t\tr\"^(?P<name>\\w+)-(?P<suffix>\\w+)-(?P<suffix>\\w+)$\",\n\t]\n\treturn resource_name_regex\n\n", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\treturn re.compile('^[a-zA-Z0-9_]+$')", "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\tresource_name_regex = re.compile(\n\t\tr'^[a-zA-Z0-9][a-zA-Z0-9-]{0,62}'\n\t)\n\n\treturn resource_name_regex\n\n#krake/resources/__init__.py\nfrom .base import BaseResource\nfrom .iam_binding import IamBinding\nfrom .iam_policy import IamPolicy\nfrom .iam_role import IamRole\nfrom .iam_user import IamUser"], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\tresource_name_regex = re.compile(r'^[a-z0-9-_]+$')\n\treturn resource_name_regex\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = r'^(?!_)(?!-)[a-zA-Z0-9-]+'\n\treturn resource_name_regex\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tname_regex = r\"^[a-zA-Z0-9_-]+\"\n\treturn name_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\tresource_name_regex = r\"^(?P<resource_type>\\w+)-(?P<resource_name>\\w+)\"\n\t\n\treturn resource_name_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = []\n\tfor resource_type in ['aws_krake_application', 'aws_krake_application_version', 'aws_krake_cluster', 'aws_krake_deployment', 'aws_krake_deployment_group', 'aws_krake_namespace', 'aws_krake_pipeline']:\n\t\tresource_name_regex.append(r'^{0}$'.format(resource_type))\n\n\treturn resource_name_regex\n\n#source/krake/resources/aws_krake_application.py\n# Copyright 2019 VMware, Inc.\n# SPDX-License-Identifier: MPL-2.0\n\nimport botocore.exceptions\n\nfrom . import _get_resource_name_regex\nfrom . import _get_resource_type_regex\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tretval = re.compile(r\"^[A-Za-z0-9][A-Za-z0-9-]{0,31}$\")\n\treturn retval\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = [\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9])$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*)$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9])$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9][a-zA-Z0-9-]*)$\",\n\t\tr\"^(?P<name>[a-zA-Z0-9][a-zA-Z0-", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\tresource_name_regex = [\n\t\tr\"^(?P<prefix>\\w+)-(?P<name>\\w+)$\",\n\t\tr\"^(?P<prefix>\\w+)-(?P<name>\\w+)-(?P<suffix>\\w+)$\",\n\t\tr\"^(?P<name>\\w+)$\",\n\t\tr\"^(?P<name>\\w+)-(?P<suffix>\\w+)$\",\n\t\tr\"^(?P<name>\\w+)-(?P<suffix>\\w+)-(?P<suffix>\\w+)$\",\n\t]\n\treturn resource_name_regex\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\timport re\n\treturn re.compile('^[a-zA-Z0-9_]+$')", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\t\"\"\"\n\tReturn the regular expressions that are used to validate the name of the Krake resources\n\t\"\"\"\n\t\n\tresource_name_regex = re.compile(\n\t\tr'^[a-zA-Z0-9][a-zA-Z0-9-]{0,62}'\n\t)\n\n\treturn resource_name_regex\n\n#krake/resources/__init__.py\nfrom .base import BaseResource\nfrom .iam_binding import IamBinding\nfrom .iam_policy import IamPolicy\nfrom .iam_role import IamRole\nfrom .iam_user import IamUser", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\tvalid_values = {\n\t\t'int': '^-?[0-9]+$',\n\t\t'float': '^-?[0-9]+\\.[0-9]+$',\n\t\t'string': '^[a-zA-Z0-9\\-\\_\\.]+$',\n\t\t'boolean': '^(true|false)$',\n\t\t'datetime': '^([0-9]{4})-([0-9]{2})-([0-9]{2}) ([0-9]{2}):([0-9]{2}):([0-9]{2})$',\n\t\t'date': '^([0-9]{4})-([0-9]{2})-([0-9]{2})$',\n\t}\n\t\n\ttry:\n\t\tregex = valid_values[value['type']]\n\texcept KeyError:\n\t\traise AttributeError('Invalid type: ' + value['type'])\n\t\n\tif not re.match(regex, value['value']):\n\t\traise AttributeError('Invalid value: ' + value['value'])\n\t\n\treturn True\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tregex = {\n\t\t'email': '^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$',\n\t\t'phone': '^[0-9]+$',\n\t\t'url': '^http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[^a-zA-Z0-9-]|(?:%[0-9a-fA-F][0-9a-fA-F]))+$',\n\t\t'ip': '^([0-9]{1,3}.){3}[0-9]{1,3}$',\n\t\t'hex': '^([0-9a-fA-F]{2}){3}$',\n\t\t'binary': '^(([0-1]+)+)$'\n\t}\n\n\tif value and regex.get(type(value)):\n\t\treturn re.search(regex[type(value)], value)\n\telse:\n\t\treturn False\n\n#tests/test_validate_value.py\nfrom validate_value import validate_value\nimport unittest\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\ttry:\n\t\tmatch = re.match(config['regex'], value)\n\t\tif match:\n\t\t\treturn match.group(1)\n\t\telse:\n\t\t\traise ValueError\n\texcept ValueError:\n\t\traise ValueError('Invalid value \"%s.\"' % value)\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tis_valid = re.match(self.value_regex, value)\n\treturn is_valid is not None\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\t# If the parameter is a string, check if it is a valid number.\n\tif type(value) == str:\n\t\ttry:\n\t\t\tfloat(value)\n\t\t\treturn True\n\t\texcept ValueError:\n\t\t\treturn False\n\t\n\t# Otherwise, check if it is a valid boolean.\n\telse:\n\t\tvalid_booleans = [True, False]\n\t\treturn value in valid_booleans\n\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tmatch = re.match(value.get(\"regex\"), value.get(\"value\"))\n\tif match:\n\t\tif len(match.groups()) == 1:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\timport re\n\t\n\tvalid = False\n\t\n\tregex = {\n\t\t'boolean': re.compile(r'^(true|false)$'),\n\t\t'integer': re.compile(r'^\\d+$'),\n\t\t'number': re.compile(r'^\\d+\\.\\d+|^\\d+$'),\n\t\t'string': re.compile(r'^\"([^\"]|\\\\\")*\"$')\n\t}\n\t\n\tregex_name = value['type']\n\t\n\tif regex_name in regex:\n\t\tregex_pattern = regex[regex_name]\n\t\tmatch = regex_pattern.match(value['value'])\n\t\t\n\t\tif match:\n\t\t\tvalid = True\n\t\n\treturn valid\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tvalue = str(value)\n\tif value == \"\":\n\t\treturn True\n\tif value.isdigit():\n\t\treturn True\n\tif value.isdecimal():\n\t\treturn True\n\tif value.isalpha():\n\t\treturn True\n\tif value.islower():\n\t\treturn True\n\tif value.isupper():\n\t\treturn True\n\tif value.isspace():\n\t\treturn True\n\tif value.isidentifier():\n\t\treturn True\n\tif value.isalnum():\n\t\treturn True\n\treturn False\n\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\tvalue = value.strip()\n\n\tif value.isdigit():\n\t\treturn int(value)\n\telif value.isalpha():\n\t\treturn str(value)\n\telif value.lower() in ('true', 'false'):\n\t\treturn bool(value)\n\telif value.lower() in ('null', 'none'):\n\t\treturn None\n\n#src/models/base.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nfrom src.settings import settings\n\nSQLALCHEMY_DATABASE_URL = settings.DB_URL\n\nengine = create_engine(\n\tSQLALCHEMY_DATABASE_URL,\n)\n\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\n#src/models/category.py\nfrom sqlalchemy import Column, Integer, String\n\nfrom .base import Base\n", "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\ttry:\n\t\tre.match(value, value)\n\t\treturn True\n\texcept:\n\t\treturn False\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\tvalid_values = {\n\t\t'int': '^-?[0-9]+$',\n\t\t'float': '^-?[0-9]+\\.[0-9]+$',\n\t\t'string': '^[a-zA-Z0-9\\-\\_\\.]+$',\n\t\t'boolean': '^(true|false)$',\n\t\t'datetime': '^([0-9]{4})-([0-9]{2})-([0-9]{2}) ([0-9]{2}):([0-9]{2}):([0-9]{2})$',\n\t\t'date': '^([0-9]{4})-([0-9]{2})-([0-9]{2})$',\n\t}\n\t\n\ttry:\n\t\tregex = valid_values[value['type']]\n\texcept KeyError:\n\t\traise AttributeError('Invalid type: ' + value['type'])\n\t\n\tif not re.match(regex, value['value']):\n\t\traise AttributeError('Invalid value: ' + value['value'])\n\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tregex = {\n\t\t'email': '^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$',\n\t\t'phone': '^[0-9]+$',\n\t\t'url': '^http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[^a-zA-Z0-9-]|(?:%[0-9a-fA-F][0-9a-fA-F]))+$',\n\t\t'ip': '^([0-9]{1,3}.){3}[0-9]{1,3}$',\n\t\t'hex': '^([0-9a-fA-F]{2}){3}$',\n\t\t'binary': '^(([0-1]+)+)$'\n\t}\n\n\tif value and regex.get(type(value)):\n\t\treturn re.search(regex[type(value)], value)\n\telse:\n\t\treturn False\n\n#tests/test_validate_value.py\nfrom validate_value import validate_value\nimport unittest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\ttry:\n\t\tmatch = re.match(config['regex'], value)\n\t\tif match:\n\t\t\treturn match.group(1)\n\t\telse:\n\t\t\traise ValueError\n\texcept ValueError:\n\t\traise ValueError('Invalid value \"%s.\"' % value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tis_valid = re.match(self.value_regex, value)\n\treturn is_valid is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\t# If the parameter is a string, check if it is a valid number.\n\tif type(value) == str:\n\t\ttry:\n\t\t\tfloat(value)\n\t\t\treturn True\n\t\texcept ValueError:\n\t\t\treturn False\n\t\n\t# Otherwise, check if it is a valid boolean.\n\telse:\n\t\tvalid_booleans = [True, False]\n\t\treturn value in valid_booleans\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tmatch = re.match(value.get(\"regex\"), value.get(\"value\"))\n\tif match:\n\t\tif len(match.groups()) == 1:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\timport re\n\t\n\tvalid = False\n\t\n\tregex = {\n\t\t'boolean': re.compile(r'^(true|false)$'),\n\t\t'integer': re.compile(r'^\\d+$'),\n\t\t'number': re.compile(r'^\\d+\\.\\d+|^\\d+$'),\n\t\t'string': re.compile(r'^\"([^\"]|\\\\\")*\"$')\n\t}\n\t\n\tregex_name = value['type']\n\t\n\tif regex_name in regex:\n\t\tregex_pattern = regex[regex_name]\n\t\tmatch = regex_pattern.match(value['value'])\n\t\t\n\t\tif match:\n\t\t\tvalid = True\n\t\n\treturn valid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\tvalue = str(value)\n\tif value == \"\":\n\t\treturn True\n\tif value.isdigit():\n\t\treturn True\n\tif value.isdecimal():\n\t\treturn True\n\tif value.isalpha():\n\t\treturn True\n\tif value.islower():\n\t\treturn True\n\tif value.isupper():\n\t\treturn True\n\tif value.isspace():\n\t\treturn True\n\tif value.isidentifier():\n\t\treturn True\n\tif value.isalnum():\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\t\n\tvalue = value.strip()\n\n\tif value.isdigit():\n\t\treturn int(value)\n\telif value.isalpha():\n\t\treturn str(value)\n\telif value.lower() in ('true', 'false'):\n\t\treturn bool(value)\n\telif value.lower() in ('null', 'none'):\n\t\treturn None\n\n#src/models/base.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\n\nfrom src.settings import settings\n\nSQLALCHEMY_DATABASE_URL = settings.DB_URL\n\nengine = create_engine(\n\tSQLALCHEMY_DATABASE_URL,\n)\n\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase = declarative_base()\n\n#src/models/category.py\nfrom sqlalchemy import Column, Integer, String\n\nfrom .base import Base\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\t\"\"\"\n\tValidate the given value against the corresponding regular expression.\n\t\"\"\"\n\ttry:\n\t\tre.match(value, value)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tregex = r\"^[A-Z]{1,2}[\\d]{1,2}[A-Z]{1,2}$\"\n\tif re.match(regex, key):\n\t\treturn True\n\treturn False\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\tregexes = {\n\t\t'account': '^[a-z0-9_]{3,15}$',\n\t\t'password': '^[a-z0-9_]{6,16}$',\n\t\t'name': '^[ a-zA-Z0-9_\\-\\.]+$',\n\t\t'email': '^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,})$'\n\t}\n\t\n\tif key in regexes.keys():\n\t\treturn True\n\telse:\n\t\treturn False\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tis_valid = False\n\tif \":\" in key:\n\t\t# \"key:value\"\n\t\tkey_type, key_value = key.split(\":\")\n\t\tif key_type == \"file\" and key_value.startswith(\"/\"):\n\t\t\t# \"file:/path/to/file\"\n\t\t\tis_valid = True\n\t\telif key_type == \"file\":\n\t\t\t# \"file:path/to/file\"\n\t\t\tis_valid = True\n\t\telif key_type == \"directory\" and key_value.startswith(\"/\"):\n\t\t\t# \"directory:/path/to/directory\"\n\t\t\tis_valid = True\n\t\telif key_type == \"directory\":\n\t\t\t# \"directory:path/to/directory\"\n\t\t\tis_valid = True\n\telif key.isdigit():\n\t\t# \"key:value\"\n\t\tis_valid = True\n\telif key.startswith(\":\"):\n\t\t# \"key:value\"\n\t\tis_valid = True\n\n\treturn is_valid\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tfrom re import search\n\tfrom sys import argv\n\t\n\twith open(\"keys.txt\", \"r\") as keys:\n\t\tfor keyfile in keys.readlines():\n\t\t\tkeyfile = keyfile.strip()\n\t\t\tif keyfile == key:\n\t\t\t\treturn True\n\t\t\tif search(keyfile, key):\n\t\t\t\treturn True\n\t\n\tprint(\"Error: Invalid key.\")\n\tprint(\"Usage:\", argv[0], \"<key>\")\n\texit(1)\n\t", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tvalid = True\n\t\n\tif len(key) != 20:\n\t\tvalid = False\n\t\n\telif key[0] != '0':\n\t\tvalid = False\n\t\n\telse:\n\t\tfor i in range(1, len(key)):\n\t\t\tif key[i] == '-':\n\t\t\t\tvalid = False\n\t\t\t\tbreak\n\t\n\treturn valid\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tmatches = key.match(r\"^[a-zA-Z0-9_]+$\")\n\treturn matches != None\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tpattern = re.compile('^[a-zA-Z0-9]{8}$')\n\treturn pattern.match(key)\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\tkey = key.lower()\n\t\n\tif key.startswith('_'):\n\t\treturn False\n\t\n\tif key.startswith('__') and len(key) > 1:\n\t\treturn False\n\t\n\tif key.endswith('_') or key.endswith('__'):\n\t\treturn False\n\t\n\tif len(key.split('_')) > 1:\n\t\treturn False\n\t\n\tif re.search('^[a-z0-9_]+$', key):\n\t\treturn True\n\t\n\treturn False\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tkey_regex = re.compile(r\"^\\d{5}-\\d{4}$\")\n\treturn key_regex.match(key)\n\n", "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tkey = key.lower()\n\ttry:\n\t\tif re.compile(key):\n\t\t\treturn True\n\texcept:\n\t\tpass\n\treturn False\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tregex = r\"^[A-Z]{1,2}[\\d]{1,2}[A-Z]{1,2}$\"\n\tif re.match(regex, key):\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\tregexes = {\n\t\t'account': '^[a-z0-9_]{3,15}$',\n\t\t'password': '^[a-z0-9_]{6,16}$',\n\t\t'name': '^[ a-zA-Z0-9_\\-\\.]+$',\n\t\t'email': '^[_a-z0-9-]+(\\.[_a-z0-9-]+)*@[a-z0-9-]+(\\.[a-z0-9-]+)*(\\.[a-z]{2,})$'\n\t}\n\t\n\tif key in regexes.keys():\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tis_valid = False\n\tif \":\" in key:\n\t\t# \"key:value\"\n\t\tkey_type, key_value = key.split(\":\")\n\t\tif key_type == \"file\" and key_value.startswith(\"/\"):\n\t\t\t# \"file:/path/to/file\"\n\t\t\tis_valid = True\n\t\telif key_type == \"file\":\n\t\t\t# \"file:path/to/file\"\n\t\t\tis_valid = True\n\t\telif key_type == \"directory\" and key_value.startswith(\"/\"):\n\t\t\t# \"directory:/path/to/directory\"\n\t\t\tis_valid = True\n\t\telif key_type == \"directory\":\n\t\t\t# \"directory:path/to/directory\"\n\t\t\tis_valid = True\n\telif key.isdigit():\n\t\t# \"key:value\"\n\t\tis_valid = True\n\telif key.startswith(\":\"):\n\t\t# \"key:value\"\n\t\tis_valid = True\n\n\treturn is_valid\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tfrom re import search\n\tfrom sys import argv\n\t\n\twith open(\"keys.txt\", \"r\") as keys:\n\t\tfor keyfile in keys.readlines():\n\t\t\tkeyfile = keyfile.strip()\n\t\t\tif keyfile == key:\n\t\t\t\treturn True\n\t\t\tif search(keyfile, key):\n\t\t\t\treturn True\n\t\n\tprint(\"Error: Invalid key.\")\n\tprint(\"Usage:\", argv[0], \"<key>\")\n\texit(1)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tvalid = True\n\t\n\tif len(key) != 20:\n\t\tvalid = False\n\t\n\telif key[0] != '0':\n\t\tvalid = False\n\t\n\telse:\n\t\tfor i in range(1, len(key)):\n\t\t\tif key[i] == '-':\n\t\t\t\tvalid = False\n\t\t\t\tbreak\n\t\n\treturn valid\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tmatches = key.match(r\"^[a-zA-Z0-9_]+$\")\n\treturn matches != None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tpattern = re.compile('^[a-zA-Z0-9]{8}$')\n\treturn pattern.match(key)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\t\n\tkey = key.lower()\n\t\n\tif key.startswith('_'):\n\t\treturn False\n\t\n\tif key.startswith('__') and len(key) > 1:\n\t\treturn False\n\t\n\tif key.endswith('_') or key.endswith('__'):\n\t\treturn False\n\t\n\tif len(key.split('_')) > 1:\n\t\treturn False\n\t\n\tif re.search('^[a-z0-9_]+$', key):\n\t\treturn True\n\t\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\timport re\n\tkey_regex = re.compile(r\"^\\d{5}-\\d{4}$\")\n\treturn key_regex.match(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\t\"\"\"\n\tValidate the given key against the corresponding regular expression.\n\t\"\"\"\n\tkey = key.lower()\n\ttry:\n\t\tif re.compile(key):\n\t\t\treturn True\n\texcept:\n\t\tpass\n\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tfor key, value in manifest_dict.iteritems():\n\t\tif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\t\telse:\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = [generate_default_observer_schema_dict(item) if isinstance(item, dict) else item for item in value]\n\n\t\telif isinstance(value, basestring):\n\t\t\tnew_dict[key] = value\n\n\t\telif isinstance(value, bool):\n\t\t\tnew_dict[key] = str(value).lower()\n\n\t\telif isinstance(value, int):\n\t\t\tnew_dict[key] = value\n\n\t\telif isinstance(value, float):\n\t\t\tnew_dict[key] = value\n\n\t\telse:\n\t\t\tnew_dict[key] = None\n\n\treturn new_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = dict()\n\t\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tnew_dict[key] = []\n\t\t\tfor element in value:\n\t\t\t\tif isinstance(element, dict):\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(element))\n\t\t\t\telif isinstance(element, list):\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(element, True))\n\t\t\t\telse:\n\t\t\t\t\tnew_dict[key].append(element)\n\t\telif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\t\telse:\n\t\t\t\tnew_dict[key] = dict()\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tnew_dict[key] = value\n\t\n\treturn new_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tschema_dict[key] = {}\n\t\t\telse:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\telif isinstance(value, list):\n\t\t\tschema_dict[key] = []\n\t\telse:\n\t\t\tschema_dict[key] = value\n\treturn schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tif first_level:\n\t\tschema_dict['name'] = 'observer'\n\t\tschema_dict['type'] = 'object'\n\t\tschema_dict['properties'] = {}\n\telse:\n\t\tschema_dict['type'] = 'object'\n\t\tproperties = {'name': {\n\t\t\t'type': 'string',\n\t\t\t'description': 'Name of the observer.',\n\t\t\t'example': 'observer-1',\n\t\t\t'required': False\n\t\t}}\n\t\tschema_dict['properties'].update(properties)\n\n\tsub_schema_dict = {'type': 'object', 'properties': {}}\n\tif manifest_dict['type'] == 'dict':\n\t\tsub_schema_dict['type'] = 'object'\n\t\tsub_schema_dict['properties'] = manifest_dict['properties']\n\t\tschema_dict['properties'] = sub_schema_dict\n\telif manifest_dict['type'] == 'list':\n\t\tsub_schema_dict['type'] = 'array'\n\t\tsub_schema_dict['items'] = manifest_dict['items']\n\t\tschema_dict['properties'] = sub_schema_dict\n\telse:\n\t\tschema_dict['properties'] = manifest_dict\n\treturn schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\n\tif first_level:\n\t\tschema_dict[\"version\"] = \"1.0.0\"\n\t\tschema_dict[\"title\"] = \"Default observer schema\"\n\n\telif \"schema\" in manifest_dict:\n\t\tschema_dict[\"schema\"] = manifest_dict[\"schema\"]\n\n\telif \"schema_version\" in manifest_dict:\n\t\tschema_dict[\"schema_version\"] = manifest_dict[\"schema_version\"]\n\n\telif \"schema_version_date\" in manifest_dict:\n\t\tschema_dict[\"schema_version_date\"] = manifest_dict[\"schema_version_date\"]\n\n\telif \"schema_version_description\" in manifest_dict:\n\t\tschema_dict[\"schema_version_description\"] = manifest_dict[\"schema_version_description\"]\n\n\telif \"schema_version_url\" in manifest_dict:\n\t\tschema_dict[\"schema_version_url\"] = manifest_dict[\"schema_version_url\"]\n\n\telse:\n\t\traise Exception(\"manifest_dict doesn't contain expected keys\")\n\n\treturn schema_dict", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tnew_dict[key] = generate_default_observer_schema_value(value, first_level)\n\treturn new_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\tschema_dict = dict()\n\t\n\tif first_level:\n\t\t# Create a placeholder dict for the first time\n\t\tfor key in manifest_dict:\n\t\t\tschema_dict[key] = dict()\n\t\n\tfor key in manifest_dict:\n\t\tif isinstance(manifest_dict[key], dict):\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], True)\n\t\telse:\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\n\treturn schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\t\n\tfor key, value in manifest_dict.iteritems():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = []\n\t\t\tfor v in value:\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(v, first_level=True))\n\t\t\t\telif isinstance(v, list):\n\t\t\t\t\tnew_dict[key].append([])\n\t\t\t\t\tfor v2 in v:\n\t\t\t\t\t\tif isinstance(v2, dict):\n\t\t\t\t\t\t\tnew_dict[key][-1].append(generate_default_observer_schema_dict(v2, first_level=True))\n\t\t\t\t\t\telif isinstance(v2, list):\n\t\t\t\t\t\t\tnew_dict[key][-1].append([])\n\t\t\t\t\t\t\tfor v3 in v2:\n\t\t\t\t\t\t\t\tif isinstance(v3, dict):\n\t\t\t\t\t\t\t\t\tnew_dict[key][-1][-1].append(generate_default_observer_schema_dict(v3, first_level=True))\n\t\t\t\t\t\t\t\telif isinstance", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tif manifest_dict['type'] == 'object':\n\t\tschema_dict['type'] = 'object'\n\t\tfor key, value in manifest_dict['properties'].items():\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value, first_level)\n\telif manifest_dict['type'] == 'array':\n\t\tschema_dict['type'] = 'array'\n\t\tschema_dict['items'] = generate_default_observer_schema_dict(manifest_dict['items'], first_level)\n\telif manifest_dict['type'] in ['string', 'number']:\n\t\tschema_dict['type'] = manifest_dict['type']\n\telse:\n\t\tschema_dict['type'] = 'string'\n\treturn schema_dict\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\n\tfor key, value in manifest_dict.items():\n\t\tif first_level:\n\t\t\tschema_dict[key] = key\n\t\telse:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\t\telif isinstance(value, list):\n\t\t\t\tschema_dict[key] = []\n\t\t\t\tfor item in value:\n\t\t\t\t\tif isinstance(item, dict):\n\t\t\t\t\t\tschema_dict[key].append(generate_default_observer_schema_dict(item, True))\n\t\t\t\t\telse:\n\t\t\t\t\t\tschema_dict[key].append(item)\n\t\t\telse:\n\t\t\t\tschema_dict[key] = value\n\n\treturn schema_dict\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tfor key, value in manifest_dict.iteritems():\n\t\tif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\t\telse:\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = [generate_default_observer_schema_dict(item) if isinstance(item, dict) else item for item in value]\n\n\t\telif isinstance(value, basestring):\n\t\t\tnew_dict[key] = value\n\n\t\telif isinstance(value, bool):\n\t\t\tnew_dict[key] = str(value).lower()\n\n\t\telif isinstance(value, int):\n\t\t\tnew_dict[key] = value\n\n\t\telif isinstance(value, float):\n\t\t\tnew_dict[key] = value\n\n\t\telse:\n\t\t\tnew_dict[key] = None\n\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = dict()\n\t\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tnew_dict[key] = []\n\t\t\tfor element in value:\n\t\t\t\tif isinstance(element, dict):\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(element))\n\t\t\t\telif isinstance(element, list):\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(element, True))\n\t\t\t\telse:\n\t\t\t\t\tnew_dict[key].append(element)\n\t\telif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\t\telse:\n\t\t\t\tnew_dict[key] = dict()\n\t\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tnew_dict[key] = value\n\t\n\treturn new_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tif first_level:\n\t\t\t\tschema_dict[key] = {}\n\t\t\telse:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\telif isinstance(value, list):\n\t\t\tschema_dict[key] = []\n\t\telse:\n\t\t\tschema_dict[key] = value\n\treturn schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tif first_level:\n\t\tschema_dict['name'] = 'observer'\n\t\tschema_dict['type'] = 'object'\n\t\tschema_dict['properties'] = {}\n\telse:\n\t\tschema_dict['type'] = 'object'\n\t\tproperties = {'name': {\n\t\t\t'type': 'string',\n\t\t\t'description': 'Name of the observer.',\n\t\t\t'example': 'observer-1',\n\t\t\t'required': False\n\t\t}}\n\t\tschema_dict['properties'].update(properties)\n\n\tsub_schema_dict = {'type': 'object', 'properties': {}}\n\tif manifest_dict['type'] == 'dict':\n\t\tsub_schema_dict['type'] = 'object'\n\t\tsub_schema_dict['properties'] = manifest_dict['properties']\n\t\tschema_dict['properties'] = sub_schema_dict\n\telif manifest_dict['type'] == 'list':\n\t\tsub_schema_dict['type'] = 'array'\n\t\tsub_schema_dict['items'] = manifest_dict['items']\n\t\tschema_dict['properties'] = sub_schema_dict\n\telse:\n\t\tschema_dict['properties'] = manifest_dict\n\treturn schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\n\tif first_level:\n\t\tschema_dict[\"version\"] = \"1.0.0\"\n\t\tschema_dict[\"title\"] = \"Default observer schema\"\n\n\telif \"schema\" in manifest_dict:\n\t\tschema_dict[\"schema\"] = manifest_dict[\"schema\"]\n\n\telif \"schema_version\" in manifest_dict:\n\t\tschema_dict[\"schema_version\"] = manifest_dict[\"schema_version\"]\n\n\telif \"schema_version_date\" in manifest_dict:\n\t\tschema_dict[\"schema_version_date\"] = manifest_dict[\"schema_version_date\"]\n\n\telif \"schema_version_description\" in manifest_dict:\n\t\tschema_dict[\"schema_version_description\"] = manifest_dict[\"schema_version_description\"]\n\n\telif \"schema_version_url\" in manifest_dict:\n\t\tschema_dict[\"schema_version_url\"] = manifest_dict[\"schema_version_url\"]\n\n\telse:\n\t\traise Exception(\"manifest_dict doesn't contain expected keys\")\n\n\treturn schema_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tnew_dict[key] = generate_default_observer_schema_value(value, first_level)\n\treturn new_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\t\n\tschema_dict = dict()\n\t\n\tif first_level:\n\t\t# Create a placeholder dict for the first time\n\t\tfor key in manifest_dict:\n\t\t\tschema_dict[key] = dict()\n\t\n\tfor key in manifest_dict:\n\t\tif isinstance(manifest_dict[key], dict):\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key], True)\n\t\telse:\n\t\t\tschema_dict[key] = manifest_dict[key]\n\t\n\treturn schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tnew_dict = {}\n\t\n\tfor key, value in manifest_dict.iteritems():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value, first_level=True)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = []\n\t\t\tfor v in value:\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(v, first_level=True))\n\t\t\t\telif isinstance(v, list):\n\t\t\t\t\tnew_dict[key].append([])\n\t\t\t\t\tfor v2 in v:\n\t\t\t\t\t\tif isinstance(v2, dict):\n\t\t\t\t\t\t\tnew_dict[key][-1].append(generate_default_observer_schema_dict(v2, first_level=True))\n\t\t\t\t\t\telif isinstance(v2, list):\n\t\t\t\t\t\t\tnew_dict[key][-1].append([])\n\t\t\t\t\t\t\tfor v3 in v2:\n\t\t\t\t\t\t\t\tif isinstance(v3, dict):\n\t\t\t\t\t\t\t\t\tnew_dict[key][-1][-1].append(generate_default_observer_schema_dict(v3, first_level=True))\n\t\t\t\t\t\t\t\telif isinstance", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\tif manifest_dict['type'] == 'object':\n\t\tschema_dict['type'] = 'object'\n\t\tfor key, value in manifest_dict['properties'].items():\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value, first_level)\n\telif manifest_dict['type'] == 'array':\n\t\tschema_dict['type'] = 'array'\n\t\tschema_dict['items'] = generate_default_observer_schema_dict(manifest_dict['items'], first_level)\n\telif manifest_dict['type'] in ['string', 'number']:\n\t\tschema_dict['type'] = manifest_dict['type']\n\telse:\n\t\tschema_dict['type'] = 'string'\n\treturn schema_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t\"\"\"\n\tThe values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.\n\t\"\"\"\n\tschema_dict = {}\n\n\tfor key, value in manifest_dict.items():\n\t\tif first_level:\n\t\t\tschema_dict[key] = key\n\t\telse:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value, True)\n\t\t\telif isinstance(value, list):\n\t\t\t\tschema_dict[key] = []\n\t\t\t\tfor item in value:\n\t\t\t\t\tif isinstance(item, dict):\n\t\t\t\t\t\tschema_dict[key].append(generate_default_observer_schema_dict(item, True))\n\t\t\t\t\telse:\n\t\t\t\t\t\tschema_dict[key].append(item)\n\t\t\telse:\n\t\t\t\tschema_dict[key] = value\n\n\treturn schema_dict\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = [\n\t\t\"lastAppliedManifest\",\n\t\t\"lastAppliedManifestHash\",\n\t\t\"observerSchema\",\n\t]\n\tfor observed_field in observed_fields:\n\t\tvalue = response.get(observed_field, None)\n\t\tif value is None:\n\t\t\tcontinue\n\t\tif observed_field == \"lastAppliedManifest\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, value\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[observed_field] = value\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfields_with_observer = get_fields_with_observer(observer_schema)\n\tfor observed_field in fields_with_observer:\n\t\tobserved_field_value = get_observed_field_value(response, observed_field)\n\t\tif observed_field_value is None:\n\t\t\tcontinue\n\t\tif isinstance(observed_field_value, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(last_applied_manifest,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observed_field_value,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observer_schema)\n\t\t\tcontinue\n\t\tif isinstance(observed_field_value, list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(last_applied_manifest,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observed_field_value,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observer_schema)\n\t\t\tcontinue\n\t\tlast_applied_manifest[observed_field] = observed_field_value\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tlast_applied_manifest_dict = {}\n\t\n\tlast_applied_manifest_dict['observer_schema'] = observer_schema\n\t\n\tfor observed_field in last_applied_manifest:\n\t\tfor observed_obj in last_applied_manifest_dict['observer_schema']:\n\t\t\tif observed_obj['name'] == observed_field:\n\t\t\t\tlast_applied_manifest_dict[observed_field] = {\n\t\t\t\t\t'last_applied_version': observed_obj['version'],\n\t\t\t\t\t'last_applied_manifest': {}\n\t\t\t\t}\n\n\tapplied_version = response.get('status', {}).get('applied_version')\n\tif applied_version:\n\t\tlast_applied_manifest_dict['last_applied_version'] = applied_version\n\t\n\tfor obj_name, observed_obj in last_applied_manifest_dict.items():\n\t\tif obj_name == 'observer_schema':\n\t\t\tcontinue\n\n\t\t# This is a list of observed field names for this object\n\t\tobserved_object_fields = [\n\t\t\tobserved_obj['name']\n\t\t]\n\n\t\t# This is a list of observed fields for this object,\n\t\t# and their respective versions\n\t\tobserved_object_versions = [\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserver_schema_index = get_observer_schema_index(observer_schema)\n\tfor field in response:\n\t\tif field[\"field\"] in last_applied_manifest:\n\t\t\tlast_applied_manifest[field[\"field\"]] = field[\"value\"]\n\t\telse:\n\t\t\tlast_applied_manifest[field[\"field\"]] = {\n\t\t\t\t\"value\": field[\"value\"],\n\t\t\t\t\"observer_schema\": observer_schema_index,\n\t\t\t}\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserver_schema = observer_schema or []\n\tresponse = response or []\n\n\t# Update the last applied manifest from the response\n\tfor r in response:\n\t\tr_fields = r.get('fields', [])\n\t\tfor r_field in r_fields:\n\t\t\tfield_name = r_field.get('name', None)\n\t\t\tvalue = r_field.get('value', None)\n\t\t\tlast_applied_manifest = update_last_applied_manifest_item(\n\t\t\t\t\tlast_applied_manifest,\n\t\t\t\t\tobserver_schema,\n\t\t\t\t\tfield_name,\n\t\t\t\t\tvalue,\n\t\t\t\t\t)\n\n\t# Recursively update last applied manifest from the response\n\tfor observer_field in observer_schema:\n\t\tobserver_field_name = observer_field.get('name', None)\n\t\tobserver_field_type = observer_field.get('type', None)\n\t\tif observer_field_type == 'array':\n\t\t\t# If this is an array field, iterate over the elements\n\t\t\tobserver_field_items = observer_field.get('items', {})\n\t\t\tobserver_field_items_observer_schema = observer_field_items.get(\n\t\t\t\t\t'observer_schema', None)\n\t\t\tobserver_field_items_", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfield_schema = {\n\t\t\"name\": \"update_last_applied_manifest_list_from_resp\",\n\t\t\"type\": \"object\",\n\t\t\"properties\": {\n\t\t\t\"observer_schema\": {\n\t\t\t\t\"type\": \"array\",\n\t\t\t\t\"items\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [\"name\", \"type\"],\n\t\t\t\t},\n\t\t\t},\n\t\t\t\"response\": {\n\t\t\t\t\"type\": \"array\",\n\t\t\t\t\"items\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"kind\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"metadata\": {\n\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"required\": [\"name\"],\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [\"kind\", \"metadata\"],\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t\"required\": [\"observer_schema\", \"response\"],\n\t}\n\n\tschema_helper.validate_object(field_schema, last_applied_manifest)\n\tschema_helper.", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tif not response:\n\t\treturn\n\n\t# Update last_applied_manifest\n\tfor field in observer_schema:\n\t\tif not field.get(\"value\"):\n\t\t\tif field.get(\"name\") in last_applied_manifest:\n\t\t\t\tcontinue\n\t\t\tlast_applied_manifest[field.get(\"name\")] = field.get(\"value\")\n\n\t# Update last_applied_manifest recursively\n\tupdate_last_applied_manifest_from_resp(last_applied_manifest, field.get(\"children\"), response)\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields_map = {}\n\tfor observed_field in observer_schema:\n\t\tobserved_fields_map[observed_field[\"fieldName\"]] = observed_field\n\n\tfor resource_field in response:\n\t\tif resource_field[\"fieldName\"] in observed_fields_map:\n\t\t\tlast_applied_manifest = update_last_applied_manifest_dict_from_resp(\n\t\t\t    last_applied_manifest,\n\t\t\t    observed_fields_map[resource_field[\"fieldName\"]],\n\t\t\t    resource_field[\"fieldValue\"],\n\t\t\t)\n\n\treturn last_applied_manifest\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tupdated_last_applied_manifest = []\n\tfor observed_field in observer_schema:\n\t\tif observed_field.name == 'last_applied_manifest':\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserved_field.value,\n\t\t\t\tresponse\n\t\t\t)\n\t\telif type(observed_field.value) == list:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserved_field.value,\n\t\t\t\tresponse\n\t\t\t)\n\t\telse:\n\t\t\tupdated_last_applied_manifest.append(\n\t\t\t\tlast_applied_manifest[observed_field.name]\n\t\t\t)\n\treturn updated_last_applied_manifest\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef _set_item_from_resp(i, resp_item):\n\t\tif i >= len(last_applied_manifest):\n\t\t\traise ValueError(\n\t\t\t\t\"last_applied_manifest has length %d, but response has length %d\" % (len(last_applied_manifest), len(response))\n\t\t\t)\n\t\tif isinstance(resp_item, list):\n\t\t\t_set_item_from_resp(i, resp_item[0])\n\t\telif isinstance(resp_item, dict):\n\t\t\tfor key, value in resp_item.items():\n\t\t\t\t_set_item_from_resp(i, value)\n\t\telse:\n\t\t\tlast_applied_manifest[i][resp_item] = None\n\n\tdef _set_from_observer_schema(i, schema_item):\n\t\tif i >= len(last_applied_manifest):\n\t\t\traise ValueError(\n\t\t\t\t\"last_applied_manifest has length %d, but observer_schema has length %d\" % (len(last_applied_manifest), len(observer_schema))\n\t\t\t)\n\t\tif isinstance(schema_item, list):\n\t\t\t_set_from_observer_schema(i, schema_item[0])\n\t\telif isinstance(schema_"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = [\n\t\t\"lastAppliedManifest\",\n\t\t\"lastAppliedManifestHash\",\n\t\t\"observerSchema\",\n\t]\n\tfor observed_field in observed_fields:\n\t\tvalue = response.get(observed_field, None)\n\t\tif value is None:\n\t\t\tcontinue\n\t\tif observed_field == \"lastAppliedManifest\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest, observer_schema, value\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[observed_field] = value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfields_with_observer = get_fields_with_observer(observer_schema)\n\tfor observed_field in fields_with_observer:\n\t\tobserved_field_value = get_observed_field_value(response, observed_field)\n\t\tif observed_field_value is None:\n\t\t\tcontinue\n\t\tif isinstance(observed_field_value, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(last_applied_manifest,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observed_field_value,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observer_schema)\n\t\t\tcontinue\n\t\tif isinstance(observed_field_value, list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(last_applied_manifest,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observed_field_value,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t observer_schema)\n\t\t\tcontinue\n\t\tlast_applied_manifest[observed_field] = observed_field_value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tlast_applied_manifest_dict = {}\n\t\n\tlast_applied_manifest_dict['observer_schema'] = observer_schema\n\t\n\tfor observed_field in last_applied_manifest:\n\t\tfor observed_obj in last_applied_manifest_dict['observer_schema']:\n\t\t\tif observed_obj['name'] == observed_field:\n\t\t\t\tlast_applied_manifest_dict[observed_field] = {\n\t\t\t\t\t'last_applied_version': observed_obj['version'],\n\t\t\t\t\t'last_applied_manifest': {}\n\t\t\t\t}\n\n\tapplied_version = response.get('status', {}).get('applied_version')\n\tif applied_version:\n\t\tlast_applied_manifest_dict['last_applied_version'] = applied_version\n\t\n\tfor obj_name, observed_obj in last_applied_manifest_dict.items():\n\t\tif obj_name == 'observer_schema':\n\t\t\tcontinue\n\n\t\t# This is a list of observed field names for this object\n\t\tobserved_object_fields = [\n\t\t\tobserved_obj['name']\n\t\t]\n\n\t\t# This is a list of observed fields for this object,\n\t\t# and their respective versions\n\t\tobserved_object_versions = [\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserver_schema_index = get_observer_schema_index(observer_schema)\n\tfor field in response:\n\t\tif field[\"field\"] in last_applied_manifest:\n\t\t\tlast_applied_manifest[field[\"field\"]] = field[\"value\"]\n\t\telse:\n\t\t\tlast_applied_manifest[field[\"field\"]] = {\n\t\t\t\t\"value\": field[\"value\"],\n\t\t\t\t\"observer_schema\": observer_schema_index,\n\t\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserver_schema = observer_schema or []\n\tresponse = response or []\n\n\t# Update the last applied manifest from the response\n\tfor r in response:\n\t\tr_fields = r.get('fields', [])\n\t\tfor r_field in r_fields:\n\t\t\tfield_name = r_field.get('name', None)\n\t\t\tvalue = r_field.get('value', None)\n\t\t\tlast_applied_manifest = update_last_applied_manifest_item(\n\t\t\t\t\tlast_applied_manifest,\n\t\t\t\t\tobserver_schema,\n\t\t\t\t\tfield_name,\n\t\t\t\t\tvalue,\n\t\t\t\t\t)\n\n\t# Recursively update last applied manifest from the response\n\tfor observer_field in observer_schema:\n\t\tobserver_field_name = observer_field.get('name', None)\n\t\tobserver_field_type = observer_field.get('type', None)\n\t\tif observer_field_type == 'array':\n\t\t\t# If this is an array field, iterate over the elements\n\t\t\tobserver_field_items = observer_field.get('items', {})\n\t\t\tobserver_field_items_observer_schema = observer_field_items.get(\n\t\t\t\t\t'observer_schema', None)\n\t\t\tobserver_field_items_", "is_pass": true, "return_code": 0}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tfield_schema = {\n\t\t\"name\": \"update_last_applied_manifest_list_from_resp\",\n\t\t\"type\": \"object\",\n\t\t\"properties\": {\n\t\t\t\"observer_schema\": {\n\t\t\t\t\"type\": \"array\",\n\t\t\t\t\"items\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [\"name\", \"type\"],\n\t\t\t\t},\n\t\t\t},\n\t\t\t\"response\": {\n\t\t\t\t\"type\": \"array\",\n\t\t\t\t\"items\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"kind\": {\n\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\"metadata\": {\n\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\"name\": {\n\t\t\t\t\t\t\t\t\t\"type\": \"string\",\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"required\": [\"name\"],\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [\"kind\", \"metadata\"],\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t\"required\": [\"observer_schema\", \"response\"],\n\t}\n\n\tschema_helper.validate_object(field_schema, last_applied_manifest)\n\tschema_helper.", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\t\n\tif not response:\n\t\treturn\n\n\t# Update last_applied_manifest\n\tfor field in observer_schema:\n\t\tif not field.get(\"value\"):\n\t\t\tif field.get(\"name\") in last_applied_manifest:\n\t\t\t\tcontinue\n\t\t\tlast_applied_manifest[field.get(\"name\")] = field.get(\"value\")\n\n\t# Update last_applied_manifest recursively\n\tupdate_last_applied_manifest_from_resp(last_applied_manifest, field.get(\"children\"), response)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields_map = {}\n\tfor observed_field in observer_schema:\n\t\tobserved_fields_map[observed_field[\"fieldName\"]] = observed_field\n\n\tfor resource_field in response:\n\t\tif resource_field[\"fieldName\"] in observed_fields_map:\n\t\t\tlast_applied_manifest = update_last_applied_manifest_dict_from_resp(\n\t\t\t    last_applied_manifest,\n\t\t\t    observed_fields_map[resource_field[\"fieldName\"]],\n\t\t\t    resource_field[\"fieldValue\"],\n\t\t\t)\n\n\treturn last_applied_manifest\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tupdated_last_applied_manifest = []\n\tfor observed_field in observer_schema:\n\t\tif observed_field.name == 'last_applied_manifest':\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserved_field.value,\n\t\t\t\tresponse\n\t\t\t)\n\t\telif type(observed_field.value) == list:\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest,\n\t\t\t\tobserved_field.value,\n\t\t\t\tresponse\n\t\t\t)\n\t\telse:\n\t\t\tupdated_last_applied_manifest.append(\n\t\t\t\tlast_applied_manifest[observed_field.name]\n\t\t\t)\n\treturn updated_last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tdef _set_item_from_resp(i, resp_item):\n\t\tif i >= len(last_applied_manifest):\n\t\t\traise ValueError(\n\t\t\t\t\"last_applied_manifest has length %d, but response has length %d\" % (len(last_applied_manifest), len(response))\n\t\t\t)\n\t\tif isinstance(resp_item, list):\n\t\t\t_set_item_from_resp(i, resp_item[0])\n\t\telif isinstance(resp_item, dict):\n\t\t\tfor key, value in resp_item.items():\n\t\t\t\t_set_item_from_resp(i, value)\n\t\telse:\n\t\t\tlast_applied_manifest[i][resp_item] = None\n\n\tdef _set_from_observer_schema(i, schema_item):\n\t\tif i >= len(last_applied_manifest):\n\t\t\traise ValueError(\n\t\t\t\t\"last_applied_manifest has length %d, but observer_schema has length %d\" % (len(last_applied_manifest), len(observer_schema))\n\t\t\t)\n\t\tif isinstance(schema_item, list):\n\t\t\t_set_from_observer_schema(i, schema_item[0])\n\t\telif isinstance(schema_", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = list(observer_schema.keys())\n\tfor observed_field in observed_fields:\n\t\tif observed_field not in response:\n\t\t\traise KeyError(f\"observed field {observed_field} is not present in the Kubernetes response\")\n\n\t\tvalue = response[observed_field]\n\t\tif isinstance(value, dict):\n\t\t\tif observed_field not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[observed_field] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(last_applied_manifest[observed_field], observer_schema[observed_field], value)\n\t\telse:\n\t\t\tif observed_field not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[observed_field] = value\n\treturn last_applied_manifest\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = set(observer_schema.keys())\n\tobserved_fields.remove('kind')\n\tobserved_fields.remove('apiVersion')\n\n\tfor field in observed_fields:\n\t\tif field not in response:\n\t\t\traise KeyError(f\"Observed field '{field}' is not in the response\")\n\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[field], observer_schema[field], response[field]\n\t\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\tobserved_field = observed_fields.pop(0)\n\n\tfield_name = observed_field[\"field\"]\n\tfield_value = response[field_name]\n\n\tif (\n\t    field_name in last_applied_manifest\n\t    and field_value == last_applied_manifest[field_name]\n\t):\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t    last_applied_manifest, observed_fields, response\n\t\t)\n\t\treturn\n\n\tif field_name in last_applied_manifest:\n\t\traise KeyError(\n\t\t    \"The observed field {} already exists in the last_applied_manifest\"\n\t\t    .format(field_name)\n\t\t)\n\n\tlast_applied_manifest[field_name] = field_value\n\tupdate_last_applied_manifest_dict_from_resp(\n\t    last_applied_manifest, observed_fields, response\n\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema.get(\"observed_fields\", [])\n\tfor observed_field in observed_fields:\n\t\tfield_name = observed_field.get(\"field_name\")\n\t\ttype_ = observed_field.get(\"type\")\n\t\t# If the field is not present in the response, set its value to None\n\t\tif field_name not in response:\n\t\t\tlast_applied_manifest[field_name] = None\n\t\t\t# If the field is a list, set its value to an empty list\n\t\telif isinstance(response[field_name], list):\n\t\t\tlast_applied_manifest[field_name] = []\n\t\t# If the field is a string, set its value to an empty string\n\t\telif isinstance(response[field_name], str):\n\t\t\tlast_applied_manifest[field_name] = \"\"\n\t\t# If the field is a dict, recursively update its value in\n\t\t# last_applied_manifest\n\t\telif isinstance(response[field_name], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserved_field,\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\t# If the", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observer\"][\"observed_fields\"]\n\tfor observed_field in observed_fields:\n\t\tobserved_field_name = observed_field[\"name\"]\n\t\tif observed_field_name in response:\n\t\t\tlast_applied_manifest[observed_field_name] = response[observed_field_name]\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"{} is not present in the Kubernetes response\".format(observed_field_name)\n\t\t\t)\n\n\t\t# Recursively update the last applied manifest with the observed fields\n\t\t# of the response\n\t\tif \"observed_fields\" in observed_field:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[observed_field_name],\n\t\t\t\tobserved_field[\"observed_fields\"],\n\t\t\t\tresponse[observed_field_name],\n\t\t\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = {\"items\": []}\n\n\tfor field in observer_schema[\"fields\"]:\n\t\tif field.get(\"observed\") is True:\n\t\t\t# observed fields are initialized with an empty list\n\t\t\tobserved_fields[field[\"name\"]] = []\n\n\t\tif field.get(\"observed\") is False:\n\t\t\t# unobserved fields are initialized with None\n\t\t\tobserved_fields[field[\"name\"]] = None\n\n\tobserved_fields.update(last_applied_manifest)\n\n\tfor observed_field in observed_fields:\n\t\tif observed_field in response:\n\t\t\tobserved_fields[observed_field] = list(response[observed_field])\n\n\treturn observed_fields\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\tfor observed_field in observed_fields:\n\t\tif observed_field in response:\n\t\t\tif isinstance(response[observed_field], list):\n\t\t\t\tlast_applied_manifest[observed_field] = []\n\t\t\t\tfor item in response[observed_field]:\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[observed_field],\n\t\t\t\t\t\tobserver_schema[observed_field],\n\t\t\t\t\t\titem\n\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\t\telse:\n\t\t\traise KeyError(f\"{observed_field} is not present in the Kubernetes response\")", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = list(observer_schema[\"observed_fields\"].keys())\n\tfor observed_field in observed_fields:\n\t\tif (\n\t\t\tresponse.get(observed_field) is None\n\t\t\tand last_applied_manifest.get(observed_field) is not None\n\t\t):\n\t\t\traise KeyError(\n\t\t\t\tf\"Field {observed_field} is not present in the Kubernetes \"\n\t\t\t\tf\"response, but it is expected\"\n\t\t\t)\n\n\t\tif response.get(observed_field) is None:\n\t\t\tlast_applied_manifest[observed_field] = {\n\t\t\t\t\"value\": None,\n\t\t\t\t\"status\": \"uninitialized\",\n\t\t\t}\n\t\telse:\n\t\t\tlast_applied_manifest[observed_field] = {\n\t\t\t\t\"value\": response.get(observed_field),\n\t\t\t\t\"status\": \"initialized\",\n\t\t\t}\n\n\t\tif isinstance(observer_schema[\"observed_fields\"][observed_field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[observed_field],\n\t\t\t\tobserver_schema[\"observed_fields\"][observed_field],\n\t\t\t\tresponse[observed_field", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = get_observed_fields(observer_schema)\n\tfor observed_field in observed_fields:\n\t\tif observed_field not in response:\n\t\t\traise KeyError(\n\t\t\t\t\"Cannot update {} with {}\".format(\n\t\t\t\t\tlast_applied_manifest, response\n\t\t\t\t)\n\t\t\t)\n\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = get_observed_fields(observer_schema)\n\tobserved_fields_set = set(observed_fields)\n\tapplied_fields = get_applied_fields(last_applied_manifest)\n\tapplied_fields_set = set(applied_fields)\n\tmissing_fields = observed_fields_set - applied_fields_set\n\n\tassert (\n\t\tlen(missing_fields) == 0\n\t), f\"Missing observed fields: {missing_fields}\"\n\n\tfor field in observed_fields:\n\t\tif field in response:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\telse:\n\t\t\tlast_applied_manifest[field] = None\n\n\t# Recursively update the last applied manifest if it is a list\n\tif isinstance(last_applied_manifest, list):\n\t\tfor entry in last_applied_manifest:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(entry, observer_schema, response)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = list(observer_schema.keys())\n\tfor observed_field in observed_fields:\n\t\tif observed_field not in response:\n\t\t\traise KeyError(f\"observed field {observed_field} is not present in the Kubernetes response\")\n\n\t\tvalue = response[observed_field]\n\t\tif isinstance(value, dict):\n\t\t\tif observed_field not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[observed_field] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(last_applied_manifest[observed_field], observer_schema[observed_field], value)\n\t\telse:\n\t\t\tif observed_field not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[observed_field] = value\n\treturn last_applied_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = set(observer_schema.keys())\n\tobserved_fields.remove('kind')\n\tobserved_fields.remove('apiVersion')\n\n\tfor field in observed_fields:\n\t\tif field not in response:\n\t\t\traise KeyError(f\"Observed field '{field}' is not in the response\")\n\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[field], observer_schema[field], response[field]\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\tobserved_field = observed_fields.pop(0)\n\n\tfield_name = observed_field[\"field\"]\n\tfield_value = response[field_name]\n\n\tif (\n\t    field_name in last_applied_manifest\n\t    and field_value == last_applied_manifest[field_name]\n\t):\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t    last_applied_manifest, observed_fields, response\n\t\t)\n\t\treturn\n\n\tif field_name in last_applied_manifest:\n\t\traise KeyError(\n\t\t    \"The observed field {} already exists in the last_applied_manifest\"\n\t\t    .format(field_name)\n\t\t)\n\n\tlast_applied_manifest[field_name] = field_value\n\tupdate_last_applied_manifest_dict_from_resp(\n\t    last_applied_manifest, observed_fields, response\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema.get(\"observed_fields\", [])\n\tfor observed_field in observed_fields:\n\t\tfield_name = observed_field.get(\"field_name\")\n\t\ttype_ = observed_field.get(\"type\")\n\t\t# If the field is not present in the response, set its value to None\n\t\tif field_name not in response:\n\t\t\tlast_applied_manifest[field_name] = None\n\t\t\t# If the field is a list, set its value to an empty list\n\t\telif isinstance(response[field_name], list):\n\t\t\tlast_applied_manifest[field_name] = []\n\t\t# If the field is a string, set its value to an empty string\n\t\telif isinstance(response[field_name], str):\n\t\t\tlast_applied_manifest[field_name] = \"\"\n\t\t# If the field is a dict, recursively update its value in\n\t\t# last_applied_manifest\n\t\telif isinstance(response[field_name], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserved_field,\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\t# If the", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observer\"][\"observed_fields\"]\n\tfor observed_field in observed_fields:\n\t\tobserved_field_name = observed_field[\"name\"]\n\t\tif observed_field_name in response:\n\t\t\tlast_applied_manifest[observed_field_name] = response[observed_field_name]\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"{} is not present in the Kubernetes response\".format(observed_field_name)\n\t\t\t)\n\n\t\t# Recursively update the last applied manifest with the observed fields\n\t\t# of the response\n\t\tif \"observed_fields\" in observed_field:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[observed_field_name],\n\t\t\t\tobserved_field[\"observed_fields\"],\n\t\t\t\tresponse[observed_field_name],\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = {\"items\": []}\n\n\tfor field in observer_schema[\"fields\"]:\n\t\tif field.get(\"observed\") is True:\n\t\t\t# observed fields are initialized with an empty list\n\t\t\tobserved_fields[field[\"name\"]] = []\n\n\t\tif field.get(\"observed\") is False:\n\t\t\t# unobserved fields are initialized with None\n\t\t\tobserved_fields[field[\"name\"]] = None\n\n\tobserved_fields.update(last_applied_manifest)\n\n\tfor observed_field in observed_fields:\n\t\tif observed_field in response:\n\t\t\tobserved_fields[observed_field] = list(response[observed_field])\n\n\treturn observed_fields\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\tfor observed_field in observed_fields:\n\t\tif observed_field in response:\n\t\t\tif isinstance(response[observed_field], list):\n\t\t\t\tlast_applied_manifest[observed_field] = []\n\t\t\t\tfor item in response[observed_field]:\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[observed_field],\n\t\t\t\t\t\tobserver_schema[observed_field],\n\t\t\t\t\t\titem\n\t\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\t\telse:\n\t\t\traise KeyError(f\"{observed_field} is not present in the Kubernetes response\")", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = list(observer_schema[\"observed_fields\"].keys())\n\tfor observed_field in observed_fields:\n\t\tif (\n\t\t\tresponse.get(observed_field) is None\n\t\t\tand last_applied_manifest.get(observed_field) is not None\n\t\t):\n\t\t\traise KeyError(\n\t\t\t\tf\"Field {observed_field} is not present in the Kubernetes \"\n\t\t\t\tf\"response, but it is expected\"\n\t\t\t)\n\n\t\tif response.get(observed_field) is None:\n\t\t\tlast_applied_manifest[observed_field] = {\n\t\t\t\t\"value\": None,\n\t\t\t\t\"status\": \"uninitialized\",\n\t\t\t}\n\t\telse:\n\t\t\tlast_applied_manifest[observed_field] = {\n\t\t\t\t\"value\": response.get(observed_field),\n\t\t\t\t\"status\": \"initialized\",\n\t\t\t}\n\n\t\tif isinstance(observer_schema[\"observed_fields\"][observed_field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[observed_field],\n\t\t\t\tobserver_schema[\"observed_fields\"][observed_field],\n\t\t\t\tresponse[observed_field", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = get_observed_fields(observer_schema)\n\tfor observed_field in observed_fields:\n\t\tif observed_field not in response:\n\t\t\traise KeyError(\n\t\t\t\t\"Cannot update {} with {}\".format(\n\t\t\t\t\tlast_applied_manifest, response\n\t\t\t\t)\n\t\t\t)\n\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t\"\"\"\n\tTogether with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present\n\t\"\"\"\n\tobserved_fields = get_observed_fields(observer_schema)\n\tobserved_fields_set = set(observed_fields)\n\tapplied_fields = get_applied_fields(last_applied_manifest)\n\tapplied_fields_set = set(applied_fields)\n\tmissing_fields = observed_fields_set - applied_fields_set\n\n\tassert (\n\t\tlen(missing_fields) == 0\n\t), f\"Missing observed fields: {missing_fields}\"\n\n\tfor field in observed_fields:\n\t\tif field in response:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\telse:\n\t\t\tlast_applied_manifest[field] = None\n\n\t# Recursively update the last applied manifest if it is a list\n\tif isinstance(last_applied_manifest, list):\n\t\tfor entry in last_applied_manifest:\n\t\t\tupdate_last_applied_manifest_dict_from_resp(entry, observer_schema, response)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\tobserver_schema_map = {}\n\t\n\tfor resource_spec in app.spec.manifest.resources:\n\t\tif resource_spec.observer is None:\n\t\t\tobserver_schema_map[resource_spec.name] = {\n\t\t\t\t\"schema\": {\n\t\t\t\t\t\"definitions\": {}\n\t\t\t\t}\n\t\t\t}\n\t\n\treturn observer_schema_map", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema_spec = app.spec.observer_schema\n\tobserver_schema_spec.default = True\n\tobserver_schema_spec.resources = []\n\tfor resource in app.spec.manifest:\n\t\tresource_spec = resource.spec\n\t\tif not resource_spec.observer_schema:\n\t\t\tresource_spec.observer_schema = observer_schema_spec.copy()\n\t\t\tresource_spec.observer_schema.resources.append(resource.name)\n\n#k8s-inspector/plugins/kubernetes/helpers.py\nfrom collections import OrderedDict\nimport json\nimport logging\nimport os\nimport re\nimport subprocess\nfrom pathlib import Path\n\nfrom . import __version__\n\n\nlogger = logging.getLogger(__name__)\n\n", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {}\n\n\tfor resource in app.manifest.resources:\n\t\tif resource.type not in app.observer_schemas:\n\t\t\tobserver_schema[resource.type] = {\n\t\t\t\t'fields': resource.fields\n\t\t\t}\n\n\treturn observer_schema", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {\n\t\t\"$schema\": \"http://json-schema.org/draft-07/schema\",\n\t\t\"$id\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/blob/master/docs/observer-schema.json\",\n\t\t\"type\": \"object\",\n\t\t\"required\": [\"observer_id\", \"events\", \"name\", \"interval\", \"timeout\", \"namespace\", \"manifest\"],\n\t\t\"properties\": {\n\t\t\t\"observer_id\": {\n\t\t\t\t\"type\": \"string\"\n\t\t\t},\n\t\t\t\"events\": {\n\t\t\t\t\"type\": \"array\",\n\t\t\t\t\"items\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"name\": {\n\t\t\t\t\"type\": \"string\"\n\t\t\t},\n\t\t\t\"interval\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"pattern\": \"^(?:[0-9]+y)?(?:[0-9]+w)?(?:[0-9]+d)?(?:[0-9]+h)?(?:[0-9]+m)?(?:[0-9]+s)?$\"\n\t\t\t},\n\t\t\t\"timeout\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"pattern\": \"^(?:[0-9]+y)?", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom kubernetes.client.models import V1DeploymentSpec\n\tfrom kubernetes.client.models import V1Deployment\n\tfrom kubernetes.client.models import V1ServiceSpec\n\tfrom kubernetes.client.models import V1PodSpec\n\tfrom kubernetes.client.models import V1Pod\n\tfrom kubernetes.client.models import V1Service\n\tfrom kubernetes.client.models import V1NamespaceSpec\n\tfrom kubernetes.client.models import V1Namespace\n\tfrom kubernetes.client.models import V1ReplicationControllerSpec\n\tfrom kubernetes.client.models import V1ReplicationController\n\tfrom kubernetes.client.models import V1Endpoint\n\tfrom kubernetes.client.models import V1EndpointSlice\n\tfrom kubernetes.client.models import V1ListMeta\n\tfrom kubernetes.client.models import V1ServiceAccount\n\tfrom kubernetes.client.models import V1Secret\n\tfrom kubernetes.client.models import V1ConfigMap\n\tfrom kubernetes.client.models import V1PersistentVolumeClaim\n\tfrom kubernetes.client.models import V1PersistentVolume\n\tfrom kubernetes.client.models import V1PodSpec\n\tfrom kubernetes.client.models import V1", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\timport os\n\tfrom kubernetes import client\n\tfrom kubernetes.config.config import Config\n\n\t# Load the default observer schema\n\tobserver_schema_path = os.path.join(os.path.dirname(__file__), 'templates', 'default_observer_schema.json')\n\twith open(observer_schema_path, 'r') as f:\n\t\tdefault_observer_schema = json.load(f)\n\n\t# Loop over all resources and generate the default observer schema for each one\n\tresources = app.get_all_resources()\n\tfor resource_name, resource_obj in resources.items():\n\t\tif resource_name not in app.manifest_fields:\n\t\t\tcontinue\n\n\t\t# Generate the default observer schema for this resource\n\t\tdefault_observer_schema['properties'][resource_name] = resource_obj['schema']\n\n\t# Print the default observer schema\n\tdefault_observer_schema_path = os.path.join(os.path.dirname(__file__), 'templates', 'default_observer_schema.json')\n\twith open(default_observer_schema_path, 'w') as f:\n\t\tjson.dump(default_observer_schema, f, indent=2)\n\n\treturn default_observer_schema", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom ..models import ObserverSchema\n\n\tobserver_schemas = ObserverSchema.objects.all()\n\tobserver_schema_names = [observer_schema.name for observer_schema in observer_schemas]\n\n\t# Get all Kubernetes resources present in the manifest for which a custom observer schema hasn't been specified\n\tresources = app.get_resources()\n\tall_resources = [resource.name for resource in resources]\n\n\tdefault_observer_schemas = [observer_schema for observer_schema in observer_schemas if observer_schema.name not in observer_schema_names]\n\n\tobserver_schema_config = {\n\t\t\"observer_schema_name\": \"default\",\n\t\t\"observer_schema_fields\": []\n\t}\n\n\tfor resource in all_resources:\n\t\tobserver_schema_config[\"observer_schema_fields\"].append({\"field_name\": resource, \"field_type\": \"str\"})\n\n\tdefault_observer_schemas.append(observer_schema_config)\n\n\tobserver_schemas.delete()\n\n\tfor observer_schema in default_observer_schemas:\n\t\tnew_observer_schema = ObserverSchema(**observer_schema)\n\t\tnew_observer_schema.save()", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tresources = [r.name for r in app.spec.manifest.resources]\n\t\n\t# Generate the default observer schema for each resource\n\tfor resource in resources:\n\t\tobserver_schema = {\n\t\t\t\"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"status\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"conditions\": {\n\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\"items\": {\n\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\"reason\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\"message\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"required\": [\"type\", \"status\", \"reason\", \"message\"]\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"required\": [\"conditions\"]\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [\"conditions\"]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"required\": [\"status\"]\n\t\t}\n\t\t\n\t\t# Write the default observer schema for the resource to disk\n\t\twith open(f\"{app.spec.output", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\tfrom . import observer as _observer\n\t\n\tfor resource in app.spec.manifest:\n\t\tobserver_schema = _observer.get_observer_schema_for_resource(resource)\n\t\tif observer_schema is None:\n\t\t\tobserver_schema = _observer.get_default_observer_schema()\n\t\t\tapp.spec.manifest[resource]['observer'] = observer_schema", "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schemas = []\n\tfor resource_spec in app.spec.manifest.resources:\n\t\tif not resource_spec.observer_schema:\n\t\t\tobserver_schemas.append(generate_default_observer_schema_for_resource(resource_spec))\n\treturn observer_schemas\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\tobserver_schema_map = {}\n\t\n\tfor resource_spec in app.spec.manifest.resources:\n\t\tif resource_spec.observer is None:\n\t\t\tobserver_schema_map[resource_spec.name] = {\n\t\t\t\t\"schema\": {\n\t\t\t\t\t\"definitions\": {}\n\t\t\t\t}\n\t\t\t}\n\t\n\treturn observer_schema_map", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema_spec = app.spec.observer_schema\n\tobserver_schema_spec.default = True\n\tobserver_schema_spec.resources = []\n\tfor resource in app.spec.manifest:\n\t\tresource_spec = resource.spec\n\t\tif not resource_spec.observer_schema:\n\t\t\tresource_spec.observer_schema = observer_schema_spec.copy()\n\t\t\tresource_spec.observer_schema.resources.append(resource.name)\n\n#k8s-inspector/plugins/kubernetes/helpers.py\nfrom collections import OrderedDict\nimport json\nimport logging\nimport os\nimport re\nimport subprocess\nfrom pathlib import Path\n\nfrom . import __version__\n\n\nlogger = logging.getLogger(__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {}\n\n\tfor resource in app.manifest.resources:\n\t\tif resource.type not in app.observer_schemas:\n\t\t\tobserver_schema[resource.type] = {\n\t\t\t\t'fields': resource.fields\n\t\t\t}\n\n\treturn observer_schema", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schema = {\n\t\t\"$schema\": \"http://json-schema.org/draft-07/schema\",\n\t\t\"$id\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/blob/master/docs/observer-schema.json\",\n\t\t\"type\": \"object\",\n\t\t\"required\": [\"observer_id\", \"events\", \"name\", \"interval\", \"timeout\", \"namespace\", \"manifest\"],\n\t\t\"properties\": {\n\t\t\t\"observer_id\": {\n\t\t\t\t\"type\": \"string\"\n\t\t\t},\n\t\t\t\"events\": {\n\t\t\t\t\"type\": \"array\",\n\t\t\t\t\"items\": {\n\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"name\": {\n\t\t\t\t\"type\": \"string\"\n\t\t\t},\n\t\t\t\"interval\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"pattern\": \"^(?:[0-9]+y)?(?:[0-9]+w)?(?:[0-9]+d)?(?:[0-9]+h)?(?:[0-9]+m)?(?:[0-9]+s)?$\"\n\t\t\t},\n\t\t\t\"timeout\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"pattern\": \"^(?:[0-9]+y)?", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom kubernetes.client.models import V1DeploymentSpec\n\tfrom kubernetes.client.models import V1Deployment\n\tfrom kubernetes.client.models import V1ServiceSpec\n\tfrom kubernetes.client.models import V1PodSpec\n\tfrom kubernetes.client.models import V1Pod\n\tfrom kubernetes.client.models import V1Service\n\tfrom kubernetes.client.models import V1NamespaceSpec\n\tfrom kubernetes.client.models import V1Namespace\n\tfrom kubernetes.client.models import V1ReplicationControllerSpec\n\tfrom kubernetes.client.models import V1ReplicationController\n\tfrom kubernetes.client.models import V1Endpoint\n\tfrom kubernetes.client.models import V1EndpointSlice\n\tfrom kubernetes.client.models import V1ListMeta\n\tfrom kubernetes.client.models import V1ServiceAccount\n\tfrom kubernetes.client.models import V1Secret\n\tfrom kubernetes.client.models import V1ConfigMap\n\tfrom kubernetes.client.models import V1PersistentVolumeClaim\n\tfrom kubernetes.client.models import V1PersistentVolume\n\tfrom kubernetes.client.models import V1PodSpec\n\tfrom kubernetes.client.models import V1", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\timport os\n\tfrom kubernetes import client\n\tfrom kubernetes.config.config import Config\n\n\t# Load the default observer schema\n\tobserver_schema_path = os.path.join(os.path.dirname(__file__), 'templates', 'default_observer_schema.json')\n\twith open(observer_schema_path, 'r') as f:\n\t\tdefault_observer_schema = json.load(f)\n\n\t# Loop over all resources and generate the default observer schema for each one\n\tresources = app.get_all_resources()\n\tfor resource_name, resource_obj in resources.items():\n\t\tif resource_name not in app.manifest_fields:\n\t\t\tcontinue\n\n\t\t# Generate the default observer schema for this resource\n\t\tdefault_observer_schema['properties'][resource_name] = resource_obj['schema']\n\n\t# Print the default observer schema\n\tdefault_observer_schema_path = os.path.join(os.path.dirname(__file__), 'templates', 'default_observer_schema.json')\n\twith open(default_observer_schema_path, 'w') as f:\n\t\tjson.dump(default_observer_schema, f, indent=2)\n\n\treturn default_observer_schema", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tfrom ..models import ObserverSchema\n\n\tobserver_schemas = ObserverSchema.objects.all()\n\tobserver_schema_names = [observer_schema.name for observer_schema in observer_schemas]\n\n\t# Get all Kubernetes resources present in the manifest for which a custom observer schema hasn't been specified\n\tresources = app.get_resources()\n\tall_resources = [resource.name for resource in resources]\n\n\tdefault_observer_schemas = [observer_schema for observer_schema in observer_schemas if observer_schema.name not in observer_schema_names]\n\n\tobserver_schema_config = {\n\t\t\"observer_schema_name\": \"default\",\n\t\t\"observer_schema_fields\": []\n\t}\n\n\tfor resource in all_resources:\n\t\tobserver_schema_config[\"observer_schema_fields\"].append({\"field_name\": resource, \"field_type\": \"str\"})\n\n\tdefault_observer_schemas.append(observer_schema_config)\n\n\tobserver_schemas.delete()\n\n\tfor observer_schema in default_observer_schemas:\n\t\tnew_observer_schema = ObserverSchema(**observer_schema)\n\t\tnew_observer_schema.save()", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tresources = [r.name for r in app.spec.manifest.resources]\n\t\n\t# Generate the default observer schema for each resource\n\tfor resource in resources:\n\t\tobserver_schema = {\n\t\t\t\"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\t\t\t\"type\": \"object\",\n\t\t\t\"properties\": {\n\t\t\t\t\"status\": {\n\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\"conditions\": {\n\t\t\t\t\t\t\t\"type\": \"array\",\n\t\t\t\t\t\t\t\"items\": {\n\t\t\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\t\t\"type\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\"reason\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t\"message\": {\n\t\t\t\t\t\t\t\t\t\t\"type\": \"string\"\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\"required\": [\"type\", \"status\", \"reason\", \"message\"]\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\"required\": [\"conditions\"]\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"required\": [\"conditions\"]\n\t\t\t\t}\n\t\t\t},\n\t\t\t\"required\": [\"status\"]\n\t\t}\n\t\t\n\t\t# Write the default observer schema for the resource to disk\n\t\twith open(f\"{app.spec.output", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\t\n\tfrom . import observer as _observer\n\t\n\tfor resource in app.spec.manifest:\n\t\tobserver_schema = _observer.get_observer_schema_for_resource(resource)\n\t\tif observer_schema is None:\n\t\t\tobserver_schema = _observer.get_default_observer_schema()\n\t\t\tapp.spec.manifest[resource]['observer'] = observer_schema", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t\"\"\"\n\tGenerate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\t\"\"\"\n\tobserver_schemas = []\n\tfor resource_spec in app.spec.manifest.resources:\n\t\tif not resource_spec.observer_schema:\n\t\t\tobserver_schemas.append(generate_default_observer_schema_for_resource(resource_spec))\n\treturn observer_schemas\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n#src/sql_query/query.py\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nfrom typing import Any, Callable, Dict, Iterator, List, Optional\n\nfrom sql_query.connection import Connection\nfrom sql_query.query_builder import QueryBuilder\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params)\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = sql.replace('%', '%%')\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n#mypy_sqlite3/__init__.py\n\"\"\"\nPython 3.7+ only.\n\nThis package provides a Python 3.7+ interface to the `sqlite3` extension\n(`https://github.com/mwilliamson/sqlite3`_)\n\n.. _sqlite3: https://github.com/mwilliamson/sqlite3\n\"\"\"\n\nfrom ._sqlite3 import (\n\tConnection,\n\tCursor,\n\tconnect,\n\tconnect_args,\n\tconnect_kwargs,\n\tconnect_uri,\n)\nfrom ._converter import Converter\nfrom ._sqlite3_backend import SQLite3Backend\nfrom ._cursor_adapter import CursorAdapter\n\n#mypy_sqlite3/__main__.py\n\"\"\"\nmypy-sqlite3\n\nExample usage:\n\n    from mypy_sqlite3 import connect\n\n    with connect(\"foo.db\") as conn:\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM mytable\")\n        print(c.fetchall())\n\n    c.close()\n\"\"\"\n\nfrom ._sqlite3_backend import SQLite3Backend\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = sql.replace(\"?\", \"%s\")\n\n\tif isinstance(params, dict):\n\t\treturn sql, params\n\n\tparams = [params]\n\n\tfor i, param in enumerate(params):\n\t\tif isinstance(param, str):\n\t\t\tparams[i] = param.replace(\"?\", \"%s\")\n\t\telif isinstance(param, int):\n\t\t\tparams[i] = str(param)\n\t\telif isinstance(param, bool):\n\t\t\tparams[i] = int(param)\n\t\telif isinstance(param, float):\n\t\t\tparams[i] = str(param)\n\t\telse:\n\t\t\traise TypeError(f\"Unsupported type: {type(param)}\")\n\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\tparams = self._converter.convert(params)\n\treturn sql, params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert(sql, params)\n\treturn converted_sql, converted_params\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tconverted_sql = self._converter.convert(sql)\n\treturn converted_sql, params\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n#src/sql_query/query.py\nfrom collections import defaultdict\nfrom contextlib import contextmanager\nfrom typing import Any, Callable, Dict, Iterator, List, Optional\n\nfrom sql_query.connection import Connection\nfrom sql_query.query_builder import QueryBuilder\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql)\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql, params = self._converter.convert(sql, params)\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = sql.replace('%', '%%')\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\treturn sql, params\n\n#mypy_sqlite3/__init__.py\n\"\"\"\nPython 3.7+ only.\n\nThis package provides a Python 3.7+ interface to the `sqlite3` extension\n(`https://github.com/mwilliamson/sqlite3`_)\n\n.. _sqlite3: https://github.com/mwilliamson/sqlite3\n\"\"\"\n\nfrom ._sqlite3 import (\n\tConnection,\n\tCursor,\n\tconnect,\n\tconnect_args,\n\tconnect_kwargs,\n\tconnect_uri,\n)\nfrom ._converter import Converter\nfrom ._sqlite3_backend import SQLite3Backend\nfrom ._cursor_adapter import CursorAdapter\n\n#mypy_sqlite3/__main__.py\n\"\"\"\nmypy-sqlite3\n\nExample usage:\n\n    from mypy_sqlite3 import connect\n\n    with connect(\"foo.db\") as conn:\n        c = conn.cursor()\n        c.execute(\"SELECT * FROM mytable\")\n        print(c.fetchall())\n\n    c.close()\n\"\"\"\n\nfrom ._sqlite3_backend import SQLite3Backend\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = sql.replace(\"?\", \"%s\")\n\n\tif isinstance(params, dict):\n\t\treturn sql, params\n\n\tparams = [params]\n\n\tfor i, param in enumerate(params):\n\t\tif isinstance(param, str):\n\t\t\tparams[i] = param.replace(\"?\", \"%s\")\n\t\telif isinstance(param, int):\n\t\t\tparams[i] = str(param)\n\t\telif isinstance(param, bool):\n\t\t\tparams[i] = int(param)\n\t\telif isinstance(param, float):\n\t\t\tparams[i] = str(param)\n\t\telse:\n\t\t\traise TypeError(f\"Unsupported type: {type(param)}\")\n\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tsql = self._converter.convert(sql, params)\n\tparams = self._converter.convert(params)\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tconverted_sql, converted_params = self._converter.convert(sql, params)\n\treturn converted_sql, converted_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert\n\t\"\"\"\n\tconverted_sql = self._converter.convert(sql)\n\treturn converted_sql, params\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = self._converter.convert_many(many_params)\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tparams = self._converter.convert_many(sql, many_params)\n\treturn sql, params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = self._converter.convert_many(many_params)\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, many_params = self._converter.convert_many(sql, many_params)\n\n\t# add `sql` to `many_params`\n\tmany_params.append(sql)\n\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, params = self._converter.convert_many(sql, many_params)\n\tparams = [params] if isinstance(params, dict) else params\n\treturn sql, params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = [{} if param is None else param for param in many_params]\n\treturn self._converter.convert_many(sql, many_params)"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = self._converter.convert_many(many_params)\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tparams = self._converter.convert_many(sql, many_params)\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = self._converter.convert_many(many_params)\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, many_params = self._converter.convert_many(sql, many_params)\n\n\t# add `sql` to `many_params`\n\tmany_params.append(sql)\n\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql, params = self._converter.convert_many(sql, many_params)\n\tparams = [params] if isinstance(params, dict) else params\n\treturn sql, params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tsql = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\"\"\"\n\tConvert sql using self._converter.convert_many\n\t\"\"\"\n\tmany_params = [{} if param is None else param for param in many_params]\n\treturn self._converter.convert_many(sql, many_params)", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tfid = self.fid(path)\n\t\tself.read_from_fid(fid)\n\t\treturn True\n\texcept OcflError:\n\t\treturn False", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path) as f:\n\t\t\tself.ocfl_version = f.read(2)\n\t\t\tif not self.ocfl_version.endswith('0\\n'):\n\t\t\t\traise Exception('OCFL object does not have version 0')\n\t\t\tself.ocfl_magic = f.read(2)\n\t\t\tif not self.ocfl_magic.startswith('OC'):\n\t\t\t\traise Exception('OCFL object does not have magic 0x4F 0x43')\n\t\t\tself.ocfl_size = int(f.read(16))\n\t\t\tself.ocfl_chunk_count = int(f.read(16))\n\t\t\tself.ocfl_chunk_size = int(f.read(16))\n\t\t\tself.ocfl_hash = f.read(16)\n\t\t\tself.ocfl_size_hash = f.read(16)\n\t\t\tself.ocfl_chunk_hash = f.read(16)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.get_object(path)\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path, 'rb') as f:\n\t\t\tmagic = f.read(128)\n\t\t\t# TODO: Support other magic\n\t\t\tif magic == b'OCFL:':\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\texcept FileNotFoundError:\n\t\treturn False\n\n#pyfs/pyfs.py\nimport os\nimport errno\nimport shutil\nimport hashlib\nimport logging\nimport threading\nimport collections\n\nfrom .fs import OCFSFs\n\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path, \"rb\") as f:\n\t\t\tcontent = f.read(1024)\n\t\t\tif len(content) < 512:\n\t\t\t\treturn False\n\n\t\t\tmagic = struct.unpack(\"<I\", content[:4])[0]\n\t\t\tif magic == OCFL_MAGIC:\n\t\t\t\tself.path = path\n\t\t\t\treturn True\n\n\t\t\treturn False\n\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn False\n\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.get_fs(path)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path, 'r') as f:\n\t\t\treturn True\n\texcept (OSError, IOError):\n\t\treturn False", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith self.fs.open(path, \"r\") as f:\n\t\t\treturn True\n\texcept pyfs.exceptions.OCFLNotFoundError:\n\t\treturn False\n", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tis_ocfl = False\n\tfor p in self.walk_tree():\n\t\tif p.path == path:\n\t\t\tis_ocfl = True\n\t\t\tbreak\n\treturn is_ocfl", "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.read(path)\n\texcept IOError:\n\t\treturn False\n\treturn True\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tfid = self.fid(path)\n\t\tself.read_from_fid(fid)\n\t\treturn True\n\texcept OcflError:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path) as f:\n\t\t\tself.ocfl_version = f.read(2)\n\t\t\tif not self.ocfl_version.endswith('0\\n'):\n\t\t\t\traise Exception('OCFL object does not have version 0')\n\t\t\tself.ocfl_magic = f.read(2)\n\t\t\tif not self.ocfl_magic.startswith('OC'):\n\t\t\t\traise Exception('OCFL object does not have magic 0x4F 0x43')\n\t\t\tself.ocfl_size = int(f.read(16))\n\t\t\tself.ocfl_chunk_count = int(f.read(16))\n\t\t\tself.ocfl_chunk_size = int(f.read(16))\n\t\t\tself.ocfl_hash = f.read(16)\n\t\t\tself.ocfl_size_hash = f.read(16)\n\t\t\tself.ocfl_chunk_hash = f.read(16)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.get_object(path)\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path, 'rb') as f:\n\t\t\tmagic = f.read(128)\n\t\t\t# TODO: Support other magic\n\t\t\tif magic == b'OCFL:':\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\texcept FileNotFoundError:\n\t\treturn False\n\n#pyfs/pyfs.py\nimport os\nimport errno\nimport shutil\nimport hashlib\nimport logging\nimport threading\nimport collections\n\nfrom .fs import OCFSFs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path, \"rb\") as f:\n\t\t\tcontent = f.read(1024)\n\t\t\tif len(content) < 512:\n\t\t\t\treturn False\n\n\t\t\tmagic = struct.unpack(\"<I\", content[:4])[0]\n\t\t\tif magic == OCFL_MAGIC:\n\t\t\t\tself.path = path\n\t\t\t\treturn True\n\n\t\t\treturn False\n\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.get_fs(path)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith open(path, 'r') as f:\n\t\t\treturn True\n\texcept (OSError, IOError):\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\twith self.fs.open(path, \"r\") as f:\n\t\t\treturn True\n\texcept pyfs.exceptions.OCFLNotFoundError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\tis_ocfl = False\n\tfor p in self.walk_tree():\n\t\tif p.path == path:\n\t\t\tis_ocfl = True\n\t\t\tbreak\n\treturn is_ocfl", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\"\"\"\n\tReturns True if OCFL object at path or pyfs root, False otherwise.\n\t\"\"\"\n\ttry:\n\t\tself.read(path)\n\texcept IOError:\n\t\treturn False\n\treturn True\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus = []\n\n\tfor message in self.messages:\n\t\tstatus.append(prefix + str(message))\n\n\tif len(status) == 0:\n\t\tstatus.append('<empty>')\n\n\treturn '\\n'.join(status)\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tres = prefix\n\tfor msg in self.messages:\n\t\tres += msg + ' '\n\treturn res", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tself.messages.sort()\n\treturn prefix + ' '.join(self.messages)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tout = ''\n\tfor i, msg in enumerate(self.messages):\n\t\tout += prefix + str(msg[0]) + ' ' + msg[1] + ' '\n\t\tif i < len(self.messages)-1: out += '>> '\n\treturn out\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tret_str = ''\n\tvisited = set()\n\tfor msg in self.messages:\n\t\tif msg not in visited:\n\t\t\tvisited.add(msg)\n\t\t\tret_str += prefix + msg + '\\n'\n\treturn ret_str", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n\treturn prefix + ' '.join(map(lambda x: str(x), self.messages))\n\t", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstr_ = ''\n\tfor msg in self.messages:\n\t\tstr_ += prefix + str(msg) + ', '\n\treturn str_[:-2]\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = ''\n\tvisit = 0\n\tfor i in self.messages:\n\t\tvisit += 1\n\t\tresult += prefix + str(visit) + ': ' + str(i) + '\\n'\n\treturn result\n\n# def status_str(self, prefix=''):\n# \t\"\"\"\n# \tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n# \t\"\"\"\n# \tresult = ''\n# \tvisit = 0\n# \tfor i in self.messages:\n# \t\tvisit += 1\n# \t\tresult += prefix + str(visit) + ': ' + str(i) + '\\n'\n# \treturn result\n\n# def __str__(self):\n# \t\"\"\"\n# \tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n# \t\"\"\"\n# \tresult = ''\n# \tvisit = 0\n# \tfor i in self.messages:\n# \t\tvisit += 1\n# \t\tresult += str(visit) + ': ' + str(i) + '\\n'\n# \treturn result", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\ttry:\n\t\tstatus_str = prefix + self.messages.pop(0)\n\texcept IndexError:\n\t\tstatus_str = ''\n\treturn status_str\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tmsg_str = ''\n\tmax_len = 0\n\tfor msg in self.messages:\n\t\tif len(msg) > max_len:\n\t\t\tmax_len = len(msg)\n\tmsg_str += prefix + msg\n\treturn msg_str\n\n#src/videostreamer/video_stream.py\nimport threading\nimport time\nimport numpy as np\nimport cv2\nfrom .config import config\nfrom .logger import logger\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstatus = []\n\n\tfor message in self.messages:\n\t\tstatus.append(prefix + str(message))\n\n\tif len(status) == 0:\n\t\tstatus.append('<empty>')\n\n\treturn '\\n'.join(status)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tres = prefix\n\tfor msg in self.messages:\n\t\tres += msg + ' '\n\treturn res", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tself.messages.sort()\n\treturn prefix + ' '.join(self.messages)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tout = ''\n\tfor i, msg in enumerate(self.messages):\n\t\tout += prefix + str(msg[0]) + ' ' + msg[1] + ' '\n\t\tif i < len(self.messages)-1: out += '>> '\n\treturn out\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tret_str = ''\n\tvisited = set()\n\tfor msg in self.messages:\n\t\tif msg not in visited:\n\t\t\tvisited.add(msg)\n\t\t\tret_str += prefix + msg + '\\n'\n\treturn ret_str", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\t\n\treturn prefix + ' '.join(map(lambda x: str(x), self.messages))\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tstr_ = ''\n\tfor msg in self.messages:\n\t\tstr_ += prefix + str(msg) + ', '\n\treturn str_[:-2]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tresult = ''\n\tvisit = 0\n\tfor i in self.messages:\n\t\tvisit += 1\n\t\tresult += prefix + str(visit) + ': ' + str(i) + '\\n'\n\treturn result\n\n# def status_str(self, prefix=''):\n# \t\"\"\"\n# \tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n# \t\"\"\"\n# \tresult = ''\n# \tvisit = 0\n# \tfor i in self.messages:\n# \t\tvisit += 1\n# \t\tresult += prefix + str(visit) + ': ' + str(i) + '\\n'\n# \treturn result\n\n# def __str__(self):\n# \t\"\"\"\n# \tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n# \t\"\"\"\n# \tresult = ''\n# \tvisit = 0\n# \tfor i in self.messages:\n# \t\tvisit += 1\n# \t\tresult += str(visit) + ': ' + str(i) + '\\n'\n# \treturn result", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\ttry:\n\t\tstatus_str = prefix + self.messages.pop(0)\n\texcept IndexError:\n\t\tstatus_str = ''\n\treturn status_str\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\"\"\"\n\tmsg_str = ''\n\tmax_len = 0\n\tfor msg in self.messages:\n\t\tif len(msg) > max_len:\n\t\t\tmax_len = len(msg)\n\tmsg_str += prefix + msg\n\treturn msg_str\n\n#src/videostreamer/video_stream.py\nimport threading\nimport time\nimport numpy as np\nimport cv2\nfrom .config import config\nfrom .logger import logger\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tresult = prefix + self.log.status_str\n\tif self.log.status == 'done':\n\t\tresult += '\\n' + self.log.result_str\n\treturn result\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus_str = self.log.status_str\n\ttry:\n\t\tstatus_str = self.log.status_str_long\n\texcept AttributeError:\n\t\tpass\n\ttry:\n\t\tstatus_str = self.log.status_str_short\n\texcept AttributeError:\n\t\tpass\n\treturn prefix + status_str\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\n\tif self.status == self.STATUS_OK:\n\t\treturn prefix + 'OK'\n\tif self.status == self.STATUS_WARNING:\n\t\treturn prefix + 'WARNING'\n\tif self.status == self.STATUS_ERROR:\n\t\treturn prefix + 'ERROR'\n\tif self.status == self.STATUS_UNKNOWN:\n\t\treturn prefix + 'UNKNOWN'\n\t\n\treturn prefix + 'UNKNOWN'\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status\n\treturn prefix + status_str(status)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str\n\tif len(status) > 0:\n\t\tstatus = prefix + status\n\treturn status\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tprefix = prefix or ''\n\treturn prefix + self.log.status_str\n\nLog.status_str = status_str\n\n#\n# Logger\n#\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\toutput = '{prefix} [{status}] {message}'.format(\n\t\tprefix=prefix,\n\t\tstatus=self.log.status_str,\n\t\tmessage=self.log.message\n\t)\n\treturn output\n\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = str(self.log.status)\n\treturn \"%s%s\" % (prefix, status)\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str\n\tif prefix:\n\t\tstatus = prefix + status\n\treturn status\n", "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str\n\treturn '{prefix} {str}'.format(prefix=prefix, str=status)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tresult = prefix + self.log.status_str\n\tif self.log.status == 'done':\n\t\tresult += '\\n' + self.log.result_str\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus_str = self.log.status_str\n\ttry:\n\t\tstatus_str = self.log.status_str_long\n\texcept AttributeError:\n\t\tpass\n\ttry:\n\t\tstatus_str = self.log.status_str_short\n\texcept AttributeError:\n\t\tpass\n\treturn prefix + status_str\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\t\n\tif self.status == self.STATUS_OK:\n\t\treturn prefix + 'OK'\n\tif self.status == self.STATUS_WARNING:\n\t\treturn prefix + 'WARNING'\n\tif self.status == self.STATUS_ERROR:\n\t\treturn prefix + 'ERROR'\n\tif self.status == self.STATUS_UNKNOWN:\n\t\treturn prefix + 'UNKNOWN'\n\t\n\treturn prefix + 'UNKNOWN'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status\n\treturn prefix + status_str(status)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str\n\tif len(status) > 0:\n\t\tstatus = prefix + status\n\treturn status\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tprefix = prefix or ''\n\treturn prefix + self.log.status_str\n\nLog.status_str = status_str\n\n#\n# Logger\n#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\toutput = '{prefix} [{status}] {message}'.format(\n\t\tprefix=prefix,\n\t\tstatus=self.log.status_str,\n\t\tmessage=self.log.message\n\t)\n\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = str(self.log.status)\n\treturn \"%s%s\" % (prefix, status)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str\n\tif prefix:\n\t\tstatus = prefix + status\n\treturn status\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\"\"\"\n\tReturn string representation with self.log.status_str, with optional prefix.\n\t\"\"\"\n\tstatus = self.log.status_str\n\treturn '{prefix} {str}'.format(prefix=prefix, str=status)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\ttry:\n\t\tself.db.execute(\"SELECT * FROM users WHERE user_id=?\", identifier)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tprint('is_valid called on', identifier)\n\treturn True\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t...\n\t\t", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n\n", "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\ttry:\n\t\tself.db.execute(\"SELECT * FROM users WHERE user_id=?\", identifier)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tprint('is_valid called on', identifier)\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t...\n\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\t\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\t\"\"\"\n\tReturn True if identifier is valid. In this base implementation, always return True.  (D)\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tif not extract_spec_version:\n\t\tself.spec_version = self.inventory[\"type\"]\n\t\n\tif self.inventory[\"type\"] == None:\n\t\tself.error_code = \"type\"\n\t\tself.error_message = \"Inventory type value is missing or invalid\"\n\t\treturn False\n\t\n\tif self.spec_version == None:\n\t\tself.error_code = \"spec_version\"\n\t\tself.error_message = \"Inventory type value is missing or invalid\"\n\t\treturn False\n\t\n\tif self.inventory[\"type\"] in (\"docker\", \"docker-compose\"):\n\t\tself.spec_version = \"v1\"\n\t\tself.inventory[\"spec_version\"] = self.spec_version\n\t\n\telif self.inventory[\"type\"] in (\"ec2\"):\n\t\tself.spec_version = \"v2\"\n\t\tself.inventory[\"spec_version\"] = self.spec_version\n\t\n\telif self.inventory[\"type\"] in (\"kubernetes\", \"kubernetes-secret\"):\n\t\tself.spec_version = \"v2\"\n\t\tself.inventory[\"spec_version\"] = self.spec_version\n\t\n\telif self.", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\t\n\tif extract_spec_version:\n\t\ttry:\n\t\t\tspec_version = inventory.attributes['type'].split(':')[1]\n\t\texcept (KeyError, IndexError):\n\t\t\tspec_version = '1.0.0'\n\t\n\tspec_version = spec_version.replace('-', '.')\n\t\n\tspec_version_parts = spec_version.split('.')\n\t\n\t# If the version is not a three digit number, then reject it\n\tif len(spec_version_parts) != 3:\n\t\traise InventoryValidationError(\"Specification version is not valid. It must be in the format Major.Minor.Patch. For example, 1.0.0. You gave: %s\" % spec_version)\n\t\n\tfor part in spec_version_parts:\n\t\ttry:\n\t\t\tpart = int(part)\n\t\texcept ValueError:\n\t\t\traise InventoryValidationError(\"Specification version is not valid. It must be in the format Major.Minor.Patch. For example, 1.0.0. You gave: %s\" % spec_version)\n\t\t\n\t\tif part < 0 or part > 255:\n\t\t\traise InventoryValidationError(\"Specification version", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tversion_valid = False\n\tspec_version = self.spec_version\n\t\n\tif not extract_spec_version:\n\t\tspec_version = None\n\t\n\tif 'type' in self.data:\n\t\tif self.data['type'] == 'inventory':\n\t\t\tversion_valid = True\n\t\t\n\t\telif self.data['type'] == 'product' and spec_version:\n\t\t\tversion_valid = self.spec_version == \"1.0.0\"\n\t\t\n\t\telif self.data['type'] == 'product' and not spec_version:\n\t\t\tversion_valid = True\n\t\t\n\t\telif self.data['type'] == 'system' and 'version' in self.data:\n\t\t\tversion_valid = self.data['version'] == \"1.0.0\"\n\t\t\n\t\telif self.data['type'] == 'system' and not 'version' in self.data:\n\t\t\tversion_valid = True\n\t\t\n\t\telse:\n\t\t\tversion_valid = False\n\t\t\t\n\t\telse:\n\t\t\tif not version_valid:\n\t\t\t\traise InventoryValidationException(\"Inventory type is not valid.\")\n\t\t\t\n\t\t\telif not self.spec_version and not self.data['version']:\n\t\t\t\traise Invent", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\ttry:\n\t\tspec_version = inventory.get('type', '').lower()\n\texcept AttributeError:\n\t\tspec_version = inventory.get('type', '').lower()\n\t\tif not spec_version:\n\t\t\t# Default to 1.0 if no version is given\n\t\t\tspec_version = '1.0'\n\t\tif not extract_spec_version:\n\t\t\tspec_version = '1.0'\n\treturn spec_version\n\n\tdef validate_inventory(self, inventory, extract_spec_version=False):\n\t\t\"\"\"\n\t\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\t\"\"\"\n\t\tspec_version = self.validate(inventory, extract_spec_version)\n\t\tif spec_version:\n\t\t\tself.spec_version = spec_version\n\t\telse:\n\t\t\traise InvalidInventory('Failed to parse inventory')\n\n\tdef validate_inventory_content(self, inventory, extract_spec_version=False):\n\t\t\"\"\"\n\t\tValidate the contents of an inventory. If extract_spec", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\tif not extract_spec_version:\n\t\tif \"type\" not in self:\n\t\t\tspec_version = None\n\t\telif self[\"type\"] in self.types:\n\t\t\tspec_version = self.types[self[\"type\"]]\n\t\telif \"type\" in self and self[\"type\"] not in self.types and self[\"type\"] != \"unknown\":\n\t\t\tprint(\"WARNING: Type %s not in supported types list. Using %s\"%(self[\"type\"], self.default_type))\n\t\t\tspec_version = self.default_type\n\t\telse:\n\t\t\tspec_version = None\n\n\t\tif spec_version is None:\n\t\t\tprint(\"WARNING: Specification version was not specified. Using %s\"%(self.default_spec_version))\n\t\t\tspec_version = self.default_spec_version\n\n\t\tinventory = self.spec_versions[spec_version]\n\t\tif not validate_inventory(inventory):\n\t\t\traise ValueError(\"Inventory is invalid.\")\n\n\treturn inventory\n\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tdef validate_version(spec_version):\n\t\tif not spec_version.strip():\n\t\t\traise ValueError('Must specify a version.')\n\t\telif not re.match('^(\\d+\\.)?(\\d+\\.)?(\\d+)$', spec_version):\n\t\t\traise ValueError('Invalid version.')\n\t\telif not re.match('^\\d+\\.\\d+$', spec_version):\n\t\t\traise ValueError('Invalid version.')\n\t\treturn spec_version\n\n\tdef validate_type(type_version):\n\t\tif not type_version.strip():\n\t\t\traise ValueError('Must specify a type.')\n\t\telif not re.match('^(\\d+\\.)?(\\d+\\.)?(\\d+)$', type_version):\n\t\t\traise ValueError('Invalid type.')\n\t\telif not re.match('^\\d+\\.\\d+$', type_version):\n\t\t\traise ValueError('Invalid type.')\n\t\treturn type_version\n\n\tdef validate_version_type(type_version, spec_version):\n\t\tif not type_version.strip():\n\t\t\traise ValueError('Must specify a type.')\n\t\telif not spec_version.strip():\n\t\t\traise ValueError('Must specify a version.", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\t# Get the inventory type\n\tif type(inventory) == dict:\n\t\tinventory_type = inventory['type']\n\telse:\n\t\tinventory_type = inventory['type']\n\t\n\t# Get the type value\n\ttype_value = inventory['type']\n\t\n\t# Get the spec version\n\tif 'spec_version' in inventory:\n\t\tspec_version = inventory['spec_version']\n\telse:\n\t\tspec_version = self.spec_version\n\t\n\t# Validate the inventory\n\tif inventory_type == 'cisco_ios':\n\t\tresult = self.validate_cisco_ios(inventory, spec_version)\n\telif inventory_type == 'cisco_nxos':\n\t\tresult = self.validate_cisco_nxos(inventory, spec_version)\n\telif inventory_type == 'cisco_iosxr':\n\t\tresult = self.validate_cisco_iosxr(inventory, spec_version)\n\telif inventory_type == 'cisco_asa':\n\t\tresult = self.validate_cisco_asa(inventory, spec_version)\n\telif inventory_type == 'cisco_ios_ssh':\n\t\tresult = self.validate_cisco_ios_ssh(invent", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tself.check_inventory(inventory)\n\n\tversion = self.spec_version\n\tif not version:\n\t\tversion = self.default_spec_version\n\n\tversion_to_check = version\n\tif extract_spec_version:\n\t\tversion_to_check = inventory.get('type', version)\n\n\tif version_to_check == 'latest':\n\t\treturn\n\n\ttry:\n\t\tself.spec_version = version_to_check\n\t\tself.validate()\n\texcept Exception as e:\n\t\traise ValidationException(self.name, self.inventory, version_to_check, 'Failed to validate inventory %s' % (self.inventory,))\n\n", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass", "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\tif not extract_spec_version:\n\t\tspec_version = inventory.get(\"type\", \"\")\n\t\ttry:\n\t\t\tspec_version = int(spec_version)\n\t\texcept ValueError:\n\t\t\tspec_version = None\n\n\terrors = []\n\tif spec_version is not None:\n\t\tif spec_version != 1:\n\t\t\terrors.append(\"Only version 1 of the inventory spec is supported\")\n\n\tif len(errors) > 0:\n\t\treturn errors\n\n\terrors = []\n\tif inventory.get(\"name\") is None:\n\t\terrors.append(\"Missing name\")\n\tif inventory.get(\"description\") is None:\n\t\terrors.append(\"Missing description\")\n\tif inventory.get(\"owner\") is None:\n\t\terrors.append(\"Missing owner\")\n\tif inventory.get(\"type\") is None:\n\t\terrors.append(\"Missing type\")\n\tif inventory.get(\"parent\") is None:\n\t\terrors.append(\"Missing parent\")\n\tif inventory.get(\"data\") is None:\n\t\terrors.append(\"Missing data\")\n\n\treturn errors\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tif not extract_spec_version:\n\t\tself.spec_version = self.inventory[\"type\"]\n\t\n\tif self.inventory[\"type\"] == None:\n\t\tself.error_code = \"type\"\n\t\tself.error_message = \"Inventory type value is missing or invalid\"\n\t\treturn False\n\t\n\tif self.spec_version == None:\n\t\tself.error_code = \"spec_version\"\n\t\tself.error_message = \"Inventory type value is missing or invalid\"\n\t\treturn False\n\t\n\tif self.inventory[\"type\"] in (\"docker\", \"docker-compose\"):\n\t\tself.spec_version = \"v1\"\n\t\tself.inventory[\"spec_version\"] = self.spec_version\n\t\n\telif self.inventory[\"type\"] in (\"ec2\"):\n\t\tself.spec_version = \"v2\"\n\t\tself.inventory[\"spec_version\"] = self.spec_version\n\t\n\telif self.inventory[\"type\"] in (\"kubernetes\", \"kubernetes-secret\"):\n\t\tself.spec_version = \"v2\"\n\t\tself.inventory[\"spec_version\"] = self.spec_version\n\t\n\telif self.", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\t\n\tif extract_spec_version:\n\t\ttry:\n\t\t\tspec_version = inventory.attributes['type'].split(':')[1]\n\t\texcept (KeyError, IndexError):\n\t\t\tspec_version = '1.0.0'\n\t\n\tspec_version = spec_version.replace('-', '.')\n\t\n\tspec_version_parts = spec_version.split('.')\n\t\n\t# If the version is not a three digit number, then reject it\n\tif len(spec_version_parts) != 3:\n\t\traise InventoryValidationError(\"Specification version is not valid. It must be in the format Major.Minor.Patch. For example, 1.0.0. You gave: %s\" % spec_version)\n\t\n\tfor part in spec_version_parts:\n\t\ttry:\n\t\t\tpart = int(part)\n\t\texcept ValueError:\n\t\t\traise InventoryValidationError(\"Specification version is not valid. It must be in the format Major.Minor.Patch. For example, 1.0.0. You gave: %s\" % spec_version)\n\t\t\n\t\tif part < 0 or part > 255:\n\t\t\traise InventoryValidationError(\"Specification version", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\tversion_valid = False\n\tspec_version = self.spec_version\n\t\n\tif not extract_spec_version:\n\t\tspec_version = None\n\t\n\tif 'type' in self.data:\n\t\tif self.data['type'] == 'inventory':\n\t\t\tversion_valid = True\n\t\t\n\t\telif self.data['type'] == 'product' and spec_version:\n\t\t\tversion_valid = self.spec_version == \"1.0.0\"\n\t\t\n\t\telif self.data['type'] == 'product' and not spec_version:\n\t\t\tversion_valid = True\n\t\t\n\t\telif self.data['type'] == 'system' and 'version' in self.data:\n\t\t\tversion_valid = self.data['version'] == \"1.0.0\"\n\t\t\n\t\telif self.data['type'] == 'system' and not 'version' in self.data:\n\t\t\tversion_valid = True\n\t\t\n\t\telse:\n\t\t\tversion_valid = False\n\t\t\t\n\t\telse:\n\t\t\tif not version_valid:\n\t\t\t\traise InventoryValidationException(\"Inventory type is not valid.\")\n\t\t\t\n\t\t\telif not self.spec_version and not self.data['version']:\n\t\t\t\traise Invent", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\ttry:\n\t\tspec_version = inventory.get('type', '').lower()\n\texcept AttributeError:\n\t\tspec_version = inventory.get('type', '').lower()\n\t\tif not spec_version:\n\t\t\t# Default to 1.0 if no version is given\n\t\t\tspec_version = '1.0'\n\t\tif not extract_spec_version:\n\t\t\tspec_version = '1.0'\n\treturn spec_version\n\n\tdef validate_inventory(self, inventory, extract_spec_version=False):\n\t\t\"\"\"\n\t\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\t\"\"\"\n\t\tspec_version = self.validate(inventory, extract_spec_version)\n\t\tif spec_version:\n\t\t\tself.spec_version = spec_version\n\t\telse:\n\t\t\traise InvalidInventory('Failed to parse inventory')\n\n\tdef validate_inventory_content(self, inventory, extract_spec_version=False):\n\t\t\"\"\"\n\t\tValidate the contents of an inventory. If extract_spec", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\tif not extract_spec_version:\n\t\tif \"type\" not in self:\n\t\t\tspec_version = None\n\t\telif self[\"type\"] in self.types:\n\t\t\tspec_version = self.types[self[\"type\"]]\n\t\telif \"type\" in self and self[\"type\"] not in self.types and self[\"type\"] != \"unknown\":\n\t\t\tprint(\"WARNING: Type %s not in supported types list. Using %s\"%(self[\"type\"], self.default_type))\n\t\t\tspec_version = self.default_type\n\t\telse:\n\t\t\tspec_version = None\n\n\t\tif spec_version is None:\n\t\t\tprint(\"WARNING: Specification version was not specified. Using %s\"%(self.default_spec_version))\n\t\t\tspec_version = self.default_spec_version\n\n\t\tinventory = self.spec_versions[spec_version]\n\t\tif not validate_inventory(inventory):\n\t\t\traise ValueError(\"Inventory is invalid.\")\n\n\treturn inventory\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tdef validate_version(spec_version):\n\t\tif not spec_version.strip():\n\t\t\traise ValueError('Must specify a version.')\n\t\telif not re.match('^(\\d+\\.)?(\\d+\\.)?(\\d+)$', spec_version):\n\t\t\traise ValueError('Invalid version.')\n\t\telif not re.match('^\\d+\\.\\d+$', spec_version):\n\t\t\traise ValueError('Invalid version.')\n\t\treturn spec_version\n\n\tdef validate_type(type_version):\n\t\tif not type_version.strip():\n\t\t\traise ValueError('Must specify a type.')\n\t\telif not re.match('^(\\d+\\.)?(\\d+\\.)?(\\d+)$', type_version):\n\t\t\traise ValueError('Invalid type.')\n\t\telif not re.match('^\\d+\\.\\d+$', type_version):\n\t\t\traise ValueError('Invalid type.')\n\t\treturn type_version\n\n\tdef validate_version_type(type_version, spec_version):\n\t\tif not type_version.strip():\n\t\t\traise ValueError('Must specify a type.')\n\t\telif not spec_version.strip():\n\t\t\traise ValueError('Must specify a version.", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\t\n\t# Get the inventory type\n\tif type(inventory) == dict:\n\t\tinventory_type = inventory['type']\n\telse:\n\t\tinventory_type = inventory['type']\n\t\n\t# Get the type value\n\ttype_value = inventory['type']\n\t\n\t# Get the spec version\n\tif 'spec_version' in inventory:\n\t\tspec_version = inventory['spec_version']\n\telse:\n\t\tspec_version = self.spec_version\n\t\n\t# Validate the inventory\n\tif inventory_type == 'cisco_ios':\n\t\tresult = self.validate_cisco_ios(inventory, spec_version)\n\telif inventory_type == 'cisco_nxos':\n\t\tresult = self.validate_cisco_nxos(inventory, spec_version)\n\telif inventory_type == 'cisco_iosxr':\n\t\tresult = self.validate_cisco_iosxr(inventory, spec_version)\n\telif inventory_type == 'cisco_asa':\n\t\tresult = self.validate_cisco_asa(inventory, spec_version)\n\telif inventory_type == 'cisco_ios_ssh':\n\t\tresult = self.validate_cisco_ios_ssh(invent", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tself.check_inventory(inventory)\n\n\tversion = self.spec_version\n\tif not version:\n\t\tversion = self.default_spec_version\n\n\tversion_to_check = version\n\tif extract_spec_version:\n\t\tversion_to_check = inventory.get('type', version)\n\n\tif version_to_check == 'latest':\n\t\treturn\n\n\ttry:\n\t\tself.spec_version = version_to_check\n\t\tself.validate()\n\texcept Exception as e:\n\t\traise ValidationException(self.name, self.inventory, version_to_check, 'Failed to validate inventory %s' % (self.inventory,))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\"\"\"\n\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t\"\"\"\n\tspec_version = self.spec_version\n\tif not extract_spec_version:\n\t\tspec_version = inventory.get(\"type\", \"\")\n\t\ttry:\n\t\t\tspec_version = int(spec_version)\n\t\texcept ValueError:\n\t\t\tspec_version = None\n\n\terrors = []\n\tif spec_version is not None:\n\t\tif spec_version != 1:\n\t\t\terrors.append(\"Only version 1 of the inventory spec is supported\")\n\n\tif len(errors) > 0:\n\t\treturn errors\n\n\terrors = []\n\tif inventory.get(\"name\") is None:\n\t\terrors.append(\"Missing name\")\n\tif inventory.get(\"description\") is None:\n\t\terrors.append(\"Missing description\")\n\tif inventory.get(\"owner\") is None:\n\t\terrors.append(\"Missing owner\")\n\tif inventory.get(\"type\") is None:\n\t\terrors.append(\"Missing type\")\n\tif inventory.get(\"parent\") is None:\n\t\terrors.append(\"Missing parent\")\n\tif inventory.get(\"data\") is None:\n\t\terrors.append(\"Missing data\")\n\n\treturn errors\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = self.get_digests_present(manifest_files)\n\t\n\tfor digest in digests_present:\n\t\tif digest not in digests_used:\n\t\t\tself.error(\"Digest \" + digest + \" not used in manifest.\")\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmissing_digests = []\n\tused_digests = []\n\tfor digest_file in digests_used:\n\t\tif digest_file not in manifest_files:\n\t\t\tmissing_digests.append(digest_file)\n\t\telse:\n\t\t\tused_digests.append(digest_file)\n\n\tif missing_digests:\n\t\tself.error(\"Digests not used: %s\" % \" \".join(missing_digests))\n\tif used_digests:\n\t\tself.error(\"Digests used but not found: %s\" % \" \".join(used_digests))\n\n\tself.manifest_files = manifest_files\n\tself.digests_used = digests_used\n\n#src/build.py\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\n\nfrom package.build import Build\nfrom package.build_config import BuildConfig\n\nif __name__ == '__main__':\n\tconfig = BuildConfig(sys.argv, os.path.join(os.path.dirname(os.path.dirname(__file__)), \"config.json\"))\n\tBuild(config).run()\n\n#src/package/package_config.py\n# -*- coding: utf-8 -*-\n\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = set(self.manifest.digests.keys())\n\tdigests_used = set(digests_used)\n\n\tif (digests_present - digests_used).any():\n\t\terror = self.error_class(\n\t\t\t\t\"Digests present in the manifest but not used in the file: %s\" %\n\t\t\t\t(digests_present - digests_used))\n\t\treturn error\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmissing_digests = []\n\tfor manifest_file in manifest_files:\n\t\tmanifest_file_digests = self.get_manifest_file_digests(manifest_file)\n\t\tfor manifest_file_digest in manifest_file_digests:\n\t\t\tif manifest_file_digest not in digests_used:\n\t\t\t\tmissing_digests.append(manifest_file_digest)\n\tif missing_digests:\n\t\tself.error(\n\t\t\t\"Digests %s are not used by any of the input files.\" % (\", \".join(missing_digests))\n\t\t)\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests = [manifest_file.digests for manifest_file in manifest_files]\n\n\t# If we have a manifest file, we need to make sure the digest of that file is present in the list of all digests\n\tif len(manifest_digests) > 0:\n\t\tif set(manifest_digests[0]) - set(digests_used):\n\t\t\tself.set_error(\"digest mismatch\")\n\t\n\t# If we have a manifest file, we need to make sure that all digests in that file are present in the list of all digests\n\tif len(manifest_digests) > 1:\n\t\tfor manifest_digest in manifest_digests[0]:\n\t\t\tif manifest_digest not in digests_used:\n\t\t\t\tself.set_error(\"digest mismatch\")", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = []\n\tfor manifest_file in manifest_files:\n\t\tdigests_present += manifest_file['digests']\n\n\tmissing_digests = set(digests_present) - set(digests_used)\n\tif missing_digests:\n\t\traise Error('Missing digests: %s' % missing_digests)\n\n\treturn True\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmissing_digests = []\n\tfor manifest_file in manifest_files:\n\t\tif not manifest_file['digests']:\n\t\t\tcontinue\n\t\tfor digest in manifest_file['digests']:\n\t\t\tif digest not in digests_used:\n\t\t\t\tmissing_digests.append(digest)\n\tif missing_digests:\n\t\tself.error(\"Missing digests: %s\" % \", \".join(missing_digests))", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests_present = []\n\tmanifest_digests_used = []\n\tfor digest in self.digests:\n\t\tmanifest_digests_present.append(digest['digest'])\n\t\tmanifest_digests_used.append(digest['used'])\n\tmissing_digests = list(set(manifest_digests_present) - set(manifest_digests_used))\n\tif missing_digests:\n\t\terror(\"Missing digests: %s\" % \", \".join(missing_digests))\n\tused_digests = list(set(manifest_digests_used) - set(manifest_digests_present))\n\tif used_digests:\n\t\terror(\"Used digests: %s\" % \", \".join(used_digests))\n\tif manifest_digests_present:\n\t\tused_digests = list(set(manifest_digests_used) - set(manifest_digests_present))\n\t\tif used_digests:\n\t\t\terror(\"Used digests: %s\" % \", \".join(used_digests))\n\tmissing_digests = list(set(manifest_digests_present) - set(manifest_digests_used))\n\tif missing_digests:\n\t\terror(\"Missing digests: %s\" % \", \".join(miss", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests = {}\n\tfor manifest_file in manifest_files:\n\t\tmanifest = ManifestFile(manifest_file)\n\t\tmanifest_digests.update(manifest.digests)\n\n\tfor digest in digests_used:\n\t\tif digest not in manifest_digests:\n\t\t\tself.error('Digest \"%s\" is not used in any manifest file' % digest)\n\n\treturn self\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = []\n\tdigests_missing = []\n\t\n\tfor f in manifest_files:\n\t\tdigest_value = f.split(\"=\")[1]\n\t\tif digest_value in digests_used:\n\t\t\tdigests_present.append(digest_value)\n\t\telse:\n\t\t\tdigests_missing.append(digest_value)\n\t\t\n\tif len(digests_missing) > 0:\n\t\tself.error(\n\t\t\t\"The following digests are not used: %s\" % (\n\t\t\t\t\", \".join(digests_missing)\n\t\t\t)\n\t\t)\n\t\n\tif len(digests_present) > 0:\n\t\tself.error(\n\t\t\t\"The following digests are present but not used: %s\" % (\n\t\t\t\t\", \".join(digests_present)\n\t\t\t)\n\t\t)"], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = self.get_digests_present(manifest_files)\n\t\n\tfor digest in digests_present:\n\t\tif digest not in digests_used:\n\t\t\tself.error(\"Digest \" + digest + \" not used in manifest.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmissing_digests = []\n\tused_digests = []\n\tfor digest_file in digests_used:\n\t\tif digest_file not in manifest_files:\n\t\t\tmissing_digests.append(digest_file)\n\t\telse:\n\t\t\tused_digests.append(digest_file)\n\n\tif missing_digests:\n\t\tself.error(\"Digests not used: %s\" % \" \".join(missing_digests))\n\tif used_digests:\n\t\tself.error(\"Digests used but not found: %s\" % \" \".join(used_digests))\n\n\tself.manifest_files = manifest_files\n\tself.digests_used = digests_used\n\n#src/build.py\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\n\nfrom package.build import Build\nfrom package.build_config import BuildConfig\n\nif __name__ == '__main__':\n\tconfig = BuildConfig(sys.argv, os.path.join(os.path.dirname(os.path.dirname(__file__)), \"config.json\"))\n\tBuild(config).run()\n\n#src/package/package_config.py\n# -*- coding: utf-8 -*-\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = set(self.manifest.digests.keys())\n\tdigests_used = set(digests_used)\n\n\tif (digests_present - digests_used).any():\n\t\terror = self.error_class(\n\t\t\t\t\"Digests present in the manifest but not used in the file: %s\" %\n\t\t\t\t(digests_present - digests_used))\n\t\treturn error\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmissing_digests = []\n\tfor manifest_file in manifest_files:\n\t\tmanifest_file_digests = self.get_manifest_file_digests(manifest_file)\n\t\tfor manifest_file_digest in manifest_file_digests:\n\t\t\tif manifest_file_digest not in digests_used:\n\t\t\t\tmissing_digests.append(manifest_file_digest)\n\tif missing_digests:\n\t\tself.error(\n\t\t\t\"Digests %s are not used by any of the input files.\" % (\", \".join(missing_digests))\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests = [manifest_file.digests for manifest_file in manifest_files]\n\n\t# If we have a manifest file, we need to make sure the digest of that file is present in the list of all digests\n\tif len(manifest_digests) > 0:\n\t\tif set(manifest_digests[0]) - set(digests_used):\n\t\t\tself.set_error(\"digest mismatch\")\n\t\n\t# If we have a manifest file, we need to make sure that all digests in that file are present in the list of all digests\n\tif len(manifest_digests) > 1:\n\t\tfor manifest_digest in manifest_digests[0]:\n\t\t\tif manifest_digest not in digests_used:\n\t\t\t\tself.set_error(\"digest mismatch\")", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = []\n\tfor manifest_file in manifest_files:\n\t\tdigests_present += manifest_file['digests']\n\n\tmissing_digests = set(digests_present) - set(digests_used)\n\tif missing_digests:\n\t\traise Error('Missing digests: %s' % missing_digests)\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmissing_digests = []\n\tfor manifest_file in manifest_files:\n\t\tif not manifest_file['digests']:\n\t\t\tcontinue\n\t\tfor digest in manifest_file['digests']:\n\t\t\tif digest not in digests_used:\n\t\t\t\tmissing_digests.append(digest)\n\tif missing_digests:\n\t\tself.error(\"Missing digests: %s\" % \", \".join(missing_digests))", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests_present = []\n\tmanifest_digests_used = []\n\tfor digest in self.digests:\n\t\tmanifest_digests_present.append(digest['digest'])\n\t\tmanifest_digests_used.append(digest['used'])\n\tmissing_digests = list(set(manifest_digests_present) - set(manifest_digests_used))\n\tif missing_digests:\n\t\terror(\"Missing digests: %s\" % \", \".join(missing_digests))\n\tused_digests = list(set(manifest_digests_used) - set(manifest_digests_present))\n\tif used_digests:\n\t\terror(\"Used digests: %s\" % \", \".join(used_digests))\n\tif manifest_digests_present:\n\t\tused_digests = list(set(manifest_digests_used) - set(manifest_digests_present))\n\t\tif used_digests:\n\t\t\terror(\"Used digests: %s\" % \", \".join(used_digests))\n\tmissing_digests = list(set(manifest_digests_present) - set(manifest_digests_used))\n\tif missing_digests:\n\t\terror(\"Missing digests: %s\" % \", \".join(miss", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tmanifest_digests = {}\n\tfor manifest_file in manifest_files:\n\t\tmanifest = ManifestFile(manifest_file)\n\t\tmanifest_digests.update(manifest.digests)\n\n\tfor digest in digests_used:\n\t\tif digest not in manifest_digests:\n\t\t\tself.error('Digest \"%s\" is not used in any manifest file' % digest)\n\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\"\"\"\n\tCheck all digests in manifest that are needed are present and used. Return error() in the class.\n\t\"\"\"\n\tdigests_present = []\n\tdigests_missing = []\n\t\n\tfor f in manifest_files:\n\t\tdigest_value = f.split(\"=\")[1]\n\t\tif digest_value in digests_used:\n\t\t\tdigests_present.append(digest_value)\n\t\telse:\n\t\t\tdigests_missing.append(digest_value)\n\t\t\n\tif len(digests_missing) > 0:\n\t\tself.error(\n\t\t\t\"The following digests are not used: %s\" % (\n\t\t\t\t\", \".join(digests_missing)\n\t\t\t)\n\t\t)\n\t\n\tif len(digests_present) > 0:\n\t\tself.error(\n\t\t\t\"The following digests are present but not used: %s\" % (\n\t\t\t\t\", \".join(digests_present)\n\t\t\t)\n\t\t)", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tdef error():\n\t\traise Exception(\"Invalid prior version for inventory {}. Prior version should be {}.\".format(self.name, prior))\n\n\t# Validate that the prior version is a higher version of the current version\n\tif self.version > prior.version:\n\t\tlogger.info(\"Inventory version is higher than prior version. Validating prior version.\")\n\t\tvalidate_as_prior_version(prior, self)\n\n\t# Validate that the prior version has the same number of inventories\n\tif len(self.inventory_list) != len(prior.inventory_list):\n\t\tlogger.info(\"Inventory version has different number of inventories. Validating prior version.\")\n\t\tvalidate_as_prior_version(prior, self)\n\n\t# Check that the prior version has the same number of inventory items\n\tfor i in range(len(self.inventory_list)):\n\t\tif len(self.inventory_list[i]) != len(prior.inventory_list[i]):\n\t\t\tlogger.info(\"Inventory version has different number of items. Validating prior version.\")\n\t\t\tvalidate_as_prior_version(prior, self)\n\n\t# Validate that the prior version has the same inventory items\n\tfor i in range(len(self.inventory_list)):", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tdef error():\n\t\traise Exception(\"prior version not valid for this inventory. version is not a valid prior version of the current inventory object.\")\n\t\n\tprior_version_names = [item.name for item in prior.inventory_items]\n\tcurrent_version_names = [item.name for item in self.inventory_items]\n\t\n\tcurrent_version_names = list(set(current_version_names))\n\tprior_version_names = list(set(prior_version_names))\n\t\n\tif current_version_names == prior_version_names:\n\t\terror()\n\t\n\tif len(current_version_names) > len(prior_version_names):\n\t\terror()\n\t\n\tfor name in current_version_names:\n\t\tif name not in prior_version_names:\n\t\t\terror()\n\t\n\tis_valid = True\n\tfor name in prior_version_names:\n\t\tif name not in current_version_names:\n\t\t\tis_valid = False\n\t\t\terror()\n\t\n\tif is_valid:\n\t\tpass\n\telse:\n\t\terror()\n\t\n\treturn is_valid\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\tif prior.version==self.version:\n\t\treturn\n\t\n\terror_list=[]\n\t\n\tif prior.version<self.version:\n\t\terror_list.append('Prior version of inventory: '+str(prior.version)+' is less than current inventory version: '+str(self.version))\n\t\n\terror_list.extend(self.check_inventory_object_for_errors(prior))\n\t\n\tif len(error_list)>0:\n\t\terror_string=', '.join(error_list)\n\t\treturn error(error_string)\n\telse:\n\t\treturn\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\tself_version = self.version\n\tprior_version = prior.version\n\t\n\tif prior_version == '':\n\t\treturn 'Prior inventory version missing'\n\t\n\tif self_version == '':\n\t\treturn 'Inventory version missing'\n\t\n\tif self_version == prior_version:\n\t\treturn 'Inventory versions match'\n\t\n\tif (self_version < prior_version):\n\t\treturn 'Prior version is older than current inventory version'\n\t\n\tif (self_version > prior_version):\n\t\treturn 'Current version is older than prior version'\n\n#inventory_validator/__init__.py\nfrom inventory_validator.validator import InventoryValidator\nfrom inventory_validator.inventory import Inventory\nfrom inventory_validator.inventory_object import InventoryObject\nfrom inventory_validator.inventory_object_validator import InventoryObjectValidator\nfrom inventory_validator.version_validator import validate_as_prior_version\n\n#tests/test_inventory_object.py\nimport unittest\nfrom inventory_validator import InventoryObject\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\t# Get the inventory version that we are looking for.\n\tif prior.is_inventory_version_1:\n\t\ttarget_version = \"version 1\"\n\t\ttarget_version_class = InventoryValidatorVersion1\n\telif prior.is_inventory_version_2:\n\t\ttarget_version = \"version 2\"\n\t\ttarget_version_class = InventoryValidatorVersion2\n\telse:\n\t\terror(\"Invalid inventory version for prior inventory: %s\" % (prior_inventory.get_version(),))\n\t\t\n\t# The version of prior must be the same as the current version.\n\tif prior.get_version() != self.get_version():\n\t\terror(\"The inventory versions must be the same: %s != %s\" % (prior.get_version(), self.get_version()))\n\t\t\n\t# The version of prior must be a valid version of our current inventory.\n\tif not prior_inventory.get_version() in target_version_class.valid_versions:\n\t\terror(\"The inventory version of the prior inventory must be a valid version of our inventory: %s (valid versions: %s)\" % (prior_inventory.get_version(), target_version_class.valid_versions))\n\t\t\n\t# The version of the prior inventory must not", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tself.check_version_match(prior)\n\tself.check_version_match(self)\n\t\n\tself.check_prior_version_match(prior)\n\tself.check_prior_version_match(self)\n\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tprior_version = prior.prior_version\n\tself_version = self.version\n\t\n\t# Check that both prior and self version are valid prior versions\n\tif not prior_version.is_valid_prior_version():\n\t\terror(f\"Invalid prior version '{prior_version}' for inventory object.\")\n\t\treturn\n\tif not self_version.is_valid_prior_version():\n\t\terror(f\"Invalid prior version '{self_version}' for inventory object.\")\n\t\treturn\n\t\n\t# Check that prior version is less than or equal to self version\n\tif prior_version.version_number > self_version.version_number:\n\t\terror(f\"Inventory prior version '{prior_version}' is greater than inventory version '{self_version}'.\")\n\t\treturn\n\t\n\t# Check that prior version is less than or equal to current inventory version\n\tif prior_version.version_number > self.version.version_number:\n\t\terror(f\"Inventory prior version '{prior_version}' is greater than inventory version '{self}'.\")\n\t\treturn\n\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tret = []\n\tret.extend(prior.validate_version(self.version))\n\tret.extend(self.validate_version(prior.version))\n\n\treturn ret\n\n@staticmethod", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tself.validate_as_prior_version_internal(prior)\n\tself.check_prior_version_consistency()\n", "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tinventory_version = self.inventory.version\n\tprior_inventory_version = prior.inventory.version\n\tif (prior_inventory_version != inventory_version):\n\t\treturn 'The prior inventory must be of the same version as the current inventory'\n\n\t\n\tprior_version_is_not_prior = prior_inventory_version > inventory_version\n\tif (prior_version_is_not_prior):\n\t\treturn 'The prior inventory version must be prior to the current inventory version'\n\n\t\n\tprior_object_version = prior.inventory.version\n\tprior_inventory_object_version = self.inventory.version\n\tprior_version_is_not_prior_object = prior_inventory_object_version > prior_object_version\n\tif (prior_version_is_not_prior_object):\n\t\treturn 'The prior inventory object version must be prior to the current inventory object version'\n\n\t\n\tprior_inventory_is_not_prior = prior_inventory_object_version > inventory_version\n\tif (prior_inventory_is_not_prior):\n\t\treturn 'The prior inventory must be prior to the current inventory'\n\n\t\n\tprior_version_is_prior = prior_inventory_object"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tdef error():\n\t\traise Exception(\"Invalid prior version for inventory {}. Prior version should be {}.\".format(self.name, prior))\n\n\t# Validate that the prior version is a higher version of the current version\n\tif self.version > prior.version:\n\t\tlogger.info(\"Inventory version is higher than prior version. Validating prior version.\")\n\t\tvalidate_as_prior_version(prior, self)\n\n\t# Validate that the prior version has the same number of inventories\n\tif len(self.inventory_list) != len(prior.inventory_list):\n\t\tlogger.info(\"Inventory version has different number of inventories. Validating prior version.\")\n\t\tvalidate_as_prior_version(prior, self)\n\n\t# Check that the prior version has the same number of inventory items\n\tfor i in range(len(self.inventory_list)):\n\t\tif len(self.inventory_list[i]) != len(prior.inventory_list[i]):\n\t\t\tlogger.info(\"Inventory version has different number of items. Validating prior version.\")\n\t\t\tvalidate_as_prior_version(prior, self)\n\n\t# Validate that the prior version has the same inventory items\n\tfor i in range(len(self.inventory_list)):", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tdef error():\n\t\traise Exception(\"prior version not valid for this inventory. version is not a valid prior version of the current inventory object.\")\n\t\n\tprior_version_names = [item.name for item in prior.inventory_items]\n\tcurrent_version_names = [item.name for item in self.inventory_items]\n\t\n\tcurrent_version_names = list(set(current_version_names))\n\tprior_version_names = list(set(prior_version_names))\n\t\n\tif current_version_names == prior_version_names:\n\t\terror()\n\t\n\tif len(current_version_names) > len(prior_version_names):\n\t\terror()\n\t\n\tfor name in current_version_names:\n\t\tif name not in prior_version_names:\n\t\t\terror()\n\t\n\tis_valid = True\n\tfor name in prior_version_names:\n\t\tif name not in current_version_names:\n\t\t\tis_valid = False\n\t\t\terror()\n\t\n\tif is_valid:\n\t\tpass\n\telse:\n\t\terror()\n\t\n\treturn is_valid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\tif prior.version==self.version:\n\t\treturn\n\t\n\terror_list=[]\n\t\n\tif prior.version<self.version:\n\t\terror_list.append('Prior version of inventory: '+str(prior.version)+' is less than current inventory version: '+str(self.version))\n\t\n\terror_list.extend(self.check_inventory_object_for_errors(prior))\n\t\n\tif len(error_list)>0:\n\t\terror_string=', '.join(error_list)\n\t\treturn error(error_string)\n\telse:\n\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\tself_version = self.version\n\tprior_version = prior.version\n\t\n\tif prior_version == '':\n\t\treturn 'Prior inventory version missing'\n\t\n\tif self_version == '':\n\t\treturn 'Inventory version missing'\n\t\n\tif self_version == prior_version:\n\t\treturn 'Inventory versions match'\n\t\n\tif (self_version < prior_version):\n\t\treturn 'Prior version is older than current inventory version'\n\t\n\tif (self_version > prior_version):\n\t\treturn 'Current version is older than prior version'\n\n#inventory_validator/__init__.py\nfrom inventory_validator.validator import InventoryValidator\nfrom inventory_validator.inventory import Inventory\nfrom inventory_validator.inventory_object import InventoryObject\nfrom inventory_validator.inventory_object_validator import InventoryObjectValidator\nfrom inventory_validator.version_validator import validate_as_prior_version\n\n#tests/test_inventory_object.py\nimport unittest\nfrom inventory_validator import InventoryObject\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\t\n\t# Get the inventory version that we are looking for.\n\tif prior.is_inventory_version_1:\n\t\ttarget_version = \"version 1\"\n\t\ttarget_version_class = InventoryValidatorVersion1\n\telif prior.is_inventory_version_2:\n\t\ttarget_version = \"version 2\"\n\t\ttarget_version_class = InventoryValidatorVersion2\n\telse:\n\t\terror(\"Invalid inventory version for prior inventory: %s\" % (prior_inventory.get_version(),))\n\t\t\n\t# The version of prior must be the same as the current version.\n\tif prior.get_version() != self.get_version():\n\t\terror(\"The inventory versions must be the same: %s != %s\" % (prior.get_version(), self.get_version()))\n\t\t\n\t# The version of prior must be a valid version of our current inventory.\n\tif not prior_inventory.get_version() in target_version_class.valid_versions:\n\t\terror(\"The inventory version of the prior inventory must be a valid version of our inventory: %s (valid versions: %s)\" % (prior_inventory.get_version(), target_version_class.valid_versions))\n\t\t\n\t# The version of the prior inventory must not", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tself.check_version_match(prior)\n\tself.check_version_match(self)\n\t\n\tself.check_prior_version_match(prior)\n\tself.check_prior_version_match(self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tprior_version = prior.prior_version\n\tself_version = self.version\n\t\n\t# Check that both prior and self version are valid prior versions\n\tif not prior_version.is_valid_prior_version():\n\t\terror(f\"Invalid prior version '{prior_version}' for inventory object.\")\n\t\treturn\n\tif not self_version.is_valid_prior_version():\n\t\terror(f\"Invalid prior version '{self_version}' for inventory object.\")\n\t\treturn\n\t\n\t# Check that prior version is less than or equal to self version\n\tif prior_version.version_number > self_version.version_number:\n\t\terror(f\"Inventory prior version '{prior_version}' is greater than inventory version '{self_version}'.\")\n\t\treturn\n\t\n\t# Check that prior version is less than or equal to current inventory version\n\tif prior_version.version_number > self.version.version_number:\n\t\terror(f\"Inventory prior version '{prior_version}' is greater than inventory version '{self}'.\")\n\t\treturn\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tret = []\n\tret.extend(prior.validate_version(self.version))\n\tret.extend(self.validate_version(prior.version))\n\n\treturn ret\n\n@staticmethod", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tself.validate_as_prior_version_internal(prior)\n\tself.check_prior_version_consistency()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t\"\"\"\n\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.\n\t\"\"\"\n\tinventory_version = self.inventory.version\n\tprior_inventory_version = prior.inventory.version\n\tif (prior_inventory_version != inventory_version):\n\t\treturn 'The prior inventory must be of the same version as the current inventory'\n\n\t\n\tprior_version_is_not_prior = prior_inventory_version > inventory_version\n\tif (prior_version_is_not_prior):\n\t\treturn 'The prior inventory version must be prior to the current inventory version'\n\n\t\n\tprior_object_version = prior.inventory.version\n\tprior_inventory_object_version = self.inventory.version\n\tprior_version_is_not_prior_object = prior_inventory_object_version > prior_object_version\n\tif (prior_version_is_not_prior_object):\n\t\treturn 'The prior inventory object version must be prior to the current inventory object version'\n\n\t\n\tprior_inventory_is_not_prior = prior_inventory_object_version > inventory_version\n\tif (prior_inventory_is_not_prior):\n\t\treturn 'The prior inventory must be prior to the current inventory'\n\n\t\n\tprior_version_is_prior = prior_inventory_object", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor item in inventory:\n\t\tlogical_path_map[item] = inventory[item][\"logical_path\"]\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor hostname, host_hash in inventory.items():\n\t\tfor key, value in host_hash.items():\n\t\t\tif key == \"path\":\n\t\t\t\tfor path in value.values():\n\t\t\t\t\tpath_map[path] = hostname\n\treturn path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tresult = {}\n\tfor path in inventory:\n\t\tresult[path] = inventory[path]\n\treturn result\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpaths = {}\n\tfor key, value in inventory.items():\n\t\tif isinstance(value, dict):\n\t\t\tpaths[key] = get_logical_path_map(value, version)\n\t\t\tcontinue\n\t\telif isinstance(value, list):\n\t\t\tpaths[key] = map(lambda x: get_logical_path_map(x, version), value)\n\t\t\tcontinue\n\t\telse:\n\t\t\tpaths[key] = (value, )\n\treturn paths\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\ttry:\n\t\tfor data in inventory:\n\t\t\tif not data.get('type') == 'file':\n\t\t\t\tcontinue\n\t\t\tpath = data.get('path')\n\t\t\tstate = data.get('state')\n\t\t\tlogical_path_map[path] = state\n\texcept Exception as e:\n\t\traise Exception(e)\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor f in inventory.files:\n\t\tlogical_path_map[f] = f.absolute_path(version)\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor i, path in enumerate(inventory):\n\t\tlogical_path_map.update({path: i})\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\t\n\tfor state in inventory.keys():\n\t\tlogical_path_map[state] = []\n\t\n\tfor state in inventory.keys():\n\t\tfor item in inventory[state].keys():\n\t\t\tif item.startswith('file://'):\n\t\t\t\tlogical_path_map[state].append(item)\n\t\n\tif inventory['start'] is not None and version.startswith('2'):\n\t\tlogical_path_map['start'] = ['start']\n\t\n\treturn logical_path_map\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor key in inventory:\n\t\tlogical_path_map[key] = []\n\t\tfor state in inventory[key]:\n\t\t\tif 'path' in state['vars']:\n\t\t\t\tpath = state['vars']['path']\n\t\t\telse:\n\t\t\t\tpath = key\n\t\t\tlogical_path_map[key].append(path)\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor host in inventory:\n\t\tpath_map[host] = get_file_path(inventory[host], version)\n\treturn path_map\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor item in inventory:\n\t\tlogical_path_map[item] = inventory[item][\"logical_path\"]\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor hostname, host_hash in inventory.items():\n\t\tfor key, value in host_hash.items():\n\t\t\tif key == \"path\":\n\t\t\t\tfor path in value.values():\n\t\t\t\t\tpath_map[path] = hostname\n\treturn path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tresult = {}\n\tfor path in inventory:\n\t\tresult[path] = inventory[path]\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpaths = {}\n\tfor key, value in inventory.items():\n\t\tif isinstance(value, dict):\n\t\t\tpaths[key] = get_logical_path_map(value, version)\n\t\t\tcontinue\n\t\telif isinstance(value, list):\n\t\t\tpaths[key] = map(lambda x: get_logical_path_map(x, version), value)\n\t\t\tcontinue\n\t\telse:\n\t\t\tpaths[key] = (value, )\n\treturn paths\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\ttry:\n\t\tfor data in inventory:\n\t\t\tif not data.get('type') == 'file':\n\t\t\t\tcontinue\n\t\t\tpath = data.get('path')\n\t\t\tstate = data.get('state')\n\t\t\tlogical_path_map[path] = state\n\texcept Exception as e:\n\t\traise Exception(e)\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor f in inventory.files:\n\t\tlogical_path_map[f] = f.absolute_path(version)\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor i, path in enumerate(inventory):\n\t\tlogical_path_map.update({path: i})\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\t\n\tfor state in inventory.keys():\n\t\tlogical_path_map[state] = []\n\t\n\tfor state in inventory.keys():\n\t\tfor item in inventory[state].keys():\n\t\t\tif item.startswith('file://'):\n\t\t\t\tlogical_path_map[state].append(item)\n\t\n\tif inventory['start'] is not None and version.startswith('2'):\n\t\tlogical_path_map['start'] = ['start']\n\t\n\treturn logical_path_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tlogical_path_map = {}\n\tfor key in inventory:\n\t\tlogical_path_map[key] = []\n\t\tfor state in inventory[key]:\n\t\t\tif 'path' in state['vars']:\n\t\t\t\tpath = state['vars']['path']\n\t\t\telse:\n\t\t\t\tpath = key\n\t\t\tlogical_path_map[key].append(path)\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\"\"\"\n\tReturns the file paths of the states in the inventory in the dict type.\n\t\"\"\"\n\tpath_map = {}\n\tfor host in inventory:\n\t\tpath_map[host] = get_file_path(inventory[host], version)\n\treturn path_map\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\traise NotImplementedError()", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.check_fixity(fixity)\n\tself.validate_fixity_file_list(manifest_files)", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\terror_msg = \"Fixity block does not exist or is incorrectly formatted\"\n\terror_type = \"inventory_fixity_err\"\n\terror_code = 1000\n\tif self.fixity == None:\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block does not exist\")\n\t\n\ttry:\n\t\tself.fixity = ast.literal_eval(self.fixity)\n\texcept SyntaxError:\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block is not a valid JSON object\")\n\t\n\t\"\"\"\n\tCheck that fixity block is a dictionary\n\t\"\"\"\n\tif not isinstance(self.fixity, dict):\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block is not a dictionary\")\n\t\n\t\"\"\"\n\tCheck that fixity block has files key\n\t\"\"\"\n\tif \"files\" not in self.fixity:\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block does not have files key\")\n\t\n\t\"\"\"\n\tCheck that files key is a list\n\t\"\"\"\n\tif", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\ttry:\n\t\tmanifest_files_list = list(manifest_files)\n\texcept AttributeError:\n\t\tmanifest_files_list = manifest_files\n\t\n\tif manifest_files_list:\n\t\tif fixity[\"file_list\"] is None:\n\t\t\traise error(\"Fixity block must have a file list\")\n\t\tfor file in fixity[\"file_list\"]:\n\t\t\ttry:\n\t\t\t\tmanifest_files_list.remove(file)\n\t\t\texcept ValueError:\n\t\t\t\traise error(\"File '%s' in fixity block file_list not found in manifest\" % (file))\n\tif fixity[\"file_list\"]:\n\t\ttry:\n\t\t\tmanifest_files_list.remove(fixity[\"file_list\"][0])\n\t\texcept ValueError:\n\t\t\traise error(\"File '%s' in fixity block file_list not found in manifest\" % (fixity[\"file_list\"][0]))\n\t\n\t# Validate fixity level\n\tif fixity[\"level\"]:\n\t\tif fixity[\"level\"] not in [\"major\", \"minor\", \"patch\"]:\n\t\t\traise error(\"Fixity level '%s' not supported, only major, minor and patch are supported\" % (fixity[\"level\"]))\n\n\t# Validate fixity version\n\tif fixity[\"", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.fixity = fixity\n\tif len(self.fixity) > 1:\n\t\terror = 'Fixity block must be a single line.'\n\t\treturn error\n\n\ttry:\n\t\tfixity_file, fixity_line = self.fixity.split(':')\n\texcept ValueError:\n\t\terror = 'Fixity block must be in format of file:line.'\n\t\treturn error\n\n\ttry:\n\t\tfixity_file_path = self.manifest_files[fixity_file]\n\texcept KeyError:\n\t\terror = 'Fixity file {} not found in manifest.'.format(fixity_file)\n\t\treturn error\n\n\ttry:\n\t\tfixity_line = int(fixity_line)\n\texcept ValueError:\n\t\terror = 'Fixity line must be an integer.'\n\t\treturn error\n\n\ttry:\n\t\tfixity_line_start = fixity_line - 1\n\texcept NameError:\n\t\terror = 'No fixity block found.'\n\t\treturn error\n\n\twith open(fixity_file_path) as f:\n\t\tfixity_lines = f.readlines()\n\n\tif len(fixity_lines) < fixity_line_start:\n\t\terror = 'Fixity line {} out of range", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tfixity_block = self.get_fixity_block()\n\n\tself.validate_fixity_block(fixity_block)\n\n\tself.validate_fixity(fixity, fixity_block, manifest_files)\n\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\tmanifest_files = self.manifest_files(manifest_files)\n\texcept Exception as err:\n\t\treturn err\n\tfixity_block = self.fixity_block\n\tfile_fixity = fixity_block.get(fixity)\n\tif file_fixity is None:\n\t\treturn error('File fixity block does not define the fixity \"%s\".' % fixity)\n\tif len(file_fixity) != len(manifest_files):\n\t\treturn error('The %s fixity block does not define the same number of files as the manifest file list.' % fixity)\n\t\n\tfor file in manifest_files:\n\t\tfile_fixity_file = file_fixity.get(file)\n\t\tif file_fixity_file is None:\n\t\t\treturn error('The manifest file \"%s\" is not listed in the fixity block for fixity \"%s\".' % (file, fixity))\n\t\telse:\n\t\t\tcontinue\n\treturn True\n\n", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\ttry:\n\t\tif not fixity.has_key(fixity_name):\n\t\t\tself.error(\"No fixity block named \" + fixity_name + \" found in inventory\")\n\t\telse:\n\t\t\tfixity_block = fixity[fixity_name]\n\t\t\tif not fixity_block.has_key('files'):\n\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" has no files key\")\n\t\t\telse:\n\t\t\t\tif not isinstance(fixity_block['files'], list):\n\t\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" has non list of files\")\n\t\t\t\telse:\n\t\t\t\t\tfor file_name in fixity_block['files']:\n\t\t\t\t\t\tif not self.inventory.has_key(file_name):\n\t\t\t\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" refers to non existing file: \" + file_name)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tfile = self.inventory[file_name]\n\t\t\t\t\t\t\t\tif not file.has_key('fixity'):\n\t\t\t\t\t\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" refers to non existing file: \" + file_name)\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tif not file['fixity'].has_key('type", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif fixity.get('block'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('fixity_block_found'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('fixity_block_not_found'):\n\t\tself.error(\"Fixity block not found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('wrong_fixity_type'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('missing_fixity_type'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('wrong_file_type'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at", "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif fixity == None:\n\t\traise error(\"Fixity block is missing in inventory\")\n\t\n\tif not isinstance(fixity, dict):\n\t\traise error(\"Fixity block must be a dictionary\")\n\t\n\tfor key in fixity.keys():\n\t\tif not isinstance(key, str):\n\t\t\traise error(\"Fixity block keys must be strings\")\n\t\t\n\t\tif key not in manifest_files:\n\t\t\traise error(\"Fixity block contains invalid file\")\n\t\n\t\n\tvalid_fixity_values = [ \"=\", \"<\", \">\", \"<=\", \">=\" ]\n\t\n\tfor value in fixity.values():\n\t\tif not isinstance(value, str):\n\t\t\traise error(\"Fixity block values must be strings\")\n\t\t\n\t\tif value not in valid_fixity_values:\n\t\t\traise error(\"Fixity block values must be one of: = < > <= >=\")\n\n#src/manifest/manifest_utils.py"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\traise NotImplementedError()", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.check_fixity(fixity)\n\tself.validate_fixity_file_list(manifest_files)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\terror_msg = \"Fixity block does not exist or is incorrectly formatted\"\n\terror_type = \"inventory_fixity_err\"\n\terror_code = 1000\n\tif self.fixity == None:\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block does not exist\")\n\t\n\ttry:\n\t\tself.fixity = ast.literal_eval(self.fixity)\n\texcept SyntaxError:\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block is not a valid JSON object\")\n\t\n\t\"\"\"\n\tCheck that fixity block is a dictionary\n\t\"\"\"\n\tif not isinstance(self.fixity, dict):\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block is not a dictionary\")\n\t\n\t\"\"\"\n\tCheck that fixity block has files key\n\t\"\"\"\n\tif \"files\" not in self.fixity:\n\t\terror(error_msg, error_type, error_code)\n\t\traise Exception(\"Inventory fixity block does not have files key\")\n\t\n\t\"\"\"\n\tCheck that files key is a list\n\t\"\"\"\n\tif", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\ttry:\n\t\tmanifest_files_list = list(manifest_files)\n\texcept AttributeError:\n\t\tmanifest_files_list = manifest_files\n\t\n\tif manifest_files_list:\n\t\tif fixity[\"file_list\"] is None:\n\t\t\traise error(\"Fixity block must have a file list\")\n\t\tfor file in fixity[\"file_list\"]:\n\t\t\ttry:\n\t\t\t\tmanifest_files_list.remove(file)\n\t\t\texcept ValueError:\n\t\t\t\traise error(\"File '%s' in fixity block file_list not found in manifest\" % (file))\n\tif fixity[\"file_list\"]:\n\t\ttry:\n\t\t\tmanifest_files_list.remove(fixity[\"file_list\"][0])\n\t\texcept ValueError:\n\t\t\traise error(\"File '%s' in fixity block file_list not found in manifest\" % (fixity[\"file_list\"][0]))\n\t\n\t# Validate fixity level\n\tif fixity[\"level\"]:\n\t\tif fixity[\"level\"] not in [\"major\", \"minor\", \"patch\"]:\n\t\t\traise error(\"Fixity level '%s' not supported, only major, minor and patch are supported\" % (fixity[\"level\"]))\n\n\t# Validate fixity version\n\tif fixity[\"", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tself.fixity = fixity\n\tif len(self.fixity) > 1:\n\t\terror = 'Fixity block must be a single line.'\n\t\treturn error\n\n\ttry:\n\t\tfixity_file, fixity_line = self.fixity.split(':')\n\texcept ValueError:\n\t\terror = 'Fixity block must be in format of file:line.'\n\t\treturn error\n\n\ttry:\n\t\tfixity_file_path = self.manifest_files[fixity_file]\n\texcept KeyError:\n\t\terror = 'Fixity file {} not found in manifest.'.format(fixity_file)\n\t\treturn error\n\n\ttry:\n\t\tfixity_line = int(fixity_line)\n\texcept ValueError:\n\t\terror = 'Fixity line must be an integer.'\n\t\treturn error\n\n\ttry:\n\t\tfixity_line_start = fixity_line - 1\n\texcept NameError:\n\t\terror = 'No fixity block found.'\n\t\treturn error\n\n\twith open(fixity_file_path) as f:\n\t\tfixity_lines = f.readlines()\n\n\tif len(fixity_lines) < fixity_line_start:\n\t\terror = 'Fixity line {} out of range", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\tfixity_block = self.get_fixity_block()\n\n\tself.validate_fixity_block(fixity_block)\n\n\tself.validate_fixity(fixity, fixity_block, manifest_files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\ttry:\n\t\tmanifest_files = self.manifest_files(manifest_files)\n\texcept Exception as err:\n\t\treturn err\n\tfixity_block = self.fixity_block\n\tfile_fixity = fixity_block.get(fixity)\n\tif file_fixity is None:\n\t\treturn error('File fixity block does not define the fixity \"%s\".' % fixity)\n\tif len(file_fixity) != len(manifest_files):\n\t\treturn error('The %s fixity block does not define the same number of files as the manifest file list.' % fixity)\n\t\n\tfor file in manifest_files:\n\t\tfile_fixity_file = file_fixity.get(file)\n\t\tif file_fixity_file is None:\n\t\t\treturn error('The manifest file \"%s\" is not listed in the fixity block for fixity \"%s\".' % (file, fixity))\n\t\telse:\n\t\t\tcontinue\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\ttry:\n\t\tif not fixity.has_key(fixity_name):\n\t\t\tself.error(\"No fixity block named \" + fixity_name + \" found in inventory\")\n\t\telse:\n\t\t\tfixity_block = fixity[fixity_name]\n\t\t\tif not fixity_block.has_key('files'):\n\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" has no files key\")\n\t\t\telse:\n\t\t\t\tif not isinstance(fixity_block['files'], list):\n\t\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" has non list of files\")\n\t\t\t\telse:\n\t\t\t\t\tfor file_name in fixity_block['files']:\n\t\t\t\t\t\tif not self.inventory.has_key(file_name):\n\t\t\t\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" refers to non existing file: \" + file_name)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tfile = self.inventory[file_name]\n\t\t\t\t\t\t\t\tif not file.has_key('fixity'):\n\t\t\t\t\t\t\t\t\tself.error(\"Fixity block \" + fixity_name + \" refers to non existing file: \" + file_name)\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\tif not file['fixity'].has_key('type", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif fixity.get('block'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('fixity_block_found'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('fixity_block_not_found'):\n\t\tself.error(\"Fixity block not found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('wrong_fixity_type'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('missing_fixity_type'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at the beginning of the manifest file. Remove the fixity block and try again.\")\n\telif fixity.get('wrong_file_type'):\n\t\tself.error(\"Fixity block found in manifest. Fixity block should be at", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\"\"\"\n\tValidate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.\n\t\"\"\"\n\t\n\tif fixity == None:\n\t\traise error(\"Fixity block is missing in inventory\")\n\t\n\tif not isinstance(fixity, dict):\n\t\traise error(\"Fixity block must be a dictionary\")\n\t\n\tfor key in fixity.keys():\n\t\tif not isinstance(key, str):\n\t\t\traise error(\"Fixity block keys must be strings\")\n\t\t\n\t\tif key not in manifest_files:\n\t\t\traise error(\"Fixity block contains invalid file\")\n\t\n\t\n\tvalid_fixity_values = [ \"=\", \"<\", \">\", \"<=\", \">=\" ]\n\t\n\tfor value in fixity.values():\n\t\tif not isinstance(value, str):\n\t\t\traise error(\"Fixity block values must be strings\")\n\t\t\n\t\tif value not in valid_fixity_values:\n\t\t\traise error(\"Fixity block values must be one of: = < > <= >=\")\n\n#src/manifest/manifest_utils.py", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "_id": "62b463153879012d19481498", "code": ["def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn os.listdir(path)", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor root, dirs, filenames in os.walk(path):\n\t\tfor filename in filenames:\n\t\t\tfiles.append(os.path.join(root, filename))\n\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor file in os.listdir(path):\n\t\tfiles.append(file)\n\treturn files\n\n#src/web/util.py", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\tfiles = os.listdir(path)\n\tfiles = [f for f in files if os.path.isfile(os.path.join(path, f))]\n\treturn files\n\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\n\tfiles = []\n\tfor dirpath, dirnames, filenames in os.walk(path):\n\t\tfiles.extend(filenames)\n\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tret = []\n\tfor file in os.listdir(path):\n\t\tret.append(os.path.join(path, file))\n\treturn ret\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tif os.path.exists(path):\n\t\tif os.path.isfile(path):\n\t\t\tfiles.append(path)\n\t\telse:\n\t\t\tfiles = files + [os.path.join(path, f) for f in os.listdir(path)]\n\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor file in os.listdir(path):\n\t\tfull_path = os.path.join(path, file)\n\t\tif os.path.isfile(full_path):\n\t\t\tfiles.append(full_path)\n\t\telse:\n\t\t\tfiles.extend(files_list(full_path))\n\treturn files\n", "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = os.listdir(path)\n\treturn files\n\n# List files in this directory\nfiles = files_list('./')\n\n# Print files\nfor file in files:\n\tprint(file)\n\n# Get file size\nfile_size = os.path.getsize(file)\nprint(file_size)\n\n# Write file\nwith open(file, 'w') as f:\n\tpass\n\n# Read file\nwith open(file, 'r') as f:\n\tpass\n\n# Delete file\nos.remove(file)\n\n# Remove directory\nos.rmdir(path)\n\n# Create directory\nos.mkdir('./my_dir')\n\n# Create file in directory\nos.mknod('./my_dir/file.txt')\n\n# Move file\nos.rename('./my_dir/file.txt', './my_dir/file_2.txt')\n\n# Change directory\nos.chdir('./my_dir')\n\n# Get current directory\nprint(os.getcwd())\n\n# Get current path\nprint(os.getcwd())\n\n# Get path of current file\nprint(os.path.dirname(__file__))\n\n# Get file name\nprint(os.path.basename(__file__))\n\n# Get parent directory"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn os.listdir(path)", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor root, dirs, filenames in os.walk(path):\n\t\tfor filename in filenames:\n\t\t\tfiles.append(os.path.join(root, filename))\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor file in os.listdir(path):\n\t\tfiles.append(file)\n\treturn files\n\n#src/web/util.py", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\tfiles = os.listdir(path)\n\tfiles = [f for f in files if os.path.isfile(os.path.join(path, f))]\n\treturn files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\t\n\tfiles = []\n\tfor dirpath, dirnames, filenames in os.walk(path):\n\t\tfiles.extend(filenames)\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tret = []\n\tfor file in os.listdir(path):\n\t\tret.append(os.path.join(path, file))\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\timport os\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tif os.path.exists(path):\n\t\tif os.path.isfile(path):\n\t\t\tfiles.append(path)\n\t\telse:\n\t\t\tfiles = files + [os.path.join(path, f) for f in os.listdir(path)]\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = []\n\tfor file in os.listdir(path):\n\t\tfull_path = os.path.join(path, file)\n\t\tif os.path.isfile(full_path):\n\t\t\tfiles.append(full_path)\n\t\telse:\n\t\t\tfiles.extend(files_list(full_path))\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\"\"\"\n\tReturn the files in given path.\n\t\"\"\"\n\tfiles = os.listdir(path)\n\treturn files\n\n# List files in this directory\nfiles = files_list('./')\n\n# Print files\nfor file in files:\n\tprint(file)\n\n# Get file size\nfile_size = os.path.getsize(file)\nprint(file_size)\n\n# Write file\nwith open(file, 'w') as f:\n\tpass\n\n# Read file\nwith open(file, 'r') as f:\n\tpass\n\n# Delete file\nos.remove(file)\n\n# Remove directory\nos.rmdir(path)\n\n# Create directory\nos.mkdir('./my_dir')\n\n# Create file in directory\nos.mknod('./my_dir/file.txt')\n\n# Move file\nos.rename('./my_dir/file.txt', './my_dir/file_2.txt')\n\n# Change directory\nos.chdir('./my_dir')\n\n# Get current directory\nprint(os.getcwd())\n\n# Get current path\nprint(os.getcwd())\n\n# Get path of current file\nprint(os.path.dirname(__file__))\n\n# Get file name\nprint(os.path.basename(__file__))\n\n# Get parent directory", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgrouped = {}\n\tfor file in files:\n\t\txml_filename = file.split('.')[0]\n\t\tif xml_filename in xmls:\n\t\t\tgrouped[xml_filename] = grouped.get(xml_filename, [])\n\t\t\tgrouped[xml_filename].append(file)\n\treturn grouped\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\n\tgroups = {}\n\tfor xml in xmls:\n\t\tgroups[xml] = []\n\t\n\tsource_files = {}\n\tfor source_file in source:\n\t\tsource_files[source_file] = []\n\t\n\tfor file in files:\n\t\tsource_files[file[\"source\"]] = file\n\t\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif file[\"source\"] == xml:\n\t\t\t\tgroups[xml].append(file)\n\t\n\treturn source_files, groups\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgrouped = {}\n\n\tfor xml in xmls:\n\t\tgrouped[xml] = []\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif xml.find(xml.split('.')[0]) == file:\n\t\t\t\tgrouped[xml].append(file)\n\t\t\t\tbreak\n\n\treturn grouped\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txml_to_files = {}\n\n\tfor file in files:\n\t\tif file.endswith(\".xml\"):\n\t\t\txml = file.replace(\".xml\", \"\")\n\t\t\txml_to_files[xml] = []\n\n\tfor file in files:\n\t\tif file.endswith(\".xml\"):\n\t\t\txml = file.replace(\".xml\", \"\")\n\t\t\txml_to_files[xml].append(file)\n\n\txml_to_files = {k: v for k, v in xml_to_files.items() if len(v) > 0}\n\n\treturn xml_to_files\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {\n\t\t\"files\": {\n\t\t\t\"xmls\": {}\n\t\t}\n\t}\n\n\tfor filename in xmls:\n\t\tdata[\"files\"][\"xmls\"][filename] = []\n\n\tfor file in files:\n\t\tfilename = os.path.basename(file)\n\t\tdata[\"files\"][\"xmls\"][filename].append(file)\n\n\treturn data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tfile_data = {}\n\tfor xml in xmls:\n\t\tfile_data[xml] = []\n\tfor file in files:\n\t\tfile_data[file.split(\".\")[0]].append(file)\n\treturn file_data\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txmls_dict = dict()\n\tfor xml in xmls:\n\t\txmls_dict[xml] = list()\n\n\tfor file in files:\n\t\txml_filename = xmls_dict.keys()[xmls.index(file)]\n\t\txmls_dict[xml_filename].append(file)\n\n\treturn xmls_dict\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txmls_files = {}\n\tfor xml, file in zip(xmls, files):\n\t\txmls_files[xml] = file\n\n\tgroups = {}\n\tfor xml, file in xmls_files.iteritems():\n\t\tgroups[xml] = []\n\t\tfor source_filename in source:\n\t\t\tif xml in source_filename:\n\t\t\t\tgroups[xml].append(file)\n\n\treturn groups\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\t\n\tfor file in files:\n\t\txml_filename = file.split('.')[0] + '.xml'\n\t\tif xml_filename in xmls:\n\t\t\tif xml_filename in result:\n\t\t\t\tresult[xml_filename].append(file)\n\t\t\telse:\n\t\t\t\tresult[xml_filename] = [file]\n\t\telse:\n\t\t\tprint('XML file not found in xmls. ' + xml_filename)\n\t\n\tfor key in result:\n\t\tresult[key].sort()\n\n\treturn result\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\t\n\tfor xml in xmls:\n\t\txml_filename = xml.get('filename')\n\t\tresult[xml_filename] = []\n\t\tfor file in files:\n\t\t\t# if file name matches xml file name, add to list\n\t\t\tif file.get('filename') == xml_filename:\n\t\t\t\tresult[xml_filename].append(file)\n\t\n\treturn result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgrouped = {}\n\tfor file in files:\n\t\txml_filename = file.split('.')[0]\n\t\tif xml_filename in xmls:\n\t\t\tgrouped[xml_filename] = grouped.get(xml_filename, [])\n\t\t\tgrouped[xml_filename].append(file)\n\treturn grouped\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\t\n\tgroups = {}\n\tfor xml in xmls:\n\t\tgroups[xml] = []\n\t\n\tsource_files = {}\n\tfor source_file in source:\n\t\tsource_files[source_file] = []\n\t\n\tfor file in files:\n\t\tsource_files[file[\"source\"]] = file\n\t\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif file[\"source\"] == xml:\n\t\t\t\tgroups[xml].append(file)\n\t\n\treturn source_files, groups\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tgrouped = {}\n\n\tfor xml in xmls:\n\t\tgrouped[xml] = []\n\n\tfor file in files:\n\t\tfor xml in xmls:\n\t\t\tif xml.find(xml.split('.')[0]) == file:\n\t\t\t\tgrouped[xml].append(file)\n\t\t\t\tbreak\n\n\treturn grouped\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txml_to_files = {}\n\n\tfor file in files:\n\t\tif file.endswith(\".xml\"):\n\t\t\txml = file.replace(\".xml\", \"\")\n\t\t\txml_to_files[xml] = []\n\n\tfor file in files:\n\t\tif file.endswith(\".xml\"):\n\t\t\txml = file.replace(\".xml\", \"\")\n\t\t\txml_to_files[xml].append(file)\n\n\txml_to_files = {k: v for k, v in xml_to_files.items() if len(v) > 0}\n\n\treturn xml_to_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tdata = {\n\t\t\"files\": {\n\t\t\t\"xmls\": {}\n\t\t}\n\t}\n\n\tfor filename in xmls:\n\t\tdata[\"files\"][\"xmls\"][filename] = []\n\n\tfor file in files:\n\t\tfilename = os.path.basename(file)\n\t\tdata[\"files\"][\"xmls\"][filename].append(file)\n\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tfile_data = {}\n\tfor xml in xmls:\n\t\tfile_data[xml] = []\n\tfor file in files:\n\t\tfile_data[file.split(\".\")[0]].append(file)\n\treturn file_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txmls_dict = dict()\n\tfor xml in xmls:\n\t\txmls_dict[xml] = list()\n\n\tfor file in files:\n\t\txml_filename = xmls_dict.keys()[xmls.index(file)]\n\t\txmls_dict[xml_filename].append(file)\n\n\treturn xmls_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\txmls_files = {}\n\tfor xml, file in zip(xmls, files):\n\t\txmls_files[xml] = file\n\n\tgroups = {}\n\tfor xml, file in xmls_files.iteritems():\n\t\tgroups[xml] = []\n\t\tfor source_filename in source:\n\t\t\tif xml in source_filename:\n\t\t\t\tgroups[xml].append(file)\n\n\treturn groups\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\t\n\tfor file in files:\n\t\txml_filename = file.split('.')[0] + '.xml'\n\t\tif xml_filename in xmls:\n\t\t\tif xml_filename in result:\n\t\t\t\tresult[xml_filename].append(file)\n\t\t\telse:\n\t\t\t\tresult[xml_filename] = [file]\n\t\telse:\n\t\t\tprint('XML file not found in xmls. ' + xml_filename)\n\t\n\tfor key in result:\n\t\tresult[key].sort()\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\"\"\"\n\tGroups files by xmls and returns data in dict format.\n\t\"\"\"\n\tresult = {}\n\t\n\tfor xml in xmls:\n\t\txml_filename = xml.get('filename')\n\t\tresult[xml_filename] = []\n\t\tfor file in files:\n\t\t\t# if file name matches xml file name, add to list\n\t\t\tif file.get('filename') == xml_filename:\n\t\t\t\tresult[xml_filename].append(file)\n\t\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n\n# -----------------------------------------------", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\tif basename.startswith(prefix + \"-\"):\n\t\treturn True\n\telif basename.startswith(prefix + \".\"):\n\t\treturn True\n\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_name = os.path.basename(file_path)\n\tif file_name.startswith(prefix):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_name = os.path.basename(file_path)\n\tif file_name.startswith(prefix + \"-\") or file_name.startswith(prefix + \".\"):\n\t\treturn True\n\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\tif basename.startswith(prefix + \"-\"):\n\t\treturn True\n\telif basename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfrom os.path import basename, splitext\n\n\tif basename(file_path).startswith(prefix):\n\t\treturn True\n\n\tif basename(file_path).startswith(prefix + \".\"):\n\t\treturn True\n\n\treturn False\n\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_basename = os.path.basename(file_path)\n\tif file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\"):\n\t\treturn True\n\treturn False\n", "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfilename = os.path.basename(file_path)\n\treturn filename.startswith(prefix + \"-\" or prefix + \".\")\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n\n# -----------------------------------------------", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\tif basename.startswith(prefix + \"-\"):\n\t\treturn True\n\telif basename.startswith(prefix + \".\"):\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_name = os.path.basename(file_path)\n\tif file_name.startswith(prefix):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_name = os.path.basename(file_path)\n\tif file_name.startswith(prefix + \"-\") or file_name.startswith(prefix + \".\"):\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\tif basename.startswith(prefix + \"-\"):\n\t\treturn True\n\telif basename.startswith(prefix + \".\"):\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfrom os.path import basename, splitext\n\n\tif basename(file_path).startswith(prefix):\n\t\treturn True\n\n\tif basename(file_path).startswith(prefix + \".\"):\n\t\treturn True\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfile_basename = os.path.basename(file_path)\n\tif file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\"):\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\t\"\"\"\n\tGiven a filepath, return true if the basename of the filepath is startswith the given prefix plus \"-\" or the given prefix plus \".\"\n\t\"\"\"\n\tfilename = os.path.basename(file_path)\n\treturn filename.startswith(prefix + \"-\" or prefix + \".\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatching_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor filename in files:\n\t\tif filename.startswith(prefix):\n\t\t\tfilenames.append(filename)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatched_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatched_files.append(file)\n\treturn matched_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tret = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tret.append(file)\n\treturn ret\n", "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatching_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatching_files.append(file)\n\treturn matching_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor filename in files:\n\t\tif filename.startswith(prefix):\n\t\t\tfilenames.append(filename)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tmatched_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tmatched_files.append(file)\n\treturn matched_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor f in files:\n\t\tif f.startswith(prefix):\n\t\t\tfilenames.append(f)\n\treturn filenames\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tselected_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tselected_files.append(file)\n\treturn selected_files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tret = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tret.append(file)\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\t\"\"\"\n\tFor each file in files, return all files taht match the given prefix\n\t\"\"\"\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tdef _group_files_by_xml_filename(filename):\n\t\treturn os.path.splitext(filename)[0].split(\"_\")[-1]\n\n\treturn _group_files_by_xml_filename\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\tfor f in os.listdir(folder):\n\t\tif f.endswith(\".xml\"):\n\t\t\tfiles.append(os.path.join(folder, f))\n\treturn _group_files_by_xml_filename(files)\n\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroups = {}\n\tfile_groups = {}\n\n\tfor f in os.listdir(folder):\n\t\tif f.endswith(\".xml\"):\n\t\t\tgroups = _group_files_by_xml_filename(f, groups)\n\n\t\telif f.endswith(\".zip\"):\n\t\t\tfile_groups = _group_files_by_zip_filename(f, file_groups)\n\n\t\telse:\n\t\t\tprint \"unknown file type: %s\" % f\n\n\treturn (groups, file_groups)\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint(\"Exploring folder: \" + folder)\n\tfiles = []\n\tfolder_contents = os.listdir(folder)\n\tfor name in folder_contents:\n\t\tif not name.endswith(\".xml\"):\n\t\t\tcontinue\n\t\tpath = os.path.join(folder, name)\n\t\tfiles.append(path)\n\tfiles = _group_files_by_xml_filename(files)\n\treturn files\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\tfor path, _, filenames in os.walk(folder):\n\t\tfor filename in filenames:\n\t\t\tfiles.append(os.path.join(path, filename))\n\treturn files\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\tfor fname in os.listdir(folder):\n\t\tif fname.endswith('.xml'):\n\t\t\tfiles.append(fname)\n\treturn _group_files_by_xml_filename(files, folder)\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroups = {}\n\tfiles = _group_files_by_xml_filename(folder)\n\t\n\tfor filename, files_in_group in files.iteritems():\n\t\tgroup = _group_files_by_xml_filename(folder, files_in_group)\n\t\tgroups[filename] = group\n\t\n\treturn groups\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tresults = []\n\tfor root, subdirs, files in os.walk(folder):\n\t\tresults.extend(_group_files_by_xml_filename(files))\n\treturn results\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\tfiles = []\n\tfor root, dirs, files in os.walk(folder):\n\t\tfiles = files + [root]\n\t\tbreak\n\t\n\tgroups = _group_files_by_xml_filename(files)\n\t\n\tfor folder, files_ in groups.items():\n\t\tprint('%s' % folder)\n\t\tfor file_ in files_:\n\t\t\tprint('  %s' % file_)\n\t\n\treturn groups\n", "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroup_files = {}\n\tfor file_path in os.listdir(folder):\n\t\tgroup_files = _group_files_by_xml_filename(group_files, file_path)\n\treturn group_files\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tdef _group_files_by_xml_filename(filename):\n\t\treturn os.path.splitext(filename)[0].split(\"_\")[-1]\n\n\treturn _group_files_by_xml_filename\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\tfor f in os.listdir(folder):\n\t\tif f.endswith(\".xml\"):\n\t\t\tfiles.append(os.path.join(folder, f))\n\treturn _group_files_by_xml_filename(files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroups = {}\n\tfile_groups = {}\n\n\tfor f in os.listdir(folder):\n\t\tif f.endswith(\".xml\"):\n\t\t\tgroups = _group_files_by_xml_filename(f, groups)\n\n\t\telif f.endswith(\".zip\"):\n\t\t\tfile_groups = _group_files_by_zip_filename(f, file_groups)\n\n\t\telse:\n\t\t\tprint \"unknown file type: %s\" % f\n\n\treturn (groups, file_groups)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tprint(\"Exploring folder: \" + folder)\n\tfiles = []\n\tfolder_contents = os.listdir(folder)\n\tfor name in folder_contents:\n\t\tif not name.endswith(\".xml\"):\n\t\t\tcontinue\n\t\tpath = os.path.join(folder, name)\n\t\tfiles.append(path)\n\tfiles = _group_files_by_xml_filename(files)\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\tfor path, _, filenames in os.walk(folder):\n\t\tfor filename in filenames:\n\t\t\tfiles.append(os.path.join(path, filename))\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = []\n\tfor fname in os.listdir(folder):\n\t\tif fname.endswith('.xml'):\n\t\t\tfiles.append(fname)\n\treturn _group_files_by_xml_filename(files, folder)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroups = {}\n\tfiles = _group_files_by_xml_filename(folder)\n\t\n\tfor filename, files_in_group in files.iteritems():\n\t\tgroup = _group_files_by_xml_filename(folder, files_in_group)\n\t\tgroups[filename] = group\n\t\n\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tresults = []\n\tfor root, subdirs, files in os.walk(folder):\n\t\tresults.extend(_group_files_by_xml_filename(files))\n\treturn results\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\t\n\tfiles = []\n\tfor root, dirs, files in os.walk(folder):\n\t\tfiles = files + [root]\n\t\tbreak\n\t\n\tgroups = _group_files_by_xml_filename(files)\n\t\n\tfor folder, files_ in groups.items():\n\t\tprint('%s' % folder)\n\t\tfor file_ in files_:\n\t\t\tprint('  %s' % file_)\n\t\n\treturn groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t\"\"\"\n\tGroups files in the given group by using _group_files_by_xml_filename.\n\t\"\"\"\n\tgroup_files = {}\n\tfor file_path in os.listdir(folder):\n\t\tgroup_files = _group_files_by_xml_filename(group_files, file_path)\n\treturn group_files\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\t\n\tfile_name = os.path.basename(file_path)\n\tfile_name_prefix = file_name[:len(prefix)]\n\tfile_name_prefix_lower = file_name_prefix.lower()\n\t\n\tif file_name_prefix_lower == prefix.lower():\n\t\ttype = \"pdf\"\n\telif file_name_prefix_lower.endswith(\".xml\"):\n\t\ttype = \"xml\"\n\telse:\n\t\treturn None\n\t\n\tif type == \"pdf\":\n\t\treturn { \"file_path\": file_path }\n\telse:\n\t\treturn { \"file_path\": file_path, \"ftype\": type }\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\timport os\n\timport xml.dom.minidom as minidom\n\t\n\tdef _get_extension(file_path):\n\t\t\"\"\"\n\t\tReturn the extension of the file.\n\t\t\"\"\"\n\t\treturn file_path.split(\".\")[-1]\n\n\tdef _get_file_type(file_path):\n\t\t\"\"\"\n\t\tReturn the type of the file.\n\t\t\"\"\"\n\t\tif file_path.endswith(\".pdf\"):\n\t\t\treturn {\n\t\t\t\t\"pdf\": \"pdf\",\n\t\t\t\t\"ppt\": \"ppt\",\n\t\t\t\t\"pptx\": \"pptx\",\n\t\t\t\t\"xls\": \"xls\",\n\t\t\t\t\"xlsx\": \"xlsx\",\n\t\t\t\t\"doc\": \"doc\",\n\t\t\t\t\"docx\": \"docx\",\n\t\t\t}\n\t\telse:\n\t\t\treturn \"unknown\"\n\n\tdef _get_file_ftype_file_path(file_path):\n\t\t\"\"\"\n\t\tReturn a tuple with the type of the file and the file path.\n\t\t\"\"\"\n\t\tif file_path.endswith(\".pdf\"):\n\t\t\treturn \"pdf\", file_path\n\t\telif file_path.endswith(\".ppt\") or file_path.endswith(\".pptx\"):\n\t\t\treturn \"p", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tfile_name = os.path.basename(file_path)\n\tftype = None\n\t\n\tif file_name.startswith(prefix):\n\t\tfile_name = file_name[len(prefix):]\n\t\tif file_name.endswith(\".pdf\"):\n\t\t\tftype = \"pdf\"\n\t\telse:\n\t\t\tftype = \"xml\"\n\tif ftype == None:\n\t\treturn None\n\treturn {\n\t\t\"file_path\": file_path,\n\t\t\"ftype\": ftype\n\t}\n\t", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport xml.etree.ElementTree as ET\n\timport re\n\n\tfile_type = None\n\tif file_path.endswith('.xlsx'):\n\t\tfile_type = 'xlsx'\n\telif file_path.endswith('.xls'):\n\t\tfile_type = 'xls'\n\telif file_path.endswith('.pdf'):\n\t\tfile_type = 'pdf'\n\telif file_path.endswith('.xml'):\n\t\tfile_type = 'xml'\n\telif file_path.endswith('.zip'):\n\t\tfile_type = 'zip'\n\telif file_path.endswith('.rar'):\n\t\tfile_type = 'rar'\n\telif file_path.endswith('.csv'):\n\t\tfile_type = 'csv'\n\telif file_path.endswith('.txt'):\n\t\tfile_type = 'txt'\n\telif file_path.endswith('.docx'):\n\t\tfile_type = 'docx'\n\telif file_path.endswith('.doc'):\n\t\tfile_type = 'doc'\n\telif file_path.endswith('.pptx'):\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\t# read the file\n\twith open(file_path, 'rb') as f:\n\t\tdata = f.read()\n\n\t# create the type of the file\n\tftype = file_path.split(\".\")[1]\n\t\n\t# check the prefix\n\tif prefix == \"pdf\":\n\t\tif ftype == \"pdf\":\n\t\t\treturn {\"ftype\": ftype, \"file_path\": file_path}\n\t\telse:\n\t\t\treturn None\n\telif prefix == \"xml\":\n\t\tif ftype == \"xml\":\n\t\t\treturn {\"ftype\": ftype, \"file_path\": file_path}\n\t\telse:\n\t\t\treturn None\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tftype = \"xml\" if file_path.endswith(\".xml\") else \"pdf\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\n\treturn {\"component_id\": file_path.split(\"/\")[-1].split(\".\")[0], \"file_path\": file_path, \"ftype\": ftype}\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_type = None\n\tcomponent_id = None\n\tftype = \"xml\"\n\tfile_path = os.path.join(prefix, file_path)\n\tfile_name = os.path.basename(file_path)\n\tif file_name.lower().endswith(\".pdf\"):\n\t\tftype = \"pdf\"\n\t\tfile_path = os.path.join(prefix, \"pdf\", file_name)\n\t\tif os.path.exists(file_path):\n\t\t\tcomponent_id = file_name[:-4].upper()\n\t\t\treturn {\n\t\t\t\t\"component_id\": component_id,\n\t\t\t\t\"file_path\": file_path,\n\t\t\t\t\"ftype\": ftype,\n\t\t\t}\n\telse:\n\t\tcomponent_id = file_name[:-4].upper()\n\treturn {\n\t\t\"component_id\": component_id,\n\t\t\"file_path\": file_path,\n\t\t\"ftype\": ftype,\n\t}\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\ttry:\n\t\tfile_path = os.path.abspath(file_path)\n\t\tfile_path = file_path.replace(prefix, '')\n\t\tfile_path = file_path.replace('\\\\', '/')\n\t\tif file_path.endswith('.pdf'):\n\t\t\treturn {\"component_id\": file_path, \"file_path\": file_path}\n\t\telif file_path.endswith('.xml'):\n\t\t\twith open(file_path, 'r') as f:\n\t\t\t\tdata = f.read()\n\t\t\t\tdata = data.split('<')\n\t\t\t\tdata = data[1].split('>')[1]\n\t\t\t\tdata = data.split('</')[0]\n\t\t\t\tdata = data.split('>')\n\t\t\t\tdata = data[1:]\n\t\t\t\tdata = data[0].split(',')\n\t\t\t\tdata = data[0].split('=')\n\t\t\t\tdata = data[1]\n\n\t\t\t\treturn {\"component_id\": file_path, \"file_path\": file_path, \"ftype\": data}\n\t\telse:\n\t\t\treturn None\n\texcept:\n\t\tprint(\"File not found\")\n\t\treturn None\n\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tdoc = xml.dom.minidom.parse(file_path)\n\troot = doc.documentElement\n\t\n\tfile_path = os.path.dirname(file_path)\n\t\n\tfile_type = None\n\t\n\tcomponent_id = root.getAttribute(\"component_id\")\n\t\n\tif prefix in component_id:\n\t\tfile_type = root.getAttribute(\"file_type\")\n\t\tif file_type == \"pdf\":\n\t\t\treturn { \"component_id\": component_id, \"file_path\": file_path }\n\t\telse:\n\t\t\treturn { \"component_id\": component_id, \"file_path\": file_path, \"ftype\": file_type, \"file_path\": file_path }\n\telse:\n\t\treturn None\n", "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_name = os.path.basename(file_path)\n\tfile_type = file_name.split('.')[-1]\n\tfile_type = 'pdf' if file_type == 'pdf' else file_type\n\n\tif file_type in ['pdf']:\n\t\treturn {\n\t\t\t'component_id': 'pdf',\n\t\t\t'file_path': file_path,\n\t\t\t'ftype': file_type,\n\t\t}\n\n\treturn None\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\t\n\tfile_name = os.path.basename(file_path)\n\tfile_name_prefix = file_name[:len(prefix)]\n\tfile_name_prefix_lower = file_name_prefix.lower()\n\t\n\tif file_name_prefix_lower == prefix.lower():\n\t\ttype = \"pdf\"\n\telif file_name_prefix_lower.endswith(\".xml\"):\n\t\ttype = \"xml\"\n\telse:\n\t\treturn None\n\t\n\tif type == \"pdf\":\n\t\treturn { \"file_path\": file_path }\n\telse:\n\t\treturn { \"file_path\": file_path, \"ftype\": type }\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\timport os\n\timport xml.dom.minidom as minidom\n\t\n\tdef _get_extension(file_path):\n\t\t\"\"\"\n\t\tReturn the extension of the file.\n\t\t\"\"\"\n\t\treturn file_path.split(\".\")[-1]\n\n\tdef _get_file_type(file_path):\n\t\t\"\"\"\n\t\tReturn the type of the file.\n\t\t\"\"\"\n\t\tif file_path.endswith(\".pdf\"):\n\t\t\treturn {\n\t\t\t\t\"pdf\": \"pdf\",\n\t\t\t\t\"ppt\": \"ppt\",\n\t\t\t\t\"pptx\": \"pptx\",\n\t\t\t\t\"xls\": \"xls\",\n\t\t\t\t\"xlsx\": \"xlsx\",\n\t\t\t\t\"doc\": \"doc\",\n\t\t\t\t\"docx\": \"docx\",\n\t\t\t}\n\t\telse:\n\t\t\treturn \"unknown\"\n\n\tdef _get_file_ftype_file_path(file_path):\n\t\t\"\"\"\n\t\tReturn a tuple with the type of the file and the file path.\n\t\t\"\"\"\n\t\tif file_path.endswith(\".pdf\"):\n\t\t\treturn \"pdf\", file_path\n\t\telif file_path.endswith(\".ppt\") or file_path.endswith(\".pptx\"):\n\t\t\treturn \"p", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tfile_name = os.path.basename(file_path)\n\tftype = None\n\t\n\tif file_name.startswith(prefix):\n\t\tfile_name = file_name[len(prefix):]\n\t\tif file_name.endswith(\".pdf\"):\n\t\t\tftype = \"pdf\"\n\t\telse:\n\t\t\tftype = \"xml\"\n\tif ftype == None:\n\t\treturn None\n\treturn {\n\t\t\"file_path\": file_path,\n\t\t\"ftype\": ftype\n\t}\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\timport os\n\timport xml.etree.ElementTree as ET\n\timport re\n\n\tfile_type = None\n\tif file_path.endswith('.xlsx'):\n\t\tfile_type = 'xlsx'\n\telif file_path.endswith('.xls'):\n\t\tfile_type = 'xls'\n\telif file_path.endswith('.pdf'):\n\t\tfile_type = 'pdf'\n\telif file_path.endswith('.xml'):\n\t\tfile_type = 'xml'\n\telif file_path.endswith('.zip'):\n\t\tfile_type = 'zip'\n\telif file_path.endswith('.rar'):\n\t\tfile_type = 'rar'\n\telif file_path.endswith('.csv'):\n\t\tfile_type = 'csv'\n\telif file_path.endswith('.txt'):\n\t\tfile_type = 'txt'\n\telif file_path.endswith('.docx'):\n\t\tfile_type = 'docx'\n\telif file_path.endswith('.doc'):\n\t\tfile_type = 'doc'\n\telif file_path.endswith('.pptx'):\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\t# read the file\n\twith open(file_path, 'rb') as f:\n\t\tdata = f.read()\n\n\t# create the type of the file\n\tftype = file_path.split(\".\")[1]\n\t\n\t# check the prefix\n\tif prefix == \"pdf\":\n\t\tif ftype == \"pdf\":\n\t\t\treturn {\"ftype\": ftype, \"file_path\": file_path}\n\t\telse:\n\t\t\treturn None\n\telif prefix == \"xml\":\n\t\tif ftype == \"xml\":\n\t\t\treturn {\"ftype\": ftype, \"file_path\": file_path}\n\t\telse:\n\t\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tftype = \"xml\" if file_path.endswith(\".xml\") else \"pdf\"\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\n\treturn {\"component_id\": file_path.split(\"/\")[-1].split(\".\")[0], \"file_path\": file_path, \"ftype\": ftype}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_type = None\n\tcomponent_id = None\n\tftype = \"xml\"\n\tfile_path = os.path.join(prefix, file_path)\n\tfile_name = os.path.basename(file_path)\n\tif file_name.lower().endswith(\".pdf\"):\n\t\tftype = \"pdf\"\n\t\tfile_path = os.path.join(prefix, \"pdf\", file_name)\n\t\tif os.path.exists(file_path):\n\t\t\tcomponent_id = file_name[:-4].upper()\n\t\t\treturn {\n\t\t\t\t\"component_id\": component_id,\n\t\t\t\t\"file_path\": file_path,\n\t\t\t\t\"ftype\": ftype,\n\t\t\t}\n\telse:\n\t\tcomponent_id = file_name[:-4].upper()\n\treturn {\n\t\t\"component_id\": component_id,\n\t\t\"file_path\": file_path,\n\t\t\"ftype\": ftype,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\ttry:\n\t\tfile_path = os.path.abspath(file_path)\n\t\tfile_path = file_path.replace(prefix, '')\n\t\tfile_path = file_path.replace('\\\\', '/')\n\t\tif file_path.endswith('.pdf'):\n\t\t\treturn {\"component_id\": file_path, \"file_path\": file_path}\n\t\telif file_path.endswith('.xml'):\n\t\t\twith open(file_path, 'r') as f:\n\t\t\t\tdata = f.read()\n\t\t\t\tdata = data.split('<')\n\t\t\t\tdata = data[1].split('>')[1]\n\t\t\t\tdata = data.split('</')[0]\n\t\t\t\tdata = data.split('>')\n\t\t\t\tdata = data[1:]\n\t\t\t\tdata = data[0].split(',')\n\t\t\t\tdata = data[0].split('=')\n\t\t\t\tdata = data[1]\n\n\t\t\t\treturn {\"component_id\": file_path, \"file_path\": file_path, \"ftype\": data}\n\t\telse:\n\t\t\treturn None\n\texcept:\n\t\tprint(\"File not found\")\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\t\n\tdoc = xml.dom.minidom.parse(file_path)\n\troot = doc.documentElement\n\t\n\tfile_path = os.path.dirname(file_path)\n\t\n\tfile_type = None\n\t\n\tcomponent_id = root.getAttribute(\"component_id\")\n\t\n\tif prefix in component_id:\n\t\tfile_type = root.getAttribute(\"file_type\")\n\t\tif file_type == \"pdf\":\n\t\t\treturn { \"component_id\": component_id, \"file_path\": file_path }\n\t\telse:\n\t\t\treturn { \"component_id\": component_id, \"file_path\": file_path, \"ftype\": file_type, \"file_path\": file_path }\n\telse:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t\"\"\"\n\tIdentify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".\n\t\"\"\"\n\tfile_name = os.path.basename(file_path)\n\tfile_type = file_name.split('.')[-1]\n\tfile_type = 'pdf' if file_type == 'pdf' else file_type\n\n\tif file_type in ['pdf']:\n\t\treturn {\n\t\t\t'component_id': 'pdf',\n\t\t\t'file_path': file_path,\n\t\t\t'ftype': file_type,\n\t\t}\n\n\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\trendition_path = os.path.join(self.path, 'renditions', lang, file_path)\n\tos.makedirs(os.path.dirname(rendition_path), exist_ok=True)\n\twith open(rendition_path, 'wb') as f:\n\t\tf.write(self.file_path(lang).read())\n\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tnew_rendition = {\n\t\t\"file_path\": file_path,\n\t\t\"lang\": lang\n\t}\n\n\tself._renditions[lang] = new_rendition\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\trendition_path = os.path.join(self.path, 'renditions', lang, file_path)\n\tos.makedirs(os.path.dirname(rendition_path), exist_ok=True)\n\twith open(rendition_path, 'wb') as f:\n\t\tf.write(self.file_path(lang).read())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tnew_rendition = {\n\t\t\"file_path\": file_path,\n\t\t\"lang\": lang\n\t}\n\n\tself._renditions[lang] = new_rendition\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_rendition(self, lang, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.\n\t\"\"\"\n\tself._renditions[lang] = file_path\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tbasename = basename.lower()\n\tfile_path = file_path\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\n\tself._assets[basename] = file_path\n\t", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\treturn basename\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tbasename = basename.lower()\n\tfile_path = file_path\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\t\n\tself._assets[basename] = file_path\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n\treturn basename\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\t\"\"\"\n\tAssign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.\n\t\"\"\"\n\tself._assets[basename] = file_path\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(zip_path)\n\t\n\tdef _make_zipfile(zip_path):\n\t\t\"\"\"\n\t\tMakes a zipfile for the given zip_path\n\t\t\"\"\"\n\t\ttry:\n\t\t\tzip_file = zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED)\n\t\texcept Exception as e:\n\t\t\tprint(\"Error creating zipfile for {0}: {1}\".format(zip_path, e))\n\t\telse:\n\t\t\treturn zip_file\n\t\n\tdef _write_to_zip(zip_file, key, values):\n\t\t\"\"\"\n\t\tWrites the given values to the given zip_file.\n\t\t\"\"\"\n\t\tfor value in values:\n\t\t\ttry:\n\t\t\t\tzip_file.write(value, key)\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Error writing to zipfile for {0}: {1}\".format(zip_path, e))\n\t\n\t# This code will be run in parallel.\n\tzip_files = dict((os.path.splitext(key)[0], _make_zipfile(key)) for key in files)\n\t\n\t# This code will be run sequentially.\n\tfor key, value in files.", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tresults = {}\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfor xml_file in _group_files_by_xml_filename(zip_file.infolist()):\n\t\t\tresults[os.path.basename(xml_file)] = zip_file.read(xml_file)\n\treturn results\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_files = []\n\twith zipfile.ZipFile(zip_path) as f:\n\t\tfor filename in f.namelist():\n\t\t\tzip_files.append(filename)\n\tgrouped_files = _group_files_by_xml_filename(zip_files)\n\treturn grouped_files\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport os\n\timport zipfile\n\n\tzip_files = os.listdir(zip_path)\n\tgrouped_files = _group_files_by_xml_filename(zip_files)\n\n\treturn grouped_files\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\ttry:\n\t\tzip_file = zipfile.ZipFile(zip_path)\n\texcept:\n\t\traise Exception('Could not open zip file at %s' % zip_path)\n\n\tfile_paths = []\n\tfor file_name in zip_file.namelist():\n\t\tif not file_name.endswith('.xml'):\n\t\t\tcontinue\n\t\tfile_paths.append(file_name)\n\n\treturn _group_files_by_xml_filename(zip_path, file_paths)\n\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tdef _group_files(group_name, files):\n\t\t\"\"\"\n\t\tGroups the given files by using _group_files_by_xml_filename.\n\t\t\"\"\"\n\t\tpath_map = {}\n\t\tfor f in files:\n\t\t\tpath_map[_group_files_by_xml_filename(f)] = f\n\t\treturn path_map\n\tdef _group_files_by_xml_filename(f):\n\t\t\"\"\"\n\t\tGroups files by the xml filename.\n\t\t\"\"\"\n\t\treturn f.split('/')[0].split('.')[0]\n\tdef _explore_zip(zip_path):\n\t\t\"\"\"\n\t\tExplore the given zip path.\n\t\t\"\"\"\n\t\tzip_files = []\n\t\tzip_dirs = []\n\t\tfor dirpath, dirnames, filenames in os.walk(zip_path):\n\t\t\tzip_files.extend(filenames)\n\t\t\tzip_dirs.extend(dirnames)\n\t\tzip_files = sorted(zip_files)\n\t\tzip_dirs = sorted(zip_dirs)\n\t\tzip_files = [os.path.join(dirpath, f) for f in zip_files]\n\t\tzip_dirs = [os.path.join(dirpath", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\txml_file_names = []\n\tfor xml_path in zip_file.namelist():\n\t\tif xml_path.endswith('.xml'):\n\t\t\txml_file_names.append(xml_path)\n\tgroups = _group_files_by_xml_filename(zip_path, xml_file_names)\n\treturn xml_file_names, groups\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip = zipfile.ZipFile(zip_path, 'r')\n\tfiles = _group_files_by_xml_filename(zip)\n\tzip.close()\n\treturn files\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\txml_files = _group_files_by_xml_filename(zip_file)\n\tzip_file.close()\n\treturn xml_files\n", "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\txml_files = [f for f in _group_files_by_xml_filename(zip_path)]\n\treturn sorted(xml_files, key=lambda x: x[0])\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tfiles = _group_files_by_xml_filename(zip_path)\n\t\n\tdef _make_zipfile(zip_path):\n\t\t\"\"\"\n\t\tMakes a zipfile for the given zip_path\n\t\t\"\"\"\n\t\ttry:\n\t\t\tzip_file = zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED)\n\t\texcept Exception as e:\n\t\t\tprint(\"Error creating zipfile for {0}: {1}\".format(zip_path, e))\n\t\telse:\n\t\t\treturn zip_file\n\t\n\tdef _write_to_zip(zip_file, key, values):\n\t\t\"\"\"\n\t\tWrites the given values to the given zip_file.\n\t\t\"\"\"\n\t\tfor value in values:\n\t\t\ttry:\n\t\t\t\tzip_file.write(value, key)\n\t\t\texcept Exception as e:\n\t\t\t\tprint(\"Error writing to zipfile for {0}: {1}\".format(zip_path, e))\n\t\n\t# This code will be run in parallel.\n\tzip_files = dict((os.path.splitext(key)[0], _make_zipfile(key)) for key in files)\n\t\n\t# This code will be run sequentially.\n\tfor key, value in files.", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tresults = {}\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfor xml_file in _group_files_by_xml_filename(zip_file.infolist()):\n\t\t\tresults[os.path.basename(xml_file)] = zip_file.read(xml_file)\n\treturn results\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_files = []\n\twith zipfile.ZipFile(zip_path) as f:\n\t\tfor filename in f.namelist():\n\t\t\tzip_files.append(filename)\n\tgrouped_files = _group_files_by_xml_filename(zip_files)\n\treturn grouped_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\timport os\n\timport zipfile\n\n\tzip_files = os.listdir(zip_path)\n\tgrouped_files = _group_files_by_xml_filename(zip_files)\n\n\treturn grouped_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\ttry:\n\t\tzip_file = zipfile.ZipFile(zip_path)\n\texcept:\n\t\traise Exception('Could not open zip file at %s' % zip_path)\n\n\tfile_paths = []\n\tfor file_name in zip_file.namelist():\n\t\tif not file_name.endswith('.xml'):\n\t\t\tcontinue\n\t\tfile_paths.append(file_name)\n\n\treturn _group_files_by_xml_filename(zip_path, file_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tdef _group_files(group_name, files):\n\t\t\"\"\"\n\t\tGroups the given files by using _group_files_by_xml_filename.\n\t\t\"\"\"\n\t\tpath_map = {}\n\t\tfor f in files:\n\t\t\tpath_map[_group_files_by_xml_filename(f)] = f\n\t\treturn path_map\n\tdef _group_files_by_xml_filename(f):\n\t\t\"\"\"\n\t\tGroups files by the xml filename.\n\t\t\"\"\"\n\t\treturn f.split('/')[0].split('.')[0]\n\tdef _explore_zip(zip_path):\n\t\t\"\"\"\n\t\tExplore the given zip path.\n\t\t\"\"\"\n\t\tzip_files = []\n\t\tzip_dirs = []\n\t\tfor dirpath, dirnames, filenames in os.walk(zip_path):\n\t\t\tzip_files.extend(filenames)\n\t\t\tzip_dirs.extend(dirnames)\n\t\tzip_files = sorted(zip_files)\n\t\tzip_dirs = sorted(zip_dirs)\n\t\tzip_files = [os.path.join(dirpath, f) for f in zip_files]\n\t\tzip_dirs = [os.path.join(dirpath", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\txml_file_names = []\n\tfor xml_path in zip_file.namelist():\n\t\tif xml_path.endswith('.xml'):\n\t\t\txml_file_names.append(xml_path)\n\tgroups = _group_files_by_xml_filename(zip_path, xml_file_names)\n\treturn xml_file_names, groups\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip = zipfile.ZipFile(zip_path, 'r')\n\tfiles = _group_files_by_xml_filename(zip)\n\tzip.close()\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\txml_files = _group_files_by_xml_filename(zip_file)\n\tzip_file.close()\n\treturn xml_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t\"\"\"\n\tGroups the given zip path by using _group_files_by_xml_filename.\n\t\"\"\"\n\txml_files = [f for f in _group_files_by_xml_filename(zip_path)]\n\treturn sorted(xml_files, key=lambda x: x[0])\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\treturn zip_file.namelist()\n\t", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tfor info in zip_ref.filelist:\n\t\t\tfiles.append(info.filename)\n\treturn files\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\tfiles = zip_file.namelist()\n\tzip_file.close()\n\treturn files\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\n\tzf = zipfile.ZipFile(zip_path)\n\tfiles = [os.path.basename(f) for f in zf.filelist]\n\tzf.close()\n\treturn files\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\ttry:\n\t\tfiles_list = zip_file.namelist()\n\texcept Exception as ex:\n\t\tzip_file.close()\n\t\traise ex\n\tzip_file.close()\n\treturn files_list\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzip_f = zipfile.ZipFile(zip_path, 'r')\n\treturn zip_f.namelist()\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfiles = zip_file.namelist()\n\treturn files", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path,'r')\n\treturn zip_file.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\tfrom tqdm import tqdm\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zf:\n\t\tfor file in tqdm(zf.namelist(), desc='extracting files from zip'):\n\t\t\tyield file\n", "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timporter = zipfile.ZipFile(zip_path)\n\timporter.extractall()\n\timporter.close()\n\tfiles_list = os.listdir(os.getcwd())\n\tfiles_list = [f for f in files_list if os.path.isfile(f)]\n\treturn files_list\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\treturn zip_file.namelist()\n\t", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n\t\tfor info in zip_ref.filelist:\n\t\t\tfiles.append(info.filename)\n\treturn files\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path)\n\tfiles = zip_file.namelist()\n\tzip_file.close()\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\n\tzf = zipfile.ZipFile(zip_path)\n\tfiles = [os.path.basename(f) for f in zf.filelist]\n\tzf.close()\n\treturn files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\ttry:\n\t\tfiles_list = zip_file.namelist()\n\texcept Exception as ex:\n\t\tzip_file.close()\n\t\traise ex\n\tzip_file.close()\n\treturn files_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\tzip_f = zipfile.ZipFile(zip_path, 'r')\n\treturn zip_f.namelist()\n", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfiles = zip_file.namelist()\n\treturn files", "is_pass": true, "return_code": 0}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\tzip_file = zipfile.ZipFile(zip_path,'r')\n\treturn zip_file.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timport zipfile\n\timport os\n\tfrom tqdm import tqdm\n\t\n\twith zipfile.ZipFile(zip_path, 'r') as zf:\n\t\tfor file in tqdm(zf.namelist(), desc='extracting files from zip'):\n\t\t\tyield file\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\t\"\"\"\n\tReturn the files in the given zip path.\n\t\"\"\"\n\timporter = zipfile.ZipFile(zip_path)\n\timporter.extractall()\n\timporter.close()\n\tfiles_list = os.listdir(os.getcwd())\n\tfiles_list = [f for f in files_list if os.path.isfile(f)]\n\treturn files_list\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = re.sub(r\"w:st=\\\"(.*?)\\\"\", r\"w-st=\\\"\\1\\\"\", content)\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tnew_content = content.replace(\"w:st=\", \"w-st=\")\n\treturn new_content\n\n", "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = re.sub(r\"w:\\w*\", \"w-\", content)\n\treturn content\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = re.sub(r\"w:st=\\\"(.*?)\\\"\", r\"w-st=\\\"\\1\\\"\", content)\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace(\"w:st=\", \"w-st=\")\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tnew_content = content.replace(\"w:st=\", \"w-st=\")\n\treturn new_content\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\t\"\"\"\n\tReplace \"w:st=\" in content with \"w-st=\".\n\t\"\"\"\n\tcontent = re.sub(r\"w:\\w*\", \"w-\", content)\n\treturn content\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate_data = node.xpath(pubdate_xpaths[0])\n\tif len(pubdate_data) > 0:\n\t\treturn pubdate_data[0]\n\telse:\n\t\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatch = None\n\tfor xpath in pubdate_xpaths:\n\t\tif node.xpath(xpath):\n\t\t\tmatch = node.xpath(xpath)[0]\n\t\t\tbreak\n\treturn match\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\txpaths = pubdate_xpaths\n\tfor xpath in xpaths:\n\t\tif xpath.find(node) != -1:\n\t\t\treturn xpath\n\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = ''\n\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tif node.xpath(pubdate_xpath):\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0]\n\t\t\tbreak\n\n\treturn pubdate\n\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tdef get_pubdate(node):\n\t\t\"\"\"\n\t\tReturns the first pubdate node in the given node's children.\n\t\t\"\"\"\n\t\tfor child in node.children:\n\t\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\t\tif child.xpath(pubdate_xpath):\n\t\t\t\t\treturn child\n\t\treturn None\n\n\tpubdate_node = get_pubdate(node)\n\tif pubdate_node:\n\t\treturn pubdate_node.attrib['pubdate']\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate_xpath = ''\n\tfor xpath in pubdate_xpaths:\n\t\tnode_list = node.xpath(xpath)\n\t\tif len(node_list) > 0:\n\t\t\tpubdate_xpath = node_list[0].xpath('string()')\n\t\t\tbreak\n\treturn pubdate_xpath\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0]\n\t\t\tbreak\n\t\texcept IndexError:\n\t\t\tpass\n\treturn pubdate", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate_xpath = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tprint(\"pubdate_xpath:\", pubdate_xpath)\n\t\tpubdate_node = node.xpath(pubdate_xpath)[0]\n\t\tprint(\"pubdate_node:\", pubdate_node.text)\n\t\tif pubdate_node:\n\t\t\treturn pubdate_node.text\n\treturn None\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfound_pubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)[0]\n\t\t# If the pubdate is a number, we have a valid date.\n\t\t# If it is not a number, it's not a valid date.\n\t\tif (len(pubdate.text) > 0) and (pubdate.text.isdigit()):\n\t\t\tfound_pubdate = pubdate.text\n\n\treturn found_pubdate\n", "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0]\n\t\texcept IndexError:\n\t\t\t# No match\n\t\t\tpass\n\treturn pubdate\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate_data = node.xpath(pubdate_xpaths[0])\n\tif len(pubdate_data) > 0:\n\t\treturn pubdate_data[0]\n\telse:\n\t\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tmatch = None\n\tfor xpath in pubdate_xpaths:\n\t\tif node.xpath(xpath):\n\t\t\tmatch = node.xpath(xpath)[0]\n\t\t\tbreak\n\treturn match\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\txpaths = pubdate_xpaths\n\tfor xpath in xpaths:\n\t\tif xpath.find(node) != -1:\n\t\t\treturn xpath\n\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = ''\n\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tif node.xpath(pubdate_xpath):\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0]\n\t\t\tbreak\n\n\treturn pubdate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tdef get_pubdate(node):\n\t\t\"\"\"\n\t\tReturns the first pubdate node in the given node's children.\n\t\t\"\"\"\n\t\tfor child in node.children:\n\t\t\tfor pubdate_xpath in pubdate_xpaths:\n\t\t\t\tif child.xpath(pubdate_xpath):\n\t\t\t\t\treturn child\n\t\treturn None\n\n\tpubdate_node = get_pubdate(node)\n\tif pubdate_node:\n\t\treturn pubdate_node.attrib['pubdate']\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate_xpath = ''\n\tfor xpath in pubdate_xpaths:\n\t\tnode_list = node.xpath(xpath)\n\t\tif len(node_list) > 0:\n\t\t\tpubdate_xpath = node_list[0].xpath('string()')\n\t\t\tbreak\n\treturn pubdate_xpath\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0]\n\t\t\tbreak\n\t\texcept IndexError:\n\t\t\tpass\n\treturn pubdate", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate_xpath = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tprint(\"pubdate_xpath:\", pubdate_xpath)\n\t\tpubdate_node = node.xpath(pubdate_xpath)[0]\n\t\tprint(\"pubdate_node:\", pubdate_node.text)\n\t\tif pubdate_node:\n\t\t\treturn pubdate_node.text\n\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tfound_pubdate = None\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)[0]\n\t\t# If the pubdate is a number, we have a valid date.\n\t\t# If it is not a number, it's not a valid date.\n\t\tif (len(pubdate.text) > 0) and (pubdate.text.isdigit()):\n\t\t\tfound_pubdate = pubdate.text\n\n\treturn found_pubdate\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\t\"\"\"\n\tFor the given node, returns the first match in the pubdate_xpaths list.\n\t\"\"\"\n\tpubdate = None\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(pubdate_xpath)[0]\n\t\texcept IndexError:\n\t\t\t# No match\n\t\t\tpass\n\treturn pubdate\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tif isinstance(issue, basestring):\n\t\tissue = [issue]\n\tfor issue_element in issue:\n\t\tif 'number' in issue_element:\n\t\t\tnumber = issue_element['number']\n\t\tif 'sup' in issue_element:\n\t\t\tsup = issue_element['sup']\n\treturn number, sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber, *sup = issue.split('.')\n\treturn number, ''.join(sup)\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\t\n\tcontents = issue.contents\n\t\n\tif len(contents) > 0 and contents[0].tag == 'number':\n\t\tnumber = contents[0].contents[0].contents[0]\n\t\tsup = contents[0].contents[1].contents[0]\n\t\n\treturn (number, sup)\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\tnumber = None\n\tsup = None\n\tfor number_element in issue.xpath('.//number'):\n\t\tif number_element.text:\n\t\t\tnumber = number_element.text\n\t\tif number_element.tail:\n\t\t\tsup = number_element.tail\n\t\n\treturn (number, sup)\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\t# Extract the number and the possible supplment from the contents of the issue.\n\tnumber = None\n\tsup = None\n\t\n\tfor line in issue.find_all('li'):\n\t\tif line.get('class') == ['number', 'number-1']:\n\t\t\tnumber = line.get_text()\n\t\telif line.get('class') == ['sup', 'sup-1']:\n\t\t\tsup = line.get_text()\n\t\n\treturn number, sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\tnumber_and_supplment = issue.contents[0].strip()\n\t\n\tnumber = number_and_supplment.split(\"-\")[0]\n\tsupplment = number_and_supplment.split(\"-\")[1]\n\n\treturn number, supplment\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = issue.xpath('@number')[0]\n\tsup = issue.xpath('@sup')[0]\n\treturn number, sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_sup = []\n\tfor elem in issue.content:\n\t\tif elem.type == 'number-and-sup':\n\t\t\tnumber_and_sup.append(elem.text)\n\treturn number_and_sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber, sup = None, None\n\ttry:\n\t\tnumber = int(issue.contents[1].contents[0])\n\texcept:\n\t\tpass\n\ttry:\n\t\tsup = int(issue.contents[1].contents[1])\n\texcept:\n\t\tpass\n\treturn number, sup\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\t\n\tfor element in issue.find_elements_by_class_name(\"item__number\"):\n\t\tnumber = element.text\n\t\n\tfor element in issue.find_elements_by_class_name(\"item__sup\"):\n\t\tsup = element.text\n\t\n\treturn (number, sup)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\tif isinstance(issue, basestring):\n\t\tissue = [issue]\n\tfor issue_element in issue:\n\t\tif 'number' in issue_element:\n\t\t\tnumber = issue_element['number']\n\t\tif 'sup' in issue_element:\n\t\t\tsup = issue_element['sup']\n\treturn number, sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber, *sup = issue.split('.')\n\treturn number, ''.join(sup)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\t\n\tcontents = issue.contents\n\t\n\tif len(contents) > 0 and contents[0].tag == 'number':\n\t\tnumber = contents[0].contents[0].contents[0]\n\t\tsup = contents[0].contents[1].contents[0]\n\t\n\treturn (number, sup)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\tnumber = None\n\tsup = None\n\tfor number_element in issue.xpath('.//number'):\n\t\tif number_element.text:\n\t\t\tnumber = number_element.text\n\t\tif number_element.tail:\n\t\t\tsup = number_element.tail\n\t\n\treturn (number, sup)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\t# Extract the number and the possible supplment from the contents of the issue.\n\tnumber = None\n\tsup = None\n\t\n\tfor line in issue.find_all('li'):\n\t\tif line.get('class') == ['number', 'number-1']:\n\t\t\tnumber = line.get_text()\n\t\telif line.get('class') == ['sup', 'sup-1']:\n\t\t\tsup = line.get_text()\n\t\n\treturn number, sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\t\n\tnumber_and_supplment = issue.contents[0].strip()\n\t\n\tnumber = number_and_supplment.split(\"-\")[0]\n\tsupplment = number_and_supplment.split(\"-\")[1]\n\n\treturn number, supplment\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = issue.xpath('@number')[0]\n\tsup = issue.xpath('@sup')[0]\n\treturn number, sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber_and_sup = []\n\tfor elem in issue.content:\n\t\tif elem.type == 'number-and-sup':\n\t\t\tnumber_and_sup.append(elem.text)\n\treturn number_and_sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber, sup = None, None\n\ttry:\n\t\tnumber = int(issue.contents[1].contents[0])\n\texcept:\n\t\tpass\n\ttry:\n\t\tsup = int(issue.contents[1].contents[1])\n\texcept:\n\t\tpass\n\treturn number, sup\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t\"\"\"\n\tReturn the possible values of number and sup from the contents of issue.\n\t\"\"\"\n\tnumber = None\n\tsup = None\n\t\n\tfor element in issue.find_elements_by_class_name(\"item__number\"):\n\t\tnumber = element.text\n\t\n\tfor element in issue.find_elements_by_class_name(\"item__sup\"):\n\t\tsup = element.text\n\t\n\treturn (number, sup)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent_str = \" \" * indent\n\tresult = f\"{indent_str}{self.__class__.__name__}({repr(self.obj)})\"\n\tif debug:\n\t\tresult = f\"{result}\\n{indent_str}    Debug: {self.debug}\"\n\treturn result\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 2\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = \", debug=True\"\n\tif isinstance(self, self.__class__):\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\telif isinstance(self, dict):\n\t\tdetails = \"\\n\"\n\t\tfor key, value in self.items():\n\t\t\tdetails += f\"  {key}: {value}\\n\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{{\\n{details}}}  )\"\n\telif isinstance(self, list):\n\t\tdetails = \"\\n\"\n\t\tfor item in self:\n\t\t\tdetails += f\"  {item}\\n\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{{\\n{details}}}  )\"\n\telse:\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 4\n\tout = (\" \" * indent) + f\"{self.__class__.__name__}({self.obj})\"\n\tif self.children:\n\t\tout += \"\\n\"\n\t\tfor child in self.children:\n\t\t\tout += child.pretty(indent=indent, debug=debug)\n\t\telse:\n\t\t\tout += \"\\n\"\n\treturn out", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = \",\\ndebug=True\"\n\t\n\tdef debug_repr(obj):\n\t\treturn str(obj)\n\t\n\tif isinstance(self.obj, str):\n\t\tdebug_repr = pretty_str\n\t\n\tindented_repr = \" \" * indent + f\"{self.__class__.__name__}({debug_details}{debug_repr(self.obj)})\"\n\t\n\treturn indented_repr\n\npretty_str = pretty.__get__(pretty)\n\n#pretty/__init__.py\nfrom ._pretty import pretty\n\n__all__ = [\"pretty\"]\n\n#pretty/__main__.py\nfrom pretty import pretty\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindentation = \" \" * indent\n\n\tdef _debug_detail(self):\n\t\t\"\"\"Returns debug details for a pretty print.\"\"\"\n\t\treturn str(self.obj)\n\n\tdef _pretty_detail(self):\n\t\t\"\"\"Returns pretty details for a pretty print.\"\"\"\n\t\treturn repr(self.obj)\n\n\tdef _get_detail(self):\n\t\t\"\"\"Returns a string for pretty printing.\"\"\"\n\t\tif debug:\n\t\t\treturn _debug_detail(self)\n\t\telse:\n\t\t\treturn _pretty_detail(self)\n\n\tdef __repr__(self):\n\t\treturn self._get_detail()\n\n\tdef __str__(self):\n\t\treturn self.__repr__()\n\n\tdef __format__(self, format_spec):\n\t\treturn self.__repr__()\n\n\tdef __int__(self):\n\t\treturn self.obj\n\n\tdef __float__(self):\n\t\treturn self.obj\n\n\tdef __bool__(self):\n\t\treturn self.obj\n\n\tdef __add__(self, other):\n\t\treturn self.obj + other.obj\n\n\tdef __sub__(self, other):\n\t\treturn self.obj - other.obj\n\n\tdef __mul__(self, other):\n\t\treturn self.obj * other.obj\n\n\tdef __truediv", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 1\n\t_indent = (\" \" * indent)\n\tindentation = \" \"\n\tif indent == 0:\n\t\tindentation = \"\"\n\tobj = self.obj\n\tif isinstance(obj, str):\n\t\tobj = f\"'{obj}'\"\n\telif debug:\n\t\tobj = debug_details(obj, indentation)\n\treturn f\"{_indent}{self.__class__.__name__}({indentation}{obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += self.get_indent()\n\tindent += 4\n\tindent_text = \" \" * indent\n\tindent_debug = \" \" * (indent + 4)\n\tindent_debug_text = \" \" * indent_debug\n\tindent_debug_details = \" \" * (indent_debug + 4)\n\tindent_debug_debug = \" \" * (indent_debug + 8)\n\tindent_debug_debug_text = \" \" * indent_debug_debug\n\tindent_debug_debug_details = \" \" * (indent_debug_debug + 4)\n\tindent_debug_debug_debug = \" \" * (indent_debug_debug + 8)\n\tindent_debug_debug_debug_text = \" \" * indent_debug_debug_debug\n\tindent_debug_debug_debug_details = \" \" * (indent_debug_debug_debug + 4)\n\tindent_debug_debug_debug_debug = \" \" * (indent_debug_debug_debug + 8)\n\tindent_debug_debug_debug_debug_text = \" \" * indent_debug_debug_debug_debug\n\tindent_debug_debug_debug_debug_details = \" \" * (indent_debug_debug_debug_debug + 4)\n\tindent_debug_debug_debug_debug_debug = \" \"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = 4\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = indent + 1\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f\"\\n{indent} - debug details: {self.debug_details}\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{self.obj})\"", "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 1\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = \" \" * indent\n\tobj = self.obj\n\tif isinstance(obj, str):\n\t\tobj = f\"'{obj}'\"\n\telif isinstance(obj, list):\n\t\tobj = \"[\"\n\t\tfor item in obj:\n\t\t\tobj += pretty(self, indent, debug)\n\t\tobj += \"]\"\n\telif isinstance(obj, dict):\n\t\tobj = \"{\"\n\t\tfor key, value in obj.items():\n\t\t\tobj += pretty(self, indent, debug)\n\t\tobj += \"}\"\n\telse:\n\t\ttry:\n\t\t\tobj = repr(obj)\n\t\texcept:\n\t\t\tobj = f\"'{obj}'\"\n\tobj = (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\treturn obj\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent_str = \" \" * indent\n\tresult = f\"{indent_str}{self.__class__.__name__}({repr(self.obj)})\"\n\tif debug:\n\t\tresult = f\"{result}\\n{indent_str}    Debug: {self.debug}\"\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 2\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = \", debug=True\"\n\tif isinstance(self, self.__class__):\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\telif isinstance(self, dict):\n\t\tdetails = \"\\n\"\n\t\tfor key, value in self.items():\n\t\t\tdetails += f\"  {key}: {value}\\n\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{{\\n{details}}}  )\"\n\telif isinstance(self, list):\n\t\tdetails = \"\\n\"\n\t\tfor item in self:\n\t\t\tdetails += f\"  {item}\\n\"\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{{\\n{details}}}  )\"\n\telse:\n\t\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 4\n\tout = (\" \" * indent) + f\"{self.__class__.__name__}({self.obj})\"\n\tif self.children:\n\t\tout += \"\\n\"\n\t\tfor child in self.children:\n\t\t\tout += child.pretty(indent=indent, debug=debug)\n\t\telse:\n\t\t\tout += \"\\n\"\n\treturn out", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = \",\\ndebug=True\"\n\t\n\tdef debug_repr(obj):\n\t\treturn str(obj)\n\t\n\tif isinstance(self.obj, str):\n\t\tdebug_repr = pretty_str\n\t\n\tindented_repr = \" \" * indent + f\"{self.__class__.__name__}({debug_details}{debug_repr(self.obj)})\"\n\t\n\treturn indented_repr\n\npretty_str = pretty.__get__(pretty)\n\n#pretty/__init__.py\nfrom ._pretty import pretty\n\n__all__ = [\"pretty\"]\n\n#pretty/__main__.py\nfrom pretty import pretty\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindentation = \" \" * indent\n\n\tdef _debug_detail(self):\n\t\t\"\"\"Returns debug details for a pretty print.\"\"\"\n\t\treturn str(self.obj)\n\n\tdef _pretty_detail(self):\n\t\t\"\"\"Returns pretty details for a pretty print.\"\"\"\n\t\treturn repr(self.obj)\n\n\tdef _get_detail(self):\n\t\t\"\"\"Returns a string for pretty printing.\"\"\"\n\t\tif debug:\n\t\t\treturn _debug_detail(self)\n\t\telse:\n\t\t\treturn _pretty_detail(self)\n\n\tdef __repr__(self):\n\t\treturn self._get_detail()\n\n\tdef __str__(self):\n\t\treturn self.__repr__()\n\n\tdef __format__(self, format_spec):\n\t\treturn self.__repr__()\n\n\tdef __int__(self):\n\t\treturn self.obj\n\n\tdef __float__(self):\n\t\treturn self.obj\n\n\tdef __bool__(self):\n\t\treturn self.obj\n\n\tdef __add__(self, other):\n\t\treturn self.obj + other.obj\n\n\tdef __sub__(self, other):\n\t\treturn self.obj - other.obj\n\n\tdef __mul__(self, other):\n\t\treturn self.obj * other.obj\n\n\tdef __truediv", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 1\n\t_indent = (\" \" * indent)\n\tindentation = \" \"\n\tif indent == 0:\n\t\tindentation = \"\"\n\tobj = self.obj\n\tif isinstance(obj, str):\n\t\tobj = f\"'{obj}'\"\n\telif debug:\n\t\tobj = debug_details(obj, indentation)\n\treturn f\"{_indent}{self.__class__.__name__}({indentation}{obj})\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += self.get_indent()\n\tindent += 4\n\tindent_text = \" \" * indent\n\tindent_debug = \" \" * (indent + 4)\n\tindent_debug_text = \" \" * indent_debug\n\tindent_debug_details = \" \" * (indent_debug + 4)\n\tindent_debug_debug = \" \" * (indent_debug + 8)\n\tindent_debug_debug_text = \" \" * indent_debug_debug\n\tindent_debug_debug_details = \" \" * (indent_debug_debug + 4)\n\tindent_debug_debug_debug = \" \" * (indent_debug_debug + 8)\n\tindent_debug_debug_debug_text = \" \" * indent_debug_debug_debug\n\tindent_debug_debug_debug_details = \" \" * (indent_debug_debug_debug + 4)\n\tindent_debug_debug_debug_debug = \" \" * (indent_debug_debug_debug + 8)\n\tindent_debug_debug_debug_debug_text = \" \" * indent_debug_debug_debug_debug\n\tindent_debug_debug_debug_debug_details = \" \" * (indent_debug_debug_debug_debug + 4)\n\tindent_debug_debug_debug_debug_debug = \" \"", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = 4\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent = indent + 1\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = f\"\\n{indent} - debug details: {self.debug_details}\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{self.obj})\"", "is_pass": true, "return_code": 0}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\t\"\"\"\n\tReturn a pretty formatted representation of self. obj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj) return (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\"\"\"\n\tindent += 1\n\tdebug_details = \"\"\n\tif debug:\n\t\tdebug_details = \" \" * indent\n\tobj = self.obj\n\tif isinstance(obj, str):\n\t\tobj = f\"'{obj}'\"\n\telif isinstance(obj, list):\n\t\tobj = \"[\"\n\t\tfor item in obj:\n\t\t\tobj += pretty(self, indent, debug)\n\t\tobj += \"]\"\n\telif isinstance(obj, dict):\n\t\tobj = \"{\"\n\t\tfor key, value in obj.items():\n\t\t\tobj += pretty(self, indent, debug)\n\t\tobj += \"}\"\n\telse:\n\t\ttry:\n\t\t\tobj = repr(obj)\n\t\texcept:\n\t\t\tobj = f\"'{obj}'\"\n\tobj = (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\treturn obj\n", "is_pass": true, "return_code": 0}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tself.assert_num_args(1, args)\n\treturn [self.absorb(args[0])]\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\n\texp = []\n\twhile args:\n\t\targ = args.pop(0)\n\t\tif isinstance(arg, list):\n\t\t\texp.append(absorb(self, arg))\n\t\telse:\n\t\t\texp.append(arg)\n\n\treturn exp\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\t# The first argument is always an integer.\n\tn = args.pop(0)\n\t# The remaining arguments are the list of arguments.\n\targs = [absorb(self, args) for args in zip(*args)]\n\t# Absorb each pair of arguments.\n\treturn [n] + args\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = self.eval_args(args)\n\tresult = []\n\tfor arg in args:\n\t\tif not result:\n\t\t\tresult.append(arg)\n\t\telse:\n\t\t\tresult.append(self.absorb(result[-1], arg))\n\treturn result\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tn = len(args)\n\te = args[0]\n\tfor i in range(1, n):\n\t\te = e.absorp(args[i])\n\treturn e\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tresult = []\n\n\tfor expr in args:\n\t\tif isinstance(expr, Expression):\n\t\t\tresult.append(expr.absorb())\n\t\telse:\n\t\t\tresult.append(expr)\n\n\treturn result\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tabsorbed = []\n\tfor arg in args:\n\t\tabsorbed.append(self.absorb(arg))\n\treturn absorbed\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs_ = []\n\tfor e in args:\n\t\targs_.append(e.absorb(self))\n\treturn self.constructor(args_)\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\targs.reverse()\n\n\twhile len(args) > 1:\n\t\texpr = args.pop()\n\t\tif isinstance(expr, self._type):\n\t\t\targs.append(expr.absorb(self))\n\n\treturn args[0]\n", "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tseq = []\n\tfor arg in args:\n\t\tif arg is self:\n\t\t\tseq.append(self.absorb())\n\t\telse:\n\t\t\tseq.append(arg)\n\treturn seq\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tself.assert_num_args(1, args)\n\treturn [self.absorb(args[0])]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\n\texp = []\n\twhile args:\n\t\targ = args.pop(0)\n\t\tif isinstance(arg, list):\n\t\t\texp.append(absorb(self, arg))\n\t\telse:\n\t\t\texp.append(arg)\n\n\treturn exp\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\t# The first argument is always an integer.\n\tn = args.pop(0)\n\t# The remaining arguments are the list of arguments.\n\targs = [absorb(self, args) for args in zip(*args)]\n\t# Absorb each pair of arguments.\n\treturn [n] + args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = self.eval_args(args)\n\tresult = []\n\tfor arg in args:\n\t\tif not result:\n\t\t\tresult.append(arg)\n\t\telse:\n\t\t\tresult.append(self.absorb(result[-1], arg))\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tn = len(args)\n\te = args[0]\n\tfor i in range(1, n):\n\t\te = e.absorp(args[i])\n\treturn e\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tresult = []\n\n\tfor expr in args:\n\t\tif isinstance(expr, Expression):\n\t\t\tresult.append(expr.absorb())\n\t\telse:\n\t\t\tresult.append(expr)\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tabsorbed = []\n\tfor arg in args:\n\t\tabsorbed.append(self.absorb(arg))\n\treturn absorbed\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs_ = []\n\tfor e in args:\n\t\targs_.append(e.absorb(self))\n\treturn self.constructor(args_)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\targs = list(args)\n\targs.reverse()\n\n\twhile len(args) > 1:\n\t\texpr = args.pop()\n\t\tif isinstance(expr, self._type):\n\t\t\targs.append(expr.absorb(self))\n\n\treturn args[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t\"\"\"\n\tGiven an `args` sequence of expressions, return a new list of expression applying absorption.\n\t\"\"\"\n\tseq = []\n\tfor arg in args:\n\t\tif arg is self:\n\t\t\tseq.append(self.absorb())\n\t\telse:\n\t\t\tseq.append(arg)\n\treturn seq\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.__hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(callback):\n\t\tself._hooks.setdefault(hook, []).append(callback)\n\t\treturn callback\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook] = self._hooks.get(hook, [])\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(f):\n\t\tself._hooks[hook].append(f)\n\t\treturn f\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(f):\n\t\tself.hooks[hook].append(f)\n\t\treturn f\n\treturn decorator\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(f):\n\t\tself.registry[hook].append(f)\n\t\treturn f\n\treturn decorator\n\n", "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.__hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(callback):\n\t\tself._hooks.setdefault(hook, []).append(callback)\n\t\treturn callback\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook] = self._hooks.get(hook, [])\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself.hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(f):\n\t\tself._hooks[hook].append(f)\n\t\treturn f\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(f):\n\t\tself.hooks[hook].append(f)\n\t\treturn f\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(f):\n\t\tself.registry[hook].append(f)\n\t\treturn f\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\t\"\"\"\n\tReturn a decorator function to add a new handler to the \"hook\" in the registry in the class.\n\t\"\"\"\n\tdef decorator(func):\n\t\tself._hooks[hook].append(func)\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"ca\": \"./certs/ca.pem\",\n\t\t\t\"cert\": \"./certs/client.crt\",\n\t\t\t\"key\": \"./certs/client.key\",\n\t\t\t\"verify\": 1\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"method\": \"certificate\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"method\": \"certificate\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enable\": True,\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 80\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\"\n\t\t}\n\t}\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = etcd.client(host=etcd_host, port=etcd_port, protocol='https')\n\tconfig.put(user, {\"key\": \"tls\", \"value\": \"true\"})\n\tconfig.put(user, {\"key\": \"authentication\", \"value\": \"true\"})\n\tconfig.put(user, {\"key\": \"authorization\", \"value\": \"true\"})\n\tconfig.put(user, {\"key\": \"etcd\", \"value\": \"https://\" + etcd_host + \":\" + etcd_port})\n\tconfig.put(user, {\"key\": \"docs\", \"value\": \"https://docs.etcd.io/latest/\"})\n\tconfig.put(user, {\"key\": \"log\", \"value\": \"true\"})\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": False,\n\t\t\"authentication\": {\n\t\t\t\"type\": None,\n\t\t\t\"options\": {}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": None,\n\t\t\t\"options\": {}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"\",\n\t\t\t\"port\": 8000,\n\t\t\t\"url\": \"\",\n\t\t\t\"auth\": {}\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"filepath\": None,\n\t\t\t\"append\": False,\n\t\t\t\"rotate\": False,\n\t\t\t\"maxsize\": 1024,\n\t\t\t\"maxlines\": 100000,\n\t\t\t\"maxbackups\": 10,\n\t\t}\n\t}\n\treturn config\n\n#app/config.py\nfrom .base_config import base_config\nfrom .base_config import *\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"key\": {\n\t\t\t\"tls\": {\n\t\t\t\t\"cert_file\": \"/etc/kubernetes/certificates/kubernetes.pem\",\n\t\t\t\t\"key_file\": \"/etc/kubernetes/certificates/kubernetes-key.pem\",\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"token_auth\": {\n\t\t\t\t\t\"enabled\": True,\n\t\t\t\t},\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"non_admin_enable\": True,\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port,\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"info\",\n\t\t\t},\n\t\t},\n\t\t\"user\": {\n\t\t\t\"name\": user,\n\t\t\t\"password\": \"password\",\n\t\t},\n\t}\n\treturn config", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = dict(\n\t\tuser = user,\n\t\tauthentication = dict(method = \"basic\"),\n\t\tauthorization = dict(method = \"none\"),\n\t\tetcd = dict(\n\t\t\thost = etcd_host,\n\t\t\tport = etcd_port\n\t\t),\n\t\tdocs = dict(\n\t\t\tenabled = True,\n\t\t\ttitle = \"Example API\",\n\t\t\tversion = \"0.0.1\",\n\t\t\tdescription = \"Example API\",\n\t\t\tlicense = \"Apache 2.0\",\n\t\t\tcontact_name = \"Example API\",\n\t\t\tcontact_url = \"https://example.com\",\n\t\t\tcontact_email = \"example@example.com\"\n\t\t),\n\t\tlog = dict(\n\t\t\tlevel = \"error\",\n\t\t\tformat = \"json\",\n\t\t\tconsole = True\n\t\t)\n\t)\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"apiVersion\": \"v1\",\n\t\t\"kind\": \"Config\",\n\t\t\"clusters\": [\n\t\t\t{\n\t\t\t\t\"name\": \"kubernetes\",\n\t\t\t\t\"cluster\": {\n\t\t\t\t\t\"server\": etcd_host + \":\" + etcd_port,\n\t\t\t\t\t\"certificate-authority\": \"/etc/kubernetes/ssl/ca.pem\"\n\t\t\t\t}\n\t\t\t}\n\t\t],\n\t\t\"contexts\": [\n\t\t\t{\n\t\t\t\t\"name\": \"kubernetes\",\n\t\t\t\t\"context\": {\n\t\t\t\t\t\"cluster\": \"kubernetes\",\n\t\t\t\t\t\"user\": user\n\t\t\t\t}\n\t\t\t}\n\t\t],\n\t\t\"current-context\": \"kubernetes\",\n\t\t\"users\": [\n\t\t\t{\n\t\t\t\t\"name\": user,\n\t\t\t\t\"user\": {\n\t\t\t\t\t\"client-certificate\": \"/etc/kubernetes/ssl/client.pem\",\n\t\t\t\t\t\"client-key\": \"/etc/kubernetes/ssl/client-key.pem\"\n\t\t\t\t}\n\t\t\t}\n\t\t]\n\t}\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"user\": user,\n\t\t\"tls\": {\n\t\t\t\"key\": \"./certs/key.pem\",\n\t\t\t\"cert\": \"./certs/cert.pem\",\n\t\t\t\"ca_cert\": \"./certs/ca.pem\",\n\t\t\t\"insecure_skip_verify\": True\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"method\": \"tls\",\n\t\t\t\"tls_config\": {\n\t\t\t\t\"cert\": \"./certs/cert.pem\",\n\t\t\t\t\"key\": \"./certs/key.pem\",\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"method\": \"tls\",\n\t\t\t\"tls_config\": {\n\t\t\t\t\"cert\": \"./certs/cert.pem\",\n\t\t\t\t\"key\": \"./certs/key.pem\",\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": \"2379\"\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": \"8080\",\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\"\n\t\t}\n\t}\n\t\n\treturn config\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\n\t\n\tconfig_dict = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"endpoint\": \"http://localhost:8000\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"level\": \"debug\"\n\t\t}\n\t}\n\t\n\treturn config_dict", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"user\": user,\n\t\t\"tls\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"tls/key.pem\",\n\t\t\t\"cert\": \"tls/cert.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"authentication/authentication.key\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"authorization/authorization.key\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"docs/docs.key\",\n\t\t\t\"cert\": \"docs/cert.pem\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"path\": \"log/log.log\"\n\t\t}\n\t}\n\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cafile\": None,\n\t\t\t\"certfile\": None,\n\t\t\t\"keyfile\": None\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"realm\": \"AuthMe\",\n\t\t\t\"auth_url\": \"http://localhost:5000/auth\",\n\t\t\t\"user_url\": \"http://localhost:5000/users\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t\t\"prefix\": \"/authme\"\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"url\": \"http://localhost:8080/api/doc\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"ERROR\",\n\t\t\t\"file\": \"authme.log\",\n\t\t\t\"max_size\": 1048576,\n\t\t\t\"max_files\": 10,\n\t\t\t\"max_backup_files\": 10\n\t\t}\n\t}\n\treturn config\n"], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"ca\": \"./certs/ca.pem\",\n\t\t\t\"cert\": \"./certs/client.crt\",\n\t\t\t\"key\": \"./certs/client.key\",\n\t\t\t\"verify\": 1\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"method\": \"certificate\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"method\": \"certificate\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enable\": True,\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 80\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\"\n\t\t}\n\t}\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = etcd.client(host=etcd_host, port=etcd_port, protocol='https')\n\tconfig.put(user, {\"key\": \"tls\", \"value\": \"true\"})\n\tconfig.put(user, {\"key\": \"authentication\", \"value\": \"true\"})\n\tconfig.put(user, {\"key\": \"authorization\", \"value\": \"true\"})\n\tconfig.put(user, {\"key\": \"etcd\", \"value\": \"https://\" + etcd_host + \":\" + etcd_port})\n\tconfig.put(user, {\"key\": \"docs\", \"value\": \"https://docs.etcd.io/latest/\"})\n\tconfig.put(user, {\"key\": \"log\", \"value\": \"true\"})\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": False,\n\t\t\"authentication\": {\n\t\t\t\"type\": None,\n\t\t\t\"options\": {}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": None,\n\t\t\t\"options\": {}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"\",\n\t\t\t\"port\": 8000,\n\t\t\t\"url\": \"\",\n\t\t\t\"auth\": {}\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"filepath\": None,\n\t\t\t\"append\": False,\n\t\t\t\"rotate\": False,\n\t\t\t\"maxsize\": 1024,\n\t\t\t\"maxlines\": 100000,\n\t\t\t\"maxbackups\": 10,\n\t\t}\n\t}\n\treturn config\n\n#app/config.py\nfrom .base_config import base_config\nfrom .base_config import *\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"key\": {\n\t\t\t\"tls\": {\n\t\t\t\t\"cert_file\": \"/etc/kubernetes/certificates/kubernetes.pem\",\n\t\t\t\t\"key_file\": \"/etc/kubernetes/certificates/kubernetes-key.pem\",\n\t\t\t},\n\t\t\t\"authentication\": {\n\t\t\t\t\"token_auth\": {\n\t\t\t\t\t\"enabled\": True,\n\t\t\t\t},\n\t\t\t},\n\t\t\t\"authorization\": {\n\t\t\t\t\"non_admin_enable\": True,\n\t\t\t},\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port,\n\t\t\t},\n\t\t\t\"docs\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t},\n\t\t\t\"log\": {\n\t\t\t\t\"level\": \"info\",\n\t\t\t},\n\t\t},\n\t\t\"user\": {\n\t\t\t\"name\": user,\n\t\t\t\"password\": \"password\",\n\t\t},\n\t}\n\treturn config", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = dict(\n\t\tuser = user,\n\t\tauthentication = dict(method = \"basic\"),\n\t\tauthorization = dict(method = \"none\"),\n\t\tetcd = dict(\n\t\t\thost = etcd_host,\n\t\t\tport = etcd_port\n\t\t),\n\t\tdocs = dict(\n\t\t\tenabled = True,\n\t\t\ttitle = \"Example API\",\n\t\t\tversion = \"0.0.1\",\n\t\t\tdescription = \"Example API\",\n\t\t\tlicense = \"Apache 2.0\",\n\t\t\tcontact_name = \"Example API\",\n\t\t\tcontact_url = \"https://example.com\",\n\t\t\tcontact_email = \"example@example.com\"\n\t\t),\n\t\tlog = dict(\n\t\t\tlevel = \"error\",\n\t\t\tformat = \"json\",\n\t\t\tconsole = True\n\t\t)\n\t)\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"apiVersion\": \"v1\",\n\t\t\"kind\": \"Config\",\n\t\t\"clusters\": [\n\t\t\t{\n\t\t\t\t\"name\": \"kubernetes\",\n\t\t\t\t\"cluster\": {\n\t\t\t\t\t\"server\": etcd_host + \":\" + etcd_port,\n\t\t\t\t\t\"certificate-authority\": \"/etc/kubernetes/ssl/ca.pem\"\n\t\t\t\t}\n\t\t\t}\n\t\t],\n\t\t\"contexts\": [\n\t\t\t{\n\t\t\t\t\"name\": \"kubernetes\",\n\t\t\t\t\"context\": {\n\t\t\t\t\t\"cluster\": \"kubernetes\",\n\t\t\t\t\t\"user\": user\n\t\t\t\t}\n\t\t\t}\n\t\t],\n\t\t\"current-context\": \"kubernetes\",\n\t\t\"users\": [\n\t\t\t{\n\t\t\t\t\"name\": user,\n\t\t\t\t\"user\": {\n\t\t\t\t\t\"client-certificate\": \"/etc/kubernetes/ssl/client.pem\",\n\t\t\t\t\t\"client-key\": \"/etc/kubernetes/ssl/client-key.pem\"\n\t\t\t\t}\n\t\t\t}\n\t\t]\n\t}\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"user\": user,\n\t\t\"tls\": {\n\t\t\t\"key\": \"./certs/key.pem\",\n\t\t\t\"cert\": \"./certs/cert.pem\",\n\t\t\t\"ca_cert\": \"./certs/ca.pem\",\n\t\t\t\"insecure_skip_verify\": True\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"method\": \"tls\",\n\t\t\t\"tls_config\": {\n\t\t\t\t\"cert\": \"./certs/cert.pem\",\n\t\t\t\t\"key\": \"./certs/key.pem\",\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"method\": \"tls\",\n\t\t\t\"tls_config\": {\n\t\t\t\t\"cert\": \"./certs/cert.pem\",\n\t\t\t\t\"key\": \"./certs/key.pem\",\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": \"2379\"\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": \"8080\",\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\"\n\t\t}\n\t}\n\t\n\treturn config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\t\n\t\n\tconfig_dict = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"endpoint\": \"http://localhost:8000\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"level\": \"debug\"\n\t\t}\n\t}\n\t\n\treturn config_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"user\": user,\n\t\t\"tls\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"tls/key.pem\",\n\t\t\t\"cert\": \"tls/cert.pem\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"authentication/authentication.key\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"authorization/authorization.key\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"key\": \"docs/docs.key\",\n\t\t\t\"cert\": \"docs/cert.pem\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"path\": \"log/log.log\"\n\t\t}\n\t}\n\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\t\"\"\"\n\tCreates a configuration with some simple parameters, the key parameters contains \"tls\", \"authentication\", \"authorization\", \"etcd\", \"docs\", and \"log\".\n\t\"\"\"\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cafile\": None,\n\t\t\t\"certfile\": None,\n\t\t\t\"keyfile\": None\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"realm\": \"AuthMe\",\n\t\t\t\"auth_url\": \"http://localhost:5000/auth\",\n\t\t\t\"user_url\": \"http://localhost:5000/users\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t\t\"prefix\": \"/authme\"\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"url\": \"http://localhost:8080/api/doc\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"ERROR\",\n\t\t\t\"file\": \"authme.log\",\n\t\t\t\"max_size\": 1048576,\n\t\t\t\"max_files\": 10,\n\t\t\t\"max_backup_files\": 10\n\t\t}\n\t}\n\treturn config\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\timport pytz\n\tfrom datetime import datetime\n\ttz = pytz.timezone(self.timezone)\n\tutc_dt = pytz.utc.localize(dt)\n\treturn tz.localize(utc_dt)\n\ntime = property(_fromutc)", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tz or timezone.utc\n\tdt = dt.replace(tzinfo=tz)\n\tif self.tz:\n\t\tdt = dt.astimezone(self.tz)\n\treturn dt\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = pytz.timezone(dt.tzinfo.tzname())\n\treturn tz.localize(dt).astimezone(pytz.timezone(self.user_timezone))\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = pytz.timezone(self.timezone)\n\tdt = dt.replace(tzinfo=pytz.utc)\n\treturn dt.astimezone(tz)\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = getattr(self, '_tz', None)\n\tif tz and self._tz is not None:\n\t\tif tz.utcoffset(dt) is None:\n\t\t\tdt = dt.replace(tzinfo=None)\n\t\telif tz.dst(dt) is None:\n\t\t\tdt = dt.replace(tzinfo=None)\n\t\telse:\n\t\t\tdt = dt.replace(tzinfo=tz)\n\t\treturn dt\n\treturn dt\n\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tfrom pytz import utc, timezone\n\tfrom datetime import datetime\n\n\tnew_dt = None\n\tif not isinstance(dt, datetime):\n\t\tnew_dt = datetime.fromtimestamp(dt)\n\telse:\n\t\tnew_dt = dt\n\n\tif self.timezone_name != 'UTC':\n\t\tnew_dt = new_dt.astimezone(timezone(self.timezone_name))\n\treturn new_dt\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tnew_dt = dt.replace(tzinfo=None)\n\treturn self.localize(new_dt)\n\n# Adds in the timezone aware now function\ndatetime.datetime.now = _now\ndatetime.datetime.utcnow = _utcnow\n\n# Adds in the timezone aware fromutc function\ndatetime.datetime.fromutc = _fromutc\ndatetime.datetime.utcfromtimestamp = utcfromtimestamp\n\n# Adds in the timezone aware fromlocal function\ndatetime.datetime.fromlocal = _fromlocal\n\n# Adds in the timezone aware now function\ndatetime.datetime.now = _now\ndatetime.datetime.utcnow = _utcnow", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo\n\tif not isinstance(tz, pytz.tzinfo.DstTzInfo):\n\t\t# Assume it's UTC if it doesn't have a tzinfo,\n\t\t# or if it's a UTC datetime already.\n\t\treturn dt\n\tdt = dt.astimezone(tz)\n\treturn dt\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo.tzname(dt)\n\tdt = dt.replace(tzinfo=pytz.utc)\n\tdt = dt.astimezone(pytz.timezone(tz))\n\treturn dt.replace(tzinfo=None)\n\n", "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo\n\n\t# If there is no timezone, return the original datetime.\n\tif not tz:\n\t\treturn dt\n\n\t# If the new timezone is the same as the original, we can do a simple timezone conversion.\n\tif self.tzinfo == tz:\n\t\treturn dt.astimezone(tz)\n\n\t# If the new timezone is not the same as the original, then we need to convert the datetime.\n\tdt = dt.astimezone(pytz.timezone(self.tzinfo.zone))\n\treturn dt.astimezone(pytz.timezone(tz.zone))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\timport pytz\n\tfrom datetime import datetime\n\ttz = pytz.timezone(self.timezone)\n\tutc_dt = pytz.utc.localize(dt)\n\treturn tz.localize(utc_dt)\n\ntime = property(_fromutc)", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tz or timezone.utc\n\tdt = dt.replace(tzinfo=tz)\n\tif self.tz:\n\t\tdt = dt.astimezone(self.tz)\n\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = pytz.timezone(dt.tzinfo.tzname())\n\treturn tz.localize(dt).astimezone(pytz.timezone(self.user_timezone))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = pytz.timezone(self.timezone)\n\tdt = dt.replace(tzinfo=pytz.utc)\n\treturn dt.astimezone(tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = getattr(self, '_tz', None)\n\tif tz and self._tz is not None:\n\t\tif tz.utcoffset(dt) is None:\n\t\t\tdt = dt.replace(tzinfo=None)\n\t\telif tz.dst(dt) is None:\n\t\t\tdt = dt.replace(tzinfo=None)\n\t\telse:\n\t\t\tdt = dt.replace(tzinfo=tz)\n\t\treturn dt\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tfrom pytz import utc, timezone\n\tfrom datetime import datetime\n\n\tnew_dt = None\n\tif not isinstance(dt, datetime):\n\t\tnew_dt = datetime.fromtimestamp(dt)\n\telse:\n\t\tnew_dt = dt\n\n\tif self.timezone_name != 'UTC':\n\t\tnew_dt = new_dt.astimezone(timezone(self.timezone_name))\n\treturn new_dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\tnew_dt = dt.replace(tzinfo=None)\n\treturn self.localize(new_dt)\n\n# Adds in the timezone aware now function\ndatetime.datetime.now = _now\ndatetime.datetime.utcnow = _utcnow\n\n# Adds in the timezone aware fromutc function\ndatetime.datetime.fromutc = _fromutc\ndatetime.datetime.utcfromtimestamp = utcfromtimestamp\n\n# Adds in the timezone aware fromlocal function\ndatetime.datetime.fromlocal = _fromlocal\n\n# Adds in the timezone aware now function\ndatetime.datetime.now = _now\ndatetime.datetime.utcnow = _utcnow", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo\n\tif not isinstance(tz, pytz.tzinfo.DstTzInfo):\n\t\t# Assume it's UTC if it doesn't have a tzinfo,\n\t\t# or if it's a UTC datetime already.\n\t\treturn dt\n\tdt = dt.astimezone(tz)\n\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo.tzname(dt)\n\tdt = dt.replace(tzinfo=pytz.utc)\n\tdt = dt.astimezone(pytz.timezone(tz))\n\treturn dt.replace(tzinfo=None)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone datetime in a given timezone, calculates a timezone datetime in a new timezone.\n\t\"\"\"\n\ttz = self.tzinfo\n\n\t# If there is no timezone, return the original datetime.\n\tif not tz:\n\t\treturn dt\n\n\t# If the new timezone is the same as the original, we can do a simple timezone conversion.\n\tif self.tzinfo == tz:\n\t\treturn dt.astimezone(tz)\n\n\t# If the new timezone is not the same as the original, then we need to convert the datetime.\n\tdt = dt.astimezone(pytz.timezone(self.tzinfo.zone))\n\treturn dt.astimezone(pytz.timezone(tz.zone))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.hours = int(self.hours)\n\tself.minutes = int(self.minutes)\n\tself.seconds = int(self.seconds)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = float(self)\n\tself.minutes = self.seconds / 60.0\n\tself.hours = self.minutes / 60.0\n\tself.days = self.hours / 24.0\n\tself.weeks = self.days / 7.0\n\tself.years = self.days / 365.0\n\tself.months = self.days / 30.0\n\tself.years_months = self.years + self.months\n\tself.years_weeks = self.years_months * 4.0\n\tself.years_days = self.years_weeks * 5.0\n\tself.years_weeks_days = self.years_days * 7.0\n\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.years = int(self.years)\n\tself.months = int(self.months)\n\tself.years_months = int(self.years_months)\n\tself.years_weeks = int(self", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.year = int(self.year)\n\tself.month = int(self.month)\n\tself.day = int(self.day)\n\tself.hour = int(self.hour)\n\tself.minute = int(self.minute)\n\tself.second = int(self.second)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.hours = self.hours / 1.0\n\tself.minutes = self.minutes / 1.0\n\tself.seconds = self.seconds / 1.0\n\tself.microseconds = self.microseconds / 1.0\n\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.year = int(self.year)\n\tself.month = int(self.month)\n\tself.day = int(self.day)\n\tself.hour = int(self.hour)\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.total_length = self.total_length.astype(int)\n\tself.total_length = self.total_length.astype(int)\n\tself.total_length = self.total_length + 1\n\tself.total_length = self.total_length.astype(int)\n\tself.total_length = self.total_length.astype(int)\n\treturn self\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.time = [int(i) for i in self.time]\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.start = int(self.start)\n\tself.end = int(self.end)\n\n\tself.start_time = datetime.datetime.strptime(self.start, '%Y-%m-%d %H:%M:%S')\n\tself.end_time = datetime.datetime.strptime(self.end, '%Y-%m-%d %H:%M:%S')\n\n\tself.start_time = self.start_time.replace(tzinfo=None)\n\tself.end_time = self.end_time.replace(tzinfo=None)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.days = int(self.days)\n\tself.hours = int(self.hours)\n\tself.minutes = int(self.minutes)\n\tself.seconds = int(self.seconds)\n\tself.microseconds = int(self.microseconds)\n\treturn self\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = int(self.seconds)\n\tself.hours = int(self.hours)\n\tself.minutes = int(self.minutes)\n\tself.seconds = int(self.seconds)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.seconds = float(self)\n\tself.minutes = self.seconds / 60.0\n\tself.hours = self.minutes / 60.0\n\tself.days = self.hours / 24.0\n\tself.weeks = self.days / 7.0\n\tself.years = self.days / 365.0\n\tself.months = self.days / 30.0\n\tself.years_months = self.years + self.months\n\tself.years_weeks = self.years_months * 4.0\n\tself.years_days = self.years_weeks * 5.0\n\tself.years_weeks_days = self.years_days * 7.0\n\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.years = int(self.years)\n\tself.months = int(self.months)\n\tself.years_months = int(self.years_months)\n\tself.years_weeks = int(self", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.year = int(self.year)\n\tself.month = int(self.month)\n\tself.day = int(self.day)\n\tself.hour = int(self.hour)\n\tself.minute = int(self.minute)\n\tself.second = int(self.second)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.hours = self.hours / 1.0\n\tself.minutes = self.minutes / 1.0\n\tself.seconds = self.seconds / 1.0\n\tself.microseconds = self.microseconds / 1.0\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.year = int(self.year)\n\tself.month = int(self.month)\n\tself.day = int(self.day)\n\tself.hour = int(self.hour)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.total_length = self.total_length.astype(int)\n\tself.total_length = self.total_length.astype(int)\n\tself.total_length = self.total_length + 1\n\tself.total_length = self.total_length.astype(int)\n\tself.total_length = self.total_length.astype(int)\n\treturn self\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.time = [int(i) for i in self.time]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.start = int(self.start)\n\tself.end = int(self.end)\n\n\tself.start_time = datetime.datetime.strptime(self.start, '%Y-%m-%d %H:%M:%S')\n\tself.end_time = datetime.datetime.strptime(self.end, '%Y-%m-%d %H:%M:%S')\n\n\tself.start_time = self.start_time.replace(tzinfo=None)\n\tself.end_time = self.end_time.replace(tzinfo=None)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalized(self):\n\t\"\"\"\n\tNormalize all units of time to integers.\n\t\"\"\"\n\tself.days = int(self.days)\n\tself.hours = int(self.hours)\n\tself.minutes = int(self.minutes)\n\tself.seconds = int(self.seconds)\n\tself.microseconds = int(self.microseconds)\n\treturn self\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(name):\n\t\tname = name.decode('utf-8')\n\t\treturn namefunc(name)\n\treturn wrapper\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef inner(*args, **kwargs):\n\t\ttzname = namefunc(*args, **kwargs)\n\t\tif isinstance(tzname, bytes):\n\t\t\treturn tzname.decode('utf-8')\n\t\telse:\n\t\t\treturn tzname\n\n\treturn inner\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef namefunc2(*args):\n\t\tname = namefunc(*args)\n\t\tif isinstance(name, str):\n\t\t\treturn name.encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(*args)\n\treturn namefunc2\n\n\nif __name__ == '__main__':\n\timport pytz\n\tprint(pytz.tzname_in_python2(pytz.timezone))\n\tprint(pytz.tzname_in_python2(pytz.timezone('America/New_York')))", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef tzname_in_python2_inner(tzname, *args):\n\t\tif isinstance(tzname, bytes):\n\t\t\ttzname = tzname.decode('utf-8')\n\t\treturn namefunc(tzname, *args)\n\treturn tzname_in_python2_inner\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef inner(*args, **kwargs):\n\t\ttzname = namefunc(*args, **kwargs)\n\t\tif isinstance(tzname, unicode):\n\t\t\ttzname = tzname.encode('utf-8')\n\t\treturn tzname\n\treturn inner\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tnamefunc = namefunc.encode(\"ascii\", \"ignore\")\n\tif namefunc == \"\":\n\t\tnamefunc = \"None\"\n\treturn namefunc\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tname = namefunc(*args, **kwargs)\n\t\treturn name.encode('utf-8')\n\treturn wrapper\n\ntzname_in_python2 = tzname_in_python2(tzname)\n\n# Copyright (c) 2015,2016,2017,2018,2019,2020,2021,2022,2023 Viktor Stinner\n# License: MIT\n#\n# The MIT License (MIT)\n#\n# Copyright (c) 2015,2016,2017,2018,2019,2020,2021,2022,2023 Viktor Stinner\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tnamefunc = namefunc.encode('ascii', 'replace')\n\n\treturn namefunc\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(name):\n\t\tname = namefunc(name)\n\t\treturn name.decode('utf-8')\n\treturn wrapper\n\n", "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(tzname):\n\t\tif isinstance(tzname, str):\n\t\t\ttzname = tzname.encode('utf-8')\n\t\treturn namefunc(tzname)\n\treturn wrapper\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(name):\n\t\tname = name.decode('utf-8')\n\t\treturn namefunc(name)\n\treturn wrapper\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef inner(*args, **kwargs):\n\t\ttzname = namefunc(*args, **kwargs)\n\t\tif isinstance(tzname, bytes):\n\t\t\treturn tzname.decode('utf-8')\n\t\telse:\n\t\t\treturn tzname\n\n\treturn inner\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef namefunc2(*args):\n\t\tname = namefunc(*args)\n\t\tif isinstance(name, str):\n\t\t\treturn name.encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(*args)\n\treturn namefunc2\n\n\nif __name__ == '__main__':\n\timport pytz\n\tprint(pytz.tzname_in_python2(pytz.timezone))\n\tprint(pytz.tzname_in_python2(pytz.timezone('America/New_York')))", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef tzname_in_python2_inner(tzname, *args):\n\t\tif isinstance(tzname, bytes):\n\t\t\ttzname = tzname.decode('utf-8')\n\t\treturn namefunc(tzname, *args)\n\treturn tzname_in_python2_inner\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef inner(*args, **kwargs):\n\t\ttzname = namefunc(*args, **kwargs)\n\t\tif isinstance(tzname, unicode):\n\t\t\ttzname = tzname.encode('utf-8')\n\t\treturn tzname\n\treturn inner\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tnamefunc = namefunc.encode(\"ascii\", \"ignore\")\n\tif namefunc == \"\":\n\t\tnamefunc = \"None\"\n\treturn namefunc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(*args, **kwargs):\n\t\tname = namefunc(*args, **kwargs)\n\t\treturn name.encode('utf-8')\n\treturn wrapper\n\ntzname_in_python2 = tzname_in_python2(tzname)\n\n# Copyright (c) 2015,2016,2017,2018,2019,2020,2021,2022,2023 Viktor Stinner\n# License: MIT\n#\n# The MIT License (MIT)\n#\n# Copyright (c) 2015,2016,2017,2018,2019,2020,2021,2022,2023 Viktor Stinner\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tnamefunc = namefunc.encode('ascii', 'replace')\n\n\treturn namefunc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(name):\n\t\tname = namefunc(name)\n\t\treturn name.decode('utf-8')\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t\"\"\"\n\tChange unicode output into bytestrings in Python 2\n\t\"\"\"\n\tdef wrapper(tzname):\n\t\tif isinstance(tzname, str):\n\t\t\ttzname = tzname.encode('utf-8')\n\t\treturn namefunc(tzname)\n\treturn wrapper\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom ._version import version as __version__\n\texcept ImportError:\n\t\t__version__ = '0.0.0'\n\treturn __version__\n\nfrom ._version import version as __version__\n\n__all__ = ['get_versions']", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport setuptools_scm\n\t\tversion = setuptools_scm.get_version()\n\texcept:\n\t\tversion = \"0.0.0\"\n\treturn version", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom pkg_resources import get_distribution\n\t\tversion = get_distribution('tubesync').version\n\texcept:\n\t\tversion = '0.0.0'\n\treturn version\n\n__version__ = get_versions()", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom setuptools_scm import get_version\n\t\treturn get_version(root='..', relative_to=__file__)\n\texcept:\n\t\treturn 'unknown'\n\n\n__version__ = get_versions()", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom pkg_resources import get_distribution\n\t\treturn get_distribution('miscutils').version\n\texcept:\n\t\treturn 'dev'\n\n#miscutils/__init__.py\n\"\"\"\nmiscutils\n========\n\nmiscutils provides a collection of useful functions for working with files, operating system, and other tasks.\n\n:copyright: (c) 2014 by David Egan.\n:license: Apache 2.0, see LICENSE for more details.\n\"\"\"\n\nfrom .version import get_versions\n\n__version__ = get_versions()\n__author__ = 'David Egan'\n__email__ = 'david.egan@gmail.com'\n__all__ = ['']\n\n#setup.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\n\nfrom setuptools import setup, find_packages\n\nfrom miscutils import get_versions\n\nversion_str = get_versions()\n\nif sys.argv[-1] == 'publish':\n\tos.system('python setup.py sdist upload')\n\tsys.exit()\n\nsetup(\n\tname='miscutils',\n\tversion=version_str,\n\tdescription='miscutils is", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution(\"pykafka\").version\n\texcept pkg_resources.DistributionNotFound:\n\t\treturn \"UNKNOWN\"\n\n__version__ = get_versions()", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom . import version\n\t\treturn version.get_versions()\n\texcept ImportError:\n\t\treturn {}\n\n\n__version__ = get_versions().get('version', '0.0.0')\n\n__version_info__ = get_versions().get('version_info', (0, 0, 0))\n\n#tests/test_version.py\nimport unittest\n\nfrom version import get_versions\n\n", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\timport subprocess\n\n\t\tversion = None\n\t\ttry:\n\t\t\tversion = pkg_resources.get_distribution(\"dipy\").version\n\t\texcept pkg_resources.DistributionNotFound:\n\t\t\tversion = None\n\n\t\ttry:\n\t\t\tversion = pkg_resources.get_distribution(\"dipy-gui\").version\n\t\texcept pkg_resources.DistributionNotFound:\n\t\t\tversion = None\n\n\t\tif version is None:\n\t\t\tversion = 'None'\n\n\t\treturn version\n\n\texcept ImportError:\n\t\treturn 'None'", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\timport os\n\timport sys\n\timport re\n\timport subprocess\n\n\t# Obtain the version information\n\tversion = os.getenv('VERSION')\n\tif version is None:\n\t\ttry:\n\t\t\tversion = subprocess.check_output(['git', 'describe', '--tags', '--dirty']).decode('utf-8').strip()\n\t\t\tversion = re.sub('^v[0-9]+\\.', '', version)\n\t\texcept:\n\t\t\tversion = '0.0.0'\n\n\t# Obtain the git hash\n\ttry:\n\t\tgit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()\n\texcept:\n\t\tgit_hash = ''\n\n\t# Obtain the build date\n\ttry:\n\t\tbuild_date = subprocess.check_output(['git', 'show', '-s', '--format=%ci']).decode('utf-8').strip()\n\texcept:\n\t\tbuild_date = ''\n\n\t# Obtain the build system\n\ttry:\n\t\tbuild_system = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode('utf-8').strip()\n\texcept:\n\t\tbuild_system", "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\treturn __import__('pkg_resources').get_distribution('py-saml').version\n\texcept:\n\t\treturn '0.0.0'"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom ._version import version as __version__\n\texcept ImportError:\n\t\t__version__ = '0.0.0'\n\treturn __version__\n\nfrom ._version import version as __version__\n\n__all__ = ['get_versions']", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport setuptools_scm\n\t\tversion = setuptools_scm.get_version()\n\texcept:\n\t\tversion = \"0.0.0\"\n\treturn version", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom pkg_resources import get_distribution\n\t\tversion = get_distribution('tubesync').version\n\texcept:\n\t\tversion = '0.0.0'\n\treturn version\n\n__version__ = get_versions()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom setuptools_scm import get_version\n\t\treturn get_version(root='..', relative_to=__file__)\n\texcept:\n\t\treturn 'unknown'\n\n\n__version__ = get_versions()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom pkg_resources import get_distribution\n\t\treturn get_distribution('miscutils').version\n\texcept:\n\t\treturn 'dev'\n\n#miscutils/__init__.py\n\"\"\"\nmiscutils\n========\n\nmiscutils provides a collection of useful functions for working with files, operating system, and other tasks.\n\n:copyright: (c) 2014 by David Egan.\n:license: Apache 2.0, see LICENSE for more details.\n\"\"\"\n\nfrom .version import get_versions\n\n__version__ = get_versions()\n__author__ = 'David Egan'\n__email__ = 'david.egan@gmail.com'\n__all__ = ['']\n\n#setup.py\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\n\nfrom setuptools import setup, find_packages\n\nfrom miscutils import get_versions\n\nversion_str = get_versions()\n\nif sys.argv[-1] == 'publish':\n\tos.system('python setup.py sdist upload')\n\tsys.exit()\n\nsetup(\n\tname='miscutils',\n\tversion=version_str,\n\tdescription='miscutils is", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution(\"pykafka\").version\n\texcept pkg_resources.DistributionNotFound:\n\t\treturn \"UNKNOWN\"\n\n__version__ = get_versions()", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\tfrom . import version\n\t\treturn version.get_versions()\n\texcept ImportError:\n\t\treturn {}\n\n\n__version__ = get_versions().get('version', '0.0.0')\n\n__version_info__ = get_versions().get('version_info', (0, 0, 0))\n\n#tests/test_version.py\nimport unittest\n\nfrom version import get_versions\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\timport pkg_resources\n\t\timport subprocess\n\n\t\tversion = None\n\t\ttry:\n\t\t\tversion = pkg_resources.get_distribution(\"dipy\").version\n\t\texcept pkg_resources.DistributionNotFound:\n\t\t\tversion = None\n\n\t\ttry:\n\t\t\tversion = pkg_resources.get_distribution(\"dipy-gui\").version\n\t\texcept pkg_resources.DistributionNotFound:\n\t\t\tversion = None\n\n\t\tif version is None:\n\t\t\tversion = 'None'\n\n\t\treturn version\n\n\texcept ImportError:\n\t\treturn 'None'", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\timport os\n\timport sys\n\timport re\n\timport subprocess\n\n\t# Obtain the version information\n\tversion = os.getenv('VERSION')\n\tif version is None:\n\t\ttry:\n\t\t\tversion = subprocess.check_output(['git', 'describe', '--tags', '--dirty']).decode('utf-8').strip()\n\t\t\tversion = re.sub('^v[0-9]+\\.', '', version)\n\t\texcept:\n\t\t\tversion = '0.0.0'\n\n\t# Obtain the git hash\n\ttry:\n\t\tgit_hash = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('utf-8').strip()\n\texcept:\n\t\tgit_hash = ''\n\n\t# Obtain the build date\n\ttry:\n\t\tbuild_date = subprocess.check_output(['git', 'show', '-s', '--format=%ci']).decode('utf-8').strip()\n\texcept:\n\t\tbuild_date = ''\n\n\t# Obtain the build system\n\ttry:\n\t\tbuild_system = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).decode('utf-8').strip()\n\texcept:\n\t\tbuild_system", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\t\"\"\"\n\tObtains the version information. If the version information cannot be obtained, the default value is returned.\n\t\"\"\"\n\ttry:\n\t\treturn __import__('pkg_resources').get_distribution('py-saml').version\n\texcept:\n\t\treturn '0.0.0'", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\trender_pieces(pieces, style, 0, 0)\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tresult = ''\n\tfor piece in pieces:\n\t\tresult += piece.render(style)\n\treturn result\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tboard = []\n\tfor i in range(3):\n\t\trow = []\n\t\tfor j in range(3):\n\t\t\trow.append(\"\")\n\t\tboard.append(row)\n\t\n\tfor piece in pieces:\n\t\trow = int(piece[0]) - 1\n\t\tcol = int(piece[2]) - 1\n\t\tboard[row][col] = piece[1]\n\t\n\tprint(style.format(*board))\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\trendered_pieces = \"\"\n\tfor piece in pieces:\n\t\trendered_pieces += piece\n\treturn rendered_pieces\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\t\n\t# Define the default style.\n\tdef default():\n\t\t\"\"\"\n\t\tDefault style.\n\t\t\"\"\"\n\t\treturn {\n\t\t\t'white': '#FFFFFF',\n\t\t\t'black': '#000000',\n\t\t\t'light_gray': '#717171',\n\t\t\t'dark_gray': '#333333',\n\t\t\t'red': '#FF0000',\n\t\t\t'orange': '#FF8000',\n\t\t\t'yellow': '#FFFF00',\n\t\t\t'green': '#00FF00',\n\t\t\t'blue': '#0000FF',\n\t\t\t'purple': '#7F00FF',\n\t\t\t'brown': '#A52A2A',\n\t\t\t'cyan': '#00FFFF',\n\t\t\t'magenta': '#FF00FF',\n\t\t\t'gray': '#808080'\n\t\t}\n\t\n\t# Define the colours.\n\tcolours = default()\n\t\n\t# Define the white.\n\tcolours['white'] = pieces.get('white', '#FFFFFF')\n\t\n\t# Define the black.\n\tcolours['black'] = pieces.get('black', '#000000')\n\t\n\t# Define the light gray.\n\tcolours['light_gray'] = pieces.", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tprint(\"<html>\")\n\tprint(\"<head>\")\n\tprint(\"<style>\")\n\n\t# Load the style\n\tprint(style)\n\tprint(\"</style>\")\n\tprint(\"</head>\")\n\tprint(\"<body>\")\n\n\t# Load each piece\n\tfor piece in pieces:\n\t\tprint(\"<div class='piece'>\")\n\t\tprint(piece)\n\t\tprint(\"</div>\")\n\n\tprint(\"</body>\")\n\tprint(\"</html>\")\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tprint(\"rendering pieces and style\", style)\n\tfor i, piece in enumerate(pieces):\n\t\tprint(f\"{piece.name}: {i}\")\n\t\tpiece.render()\n\t\tprint()\n\n#test/test_piece.py\nfrom piece import Piece\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\timport os\n\timport sys\n\timport re\n\tfrom optparse import OptionParser\n\t\n\tparser = OptionParser(usage=\"Usage: %prog [options] <style_file>\")\n\tparser.add_option(\"-o\", \"--output\", help=\"Output file\")\n\tparser.add_option(\"--output-dir\", help=\"Output directory\")\n\tparser.add_option(\"--prefix\", help=\"Prefix for output image name\")\n\tparser.add_option(\"--suffix\", help=\"Suffix for output image name\")\n\tparser.add_option(\"--width\", help=\"Width of image\")\n\tparser.add_option(\"--height\", help=\"Height of image\")\n\tparser.add_option(\"--font\", help=\"Font to use\")\n\tparser.add_option(\"--font-size\", help=\"Font size\")\n\tparser.add_option(\"--font-color\", help=\"Font color\")\n\tparser.add_option(\"--text\", help=\"Text to draw\")\n\tparser.add_option(\"--text-color\", help=\"Text color\")\n\tparser.add_option(\"--image-format\", help=\"Image format\")\n\tparser.add_option(\"-t\", \"--text-align\", help=\"Text align (", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n\n", "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\trender_functions = {\n\t\t\"square\": render_square,\n\t\t\"circle\": render_circle,\n\t\t\"triangle\": render_triangle\n\t}\n\treturn render_functions[style](pieces)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\trender_pieces(pieces, style, 0, 0)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tresult = ''\n\tfor piece in pieces:\n\t\tresult += piece.render(style)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tboard = []\n\tfor i in range(3):\n\t\trow = []\n\t\tfor j in range(3):\n\t\t\trow.append(\"\")\n\t\tboard.append(row)\n\t\n\tfor piece in pieces:\n\t\trow = int(piece[0]) - 1\n\t\tcol = int(piece[2]) - 1\n\t\tboard[row][col] = piece[1]\n\t\n\tprint(style.format(*board))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\trendered_pieces = \"\"\n\tfor piece in pieces:\n\t\trendered_pieces += piece\n\treturn rendered_pieces\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\t\n\t# Define the default style.\n\tdef default():\n\t\t\"\"\"\n\t\tDefault style.\n\t\t\"\"\"\n\t\treturn {\n\t\t\t'white': '#FFFFFF',\n\t\t\t'black': '#000000',\n\t\t\t'light_gray': '#717171',\n\t\t\t'dark_gray': '#333333',\n\t\t\t'red': '#FF0000',\n\t\t\t'orange': '#FF8000',\n\t\t\t'yellow': '#FFFF00',\n\t\t\t'green': '#00FF00',\n\t\t\t'blue': '#0000FF',\n\t\t\t'purple': '#7F00FF',\n\t\t\t'brown': '#A52A2A',\n\t\t\t'cyan': '#00FFFF',\n\t\t\t'magenta': '#FF00FF',\n\t\t\t'gray': '#808080'\n\t\t}\n\t\n\t# Define the colours.\n\tcolours = default()\n\t\n\t# Define the white.\n\tcolours['white'] = pieces.get('white', '#FFFFFF')\n\t\n\t# Define the black.\n\tcolours['black'] = pieces.get('black', '#000000')\n\t\n\t# Define the light gray.\n\tcolours['light_gray'] = pieces.", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tprint(\"<html>\")\n\tprint(\"<head>\")\n\tprint(\"<style>\")\n\n\t# Load the style\n\tprint(style)\n\tprint(\"</style>\")\n\tprint(\"</head>\")\n\tprint(\"<body>\")\n\n\t# Load each piece\n\tfor piece in pieces:\n\t\tprint(\"<div class='piece'>\")\n\t\tprint(piece)\n\t\tprint(\"</div>\")\n\n\tprint(\"</body>\")\n\tprint(\"</html>\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tprint(\"rendering pieces and style\", style)\n\tfor i, piece in enumerate(pieces):\n\t\tprint(f\"{piece.name}: {i}\")\n\t\tpiece.render()\n\t\tprint()\n\n#test/test_piece.py\nfrom piece import Piece\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\t\n\timport os\n\timport sys\n\timport re\n\tfrom optparse import OptionParser\n\t\n\tparser = OptionParser(usage=\"Usage: %prog [options] <style_file>\")\n\tparser.add_option(\"-o\", \"--output\", help=\"Output file\")\n\tparser.add_option(\"--output-dir\", help=\"Output directory\")\n\tparser.add_option(\"--prefix\", help=\"Prefix for output image name\")\n\tparser.add_option(\"--suffix\", help=\"Suffix for output image name\")\n\tparser.add_option(\"--width\", help=\"Width of image\")\n\tparser.add_option(\"--height\", help=\"Height of image\")\n\tparser.add_option(\"--font\", help=\"Font to use\")\n\tparser.add_option(\"--font-size\", help=\"Font size\")\n\tparser.add_option(\"--font-color\", help=\"Font color\")\n\tparser.add_option(\"--text\", help=\"Text to draw\")\n\tparser.add_option(\"--text-color\", help=\"Text color\")\n\tparser.add_option(\"--image-format\", help=\"Image format\")\n\tparser.add_option(\"-t\", \"--text-align\", help=\"Text align (", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\"\"\"\n\tInput pieces and a style, render the pieces to the corresponding style.\n\t\"\"\"\n\trender_functions = {\n\t\t\"square\": render_square,\n\t\t\"circle\": render_circle,\n\t\t\"triangle\": render_triangle\n\t}\n\treturn render_functions[style](pieces)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\ttag = pieces[0]\n\tfor i in range(1, len(pieces)):\n\t\tif pieces[i] == tag:\n\t\t\treturn '.'\n\treturn '+'", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\tresult = \"+\"\n\n\tfor piece in pieces:\n\t\tif piece.find(\"+\") != -1:\n\t\t\tresult = \".\"\n\t\t\tbreak\n\n\treturn result\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tpieces.sort()\n\tclosest_tag = pieces[0]\n\tclosest_tag_idx = pieces.index(closest_tag)\n\tclosest_tag_char_count = len(closest_tag)\n\n\tfor piece in pieces:\n\t\tif len(piece) > closest_tag_char_count:\n\t\t\tclosest_tag = piece\n\t\t\tclosest_tag_idx = pieces.index(closest_tag)\n\t\t\tclosest_tag_char_count = len(closest_tag)\n\n\tif pieces[closest_tag_idx] == \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[0]\n\tcloset_tag_index = 0\n\tfor i in range(len(pieces)):\n\t\tif pieces[i].find(\"<\") >= 0:\n\t\t\tcloset_tag = pieces[i]\n\t\t\tcloset_tag_index = i\n\t\telif pieces[i].find(\">\") >= 0:\n\t\t\tcloset_tag = pieces[i]\n\t\t\tcloset_tag_index = i\n\t\telse:\n\t\t\treturn \"+\"\n\treturn closet_tag_index\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tclosest_tag = get_closest_tag(pieces)\n\tif closest_tag == \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\treturn \"+\"", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\ts = \"\"\n\t\n\tfor piece in pieces:\n\t\tif \"+\" in piece:\n\t\t\ts += \"+\"\n\t\telif \"<\" in piece:\n\t\t\ts += \"+\"\n\t\t\n\treturn s\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\ttag = pieces[-1]\n\tif tag == \"+\" or tag == \".\":\n\t\treturn tag\n\telse:\n\t\treturn \"+\"\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\ttag = pieces[-1]\n\tif tag == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n\n", "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tclosest_tag_pos = 0\n\tclosest_tag = \"\"\n\t\n\tfor tag in pieces:\n\t\tif len(tag) > len(closest_tag):\n\t\t\tclosest_tag_pos = pieces.index(tag)\n\t\t\tclosest_tag = tag\n\t\n\treturn closest_tag if closest_tag_pos == 0 else \"+\"\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\ttag = pieces[0]\n\tfor i in range(1, len(pieces)):\n\t\tif pieces[i] == tag:\n\t\t\treturn '.'\n\treturn '+'", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\tresult = \"+\"\n\n\tfor piece in pieces:\n\t\tif piece.find(\"+\") != -1:\n\t\t\tresult = \".\"\n\t\t\tbreak\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tpieces.sort()\n\tclosest_tag = pieces[0]\n\tclosest_tag_idx = pieces.index(closest_tag)\n\tclosest_tag_char_count = len(closest_tag)\n\n\tfor piece in pieces:\n\t\tif len(piece) > closest_tag_char_count:\n\t\t\tclosest_tag = piece\n\t\t\tclosest_tag_idx = pieces.index(closest_tag)\n\t\t\tclosest_tag_char_count = len(closest_tag)\n\n\tif pieces[closest_tag_idx] == \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tcloset_tag = pieces[0]\n\tcloset_tag_index = 0\n\tfor i in range(len(pieces)):\n\t\tif pieces[i].find(\"<\") >= 0:\n\t\t\tcloset_tag = pieces[i]\n\t\t\tcloset_tag_index = i\n\t\telif pieces[i].find(\">\") >= 0:\n\t\t\tcloset_tag = pieces[i]\n\t\t\tcloset_tag_index = i\n\t\telse:\n\t\t\treturn \"+\"\n\treturn closet_tag_index\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tclosest_tag = get_closest_tag(pieces)\n\tif closest_tag == \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\treturn \"+\"", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\t\n\ts = \"\"\n\t\n\tfor piece in pieces:\n\t\tif \"+\" in piece:\n\t\t\ts += \"+\"\n\t\telif \"<\" in piece:\n\t\t\ts += \"+\"\n\t\t\n\treturn s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\ttag = pieces[-1]\n\tif tag == \"+\" or tag == \".\":\n\t\treturn tag\n\telse:\n\t\treturn \"+\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\ttag = pieces[-1]\n\tif tag == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t\"\"\"\n\tReturn \".\" If the closet-tag of the pieces contains \"+\", otherwise, return \"+\".\n\t\"\"\"\n\tclosest_tag_pos = 0\n\tclosest_tag = \"\"\n\t\n\tfor tag in pieces:\n\t\tif len(tag) > len(closest_tag):\n\t\t\tclosest_tag_pos = pieces.index(tag)\n\t\t\tclosest_tag = tag\n\t\n\treturn closest_tag if closest_tag_pos == 0 else \"+\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tenv = env or os.environ.copy()\n\tenv['PYTHONPATH'] = '.'\n\n\tprocess = subprocess.Popen(commands,\n\t\tstdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tenv=env, cwd=cwd)\n\n\tprocess.wait()\n\tstdout, stderr = process.communicate(input=args)\n\n\tif verbose:\n\t\tprint('stdout:\\n%s' % stdout)\n\t\tprint('stderr:\\n%s' % stderr)\n\n\tif hide_stderr and stderr:\n\t\treturn stdout\n\telse:\n\t\treturn stdout + stderr\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.STDOUT if hide_stderr else None, cwd=cwd, env=env)\n\n\tlines = []\n\twhile True:\n\t\tline = cmd.stdout.readline()\n\t\tif not line:\n\t\t\tbreak\n\t\tlines.append(line)\n\n\tif cmd.wait() != 0:\n\t\tprint(\" \".join(commands))\n\t\tprint(\" \".join(lines))\n\t\tprint(\"Command failed with exit code: {0}\".format(cmd.returncode))\n\t\tsys.exit(1)\n\n\tstdout = \"\".join(lines)\n\n\tif not stdout:\n\t\tprint(\"Command produced no output.\")\n\t\tsys.exit(1)\n\n\tstdout = stdout.rstrip()\n\n\tif verbose:\n\t\tprint(stdout)\n\n\tout = []\n\n\tif args:\n\t\tout = stdout.split(args)\n\telif not args:\n\t\tout = stdout\n\n\treturn out\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport os\n\timport shlex\n\timport subprocess\n\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\tenv.update(os.environ)\n\n\tcommand = [commands]\n\tif args is not None:\n\t\tcommand.extend(shlex.split(args))\n\n\tprocess = subprocess.Popen(command,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tcwd=cwd,\n\t\tenv=env)\n\n\tstdout, stderr = process.communicate()\n\n\tif verbose:\n\t\tprint(stdout)\n\t\tprint(stderr)\n\n\tif hide_stderr:\n\t\tstdout, stderr = process.communicate()\n\n\treturn stdout, stderr\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = commands[args[0]]\n\tif len(args) == 1:\n\t\tcmd(cwd=cwd)\n\telse:\n\t\tcmd(*args[1:])\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\n\tdef run(*args, **kwargs):\n\t\tkwargs.setdefault('stdout', subprocess.PIPE)\n\t\tkwargs.setdefault('stderr', subprocess.PIPE)\n\t\tkwargs.setdefault('env', env)\n\t\tif verbose:\n\t\t\tprint \" \".join(args)\n\t\ttry:\n\t\t\tproc = subprocess.Popen(args, cwd=cwd, **kwargs)\n\t\t\tproc.wait()\n\t\texcept:\n\t\t\tsys.stderr.write(\"Error running command: %s\\n\" % \" \".join(args))\n\t\t\traise\n\t\tif proc.returncode < 0:\n\t\t\traise subprocess.CalledProcessError(proc.returncode, args, proc.stdout)\n\t\tif proc.stdout:\n\t\t\tsys.stdout.write(proc.stdout)\n\t\tif proc.stderr:\n\t\t\tsys.stderr.write(proc.stderr)\n\n\tdef run_shell(*args, **kwargs):\n\t\tkwargs.setdefault('stdout', subprocess.PIPE)\n\t\tkwargs.setdefault('stderr', subprocess.PIPE)\n\t\tkwargs.setdefault('env', env)\n\t\tif verbose:\n\t\t\tprint \" \".join(args)", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs.sort()\n\tif env:\n\t\tenv.update(os.environ)\n\tenv_str = ' '.join(['%s=%s' % (k, v) for (k, v) in env.items()])\n\tcmd = [commands[args[0]]]\n\tcmd.extend(args[1:])\n\tif cwd:\n\t\tcmd.insert(0, 'cd')\n\t\tcmd.insert(0, cwd)\n\tif verbose:\n\t\tcmd.insert(0, 'echo')\n\t\tcmd.insert(0, 'env')\n\t\tcmd.insert(0, 'set -x')\n\tif hide_stderr:\n\t\tcmd.insert(0, '2>/dev/null')\n\tif verbose:\n\t\tcmd.insert(0, 'echo')\n\t\tcmd.insert(0, 'set -x')\n\tcmd.insert(0, 'env')\n\tcmd.insert(0, env_str)\n\tr = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tuniversal_newlines=True)\n\tr.wait()\n\treturn r.returncode\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport sh\n\timport subprocess\n\timport sys\n\t\n\tcmd = sh.Command(commands)\n\tif verbose:\n\t\tcmd = cmd.bake(verbose=True)\n\tif cwd:\n\t\tcmd = cmd.directory(cwd)\n\tif hide_stderr:\n\t\tcmd = cmd.bake(stdout=sys.stdout, stderr=subprocess.DEVNULL)\n\tif env:\n\t\tcmd = cmd.env(env)\n\tif args:\n\t\tcmd = cmd(*args)\n\treturn cmd.run()\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\n\tcmd = list()\n\tif verbose:\n\t\tcmd.append('-v')\n\n\tif hide_stderr:\n\t\tcmd.append('--hide-stderr')\n\n\tif env:\n\t\tcmd.append('--env')\n\t\tfor key, value in env.items():\n\t\t\tcmd.append('{}={}'.format(key, value))\n\n\tif cwd:\n\t\tcmd.append('--cwd')\n\t\tcmd.append(cwd)\n\n\tcmd.append(commands[0])\n\n\tif len(commands) > 1:\n\t\tcmd.extend(args)\n\t\n\ttry:\n\t\tsubprocess.check_call(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(e)\n\t\traise\n\nif __name__ == '__main__':\n\trun_command(sys.argv[1:], sys.argv[2:])", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs = args or {}\n\n\tenv = env or {}\n\n\tenv.update(get_env())\n\n\targs.update({\n\t\t'cwd': cwd,\n\t\t'silent': False,\n\t\t'stdin': commands,\n\t})\n\n\tif verbose:\n\t\tenv['TERM'] = 'dumb'\n\n\tcmd = subprocess.Popen(commands, shell=False, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE, env=env, cwd=cwd)\n\n\tstdout, stderr = cmd.communicate(args['stdin'])\n\n\tif cmd.returncode:\n\t\traise CommandError(cmd.returncode, stdout=stdout, stderr=stderr)\n\n\treturn stdout\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\n\tif env:\n\t\tprocess = subprocess.Popen(args, stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\telse:\n\t\tprocess = subprocess.Popen(args, stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n\n\tstdout, stderr = process.communicate()\n\n\tif process.returncode != 0:\n\t\traise CommandFailed(process.returncode, stderr)\n\n\tif verbose:\n\t\tprint(stdout)\n\n\tif hide_stderr:\n\t\tstderr = '\\033[0m'\n\t\tstdout = stdout.replace(stderr, '')\n\n\tif commands:\n\t\tcommands.append({'stdout': stdout, 'stderr': stderr})\n\n\treturn stdout, stderr\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tenv = env or os.environ.copy()\n\tenv['PYTHONPATH'] = '.'\n\n\tprocess = subprocess.Popen(commands,\n\t\tstdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tenv=env, cwd=cwd)\n\n\tprocess.wait()\n\tstdout, stderr = process.communicate(input=args)\n\n\tif verbose:\n\t\tprint('stdout:\\n%s' % stdout)\n\t\tprint('stderr:\\n%s' % stderr)\n\n\tif hide_stderr and stderr:\n\t\treturn stdout\n\telse:\n\t\treturn stdout + stderr\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.STDOUT if hide_stderr else None, cwd=cwd, env=env)\n\n\tlines = []\n\twhile True:\n\t\tline = cmd.stdout.readline()\n\t\tif not line:\n\t\t\tbreak\n\t\tlines.append(line)\n\n\tif cmd.wait() != 0:\n\t\tprint(\" \".join(commands))\n\t\tprint(\" \".join(lines))\n\t\tprint(\"Command failed with exit code: {0}\".format(cmd.returncode))\n\t\tsys.exit(1)\n\n\tstdout = \"\".join(lines)\n\n\tif not stdout:\n\t\tprint(\"Command produced no output.\")\n\t\tsys.exit(1)\n\n\tstdout = stdout.rstrip()\n\n\tif verbose:\n\t\tprint(stdout)\n\n\tout = []\n\n\tif args:\n\t\tout = stdout.split(args)\n\telif not args:\n\t\tout = stdout\n\n\treturn out\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport os\n\timport shlex\n\timport subprocess\n\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\tenv.update(os.environ)\n\n\tcommand = [commands]\n\tif args is not None:\n\t\tcommand.extend(shlex.split(args))\n\n\tprocess = subprocess.Popen(command,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tcwd=cwd,\n\t\tenv=env)\n\n\tstdout, stderr = process.communicate()\n\n\tif verbose:\n\t\tprint(stdout)\n\t\tprint(stderr)\n\n\tif hide_stderr:\n\t\tstdout, stderr = process.communicate()\n\n\treturn stdout, stderr\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\tcmd = commands[args[0]]\n\tif len(args) == 1:\n\t\tcmd(cwd=cwd)\n\telse:\n\t\tcmd(*args[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\n\tdef run(*args, **kwargs):\n\t\tkwargs.setdefault('stdout', subprocess.PIPE)\n\t\tkwargs.setdefault('stderr', subprocess.PIPE)\n\t\tkwargs.setdefault('env', env)\n\t\tif verbose:\n\t\t\tprint \" \".join(args)\n\t\ttry:\n\t\t\tproc = subprocess.Popen(args, cwd=cwd, **kwargs)\n\t\t\tproc.wait()\n\t\texcept:\n\t\t\tsys.stderr.write(\"Error running command: %s\\n\" % \" \".join(args))\n\t\t\traise\n\t\tif proc.returncode < 0:\n\t\t\traise subprocess.CalledProcessError(proc.returncode, args, proc.stdout)\n\t\tif proc.stdout:\n\t\t\tsys.stdout.write(proc.stdout)\n\t\tif proc.stderr:\n\t\t\tsys.stderr.write(proc.stderr)\n\n\tdef run_shell(*args, **kwargs):\n\t\tkwargs.setdefault('stdout', subprocess.PIPE)\n\t\tkwargs.setdefault('stderr', subprocess.PIPE)\n\t\tkwargs.setdefault('env', env)\n\t\tif verbose:\n\t\t\tprint \" \".join(args)", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs.sort()\n\tif env:\n\t\tenv.update(os.environ)\n\tenv_str = ' '.join(['%s=%s' % (k, v) for (k, v) in env.items()])\n\tcmd = [commands[args[0]]]\n\tcmd.extend(args[1:])\n\tif cwd:\n\t\tcmd.insert(0, 'cd')\n\t\tcmd.insert(0, cwd)\n\tif verbose:\n\t\tcmd.insert(0, 'echo')\n\t\tcmd.insert(0, 'env')\n\t\tcmd.insert(0, 'set -x')\n\tif hide_stderr:\n\t\tcmd.insert(0, '2>/dev/null')\n\tif verbose:\n\t\tcmd.insert(0, 'echo')\n\t\tcmd.insert(0, 'set -x')\n\tcmd.insert(0, 'env')\n\tcmd.insert(0, env_str)\n\tr = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tuniversal_newlines=True)\n\tr.wait()\n\treturn r.returncode\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport sh\n\timport subprocess\n\timport sys\n\t\n\tcmd = sh.Command(commands)\n\tif verbose:\n\t\tcmd = cmd.bake(verbose=True)\n\tif cwd:\n\t\tcmd = cmd.directory(cwd)\n\tif hide_stderr:\n\t\tcmd = cmd.bake(stdout=sys.stdout, stderr=subprocess.DEVNULL)\n\tif env:\n\t\tcmd = cmd.env(env)\n\tif args:\n\t\tcmd = cmd(*args)\n\treturn cmd.run()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\n\tcmd = list()\n\tif verbose:\n\t\tcmd.append('-v')\n\n\tif hide_stderr:\n\t\tcmd.append('--hide-stderr')\n\n\tif env:\n\t\tcmd.append('--env')\n\t\tfor key, value in env.items():\n\t\t\tcmd.append('{}={}'.format(key, value))\n\n\tif cwd:\n\t\tcmd.append('--cwd')\n\t\tcmd.append(cwd)\n\n\tcmd.append(commands[0])\n\n\tif len(commands) > 1:\n\t\tcmd.extend(args)\n\t\n\ttry:\n\t\tsubprocess.check_call(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\texcept subprocess.CalledProcessError as e:\n\t\tprint(e)\n\t\traise\n\nif __name__ == '__main__':\n\trun_command(sys.argv[1:], sys.argv[2:])", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\targs = args or {}\n\n\tenv = env or {}\n\n\tenv.update(get_env())\n\n\targs.update({\n\t\t'cwd': cwd,\n\t\t'silent': False,\n\t\t'stdin': commands,\n\t})\n\n\tif verbose:\n\t\tenv['TERM'] = 'dumb'\n\n\tcmd = subprocess.Popen(commands, shell=False, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE, env=env, cwd=cwd)\n\n\tstdout, stderr = cmd.communicate(args['stdin'])\n\n\tif cmd.returncode:\n\t\traise CommandError(cmd.returncode, stdout=stdout, stderr=stderr)\n\n\treturn stdout\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tCall the given command(s).\n\t\"\"\"\n\timport subprocess\n\n\tif env:\n\t\tprocess = subprocess.Popen(args, stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\telse:\n\t\tprocess = subprocess.Popen(args, stdin=None, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)\n\n\tstdout, stderr = process.communicate()\n\n\tif process.returncode != 0:\n\t\traise CommandFailed(process.returncode, stderr)\n\n\tif verbose:\n\t\tprint(stdout)\n\n\tif hide_stderr:\n\t\tstderr = '\\033[0m'\n\t\tstdout = stdout.replace(stderr, '')\n\n\tif commands:\n\t\tcommands.append({'stdout': stdout, 'stderr': stderr})\n\n\treturn stdout, stderr\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'VERSION'\n\tconfig.tag_prefix = 'v'\n\treturn config\n\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.versionfile_source = \"{project}/_version.py\"\n\tconfig.versionfile_build = \"{project}/_version.py\"\n\tconfig.versionfile_tag = \"{project}/_version.py\"\n\n\tconfig.parentdir_prefix = \"{project}-\"\n\n\tconfig.version_dev_prefix = \"{project}-dev-\"\n\n\tconfig.parentdir_prefix_development = \"{project}-dev-\"\n\n\tconfig.parentdir_prefix_release = \"\"\n\n\tconfig.parentdir_prefix_post = \"\"\n\n\tconfig.parentdir_prefix_local = \"{project}-local-\"\n\n\tconfig.version_dev_suffix = \"-dev\"\n\n\tconfig.parentdir_prefix_local_post = \"\"\n\n\tconfig.parentdir_prefix_local_dev = \"{project}-local-dev-\"\n\n\tconfig.tag_prefix = \"{project}-\"\n\n\tconfig.parentdir_default = \"{project}-\"\n\n\tconfig.version_min_length = 2\n\n\tconfig.rename_threshold = 2\n\n\tconfig.write_to_template = True\n\n\tconfig.write_to_versioneer_file = True\n\n\tconfig.write_to_tag =", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'VERSION'\n\tconfig.versionfile_build = 'git+git://github.com/stellar/python-stellar-base.git@master#egg=stellar-base'\n\tconfig.versionfile_tag = 'VERSION'\n\tconfig.parentdir_prefix = '.'\n\treturn config\n\n#setup.py\n#!/usr/bin/env python\n\nfrom setuptools import setup\n\nfrom setup_base import get_config\nfrom setup_base import get_config_git\n\nconfig = get_config()\nconfig.update(get_config_git())\n\nsetup(**config)", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.version_file_pattern = 'VERSION'\n\tconfig.version_file_required = True\n\tconfig.version_file_safe = True\n\n\tconfig.commit_message_pattern = r'^(?P<prefix>v?\\d+\\.\\d+\\.\\d+(-.+)?)\\s+(?P<suffix>.*)$'\n\tconfig.commit_message_required = True\n\tconfig.commit_message_safe = True\n\n\tconfig.commit_message_append_release = True\n\tconfig.commit_message_append_version = True\n\tconfig.commit_message_append_branch = True\n\tconfig.commit_message_default = 'v{version}{suffix}'\n\n\tconfig.tag_pattern = r'^v?(?P<version>\\d+\\.\\d+\\.\\d+(-.+)?)$'\n\tconfig.tag_required = True\n\tconfig.tag_message = 'Version {version}'\n\tconfig.tag_message_prefix = 'v'\n\tconfig.tag_message_suffix = ''\n\tconfig.tag_message_append_release = False\n\tconfig.tag_message_append_version = True\n\tconfig.tag_message_append_branch = False\n\t", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.versionfile_source = '{project}/VERSION'\n\t# config.tag_prefix = 'v'\n\n\tconfig.parentdir_prefix = '{project}-'\n\n\tconfig.vcs_command_name = 'git'\n\tconfig.vcs_mechanism = 'GIT'\n\n\t# config.versionfile_template = '{project}-{version}'\n\t# config.tag_template = '{project}-{version}'\n\t# config.parentdir_template = '{project}-{version}'\n\n\treturn config", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.tag_prefix = \"v\"\n\tconfig.versionfile_suffix = \"\"\n\treturn config\n\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'fibonacci_versioneer/VERSION'\n\tconfig.versionfile_build = 'VERSION'\n\tconfig.versionfile_template = '# {version}'\n\tconfig.tag_prefix = 'v'\n\treturn config\n\n\nconfig = get_config()\n\n", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\t# This is a path to a file with the version and date info.\n\tconfig.version_file_pattern = 'VERSION'\n\t# This is a path to a file with the version and date info.\n\tconfig.version_file_pattern_prerelease = 'VERSION_PRERELEASE'\n\t# This is the version number.\n\tconfig.version_style = '{tag}'\n\t# This is the date of the release.\n\tconfig.release_command = 'date +%Y-%m-%d'\n\t# This is a separator between version and date.\n\tconfig.tag_prefix = ''\n\t# This is the regex pattern for matching tags.\n\tconfig.parent_annotated_tags = '^(v?(?:0|[1-9][0-9]*))'\n\treturn config", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tvc = VersioneerConfig()\n\tvc.versionfile_source = 'versioneer/_version.py'\n\tvc.versionfile_build = 'versioneer/_version.py'\n\tvc.versionfile_template = '{version}'\n\tvc.parentdir_prefix = 'Versioneer'\n\treturn vc\n\n#setup.py\nfrom setuptools import setup, find_packages\n\nfrom versioneer import get_config\n\nconfig = get_config()\nconfig.parentdir_prefix = ''\n\nsetup(\n\tname='Versioneer',\n\tversion=config.get_version(),\n\tcmdclass=config.get_cmdclass(),\n\tpackages=find_packages(),\n\tinstall_requires=[\n\t],\n)\n\n#tests/conftest.py\nfrom unittest import mock\n\nimport pytest\n\nimport versioneer\n\n\n@pytest.fixture", "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.author_name = \"Pepijn van Gemert\"\n\tconfig.author_email = \"pepijn.van.gemert@gmail.com\"\n\tconfig.author_github = \"pepijnvan\"\n\tconfig.version_file_name = \"__version__.py\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'VERSION'\n\tconfig.tag_prefix = 'v'\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.versionfile_source = \"{project}/_version.py\"\n\tconfig.versionfile_build = \"{project}/_version.py\"\n\tconfig.versionfile_tag = \"{project}/_version.py\"\n\n\tconfig.parentdir_prefix = \"{project}-\"\n\n\tconfig.version_dev_prefix = \"{project}-dev-\"\n\n\tconfig.parentdir_prefix_development = \"{project}-dev-\"\n\n\tconfig.parentdir_prefix_release = \"\"\n\n\tconfig.parentdir_prefix_post = \"\"\n\n\tconfig.parentdir_prefix_local = \"{project}-local-\"\n\n\tconfig.version_dev_suffix = \"-dev\"\n\n\tconfig.parentdir_prefix_local_post = \"\"\n\n\tconfig.parentdir_prefix_local_dev = \"{project}-local-dev-\"\n\n\tconfig.tag_prefix = \"{project}-\"\n\n\tconfig.parentdir_default = \"{project}-\"\n\n\tconfig.version_min_length = 2\n\n\tconfig.rename_threshold = 2\n\n\tconfig.write_to_template = True\n\n\tconfig.write_to_versioneer_file = True\n\n\tconfig.write_to_tag =", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'VERSION'\n\tconfig.versionfile_build = 'git+git://github.com/stellar/python-stellar-base.git@master#egg=stellar-base'\n\tconfig.versionfile_tag = 'VERSION'\n\tconfig.parentdir_prefix = '.'\n\treturn config\n\n#setup.py\n#!/usr/bin/env python\n\nfrom setuptools import setup\n\nfrom setup_base import get_config\nfrom setup_base import get_config_git\n\nconfig = get_config()\nconfig.update(get_config_git())\n\nsetup(**config)", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.version_file_pattern = 'VERSION'\n\tconfig.version_file_required = True\n\tconfig.version_file_safe = True\n\n\tconfig.commit_message_pattern = r'^(?P<prefix>v?\\d+\\.\\d+\\.\\d+(-.+)?)\\s+(?P<suffix>.*)$'\n\tconfig.commit_message_required = True\n\tconfig.commit_message_safe = True\n\n\tconfig.commit_message_append_release = True\n\tconfig.commit_message_append_version = True\n\tconfig.commit_message_append_branch = True\n\tconfig.commit_message_default = 'v{version}{suffix}'\n\n\tconfig.tag_pattern = r'^v?(?P<version>\\d+\\.\\d+\\.\\d+(-.+)?)$'\n\tconfig.tag_required = True\n\tconfig.tag_message = 'Version {version}'\n\tconfig.tag_message_prefix = 'v'\n\tconfig.tag_message_suffix = ''\n\tconfig.tag_message_append_release = False\n\tconfig.tag_message_append_version = True\n\tconfig.tag_message_append_branch = False\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\n\tconfig.versionfile_source = '{project}/VERSION'\n\t# config.tag_prefix = 'v'\n\n\tconfig.parentdir_prefix = '{project}-'\n\n\tconfig.vcs_command_name = 'git'\n\tconfig.vcs_mechanism = 'GIT'\n\n\t# config.versionfile_template = '{project}-{version}'\n\t# config.tag_template = '{project}-{version}'\n\t# config.parentdir_template = '{project}-{version}'\n\n\treturn config", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.tag_prefix = \"v\"\n\tconfig.versionfile_suffix = \"\"\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.versionfile_source = 'fibonacci_versioneer/VERSION'\n\tconfig.versionfile_build = 'VERSION'\n\tconfig.versionfile_template = '# {version}'\n\tconfig.tag_prefix = 'v'\n\treturn config\n\n\nconfig = get_config()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\t# This is a path to a file with the version and date info.\n\tconfig.version_file_pattern = 'VERSION'\n\t# This is a path to a file with the version and date info.\n\tconfig.version_file_pattern_prerelease = 'VERSION_PRERELEASE'\n\t# This is the version number.\n\tconfig.version_style = '{tag}'\n\t# This is the date of the release.\n\tconfig.release_command = 'date +%Y-%m-%d'\n\t# This is a separator between version and date.\n\tconfig.tag_prefix = ''\n\t# This is the regex pattern for matching tags.\n\tconfig.parent_annotated_tags = '^(v?(?:0|[1-9][0-9]*))'\n\treturn config", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tvc = VersioneerConfig()\n\tvc.versionfile_source = 'versioneer/_version.py'\n\tvc.versionfile_build = 'versioneer/_version.py'\n\tvc.versionfile_template = '{version}'\n\tvc.parentdir_prefix = 'Versioneer'\n\treturn vc\n\n#setup.py\nfrom setuptools import setup, find_packages\n\nfrom versioneer import get_config\n\nconfig = get_config()\nconfig.parentdir_prefix = ''\n\nsetup(\n\tname='Versioneer',\n\tversion=config.get_version(),\n\tcmdclass=config.get_cmdclass(),\n\tpackages=find_packages(),\n\tinstall_requires=[\n\t],\n)\n\n#tests/conftest.py\nfrom unittest import mock\n\nimport pytest\n\nimport versioneer\n\n\n@pytest.fixture", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t\"\"\"\n\tReturn a new VersioneerConfig() and set various attribute of it.\n\t\"\"\"\n\tconfig = VersioneerConfig()\n\tconfig.author_name = \"Pepijn van Gemert\"\n\tconfig.author_email = \"pepijn.van.gemert@gmail.com\"\n\tconfig.author_github = \"pepijnvan\"\n\tconfig.version_file_name = \"__version__.py\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_mode = \"infer\"\n\tconfig.version_discovery_", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tif method not in vcs.handler_methods:\n\t\t\tvcs.handler_methods[method] = []\n\t\tvcs.handler_methods[method].append(f)\n\t\treturn f\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn._vcs_method = method\n\t\treturn fn\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tsetattr(fn, 'vcs_handler', True)\n\t\tsetattr(fn, 'vcs_method', method)\n\t\tsetattr(fn, 'vcs', vcs)\n\t\treturn fn\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tsetattr(f, '__vcs_handler__', True)\n\t\tsetattr(f, '__vcs_method__', method)\n\t\tsetattr(f, '__vcs__', vcs)\n\t\treturn f\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tsetattr(vcs, method, func)\n\t\treturn func\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef register_vcs_decorator(object):\n\t\tsetattr(object, method, vcs)\n\t\treturn object\n\treturn register_vcs_decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs[method] = func\n\t\treturn func\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(handler):\n\t\tsetattr(vcs, method, handler)\n\t\treturn handler\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs.handlers[method] = func\n\t\treturn func\n\treturn decorator\n", "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(method):\n\t\tsetattr(vcs, method.__name__, method)\n\t\treturn method\n\treturn decorator\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tif method not in vcs.handler_methods:\n\t\t\tvcs.handler_methods[method] = []\n\t\tvcs.handler_methods[method].append(f)\n\t\treturn f\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tfn._vcs_method = method\n\t\treturn fn\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(fn):\n\t\tsetattr(fn, 'vcs_handler', True)\n\t\tsetattr(fn, 'vcs_method', method)\n\t\tsetattr(fn, 'vcs', vcs)\n\t\treturn fn\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(f):\n\t\tsetattr(f, '__vcs_handler__', True)\n\t\tsetattr(f, '__vcs_method__', method)\n\t\tsetattr(f, '__vcs__', vcs)\n\t\treturn f\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tsetattr(vcs, method, func)\n\t\treturn func\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef register_vcs_decorator(object):\n\t\tsetattr(object, method, vcs)\n\t\treturn object\n\treturn register_vcs_decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs[method] = func\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(handler):\n\t\tsetattr(vcs, method, handler)\n\t\treturn handler\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(func):\n\t\tvcs.handlers[method] = func\n\t\treturn func\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\"\"\"\n\tCreate decorator to mark a method as the handler of a object\n\t\"\"\"\n\tdef decorator(method):\n\t\tsetattr(vcs, method.__name__, method)\n\t\treturn method\n\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor obj in self.storage_roots:\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\tgood_objects += obj.validate()\n\n\tif check_digests:\n\t\tgood_objects += self.validate_digests()\n\n\tif show_warnings:\n\t\tif good_objects < num_objects:\n\t\t\twarnings.warn('Found {} objects which were valid but not all of them were checked'.format(num_objects - good_objects))\n\n\treturn num_objects, good_objects", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor node in self.root.descendants():\n\t\tnum_objects += 1\n\n\t\tif node.is_object:\n\t\t\tif validate_objects:\n\t\t\t\tif not node.validate(check_digests):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Warning: %s is invalid' % node.path)\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint('Error: %s is invalid' % node.path)\n\t\t\t\telse:\n\t\t\t\t\tgood_objects += 1\n\n\t\telse:\n\t\t\tif node.is_dir:\n\t\t\t\tif not node.validate(check_digests):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Warning: %s is invalid' % node.path)\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint('Error: %s is invalid' % node.path)\n\t\t\t\telse:\n\t\t\t\t\tgood_objects += 1\n\n\treturn (num_objects, good_objects)\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor obj in self.get_objects():\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\ttry:\n\t\t\t\tself.validate_object(obj)\n\t\t\texcept:\n\t\t\t\tcontinue\n\t\tif check_digests:\n\t\t\ttry:\n\t\t\t\tself.check_digest(obj)\n\t\t\texcept:\n\t\t\t\tcontinue\n\t\tif show_warnings:\n\t\t\tif obj.warnings:\n\t\t\t\tprint('Warning: ' + str(obj.warnings))\n\t\tgood_objects += 1\n\treturn num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tgood_objects = 0\n\tnum_objects = 0\n\n\tfor obj in self.iter_root():\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\t(error, errmsg) = obj.validate()\n\t\t\tif error:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(errmsg)\n\t\t\t\tcontinue\n\n\t\tif check_digests:\n\t\t\t(error, errmsg) = obj.check_digests()\n\t\t\tif error:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(errmsg)\n\t\t\t\tcontinue\n\n\t\tgood_objects += 1\n\n\treturn (num_objects, good_objects)\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\troots = self.storage_roots.get_roots()\n\n\tfor root in roots:\n\t\tnum_objects += root.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n\t\tgood_objects += root.good_objects\n\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tdef _validate(obj, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t\"\"\"\n\t\tValidate the object and its children.\n\n    Returns:\n        good - True if the object is valid\n        message - error message if not valid\n\t\t\"\"\"\n\t\tgood = True\n\t\tmessage = None\n\t\tif validate_objects:\n\t\t\tif obj.name != obj.storage_root_name:\n\t\t\t\tmessage = 'Name mismatch: expected %s but got %s' % (obj.storage_root_name, obj.name)\n\t\t\t\tgood = False\n\t\t\tif obj.type != obj.storage_type:\n\t\t\t\tmessage = 'Type mismatch: expected %s but got %s' % (obj.storage_type, obj.type)\n\t\t\t\tgood = False\n\t\t\tif obj.level != obj.storage_level:\n\t\t\t\tmessage = 'Level mismatch: expected %s but got %s' % (obj.storage_level, obj.level)\n\t\t\t\tgood = False\n\t\tif check_digests:\n\t\t\tdigest, digest_algorithm = obj.storage_digest\n\t\t\tif digest is None:\n\t\t\t\tmessage = 'No storage digest'\n\t\t\t\tgood = False\n\t\t\telse:\n\t\t\t\texpected_digest", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor item in self.iteritems():\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\tif not item.validate_object():\n\t\t\t\tcontinue\n\t\tif check_digests:\n\t\t\tif not item.check_digests():\n\t\t\t\tcontinue\n\t\tif show_warnings:\n\t\t\titem.show_warnings()\n\t\tgood_objects += 1\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\twarnings = []\n\tfor root in self.storage_roots.values():\n\t\tnum_objects += root.count_objects()\n\t\tgood_objects += root.validate(validate_objects, check_digests, show_warnings)\n\t\twarnings += root.get_warnings()\n\tself.warnings = warnings\n\treturn num_objects, good_objects\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tgood_objects = 0\n\tnum_objects = 0\n\tfor obj in self.objects:\n\t\tnum_objects += 1\n\t\tobj_is_valid = self.validate_object(obj, validate_objects, check_digests, show_warnings)\n\t\tif obj_is_valid:\n\t\t\tgood_objects += 1\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tif validate_objects:\n\t\tnum_objects = self.validate_objects()\n\t\tgood_objects += num_objects\n\n\tif check_digests:\n\t\tnum_objects = self.check_digests()\n\t\tgood_objects += num_objects\n\n\tif show_warnings:\n\t\tself.show_warnings()\n\n\treturn num_objects, good_objects\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor obj in self.storage_roots:\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\tgood_objects += obj.validate()\n\n\tif check_digests:\n\t\tgood_objects += self.validate_digests()\n\n\tif show_warnings:\n\t\tif good_objects < num_objects:\n\t\t\twarnings.warn('Found {} objects which were valid but not all of them were checked'.format(num_objects - good_objects))\n\n\treturn num_objects, good_objects", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor node in self.root.descendants():\n\t\tnum_objects += 1\n\n\t\tif node.is_object:\n\t\t\tif validate_objects:\n\t\t\t\tif not node.validate(check_digests):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Warning: %s is invalid' % node.path)\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint('Error: %s is invalid' % node.path)\n\t\t\t\telse:\n\t\t\t\t\tgood_objects += 1\n\n\t\telse:\n\t\t\tif node.is_dir:\n\t\t\t\tif not node.validate(check_digests):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint('Warning: %s is invalid' % node.path)\n\t\t\t\t\telse:\n\t\t\t\t\t\tprint('Error: %s is invalid' % node.path)\n\t\t\t\telse:\n\t\t\t\t\tgood_objects += 1\n\n\treturn (num_objects, good_objects)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tfor obj in self.get_objects():\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\ttry:\n\t\t\t\tself.validate_object(obj)\n\t\t\texcept:\n\t\t\t\tcontinue\n\t\tif check_digests:\n\t\t\ttry:\n\t\t\t\tself.check_digest(obj)\n\t\t\texcept:\n\t\t\t\tcontinue\n\t\tif show_warnings:\n\t\t\tif obj.warnings:\n\t\t\t\tprint('Warning: ' + str(obj.warnings))\n\t\tgood_objects += 1\n\treturn num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tgood_objects = 0\n\tnum_objects = 0\n\n\tfor obj in self.iter_root():\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\t(error, errmsg) = obj.validate()\n\t\t\tif error:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(errmsg)\n\t\t\t\tcontinue\n\n\t\tif check_digests:\n\t\t\t(error, errmsg) = obj.check_digests()\n\t\t\tif error:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(errmsg)\n\t\t\t\tcontinue\n\n\t\tgood_objects += 1\n\n\treturn (num_objects, good_objects)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\troots = self.storage_roots.get_roots()\n\n\tfor root in roots:\n\t\tnum_objects += root.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n\t\tgood_objects += root.good_objects\n\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tdef _validate(obj, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t\"\"\"\n\t\tValidate the object and its children.\n\n    Returns:\n        good - True if the object is valid\n        message - error message if not valid\n\t\t\"\"\"\n\t\tgood = True\n\t\tmessage = None\n\t\tif validate_objects:\n\t\t\tif obj.name != obj.storage_root_name:\n\t\t\t\tmessage = 'Name mismatch: expected %s but got %s' % (obj.storage_root_name, obj.name)\n\t\t\t\tgood = False\n\t\t\tif obj.type != obj.storage_type:\n\t\t\t\tmessage = 'Type mismatch: expected %s but got %s' % (obj.storage_type, obj.type)\n\t\t\t\tgood = False\n\t\t\tif obj.level != obj.storage_level:\n\t\t\t\tmessage = 'Level mismatch: expected %s but got %s' % (obj.storage_level, obj.level)\n\t\t\t\tgood = False\n\t\tif check_digests:\n\t\t\tdigest, digest_algorithm = obj.storage_digest\n\t\t\tif digest is None:\n\t\t\t\tmessage = 'No storage digest'\n\t\t\t\tgood = False\n\t\t\telse:\n\t\t\t\texpected_digest", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor item in self.iteritems():\n\t\tnum_objects += 1\n\t\tif validate_objects:\n\t\t\tif not item.validate_object():\n\t\t\t\tcontinue\n\t\tif check_digests:\n\t\t\tif not item.check_digests():\n\t\t\t\tcontinue\n\t\tif show_warnings:\n\t\t\titem.show_warnings()\n\t\tgood_objects += 1\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\twarnings = []\n\tfor root in self.storage_roots.values():\n\t\tnum_objects += root.count_objects()\n\t\tgood_objects += root.validate(validate_objects, check_digests, show_warnings)\n\t\twarnings += root.get_warnings()\n\tself.warnings = warnings\n\treturn num_objects, good_objects\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tgood_objects = 0\n\tnum_objects = 0\n\tfor obj in self.objects:\n\t\tnum_objects += 1\n\t\tobj_is_valid = self.validate_object(obj, validate_objects, check_digests, show_warnings)\n\t\tif obj_is_valid:\n\t\t\tgood_objects += 1\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\"\"\"\n\tValidate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid\n\t\"\"\"\n\tnum_objects = 0\n\tgood_objects = 0\n\n\tif validate_objects:\n\t\tnum_objects = self.validate_objects()\n\t\tgood_objects += num_objects\n\n\tif check_digests:\n\t\tnum_objects = self.check_digests()\n\t\tgood_objects += num_objects\n\n\tif show_warnings:\n\t\tself.show_warnings()\n\n\treturn num_objects, good_objects\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself._init_storage_root()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLStorageRoot()\n\n\tself.root.create_root()\n\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage = ocfl_storage.OCFLStorage(self.storage_dir)", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLRoot(self.fs, self.fsroot)\n\tself.root.load()\n\tself.root.create()\n\tself.root.commit()\n\tself.root.close()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create_root()\n\tself.set_root(self.root_path)\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLStorageRoot.create()\n\tself.root.open()\n\tself.root.mount()\n\tself.root.set_config('catalog', 'OCFL')\n\tself.root.set_config('catalog', 'default', 'OCFL')\n\tself.root.set_config('catalog', 'default', 'path', self.root.path)\n\tself.root.close()\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage = OCFLStorage()\n\n\t# Create the directory structure and the content directory\n\tfor path in self.paths:\n\t\tself.storage.mkdir(path)\n\n\tself.storage.mkdir(self.paths[0] + '/content')\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLRoot(self.storage_root_name, self.storage_root_path)\n\tself.root.create()\n\tself.root.upload()\n\n\tself.root.create_directory(self.base_directory_name)\n\n\tself.base_directory = self.root.get_directory(self.base_directory_name)\n\tself.base_directory.create_directory(self.base_directory_name)\n\tself.base_directory.create_directory(self.base_directory_name)\n\n\tself.base_directory.create_file(\n\t\t'file_1',\n\t\tself.base_directory.get_file_path('file_1'),\n\t\tself.base_directory.get_file_path('file_1')\n\t\t)\n\tself.base_directory.create_file(\n\t\t'file_2',\n\t\tself.base_directory.get_file_path('file_2'),\n\t\tself.base_directory.get_file_path('file_2')\n\t\t)\n\n\tself.base_directory.create_file(\n\t\t'file_3',\n\t\tself.base_directory.get_file_path('file_3'),\n\t\tself.base_directory.get_file_path", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLStorageRoot(self.root_name)\n\n\tself.root.add_child(OCFLStorageVolume(self.root_name))\n\n\tself.root.add_child(OCFLStorageDirectory('root'))\n\n\tself.root.add_child(OCFLStorageDirectory('var'))\n\tself.root.add_child(OCFLStorageDirectory('var/log'))\n\n\tself.root.add_child(OCFLStorageDirectory('home'))\n\tself.root.add_child(OCFLStorageDirectory('home/user'))\n", "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\ttry:\n\t\tself.metadata = self.get_metadata()\n\t\tself.cache_dir = None\n\t\tself.cache_dir_created = False\n\texcept Exception as e:\n\t\traise OcflStorageError(e)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself._init_storage_root()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLStorageRoot()\n\n\tself.root.create_root()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage = ocfl_storage.OCFLStorage(self.storage_dir)", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLRoot(self.fs, self.fsroot)\n\tself.root.load()\n\tself.root.create()\n\tself.root.commit()\n\tself.root.close()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.create_root()\n\tself.set_root(self.root_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLStorageRoot.create()\n\tself.root.open()\n\tself.root.mount()\n\tself.root.set_config('catalog', 'OCFL')\n\tself.root.set_config('catalog', 'default', 'OCFL')\n\tself.root.set_config('catalog', 'default', 'path', self.root.path)\n\tself.root.close()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.storage = OCFLStorage()\n\n\t# Create the directory structure and the content directory\n\tfor path in self.paths:\n\t\tself.storage.mkdir(path)\n\n\tself.storage.mkdir(self.paths[0] + '/content')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLRoot(self.storage_root_name, self.storage_root_path)\n\tself.root.create()\n\tself.root.upload()\n\n\tself.root.create_directory(self.base_directory_name)\n\n\tself.base_directory = self.root.get_directory(self.base_directory_name)\n\tself.base_directory.create_directory(self.base_directory_name)\n\tself.base_directory.create_directory(self.base_directory_name)\n\n\tself.base_directory.create_file(\n\t\t'file_1',\n\t\tself.base_directory.get_file_path('file_1'),\n\t\tself.base_directory.get_file_path('file_1')\n\t\t)\n\tself.base_directory.create_file(\n\t\t'file_2',\n\t\tself.base_directory.get_file_path('file_2'),\n\t\tself.base_directory.get_file_path('file_2')\n\t\t)\n\n\tself.base_directory.create_file(\n\t\t'file_3',\n\t\tself.base_directory.get_file_path('file_3'),\n\t\tself.base_directory.get_file_path", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\tself.root = OCFLStorageRoot(self.root_name)\n\n\tself.root.add_child(OCFLStorageVolume(self.root_name))\n\n\tself.root.add_child(OCFLStorageDirectory('root'))\n\n\tself.root.add_child(OCFLStorageDirectory('var'))\n\tself.root.add_child(OCFLStorageDirectory('var/log'))\n\n\tself.root.add_child(OCFLStorageDirectory('home'))\n\tself.root.add_child(OCFLStorageDirectory('home/user'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t\"\"\"\n\tCreate and initialize a new OCFL storage root.\n\t\"\"\"\n\ttry:\n\t\tself.metadata = self.get_metadata()\n\t\tself.cache_dir = None\n\t\tself.cache_dir_created = False\n\texcept Exception as e:\n\t\traise OcflStorageError(e)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = list(map(int, version.split('.')))\n\tversion[-1] = version[-1] + 1\n\n\treturn '.'.join(map(str, version))\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = list(version)\n\tlast_digit = version[-1]\n\tlast_digit = int(last_digit)\n\tlast_digit += 1\n\tversion[-1] = str(last_digit)\n\treturn \"\".join(version)\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tparts[-1] = str(int(parts[-1]) + 1)\n\treturn '.'.join(parts)\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[-1] = str(int(version[-1]) + 1)\n\treturn '.'.join(version)\n\nif __name__ == '__main__':\n\tfrom sys import argv, exit\n\tif len(argv) < 2:\n\t\tprint(\"Usage: %s version\" % argv[0])\n\t\texit(1)\n\tprint(next_version(argv[1]))", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = list(version)\n\ti = len(version) - 1\n\twhile i >= 0:\n\t\tif version[i] != '9':\n\t\t\tversion[i] = chr(ord(version[i]) + 1)\n\t\t\tbreak\n\t\telse:\n\t\t\tversion[i] = '0'\n\t\ti -= 1\n\treturn ''.join(version)\n\n\n#TODO: Check if version is valid", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch = version\n\tpatch += 1\n\treturn (major, minor, patch)\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\tversion_parts[-1] = str(int(version_parts[-1]) + 1)\n\treturn '.'.join(version_parts)\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split(\".\")\n\tversion[-1] = int(version[-1]) + 1\n\tversion = \".\".join(version)\n\treturn version\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[2] = str(int(version[2]) + 1)\n\treturn '.'.join(version)\n", "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = int(version)\n\tv = version + 1\n\tv_str = str(v)\n\twhile len(v_str) < 4:\n\t\tv_str = '0' + v_str\n\treturn v_str\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = list(map(int, version.split('.')))\n\tversion[-1] = version[-1] + 1\n\n\treturn '.'.join(map(str, version))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = list(version)\n\tlast_digit = version[-1]\n\tlast_digit = int(last_digit)\n\tlast_digit += 1\n\tversion[-1] = str(last_digit)\n\treturn \"\".join(version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tparts = version.split('.')\n\tparts[-1] = str(int(parts[-1]) + 1)\n\treturn '.'.join(parts)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[-1] = str(int(version[-1]) + 1)\n\treturn '.'.join(version)\n\nif __name__ == '__main__':\n\tfrom sys import argv, exit\n\tif len(argv) < 2:\n\t\tprint(\"Usage: %s version\" % argv[0])\n\t\texit(1)\n\tprint(next_version(argv[1]))", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = list(version)\n\ti = len(version) - 1\n\twhile i >= 0:\n\t\tif version[i] != '9':\n\t\t\tversion[i] = chr(ord(version[i]) + 1)\n\t\t\tbreak\n\t\telse:\n\t\t\tversion[i] = '0'\n\t\ti -= 1\n\treturn ''.join(version)\n\n\n#TODO: Check if version is valid", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tmajor, minor, patch = version\n\tpatch += 1\n\treturn (major, minor, patch)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion_parts = version.split('.')\n\tversion_parts[-1] = str(int(version_parts[-1]) + 1)\n\treturn '.'.join(version_parts)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split(\".\")\n\tversion[-1] = int(version[-1]) + 1\n\tversion = \".\".join(version)\n\treturn version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = version.split('.')\n\tversion[2] = str(int(version[2]) + 1)\n\treturn '.'.join(version)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t\"\"\"\n\tGiven next version identifier following existing pattern\n\t\"\"\"\n\tversion = int(version)\n\tv = version + 1\n\tv_str = str(v)\n\twhile len(v_str) < 4:\n\t\tv_str = '0' + v_str\n\treturn v_str\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = {}\n\tversion_digests = {}\n\n\tfor version_dir in version_dirs:\n\t\tversion_inventories[version_dir] = {\n\t\t\t'version': version_dir,\n\t\t\t'inventory': None,\n\t\t\t'content_digest': None,\n\t\t}\n\t\tversion_digests[version_dir] = None\n\n\tinventory_path = self.get_inventory_path()\n\tif not os.path.isfile(inventory_path):\n\t\traise ValidationError(\"No inventory found at %s\" % inventory_path)\n\tinventory_digest = hashlib.md5(open(inventory_path, 'rb').read()).hexdigest()\n\tversion_inventories['root'] = {\n\t\t'version': 'root',\n\t\t'inventory': inventory_path,\n\t\t'content_digest': inventory_digest,\n\t}\n\tversion_digests['root'] = inventory_digest\n\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_path = os.path.join(self.repo_root, version_dir, 'inventory')\n\t\tif not os.path.isfile(version_inventory_path):\n\t\t\t", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_dirs = self._get_inventory_dirs()\n\tinventory_dirs = [os.path.join(self.root_dir, version_dir) for version_dir in version_dirs]\n\tinventory_dirs = [os.path.join(dir_path, 'inventory.yaml') for dir_path in inventory_dirs]\n\n\tinventory_dirs_to_check = list(set(inventory_dirs) - set(self.inventory_dirs))\n\tself.inventory_dirs = inventory_dirs\n\n\tif inventory_dirs_to_check:\n\t\traise ValueError('inventory dirs to check: {}'.format(inventory_dirs_to_check))\n\n\tself._get_content_dirs()\n\n\tversion_dirs_to_check = list(set(version_dirs) - set(self.version_dirs))\n\tself.version_dirs = version_dirs\n\n\tif version_dirs_to_check:\n\t\traise ValueError('version dirs to check: {}'.format(version_dirs_to_check))\n\n\tself._update_content_digests()\n\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdigests = {}\n\tinventories = {}\n\tfor version_dir in version_dirs:\n\t\tinventory_path = os.path.join(self.version_dir, version_dir, 'inventory')\n\t\tcontent_path = os.path.join(self.version_dir, version_dir, 'content')\n\t\tdigests[version_dir] = {}\n\t\tdigests[version_dir]['inventory'] = self.get_file_digest(inventory_path)\n\t\tdigests[version_dir]['content'] = self.get_file_digest(content_path)\n\t\tinventory = self.get_inventory(inventory_path)\n\t\tinventories[version_dir] = inventory\n\t\tself.validate_inventory(inventory_path, inventory)\n\t\tself.validate_content(content_path, inventory)\n\t\n\tself.validate_content_digests(digests)\n\tself.validate_inventory_digests(inventories)\n\treturn\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_inventory = self.root_inventory()\n\troot_inventory_digest = root_inventory.content_digest()\n\n\tversion_inventories = {}\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.version_inventory(version_dir)\n\t\tversion_inventory_digest = version_inventory.content_digest()\n\t\tversion_inventories[version_dir] = version_inventory_digest\n\n\t\tif root_inventory_digest != version_inventory_digest:\n\t\t\traise InvalidInventoryDigestError(\n\t\t\t\t\"The root inventory digest differs from the inventory for version '{}'\".format(version_dir))\n\n\treturn version_inventories\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory = self.get_inventory()\n\n\tversion_inventory = set()\n\n\tversion_content_digests = {}\n\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.get_inventory(version_dir)\n\t\tversion_content_digests = self.get_content_digests(version_inventory)\n\n\t\tself._verify_version_inventory(version_inventory, version_content_digests)\n", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tcontentDigests = {}\n\tcontent_dirs = Version.content_dirs(version_dirs)\n\tfor version_dir in version_dirs:\n\t\tversion_dir = os.path.join(self.repo_dir, version_dir)\n\t\tversion_content_dirs = content_dirs(version_dir)\n\t\t\n\t\tinventory_path = os.path.join(version_dir, 'inventory.yaml')\n\t\tif not os.path.exists(inventory_path):\n\t\t\traise Exception('No inventory found for version %s' % version_dir)\n\t\tinventory = self.load_yaml(inventory_path)\n\t\t\n\t\tinventory_digest = self.content_digest(version_content_dirs, 'inventory.yaml')\n\t\tif inventory_digest not in contentDigests:\n\t\t\tcontentDigests[inventory_digest] = {}\n\t\tcontentDigests[inventory_digest]['inventory'] = inventory\n\t\t\n\t\tcontent_dirs = inventory['content_dirs']\n\t\tfor content_dir in content_dirs:\n\t\t\tcontent_digest = self.content_digest(version_content_dirs, content_", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdigests = {}\n\thas_versions = False\n\thas_content = False\n\tfor version_dir in version_dirs:\n\t\tif not os.path.exists(os.path.join(self.root_directory, version_dir, 'inventory')):\n\t\t\traise Exception('Missing inventory for version %s' % version_dir)\n\n\t\tversion_dir_path = os.path.join(self.root_directory, version_dir)\n\t\tversion_inventory_path = os.path.join(version_dir_path, 'inventory')\n\t\tversion_content_dir = os.path.join(version_dir_path, 'content')\n\t\tversion_content_path = os.path.join(version_content_dir, 'content.json')\n\t\tversion_content_digest_path = os.path.join(version_content_dir, 'content_digest.json')\n\t\tversion_directory_digest_path = os.path.join(version_dir_path, 'directory_digest.json')\n\n\t\t# Validate inventory\n\t\twith open(version_inventory_path) as inventory:\n\t\t\tinventory_dict = json.load(inventory)\n\t\tif not inventory_dict:\n\t\t\traise", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = self.get_version_inventories()\n\tdigests = {}\n\tmissing_inventories = []\n\tversion_inventories_to_check = []\n\tfor version_dir in version_dirs:\n\t\tversion = version_dir.split('-')[1]\n\t\tversion_inventories_to_check.append(version)\n\t\tif version not in version_inventories:\n\t\t\tmissing_inventories.append(version)\n\t\tif 'content' in version_inventories[version]:\n\t\t\tdigests[version] = version_inventories[version]['content']['digest']\n\treturn missing_inventories, digests", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\troot_inventory = self.inventory_dir.join('inventory')\n\t\n\tinventories = {}\n\t\n\tversion_dirs.each do |version|\n\t\tinventory_file = self.inventory_dir.join(version, 'inventory')\n\t\tif File.exist?(inventory_file)\n\t\t\tdigests = {}\n\t\t\tinventory_file.open do |f|\n\t\t\t\tf.each_line do |line|\n\t\t\t\t\tif line =~ /^([a-f0-9]+) ([a-f0-9]+) (.+)$/\n\t\t\t\t\t\tdigests[$1] = $3\n\t\t\t\t\telse\n\t\t\t\t\t\traise \"Invalid inventory line: #{line}\"\n\t\t\t\t\tend\n\t\t\t\tend\n\t\t\tend\n\t\t\tinventories[version] = digests\n\t\telse\n\t\t\traise \"No inventory for version #{version}\"\n\t\tend\n\tend\n\t\n\tinventory_file = self.inventory_dir.join('inventory')\n\tif File.exist?(inventory_file)\n\t\tdigests = {}\n\t\tinventory_file.open do |f|\n\t\t\tf.each_line do |line|\n\t\t\t\tif line =~ /", "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tself.logger.info('Validating inventory versions')\n\n\t# List of inventory versions that have been processed\n\tversions_processed = []\n\n\t# List of inventory versions that have been processed\n\tversions_failed = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_digests = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_hash_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_size_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_version_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_size = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash_size = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash_size_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash_size_valid_valid = []\n\n\t# List of inventory versions that have"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = {}\n\tversion_digests = {}\n\n\tfor version_dir in version_dirs:\n\t\tversion_inventories[version_dir] = {\n\t\t\t'version': version_dir,\n\t\t\t'inventory': None,\n\t\t\t'content_digest': None,\n\t\t}\n\t\tversion_digests[version_dir] = None\n\n\tinventory_path = self.get_inventory_path()\n\tif not os.path.isfile(inventory_path):\n\t\traise ValidationError(\"No inventory found at %s\" % inventory_path)\n\tinventory_digest = hashlib.md5(open(inventory_path, 'rb').read()).hexdigest()\n\tversion_inventories['root'] = {\n\t\t'version': 'root',\n\t\t'inventory': inventory_path,\n\t\t'content_digest': inventory_digest,\n\t}\n\tversion_digests['root'] = inventory_digest\n\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_path = os.path.join(self.repo_root, version_dir, 'inventory')\n\t\tif not os.path.isfile(version_inventory_path):\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory_dirs = self._get_inventory_dirs()\n\tinventory_dirs = [os.path.join(self.root_dir, version_dir) for version_dir in version_dirs]\n\tinventory_dirs = [os.path.join(dir_path, 'inventory.yaml') for dir_path in inventory_dirs]\n\n\tinventory_dirs_to_check = list(set(inventory_dirs) - set(self.inventory_dirs))\n\tself.inventory_dirs = inventory_dirs\n\n\tif inventory_dirs_to_check:\n\t\traise ValueError('inventory dirs to check: {}'.format(inventory_dirs_to_check))\n\n\tself._get_content_dirs()\n\n\tversion_dirs_to_check = list(set(version_dirs) - set(self.version_dirs))\n\tself.version_dirs = version_dirs\n\n\tif version_dirs_to_check:\n\t\traise ValueError('version dirs to check: {}'.format(version_dirs_to_check))\n\n\tself._update_content_digests()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdigests = {}\n\tinventories = {}\n\tfor version_dir in version_dirs:\n\t\tinventory_path = os.path.join(self.version_dir, version_dir, 'inventory')\n\t\tcontent_path = os.path.join(self.version_dir, version_dir, 'content')\n\t\tdigests[version_dir] = {}\n\t\tdigests[version_dir]['inventory'] = self.get_file_digest(inventory_path)\n\t\tdigests[version_dir]['content'] = self.get_file_digest(content_path)\n\t\tinventory = self.get_inventory(inventory_path)\n\t\tinventories[version_dir] = inventory\n\t\tself.validate_inventory(inventory_path, inventory)\n\t\tself.validate_content(content_path, inventory)\n\t\n\tself.validate_content_digests(digests)\n\tself.validate_inventory_digests(inventories)\n\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\troot_inventory = self.root_inventory()\n\troot_inventory_digest = root_inventory.content_digest()\n\n\tversion_inventories = {}\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.version_inventory(version_dir)\n\t\tversion_inventory_digest = version_inventory.content_digest()\n\t\tversion_inventories[version_dir] = version_inventory_digest\n\n\t\tif root_inventory_digest != version_inventory_digest:\n\t\t\traise InvalidInventoryDigestError(\n\t\t\t\t\"The root inventory digest differs from the inventory for version '{}'\".format(version_dir))\n\n\treturn version_inventories\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tinventory = self.get_inventory()\n\n\tversion_inventory = set()\n\n\tversion_content_digests = {}\n\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.get_inventory(version_dir)\n\t\tversion_content_digests = self.get_content_digests(version_inventory)\n\n\t\tself._verify_version_inventory(version_inventory, version_content_digests)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\tcontentDigests = {}\n\tcontent_dirs = Version.content_dirs(version_dirs)\n\tfor version_dir in version_dirs:\n\t\tversion_dir = os.path.join(self.repo_dir, version_dir)\n\t\tversion_content_dirs = content_dirs(version_dir)\n\t\t\n\t\tinventory_path = os.path.join(version_dir, 'inventory.yaml')\n\t\tif not os.path.exists(inventory_path):\n\t\t\traise Exception('No inventory found for version %s' % version_dir)\n\t\tinventory = self.load_yaml(inventory_path)\n\t\t\n\t\tinventory_digest = self.content_digest(version_content_dirs, 'inventory.yaml')\n\t\tif inventory_digest not in contentDigests:\n\t\t\tcontentDigests[inventory_digest] = {}\n\t\tcontentDigests[inventory_digest]['inventory'] = inventory\n\t\t\n\t\tcontent_dirs = inventory['content_dirs']\n\t\tfor content_dir in content_dirs:\n\t\t\tcontent_digest = self.content_digest(version_content_dirs, content_", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tdigests = {}\n\thas_versions = False\n\thas_content = False\n\tfor version_dir in version_dirs:\n\t\tif not os.path.exists(os.path.join(self.root_directory, version_dir, 'inventory')):\n\t\t\traise Exception('Missing inventory for version %s' % version_dir)\n\n\t\tversion_dir_path = os.path.join(self.root_directory, version_dir)\n\t\tversion_inventory_path = os.path.join(version_dir_path, 'inventory')\n\t\tversion_content_dir = os.path.join(version_dir_path, 'content')\n\t\tversion_content_path = os.path.join(version_content_dir, 'content.json')\n\t\tversion_content_digest_path = os.path.join(version_content_dir, 'content_digest.json')\n\t\tversion_directory_digest_path = os.path.join(version_dir_path, 'directory_digest.json')\n\n\t\t# Validate inventory\n\t\twith open(version_inventory_path) as inventory:\n\t\t\tinventory_dict = json.load(inventory)\n\t\tif not inventory_dict:\n\t\t\traise", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tversion_inventories = self.get_version_inventories()\n\tdigests = {}\n\tmissing_inventories = []\n\tversion_inventories_to_check = []\n\tfor version_dir in version_dirs:\n\t\tversion = version_dir.split('-')[1]\n\t\tversion_inventories_to_check.append(version)\n\t\tif version not in version_inventories:\n\t\t\tmissing_inventories.append(version)\n\t\tif 'content' in version_inventories[version]:\n\t\t\tdigests[version] = version_inventories[version]['content']['digest']\n\treturn missing_inventories, digests", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\t\n\troot_inventory = self.inventory_dir.join('inventory')\n\t\n\tinventories = {}\n\t\n\tversion_dirs.each do |version|\n\t\tinventory_file = self.inventory_dir.join(version, 'inventory')\n\t\tif File.exist?(inventory_file)\n\t\t\tdigests = {}\n\t\t\tinventory_file.open do |f|\n\t\t\t\tf.each_line do |line|\n\t\t\t\t\tif line =~ /^([a-f0-9]+) ([a-f0-9]+) (.+)$/\n\t\t\t\t\t\tdigests[$1] = $3\n\t\t\t\t\telse\n\t\t\t\t\t\traise \"Invalid inventory line: #{line}\"\n\t\t\t\t\tend\n\t\t\t\tend\n\t\t\tend\n\t\t\tinventories[version] = digests\n\t\telse\n\t\t\traise \"No inventory for version #{version}\"\n\t\tend\n\tend\n\t\n\tinventory_file = self.inventory_dir.join('inventory')\n\tif File.exist?(inventory_file)\n\t\tdigests = {}\n\t\tinventory_file.open do |f|\n\t\t\tf.each_line do |line|\n\t\t\t\tif line =~ /", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t\"\"\"\n\tEach version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).\n\t\"\"\"\n\tself.logger.info('Validating inventory versions')\n\n\t# List of inventory versions that have been processed\n\tversions_processed = []\n\n\t# List of inventory versions that have been processed\n\tversions_failed = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_digests = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_hash_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_size_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_version_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_size = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash_size = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash_size_valid = []\n\n\t# List of inventory versions that have failed validation\n\tversions_failed_content_valid_hash_size_valid_valid = []\n\n\t# List of inventory versions that have", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n\tpath_type = ''\n\t\n\tif os.path.isdir(path):\n\t\tpath_type = \"D\"\n\telif os.path.isfile(path):\n\t\tpath_type = \"F\"\n\telse:\n\t\tpath_type = \"N\"\n\t\t\n\treturn path_type\n\t\n\t\n\t", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = os.path.abspath(path)\n\tpath_type = ''\n\t\n\tpath_type = os.path.splitext(path)[1]\n\tpath_type = path_type.strip('.')\n\t\n\tpath_type = path_type.lower()\n\t\n\treturn path_type", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = path.lower()\n\tif path.endswith(\".jpeg\") or path.endswith(\".jpg\"):\n\t\treturn \"image\"\n\telif path.endswith(\".png\"):\n\t\treturn \"image\"\n\telif path.endswith(\".gif\"):\n\t\treturn \"image\"\n\telif path.endswith(\".css\") or path.endswith(\".js\"):\n\t\treturn \"script\"\n\telse:\n\t\treturn \"text\"", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tdef get_type(obj):\n\t\tif not hasattr(obj, '__name__'):\n\t\t\treturn type(obj).__name__\n\t\treturn obj.__name__\n\t\n\tdef get_types(obj):\n\t\tif not hasattr(obj, '__name__'):\n\t\t\treturn [type(obj).__name__]\n\t\treturn [obj.__name__]\n\t\n\tdef get_path_type(path, path_types):\n\t\t\"\"\"\n\t\tReturn a string indicating the type of thing at the given path\n\t\t\"\"\"\n\t\tif not path:\n\t\t\treturn None\n\t\t\n\t\ttry:\n\t\t\tpath = path[:-1]\n\t\t\tpath_type = path_types[path]\n\t\t\treturn path_type\n\t\texcept:\n\t\t\treturn None\n\t\t\n\tdef get_path_type_recursive(path, path_types, depth):\n\t\t\"\"\"\n\t\tReturn a string indicating the type of thing at the given path\n\t\t\"\"\"\n\t\tif not path:\n\t\t\treturn None\n\t\t\n\t\ttry:\n\t\t\tpath = path[:-1]\n\t\t\tpath_type = path_types[path]\n\t\t\tif depth > 0:\n\t\t\t\treturn get_path_type_recursive(path, path_type, depth-1)\n\t\t\telse", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\ttry:\n\t\treturn os.stat(path).st_mode\n\texcept OSError:\n\t\treturn None\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = path.split(\"/\")[-1].split(\".\")[0]\n\treturn path_type\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\ttype = path.split('/')[-1].split('.')[0]\n\treturn (type)\n\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = None\n\t\n\tpath_type = path.split('.')[-1]\n\t\n\tif path_type == 'txt':\n\t\tpath_type = 'text'\n\telif path_type == 'html':\n\t\tpath_type = 'html'\n\telif path_type == 'xhtml':\n\t\tpath_type = 'html'\n\telif path_type == 'jpg' or path_type == 'jpeg':\n\t\tpath_type = 'image'\n\telif path_type == 'gif':\n\t\tpath_type = 'image'\n\telif path_type == 'png':\n\t\tpath_type = 'image'\n\telif path_type == 'svg':\n\t\tpath_type = 'image'\n\telif path_type == 'eot':\n\t\tpath_type = 'font'\n\telif path_type == 'ttf':\n\t\tpath_type = 'font'\n\telif path_type == 'woff':\n\t\tpath_type = 'font'\n\telif path_type == 'woff2':\n\t\tpath_type = 'font'\n\telif path_type == 'otf':\n\t\tpath_type = 'font'\n\telif path_type == 'css':\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = path.split(\"/\")\n\tpath = [p for p in path if p]\n\tif len(path) < 1:\n\t\treturn \"root\"\n\telif len(path) < 2:\n\t\treturn \"dir\"\n\telse:\n\t\treturn \"file\"\n", "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = ''\n\tif path.endswith('/'):\n\t\tpath = path[:-1]\n\tpath_type = os.path.basename(path)\n\treturn path_type\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\t\n\tpath_type = ''\n\t\n\tif os.path.isdir(path):\n\t\tpath_type = \"D\"\n\telif os.path.isfile(path):\n\t\tpath_type = \"F\"\n\telse:\n\t\tpath_type = \"N\"\n\t\t\n\treturn path_type\n\t\n\t\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = os.path.abspath(path)\n\tpath_type = ''\n\t\n\tpath_type = os.path.splitext(path)[1]\n\tpath_type = path_type.strip('.')\n\t\n\tpath_type = path_type.lower()\n\t\n\treturn path_type", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = path.lower()\n\tif path.endswith(\".jpeg\") or path.endswith(\".jpg\"):\n\t\treturn \"image\"\n\telif path.endswith(\".png\"):\n\t\treturn \"image\"\n\telif path.endswith(\".gif\"):\n\t\treturn \"image\"\n\telif path.endswith(\".css\") or path.endswith(\".js\"):\n\t\treturn \"script\"\n\telse:\n\t\treturn \"text\"", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tdef get_type(obj):\n\t\tif not hasattr(obj, '__name__'):\n\t\t\treturn type(obj).__name__\n\t\treturn obj.__name__\n\t\n\tdef get_types(obj):\n\t\tif not hasattr(obj, '__name__'):\n\t\t\treturn [type(obj).__name__]\n\t\treturn [obj.__name__]\n\t\n\tdef get_path_type(path, path_types):\n\t\t\"\"\"\n\t\tReturn a string indicating the type of thing at the given path\n\t\t\"\"\"\n\t\tif not path:\n\t\t\treturn None\n\t\t\n\t\ttry:\n\t\t\tpath = path[:-1]\n\t\t\tpath_type = path_types[path]\n\t\t\treturn path_type\n\t\texcept:\n\t\t\treturn None\n\t\t\n\tdef get_path_type_recursive(path, path_types, depth):\n\t\t\"\"\"\n\t\tReturn a string indicating the type of thing at the given path\n\t\t\"\"\"\n\t\tif not path:\n\t\t\treturn None\n\t\t\n\t\ttry:\n\t\t\tpath = path[:-1]\n\t\t\tpath_type = path_types[path]\n\t\t\tif depth > 0:\n\t\t\t\treturn get_path_type_recursive(path, path_type, depth-1)\n\t\t\telse", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\ttry:\n\t\treturn os.stat(path).st_mode\n\texcept OSError:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = path.split(\"/\")[-1].split(\".\")[0]\n\treturn path_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\ttype = path.split('/')[-1].split('.')[0]\n\treturn (type)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = None\n\t\n\tpath_type = path.split('.')[-1]\n\t\n\tif path_type == 'txt':\n\t\tpath_type = 'text'\n\telif path_type == 'html':\n\t\tpath_type = 'html'\n\telif path_type == 'xhtml':\n\t\tpath_type = 'html'\n\telif path_type == 'jpg' or path_type == 'jpeg':\n\t\tpath_type = 'image'\n\telif path_type == 'gif':\n\t\tpath_type = 'image'\n\telif path_type == 'png':\n\t\tpath_type = 'image'\n\telif path_type == 'svg':\n\t\tpath_type = 'image'\n\telif path_type == 'eot':\n\t\tpath_type = 'font'\n\telif path_type == 'ttf':\n\t\tpath_type = 'font'\n\telif path_type == 'woff':\n\t\tpath_type = 'font'\n\telif path_type == 'woff2':\n\t\tpath_type = 'font'\n\telif path_type == 'otf':\n\t\tpath_type = 'font'\n\telif path_type == 'css':\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath = path.split(\"/\")\n\tpath = [p for p in path if p]\n\tif len(path) < 1:\n\t\treturn \"root\"\n\telif len(path) < 2:\n\t\treturn \"dir\"\n\telse:\n\t\treturn \"file\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t\"\"\"\n\tReturn a string indicating the type of thing at the given path\n\t\"\"\"\n\tpath_type = ''\n\tif path.endswith('/'):\n\t\tpath = path[:-1]\n\tpath_type = os.path.basename(path)\n\treturn path_type\n", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bids'] = bug_ids\n\tparams['fields'] = ['id', 'summary', 'assigned_to', 'resolution', 'status', 'assigned_to', 'resolution', 'opened_by', 'last_change_time', 'last_change_by', 'last_change_comment', 'last_change_time', 'last_change_by', 'last_change_comment']\n\tparams['format'] = 'json'\n\treturn params", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.bz.set_params(params)\n\n\tfor bug_id in bug_ids:\n\t\tself.bz.update_bug(bug_id)\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.bug_ids = bug_ids\n\tself.params = params\n\tself.bug_ids = self.bug_ids.split(\"'\")\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams.update({\n\t\t\"bug_ids\": bug_ids,\n\t\t\"bug_id\": bug_ids[0],\n\t})\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_ids'] = ','.join(bug_ids)\n\n\treturn params\n", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams = params.copy()\n\tparams.setdefault('bug_ids', ','.join(bug_ids))\n\treturn params", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.bzparams['project'] = params['project']\n\tself.bzparams['summary'] = params['summary']\n\tself.bzparams['component'] = params['component']\n\tself.bzparams['version'] = params['version']\n\tself.bzparams['keywords'] = params['keywords']\n\tself.bzparams['assigned_to'] = params['assigned_to']\n\tself.bzparams['reported_by'] = params['reported_by']\n\tself.bzparams['bug_severity'] = params['bug_severity']\n\tself.bzparams['bug_status'] = params['bug_status']\n\tself.bzparams['target_milestone'] = params['target_milestone']\n\tself.bzparams['target_release'] = params['target_release']\n\tself.bzparams['priority'] = params['priority']\n\tself.bzparams['affected_release'] = params['affected_release']\n\tself.bzparams['cc'] = params['cc']\n\tself.bzparams['cc_list'] = params['cc_list']\n\tself.bzparams['cc_list_new'] = params['cc_list_new']\n\tself.bzparams['", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams[\"bz_bug_ids\"] = bug_ids\n\tparams[\"bz_reporter\"] = self.bz_reporter\n\tparams[\"bz_assigned_to\"] = self.bz_assigned_to\n\t#params[\"bz_component\"] = self.bz_component\n\t#params[\"bz_version\"] = self.bz_version\n\tparams[\"bz_classification\"] = self.bz_classification\n\n\tself.bz_params = params", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bz_version'] = self.BUGS_VERSION\n\tif self.BUGS_USER is not None:\n\t\tparams['bz_user'] = self.BUGS_USER\n\tif self.BUGS_PASSWORD is not None:\n\t\tparams['bz_password'] = self.BUGS_PASSWORD\n\tif self.BUGS_URL is not None:\n\t\tparams['bz_url'] = self.BUGS_URL\n\tif self.BUGS_PROJECT is not None:\n\t\tparams['bz_project'] = self.BUGS_PROJECT\n\tif self.BUGS_BUGS is not None:\n\t\tparams['bz_bugs'] = self.BUGS_BUGS\n\tif self.BUGS_COMPONENTS is not None:\n\t\tparams['bz_components'] = self.BUGS_COMPONENTS\n\tif self.BUGS_INCLUDE is not None:\n\t\tparams['bz_include'] = self.BUGS_INCLUDE\n\tif self.BUGS_EXCLUDE is not None:\n\t\tparams['bz_exclude'] = self.BUGS_EXCLUDE\n\tif self.BUGS_SEVERITY is not None:\n\t\tparams['bz_severity'] = self.BUGS_SE", "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['product_id'] = self.product_id\n\tparams['component_id'] = self.component_id\n\tparams['bug_id'] = bug_ids\n\n\t# Add the product and component names\n\tparams['product_name'] = self.product_name\n\tparams['component_name'] = self.component_name\n\n\t# Add the product and component descriptions\n\tparams['product_description'] = self.product_description\n\tparams['component_description'] = self.component_description\n\n\t# Add the bugzilla version\n\tparams['bugzilla_version'] = self.bugzilla_version\n\n\t# Add the user agent\n\tparams['user_agent'] = self.user_agent"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bids'] = bug_ids\n\tparams['fields'] = ['id', 'summary', 'assigned_to', 'resolution', 'status', 'assigned_to', 'resolution', 'opened_by', 'last_change_time', 'last_change_by', 'last_change_comment', 'last_change_time', 'last_change_by', 'last_change_comment']\n\tparams['format'] = 'json'\n\treturn params", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.bz.set_params(params)\n\n\tfor bug_id in bug_ids:\n\t\tself.bz.update_bug(bug_id)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.bug_ids = bug_ids\n\tself.params = params\n\tself.bug_ids = self.bug_ids.split(\"'\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams.update({\n\t\t\"bug_ids\": bug_ids,\n\t\t\"bug_id\": bug_ids[0],\n\t})\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bug_ids'] = ','.join(bug_ids)\n\n\treturn params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams = params.copy()\n\tparams.setdefault('bug_ids', ','.join(bug_ids))\n\treturn params", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tself.bzparams['project'] = params['project']\n\tself.bzparams['summary'] = params['summary']\n\tself.bzparams['component'] = params['component']\n\tself.bzparams['version'] = params['version']\n\tself.bzparams['keywords'] = params['keywords']\n\tself.bzparams['assigned_to'] = params['assigned_to']\n\tself.bzparams['reported_by'] = params['reported_by']\n\tself.bzparams['bug_severity'] = params['bug_severity']\n\tself.bzparams['bug_status'] = params['bug_status']\n\tself.bzparams['target_milestone'] = params['target_milestone']\n\tself.bzparams['target_release'] = params['target_release']\n\tself.bzparams['priority'] = params['priority']\n\tself.bzparams['affected_release'] = params['affected_release']\n\tself.bzparams['cc'] = params['cc']\n\tself.bzparams['cc_list'] = params['cc_list']\n\tself.bzparams['cc_list_new'] = params['cc_list_new']\n\tself.bzparams['", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams[\"bz_bug_ids\"] = bug_ids\n\tparams[\"bz_reporter\"] = self.bz_reporter\n\tparams[\"bz_assigned_to\"] = self.bz_assigned_to\n\t#params[\"bz_component\"] = self.bz_component\n\t#params[\"bz_version\"] = self.bz_version\n\tparams[\"bz_classification\"] = self.bz_classification\n\n\tself.bz_params = params", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['bz_version'] = self.BUGS_VERSION\n\tif self.BUGS_USER is not None:\n\t\tparams['bz_user'] = self.BUGS_USER\n\tif self.BUGS_PASSWORD is not None:\n\t\tparams['bz_password'] = self.BUGS_PASSWORD\n\tif self.BUGS_URL is not None:\n\t\tparams['bz_url'] = self.BUGS_URL\n\tif self.BUGS_PROJECT is not None:\n\t\tparams['bz_project'] = self.BUGS_PROJECT\n\tif self.BUGS_BUGS is not None:\n\t\tparams['bz_bugs'] = self.BUGS_BUGS\n\tif self.BUGS_COMPONENTS is not None:\n\t\tparams['bz_components'] = self.BUGS_COMPONENTS\n\tif self.BUGS_INCLUDE is not None:\n\t\tparams['bz_include'] = self.BUGS_INCLUDE\n\tif self.BUGS_EXCLUDE is not None:\n\t\tparams['bz_exclude'] = self.BUGS_EXCLUDE\n\tif self.BUGS_SEVERITY is not None:\n\t\tparams['bz_severity'] = self.BUGS_SE", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t\"\"\"\n\tAmend the Bugzilla params\n\t\"\"\"\n\tparams['product_id'] = self.product_id\n\tparams['component_id'] = self.component_id\n\tparams['bug_id'] = bug_ids\n\n\t# Add the product and component names\n\tparams['product_name'] = self.product_name\n\tparams['component_name'] = self.component_name\n\n\t# Add the product and component descriptions\n\tparams['product_description'] = self.product_description\n\tparams['component_description'] = self.component_description\n\n\t# Add the bugzilla version\n\tparams['bugzilla_version'] = self.bugzilla_version\n\n\t# Add the user agent\n\tparams['user_agent'] = self.user_agent", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdef _merge_node(node1, node2):\n\t\tif isinstance(node1, MappingNode):\n\t\t\tfor key in node1.iterkeys():\n\t\t\t\tif key in node2:\n\t\t\t\t\tnode1[key] = deep_merge_nodes(node1[key], node2[key])\n\t\treturn node2\n\n\tmerged_nodes = []\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tmerged_nodes.append(_merge_node(merged_nodes[-1], node))\n\t\telse:\n\t\t\tmerged_nodes.append(node)\n\treturn merged_nodes\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = []\n\tcolliding_keys = {}\n\tfor node in nodes:\n\t\tif node.key in colliding_keys:\n\t\t\tmerged_nodes.append(node)\n\t\telse:\n\t\t\tcolliding_keys[node.key] = node\n\tmerged_nodes.extend(colliding_keys.values())\n\treturn merged_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn MappingNode(result)\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tnew_nodes = {}\n\tfor node in nodes:\n\t\tkey = node.key\n\t\tif key not in new_nodes:\n\t\t\tnew_nodes[key] = node\n\t\telif not isinstance(new_nodes[key], MappingNode):\n\t\t\tnew_nodes[key] = node\n\n\treturn new_nodes\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tmerged_nodes = {}\n\t\n\tfor node in nodes:\n\t\tif node.key in merged_nodes:\n\t\t\tif isinstance(merged_nodes[node.key], Node):\n\t\t\t\tmerged_nodes[node.key].value = node.value\n\t\t\telse:\n\t\t\t\tmerged_nodes[node.key] = node.value\n\t\telse:\n\t\t\tmerged_nodes[node.key] = node.value\n\t\n\treturn merged_nodes\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key in node.keys():\n\t\t\tif key in merged:\n\t\t\t\tvalue = node[key]\n\t\t\t\tif not isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tmerged[key] = node[key]\n\treturn merged\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdupes = {}\n\tresult = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\tcontinue\n\t\tkey = node['key']\n\t\tif key in dupes:\n\t\t\tcontinue\n\t\tdupes[key] = 1\n\t\tif key in result:\n\t\t\tresult[key] = deep_merge_nodes([result[key], node])\n\t\telse:\n\t\t\tresult[key] = node\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tif node.value is None:\n\t\t\tcontinue\n\t\tif isinstance(node.value, MappingNode):\n\t\t\tresult.update(node.value.data)\n\t\telse:\n\t\t\tresult[node.key] = node.value\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = dict()\n\tfor k, v in nodes.items():\n\t\tif isinstance(v, MappingNode):\n\t\t\tresult = deep_merge_nodes(result.setdefault(k, dict()))\n\t\tresult[k] = v\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tif node.key in result:\n\t\t\tresult[node.key] = node.value\n\t\telse:\n\t\t\tresult[node.key] = node.value\n\treturn result\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdef _merge_node(node1, node2):\n\t\tif isinstance(node1, MappingNode):\n\t\t\tfor key in node1.iterkeys():\n\t\t\t\tif key in node2:\n\t\t\t\t\tnode1[key] = deep_merge_nodes(node1[key], node2[key])\n\t\treturn node2\n\n\tmerged_nodes = []\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tmerged_nodes.append(_merge_node(merged_nodes[-1], node))\n\t\telse:\n\t\t\tmerged_nodes.append(node)\n\treturn merged_nodes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = []\n\tcolliding_keys = {}\n\tfor node in nodes:\n\t\tif node.key in colliding_keys:\n\t\t\tmerged_nodes.append(node)\n\t\telse:\n\t\t\tcolliding_keys[node.key] = node\n\tmerged_nodes.extend(colliding_keys.values())\n\treturn merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn MappingNode(result)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tnew_nodes = {}\n\tfor node in nodes:\n\t\tkey = node.key\n\t\tif key not in new_nodes:\n\t\t\tnew_nodes[key] = node\n\t\telif not isinstance(new_nodes[key], MappingNode):\n\t\t\tnew_nodes[key] = node\n\n\treturn new_nodes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tmerged_nodes = {}\n\t\n\tfor node in nodes:\n\t\tif node.key in merged_nodes:\n\t\t\tif isinstance(merged_nodes[node.key], Node):\n\t\t\t\tmerged_nodes[node.key].value = node.value\n\t\t\telse:\n\t\t\t\tmerged_nodes[node.key] = node.value\n\t\telse:\n\t\t\tmerged_nodes[node.key] = node.value\n\t\n\treturn merged_nodes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key in node.keys():\n\t\t\tif key in merged:\n\t\t\t\tvalue = node[key]\n\t\t\t\tif not isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = value\n\t\t\telse:\n\t\t\t\tmerged[key] = node[key]\n\treturn merged\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tdupes = {}\n\tresult = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\tcontinue\n\t\tkey = node['key']\n\t\tif key in dupes:\n\t\t\tcontinue\n\t\tdupes[key] = 1\n\t\tif key in result:\n\t\t\tresult[key] = deep_merge_nodes([result[key], node])\n\t\telse:\n\t\t\tresult[key] = node\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tif node.value is None:\n\t\t\tcontinue\n\t\tif isinstance(node.value, MappingNode):\n\t\t\tresult.update(node.value.data)\n\t\telse:\n\t\t\tresult[node.key] = node.value\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = dict()\n\tfor k, v in nodes.items():\n\t\tif isinstance(v, MappingNode):\n\t\t\tresult = deep_merge_nodes(result.setdefault(k, dict()))\n\t\tresult[k] = v\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tMerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tif node.key in result:\n\t\t\tresult[node.key] = node.value\n\t\telse:\n\t\t\tresult[node.key] = node.value\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-t\", \"--template\", dest=\"template\", help=\"name of the template\", default=\"default.yaml\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", help=\"name of the output file\", default=\"default.yaml\")\n\tparser.add_argument(\"-d\", \"--directory\", dest=\"directory\", help=\"directory to look for templates in\", default=\"templates\")\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Parse a file containing a list of files to be checked for duplicate\")\n\tparser.add_argument(\"-d\", \"--directory\", required=True, help=\"The directory containing one or more files to be checked for duplicate\")\n\tparser.add_argument(\"-o\", \"--output\", required=True, help=\"The file in which the output will be written\")\n\tparser.add_argument(\"-i\", \"--ignore\", action=\"append\", help=\"A list of file extensions to be ignored (multiple use allowed)\")\n\treturn parser.parse_args(*arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"This script will convert a given directory of CSV files into a single CSV file\")\n\tparser.add_argument('-i', '--input', required=True, help='The directory containing the CSV files to be combined')\n\tparser.add_argument('-o', '--output', required=True, help='The directory where the combined CSV file is to be saved')\n\tparser.add_argument('-f', '--filename', required=True, help='The name of the combined CSV file')\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"This script computes the most likely sequence of amino acids that can be generated from a given sequence\")\n\tparser.add_argument(\"-f\", \"--file\", help=\"Input file to parse\")\n\tparser.add_argument(\"-s\", \"--sequence\", help=\"Sequence to parse\")\n\tparser.add_argument(\"-t\", \"--threshold\", type=float, default=0.3, help=\"Threshold for probability of amino acid to be chosen\")\n\tparser.add_argument(\"-p\", \"--protein\", action=\"store_true\", help=\"If specified, output protein sequence instead of amino acid sequence\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"If specified, print out the probability of each amino acid being chosen\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file to write to\")\n\tparser.add_argument(\"-a\", \"--all\", action=\"store_true\", help=\"If specified, output all of the amino acids that can be generated\")\n\targs = parser.parse_args(arguments)\n\tif args.file:\n\t\twith open(args.file,", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-s\", \"--source\", help=\"source file to generate\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", help=\"output file to generate\", required=True)\n\tparser.add_argument(\"-i\", \"--input\", help=\"input file to generate\")\n\tparser.add_argument(\"-t\", \"--template\", help=\"template file to use\")\n\treturn parser.parse_args(arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description = \"BamGenomeParser\")\n\tparser.add_argument('--bam', required = True, help = 'Input BAM file')\n\tparser.add_argument('--bam-index', help = 'Input BAM index file')\n\tparser.add_argument('--bam-prefix', help = 'Output BAM file prefix')\n\tparser.add_argument('--bam-chrom-prefix', help = 'Output BAM chromosome prefix')\n\tparser.add_argument('--bam-chrom-suffix', help = 'Output BAM chromosome suffix')\n\tparser.add_argument('--bam-output', help = 'Output BAM file')\n\tparser.add_argument('--bam-index-output', help = 'Output BAM index file')\n\tparser.add_argument('--bam-region', help = 'Output BAM region')\n\tparser.add_argument('--bam-region-output', help = 'Output BAM region file')\n\tparser.add_argument('--bam-region-output-prefix', help = 'Output BAM region file prefix')\n\tparser.add_argument('--bam-region-output-suffix', help = 'Output BAM region file", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Run a set of experiments\")\n\tparser.add_argument('-f', '--file', required=True, help='File containing the data to be used for the experiment')\n\tparser.add_argument('-n', '--name', required=True, help='Name of the experiment')\n\tparser.add_argument('-p', '--plot', action='store_true', help='Whether to include plotting code')\n\tparser.add_argument('-t', '--tasks', nargs='+', help='The tasks to be run')\n\tparser.add_argument('-s', '--sequences', nargs='+', help='The sequences to be run')\n\tparser.add_argument('-c', '--combine', nargs='+', help='The combination of the tasks to be run')\n\tparser.add_argument('-b', '--baseline', nargs='+', help='The baseline tasks to be run')\n\tparser.add_argument('-x', '--xtasks', nargs='+', help='The tasks to be run with xtasks')\n\tparser.add_argument('-y', '--ytasks', nargs='+',", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"A tool for managing the database of a local library\")\n\tparser.add_argument(\"-a\", \"--add\", action=\"store_true\", help=\"Add a book to the library\")\n\tparser.add_argument(\"-d\", \"--delete\", action=\"store_true\", help=\"Delete a book from the library\")\n\tparser.add_argument(\"-l\", \"--list\", action=\"store_true\", help=\"List all books in the library\")\n\tparser.add_argument(\"-r\", \"--remove\", action=\"store_true\", help=\"Remove a book from the library\")\n\tparser.add_argument(\"-u\", \"--update\", action=\"store_true\", help=\"Update a book in the library\")\n\tparser.add_argument(\"-s\", \"--search\", action=\"store_true\", help=\"Search for a book in the library\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"1.0.0\")\n\targs = parser.parse_args(arguments)\n\treturn args\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-t\", \"--template\", dest=\"template\", help=\"name of the template\", default=\"default.yaml\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", help=\"name of the output file\", default=\"default.yaml\")\n\tparser.add_argument(\"-d\", \"--directory\", dest=\"directory\", help=\"directory to look for templates in\", default=\"templates\")\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Parse a file containing a list of files to be checked for duplicate\")\n\tparser.add_argument(\"-d\", \"--directory\", required=True, help=\"The directory containing one or more files to be checked for duplicate\")\n\tparser.add_argument(\"-o\", \"--output\", required=True, help=\"The file in which the output will be written\")\n\tparser.add_argument(\"-i\", \"--ignore\", action=\"append\", help=\"A list of file extensions to be ignored (multiple use allowed)\")\n\treturn parser.parse_args(*arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"This script will convert a given directory of CSV files into a single CSV file\")\n\tparser.add_argument('-i', '--input', required=True, help='The directory containing the CSV files to be combined')\n\tparser.add_argument('-o', '--output', required=True, help='The directory where the combined CSV file is to be saved')\n\tparser.add_argument('-f', '--filename', required=True, help='The name of the combined CSV file')\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"This script computes the most likely sequence of amino acids that can be generated from a given sequence\")\n\tparser.add_argument(\"-f\", \"--file\", help=\"Input file to parse\")\n\tparser.add_argument(\"-s\", \"--sequence\", help=\"Sequence to parse\")\n\tparser.add_argument(\"-t\", \"--threshold\", type=float, default=0.3, help=\"Threshold for probability of amino acid to be chosen\")\n\tparser.add_argument(\"-p\", \"--protein\", action=\"store_true\", help=\"If specified, output protein sequence instead of amino acid sequence\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"If specified, print out the probability of each amino acid being chosen\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file to write to\")\n\tparser.add_argument(\"-a\", \"--all\", action=\"store_true\", help=\"If specified, output all of the amino acids that can be generated\")\n\targs = parser.parse_args(arguments)\n\tif args.file:\n\t\twith open(args.file,", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tparser.add_argument(\"-s\", \"--source\", help=\"source file to generate\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", help=\"output file to generate\", required=True)\n\tparser.add_argument(\"-i\", \"--input\", help=\"input file to generate\")\n\tparser.add_argument(\"-t\", \"--template\", help=\"template file to use\")\n\treturn parser.parse_args(arguments)\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description = \"BamGenomeParser\")\n\tparser.add_argument('--bam', required = True, help = 'Input BAM file')\n\tparser.add_argument('--bam-index', help = 'Input BAM index file')\n\tparser.add_argument('--bam-prefix', help = 'Output BAM file prefix')\n\tparser.add_argument('--bam-chrom-prefix', help = 'Output BAM chromosome prefix')\n\tparser.add_argument('--bam-chrom-suffix', help = 'Output BAM chromosome suffix')\n\tparser.add_argument('--bam-output', help = 'Output BAM file')\n\tparser.add_argument('--bam-index-output', help = 'Output BAM index file')\n\tparser.add_argument('--bam-region', help = 'Output BAM region')\n\tparser.add_argument('--bam-region-output', help = 'Output BAM region file')\n\tparser.add_argument('--bam-region-output-prefix', help = 'Output BAM region file prefix')\n\tparser.add_argument('--bam-region-output-suffix', help = 'Output BAM region file", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Run a set of experiments\")\n\tparser.add_argument('-f', '--file', required=True, help='File containing the data to be used for the experiment')\n\tparser.add_argument('-n', '--name', required=True, help='Name of the experiment')\n\tparser.add_argument('-p', '--plot', action='store_true', help='Whether to include plotting code')\n\tparser.add_argument('-t', '--tasks', nargs='+', help='The tasks to be run')\n\tparser.add_argument('-s', '--sequences', nargs='+', help='The sequences to be run')\n\tparser.add_argument('-c', '--combine', nargs='+', help='The combination of the tasks to be run')\n\tparser.add_argument('-b', '--baseline', nargs='+', help='The baseline tasks to be run')\n\tparser.add_argument('-x', '--xtasks', nargs='+', help='The tasks to be run with xtasks')\n\tparser.add_argument('-y', '--ytasks', nargs='+',", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"A tool for managing the database of a local library\")\n\tparser.add_argument(\"-a\", \"--add\", action=\"store_true\", help=\"Add a book to the library\")\n\tparser.add_argument(\"-d\", \"--delete\", action=\"store_true\", help=\"Delete a book from the library\")\n\tparser.add_argument(\"-l\", \"--list\", action=\"store_true\", help=\"List all books in the library\")\n\tparser.add_argument(\"-r\", \"--remove\", action=\"store_true\", help=\"Remove a book from the library\")\n\tparser.add_argument(\"-u\", \"--update\", action=\"store_true\", help=\"Update a book in the library\")\n\tparser.add_argument(\"-s\", \"--search\", action=\"store_true\", help=\"Search for a book in the library\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=\"1.0.0\")\n\targs = parser.parse_args(arguments)\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tdef to_string(flags):\n\t\treturn ' '.join(flags)\n\t\n\treturn to_string(parser.parse_args().__flags__())", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\treturn \" \".join(parser.parse_args().__dict__.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._option_string_actions:\n\t\tflags.append(flag.dest)\n\treturn \" \".join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targ_list = []\n\tfor flag in parser._option_string_actions:\n\t\targ_list.append(flag.dest)\n\treturn ' '.join(arg_list)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tparser.print_usage = lambda x: None\n\tparser.print_usage = lambda x: None\n\treturn \" \".join(parser.parse_args().__dict__.keys())\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.flags:\n\t\tflags += flag.name + \" \"\n\treturn flags\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest == 'help':\n\t\t\tcontinue\n\t\tif (action.option_strings or [])[0].startswith('--'):\n\t\t\tflags.append(action.dest)\n\t\telse:\n\t\t\tflags.append(action.dest + '=')\n\n\treturn ' '.join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._StoreAction):\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(action.dest)\n\t\t\tflags.extend(action.option_strings)\n\treturn \" \".join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest in ('.', '+'):\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tdef to_string(flags):\n\t\treturn ' '.join(flags)\n\t\n\treturn to_string(parser.parse_args().__flags__())", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\treturn \" \".join(parser.parse_args().__dict__.keys())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._option_string_actions:\n\t\tflags.append(flag.dest)\n\treturn \" \".join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targ_list = []\n\tfor flag in parser._option_string_actions:\n\t\targ_list.append(flag.dest)\n\treturn ' '.join(arg_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tparser.print_usage = lambda x: None\n\tparser.print_usage = lambda x: None\n\treturn \" \".join(parser.parse_args().__dict__.keys())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.flags:\n\t\tflags += flag.name + \" \"\n\treturn flags\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest == 'help':\n\t\t\tcontinue\n\t\tif (action.option_strings or [])[0].startswith('--'):\n\t\t\tflags.append(action.dest)\n\t\telse:\n\t\t\tflags.append(action.dest + '=')\n\n\treturn ' '.join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._StoreAction):\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(action.dest)\n\t\t\tflags.extend(action.option_strings)\n\treturn \" \".join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest in ('.', '+'):\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparameters = {}\n\tcurrent_key = None\n\tfor unparsed_parameter in unparsed_arguments:\n\t\tif unparsed_parameter.startswith('--'):\n\t\t\tcurrent_key = unparsed_parameter.replace('--', '')\n\t\t\tparameters[current_key] = []\n\t\telse:\n\t\t\tparameters[current_key].append(unparsed_parameter)\n\treturn parameters\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tunparsed_arguments = list(unparsed_arguments)\n\tparameters = {}\n\t\n\t# Default parameters\n\tparameters['config_file'] = 'config.py'\n\tparameters['output_dir'] = 'output'\n\t\n\t# Parse default parameters\n\tdefaults = {}\n\tdefaults['debug_mode'] = False\n\tdefaults['max_threads'] = 4\n\tdefaults['print_debug'] = False\n\t\n\t# Parse parameters\n\tfor argument in unparsed_arguments:\n\t\tif argument.startswith('-'):\n\t\t\tkey = argument[1:]\n\t\t\tif key not in defaults:\n\t\t\t\traise Exception(\"Unknown parameter: {}\".format(key))\n\t\t\tvalue = arguments[key]\n\t\t\tparameters[key] = value\n\t\telse:\n\t\t\traise Exception(\"Unknown parameter: {}\".format(argument))\n\t\n\treturn parameters\n\nif __name__ == '__main__':\n\tprint(parse_arguments('-c', 'config.py'))\n\tprint(parse_arguments('-c', 'config.py', '-o', 'output'))\n\tprint(parse_arguments('-c', 'config.py', '-o', 'output', '-d'))", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\ttry:\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"-p\", \"--path\", help=\"path to directory containing files\", required=True)\n\t\tparser.add_argument(\"-o\", \"--output\", help=\"path to output file\", required=True)\n\t\tparser.add_argument(\"-g\", \"--glob\", help=\"glob pattern to search for files\", required=True)\n\t\tparser.add_argument(\"-r\", \"--recursive\", help=\"search for files recursively\", action=\"store_true\")\n\t\tparser.add_argument(\"-l\", \"--lines\", help=\"number of lines to parse\", type=int, default=10)\n\t\tparser.add_argument(\"-v\", \"--verbose\", help=\"verbose output\", action=\"store_true\")\n\t\tparser.add_argument(\"-f\", \"--file\", help=\"name of file to create\", required=True)\n\n\t\targs = parser.parse_args(unparsed_arguments)\n\n\t\treturn {\n\t\t\t\"path\": args.path,\n\t\t\t\"output\": args.output,\n\t\t\t\"glob\": args.glob,\n\t\t\t\"recursive\": args.recursive,\n\t\t\t\"lines\": args.lines,", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\t\n\targuments['version'] = None\n\targuments['input_file'] = None\n\targuments['output_file'] = None\n\targuments['output_file_format'] = None\n\targuments['output_file_format_options'] = None\n\targuments['output_file_format_options_file'] = None\n\targuments['output_file_format_options_file_file'] = None\n\targuments['output_file_format_options_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file_file_file_file'] = None\n\t\n\targuments['language_options'] = None\n\targuments['language_options_file'] = None\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-f\", \"--file\", help=\"File to be parsed\")\n\tparser.add_argument(\"-p\", \"--pattern\", help=\"Pattern to be parsed\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file\")\n\tparser.add_argument(\"-l\", \"--limit\", help=\"Limit to the number of lines\")\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"Verbose mode\", action=\"store_true\")\n\tparser.add_argument(\"-n\", \"--nested\", help=\"Nested mode\", action=\"store_true\")\n\tparser.add_argument(\"-c\", \"--count\", help=\"Count lines\", action=\"store_true\")\n\tparser.add_argument(\"-m\", \"--match\", help=\"Match lines\", action=\"store_true\")\n\tparser.add_argument(\"-t\", \"--trim\", help=\"Trim lines\", action=\"store_true\")\n\tparser.add_argument(\"-k\", \"--keep\", help=\"Keep lines\", action=\"store_true\")\n\tparser.add_argument(\"-e\", \"--esc\", help=\"Escape characters\", action=\"store_", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparsed_arguments = {}\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif '=' in unparsed_argument:\n\t\t\tkey, value = unparsed_argument.split('=')\n\t\t\tparsed_arguments[key] = value\n\t\telse:\n\t\t\tparsed_arguments[unparsed_argument] = True\n\treturn parsed_arguments\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Gets arguments for the experiment')\n\tparser.add_argument('--epochs', type=int, default=100, help='number of epochs to train')\n\tparser.add_argument('--batch_size', type=int, default=128, help='batch size')\n\tparser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n\tparser.add_argument('--load_pretrained', type=bool, default=False, help='load pretrained model')\n\tparser.add_argument('--model_name', type=str, default='resnet18', help='model name')\n\tparser.add_argument('--load_weights', type=str, default=None, help='load weights file')\n\tparser.add_argument('--save_weights', type=str, default='weights.pth', help='save weights file')\n\tparser.add_argument('--dataset', type=str, default='cifar10', help='dataset to use')\n\tparser.add_argument('--dataset_path', type=str, default='../data', help='path to dataset')", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\tkeys = ['-i', '-o', '-f', '-g', '-l', '-s', '-t', '-p', '-n', '-w', '-c', '-x', '-k', '-d', '-r', '-a', '-v', '-h', '-b', '-e', '-o']\n\n\tfor argument in unparsed_arguments:\n\t\tindex = 0\n\t\tfor key in keys:\n\t\t\tif argument.startswith(key):\n\t\t\t\targuments[key] = argument[1:].strip()\n\t\t\t\tbreak\n\t\t\tindex += 1\n\n\treturn arguments\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targs = {}\n\targs_list = unparsed_arguments[0].split(' ')\n\tfor i in range(0, len(args_list), 2):\n\t\targs[args_list[i]] = args_list[i+1]\n\treturn args\n\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Description of what the parser does.\")\n\tparser.add_argument(\"-f\", \"--file\", help=\"Location of file\", default=\"\")\n\tparser.add_argument(\"-n\", \"--number\", help=\"Number of lines to read\", default=0)\n\t\n\targs = parser.parse_args(*unparsed_arguments)\n\n\treturn args"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparameters = {}\n\tcurrent_key = None\n\tfor unparsed_parameter in unparsed_arguments:\n\t\tif unparsed_parameter.startswith('--'):\n\t\t\tcurrent_key = unparsed_parameter.replace('--', '')\n\t\t\tparameters[current_key] = []\n\t\telse:\n\t\t\tparameters[current_key].append(unparsed_parameter)\n\treturn parameters\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tunparsed_arguments = list(unparsed_arguments)\n\tparameters = {}\n\t\n\t# Default parameters\n\tparameters['config_file'] = 'config.py'\n\tparameters['output_dir'] = 'output'\n\t\n\t# Parse default parameters\n\tdefaults = {}\n\tdefaults['debug_mode'] = False\n\tdefaults['max_threads'] = 4\n\tdefaults['print_debug'] = False\n\t\n\t# Parse parameters\n\tfor argument in unparsed_arguments:\n\t\tif argument.startswith('-'):\n\t\t\tkey = argument[1:]\n\t\t\tif key not in defaults:\n\t\t\t\traise Exception(\"Unknown parameter: {}\".format(key))\n\t\t\tvalue = arguments[key]\n\t\t\tparameters[key] = value\n\t\telse:\n\t\t\traise Exception(\"Unknown parameter: {}\".format(argument))\n\t\n\treturn parameters\n\nif __name__ == '__main__':\n\tprint(parse_arguments('-c', 'config.py'))\n\tprint(parse_arguments('-c', 'config.py', '-o', 'output'))\n\tprint(parse_arguments('-c', 'config.py', '-o', 'output', '-d'))", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\ttry:\n\t\tparser = argparse.ArgumentParser()\n\t\tparser.add_argument(\"-p\", \"--path\", help=\"path to directory containing files\", required=True)\n\t\tparser.add_argument(\"-o\", \"--output\", help=\"path to output file\", required=True)\n\t\tparser.add_argument(\"-g\", \"--glob\", help=\"glob pattern to search for files\", required=True)\n\t\tparser.add_argument(\"-r\", \"--recursive\", help=\"search for files recursively\", action=\"store_true\")\n\t\tparser.add_argument(\"-l\", \"--lines\", help=\"number of lines to parse\", type=int, default=10)\n\t\tparser.add_argument(\"-v\", \"--verbose\", help=\"verbose output\", action=\"store_true\")\n\t\tparser.add_argument(\"-f\", \"--file\", help=\"name of file to create\", required=True)\n\n\t\targs = parser.parse_args(unparsed_arguments)\n\n\t\treturn {\n\t\t\t\"path\": args.path,\n\t\t\t\"output\": args.output,\n\t\t\t\"glob\": args.glob,\n\t\t\t\"recursive\": args.recursive,\n\t\t\t\"lines\": args.lines,", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\t\n\targuments['version'] = None\n\targuments['input_file'] = None\n\targuments['output_file'] = None\n\targuments['output_file_format'] = None\n\targuments['output_file_format_options'] = None\n\targuments['output_file_format_options_file'] = None\n\targuments['output_file_format_options_file_file'] = None\n\targuments['output_file_format_options_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file_file_file'] = None\n\targuments['output_file_format_options_file_file_file_file_file_file_file_file_file'] = None\n\t\n\targuments['language_options'] = None\n\targuments['language_options_file'] = None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-f\", \"--file\", help=\"File to be parsed\")\n\tparser.add_argument(\"-p\", \"--pattern\", help=\"Pattern to be parsed\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"Output file\")\n\tparser.add_argument(\"-l\", \"--limit\", help=\"Limit to the number of lines\")\n\tparser.add_argument(\"-v\", \"--verbose\", help=\"Verbose mode\", action=\"store_true\")\n\tparser.add_argument(\"-n\", \"--nested\", help=\"Nested mode\", action=\"store_true\")\n\tparser.add_argument(\"-c\", \"--count\", help=\"Count lines\", action=\"store_true\")\n\tparser.add_argument(\"-m\", \"--match\", help=\"Match lines\", action=\"store_true\")\n\tparser.add_argument(\"-t\", \"--trim\", help=\"Trim lines\", action=\"store_true\")\n\tparser.add_argument(\"-k\", \"--keep\", help=\"Keep lines\", action=\"store_true\")\n\tparser.add_argument(\"-e\", \"--esc\", help=\"Escape characters\", action=\"store_", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparsed_arguments = {}\n\tfor unparsed_argument in unparsed_arguments:\n\t\tif '=' in unparsed_argument:\n\t\t\tkey, value = unparsed_argument.split('=')\n\t\t\tparsed_arguments[key] = value\n\t\telse:\n\t\t\tparsed_arguments[unparsed_argument] = True\n\treturn parsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Gets arguments for the experiment')\n\tparser.add_argument('--epochs', type=int, default=100, help='number of epochs to train')\n\tparser.add_argument('--batch_size', type=int, default=128, help='batch size')\n\tparser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n\tparser.add_argument('--load_pretrained', type=bool, default=False, help='load pretrained model')\n\tparser.add_argument('--model_name', type=str, default='resnet18', help='model name')\n\tparser.add_argument('--load_weights', type=str, default=None, help='load weights file')\n\tparser.add_argument('--save_weights', type=str, default='weights.pth', help='save weights file')\n\tparser.add_argument('--dataset', type=str, default='cifar10', help='dataset to use')\n\tparser.add_argument('--dataset_path', type=str, default='../data', help='path to dataset')", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targuments = {}\n\tkeys = ['-i', '-o', '-f', '-g', '-l', '-s', '-t', '-p', '-n', '-w', '-c', '-x', '-k', '-d', '-r', '-a', '-v', '-h', '-b', '-e', '-o']\n\n\tfor argument in unparsed_arguments:\n\t\tindex = 0\n\t\tfor key in keys:\n\t\t\tif argument.startswith(key):\n\t\t\t\targuments[key] = argument[1:].strip()\n\t\t\t\tbreak\n\t\t\tindex += 1\n\n\treturn arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\targs = {}\n\targs_list = unparsed_arguments[0].split(' ')\n\tfor i in range(0, len(args_list), 2):\n\t\targs[args_list[i]] = args_list[i+1]\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tParses parameters and returns them as dict maps\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Description of what the parser does.\")\n\tparser.add_argument(\"-f\", \"--file\", help=\"Location of file\", default=\"\")\n\tparser.add_argument(\"-n\", \"--number\", help=\"Number of lines to read\", default=0)\n\t\n\targs = parser.parse_args(*unparsed_arguments)\n\n\treturn args", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tnamespace_dict = {}\n\tremaining_arguments = []\n\tfor arg in unparsed_arguments:\n\t\tfor name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tnamespace = subparser.parse_args(arg)\n\t\t\texcept IOError as e:\n\t\t\t\tprint(f\"Unable to parse subparser '{name}': {e}\")\n\t\t\telse:\n\t\t\t\tnamespace_dict[name] = namespace\n\t\t\t\tbreak\n\t\telse:\n\t\t\tremaining_arguments.append(arg)\n\treturn namespace_dict, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = set(subparsers.keys())\n\tremaining_arguments = []\n\tparsed_arguments = {}\n\n\tfor action in subparsers:\n\t\tsubparser = subparsers[action]\n\t\tsubparser_name = subparser.prog\n\t\tsubparser_name_parsed_args = subparser.parse_args(unparsed_arguments)\n\t\tparsed_arguments[subparser_name] = subparser_name_parsed_args\n\n\t\tfor arg in remaining_arguments:\n\t\t\tif arg not in subparser_names:\n\t\t\t\tsubparser.error(f\"Unknown argument '{arg}' to '{subparser_name}'\")\n\n\t\tif subparser_name_parsed_args.help:\n\t\t\tsubparser.print_help()\n\t\t\tsys.exit(0)\n\n\t\tremaining_arguments.remove(subparser_name)\n\n\treturn (parsed_arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = unparsed_arguments[1:]\n\twhile unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tnamespace, unparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\tif namespace:\n\t\t\t\tparsed_arguments[subparser_name] = namespace\n\treturn (parsed_arguments, unparsed_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = [\n\t\tname for name, subparser in subparsers.items()\n\t\tif hasattr(subparser, \"parse_args\")\n\t]\n\tif not subparser_names:\n\t\t# No subparsers are registered\n\t\treturn ({}, unparsed_arguments)\n\n\tif len(unparsed_arguments) < 2:\n\t\t# No subparser specified, fall back to the first one.\n\t\treturn ({}, unparsed_arguments)\n\n\tsubparser = subparsers[unparsed_arguments[1]]\n\ttry:\n\t\tnamespace = subparser.parse_args(unparsed_arguments[2:])\n\texcept SystemExit:\n\t\t# Exit code 2 indicates that the subparser wants to handle the\n\t\t# arguments, which they didn't provide.\n\t\treturn ({}, unparsed_arguments)\n\telse:\n\t\treturn ({subparser.name: namespace}, [])\n\n#pypi/__init__.py\nfrom . import (\n\tcommand,\n\tconfig,\n\tdatabase,\n\tdownload,\n\trepository,\n\tsearch,\n\tupload,\n\tversion,\n)\n\n__all__ = [\n\t\"command\",\n\t\"config\",\n\t\"database\",\n\t\"download\",\n\t\"repository\",\n\t\"search", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor subparser in subparsers.values():\n\t\tparsed_arguments[subparser.name] = subparser.parse_args(unparsed_arguments)\n\t\tunparsed_arguments = subparser.parse_known_args(unparsed_arguments)[0].args\n\n\tremaining_arguments = unparsed_arguments\n\n\treturn parsed_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = []\n\tremaining_arguments = []\n\n\tfor action in subparsers:\n\t\tnamespace = subparsers[action].parse_args(unparsed_arguments)\n\t\tparsed_arguments.append(namespace)\n\n\t\tif namespace:\n\t\t\tunparsed_arguments = []\n\t\telif unparsed_arguments:\n\t\t\tunparsed_arguments.pop(0)\n\n\treturn (parsed_arguments, unparsed_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_output = []\n\tremaining_arguments = []\n\tsubparsers = subparsers.copy()\n\tfor unparsed_argument in unparsed_arguments:\n\t\tremaining_arguments.append(unparsed_argument)\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser_output = subparser.parse_args(remaining_arguments)\n\t\t\t\tparser_output.append((subparser_name, subparser_output))\n\t\t\t\tremaining_arguments = []\n\t\t\t\tbreak\n\t\t\texcept SystemExit as e:\n\t\t\t\tpass\n\tif parser_output == []:\n\t\traise SystemExit(\"Invalid arguments\")\n\treturn tuple(parser_output)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\tsubparser_arguments_remainder = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tsubparser_arguments_remainder.extend(subparser_arguments[subparser_name].__dict__.items())\n\t\tunparsed_arguments = [arg for arg in unparsed_arguments if arg not in subparser_arguments[subparser_name].__dict__.items()]\n\n\treturn subparser_arguments, unparsed_arguments\n\n\nif __name__ == '__main__':\n\timport argparse\n\timport sys\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--repository', type=str, help='Repository path')\n\tparser.add_argument('--remote', type=str, help='Remote repository')\n\tparser.add_argument('--branch', type=str, help='Branch to check out')\n\tparser.add_argument('--commit', type=str, help='Commit to check out')\n\tparser.add_argument('--path', type=str, help='Path to check out')\n\tparser.add_argument('", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tparser_options = {}\n\tfor subparser_name, subparser in subparsers.iteritems():\n\t\toptions = subparser.parse_args(unparsed_arguments, parser_options)\n\t\tparsed_arguments[subparser_name] = options\n\t\tremaining_arguments.extend(unparsed_arguments[len(options.__dict__):])\n\t\tunparsed_arguments = remaining_arguments\n\treturn (parsed_arguments, remaining_arguments)", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = unparsed_arguments\n\tall_subparsers = {}\n\tfor subparser in subparsers.values():\n\t\tparsed, remaining = parse_subparser_arguments(remaining_arguments, subparser)\n\t\tparsed_arguments.update(parsed)\n\t\tall_subparsers[subparser.name] = subparser\n\t\tremaining_arguments = remaining\n\treturn parsed_arguments, remaining_arguments, all_subparsers\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tnamespace_dict = {}\n\tremaining_arguments = []\n\tfor arg in unparsed_arguments:\n\t\tfor name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tnamespace = subparser.parse_args(arg)\n\t\t\texcept IOError as e:\n\t\t\t\tprint(f\"Unable to parse subparser '{name}': {e}\")\n\t\t\telse:\n\t\t\t\tnamespace_dict[name] = namespace\n\t\t\t\tbreak\n\t\telse:\n\t\t\tremaining_arguments.append(arg)\n\treturn namespace_dict, remaining_arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = set(subparsers.keys())\n\tremaining_arguments = []\n\tparsed_arguments = {}\n\n\tfor action in subparsers:\n\t\tsubparser = subparsers[action]\n\t\tsubparser_name = subparser.prog\n\t\tsubparser_name_parsed_args = subparser.parse_args(unparsed_arguments)\n\t\tparsed_arguments[subparser_name] = subparser_name_parsed_args\n\n\t\tfor arg in remaining_arguments:\n\t\t\tif arg not in subparser_names:\n\t\t\t\tsubparser.error(f\"Unknown argument '{arg}' to '{subparser_name}'\")\n\n\t\tif subparser_name_parsed_args.help:\n\t\t\tsubparser.print_help()\n\t\t\tsys.exit(0)\n\n\t\tremaining_arguments.remove(subparser_name)\n\n\treturn (parsed_arguments, remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tunparsed_arguments = unparsed_arguments[1:]\n\twhile unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tnamespace, unparsed_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\t\tif namespace:\n\t\t\t\tparsed_arguments[subparser_name] = namespace\n\treturn (parsed_arguments, unparsed_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = [\n\t\tname for name, subparser in subparsers.items()\n\t\tif hasattr(subparser, \"parse_args\")\n\t]\n\tif not subparser_names:\n\t\t# No subparsers are registered\n\t\treturn ({}, unparsed_arguments)\n\n\tif len(unparsed_arguments) < 2:\n\t\t# No subparser specified, fall back to the first one.\n\t\treturn ({}, unparsed_arguments)\n\n\tsubparser = subparsers[unparsed_arguments[1]]\n\ttry:\n\t\tnamespace = subparser.parse_args(unparsed_arguments[2:])\n\texcept SystemExit:\n\t\t# Exit code 2 indicates that the subparser wants to handle the\n\t\t# arguments, which they didn't provide.\n\t\treturn ({}, unparsed_arguments)\n\telse:\n\t\treturn ({subparser.name: namespace}, [])\n\n#pypi/__init__.py\nfrom . import (\n\tcommand,\n\tconfig,\n\tdatabase,\n\tdownload,\n\trepository,\n\tsearch,\n\tupload,\n\tversion,\n)\n\n__all__ = [\n\t\"command\",\n\t\"config\",\n\t\"database\",\n\t\"download\",\n\t\"repository\",\n\t\"search", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor subparser in subparsers.values():\n\t\tparsed_arguments[subparser.name] = subparser.parse_args(unparsed_arguments)\n\t\tunparsed_arguments = subparser.parse_known_args(unparsed_arguments)[0].args\n\n\tremaining_arguments = unparsed_arguments\n\n\treturn parsed_arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = []\n\tremaining_arguments = []\n\n\tfor action in subparsers:\n\t\tnamespace = subparsers[action].parse_args(unparsed_arguments)\n\t\tparsed_arguments.append(namespace)\n\n\t\tif namespace:\n\t\t\tunparsed_arguments = []\n\t\telif unparsed_arguments:\n\t\t\tunparsed_arguments.pop(0)\n\n\treturn (parsed_arguments, unparsed_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_output = []\n\tremaining_arguments = []\n\tsubparsers = subparsers.copy()\n\tfor unparsed_argument in unparsed_arguments:\n\t\tremaining_arguments.append(unparsed_argument)\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser_output = subparser.parse_args(remaining_arguments)\n\t\t\t\tparser_output.append((subparser_name, subparser_output))\n\t\t\t\tremaining_arguments = []\n\t\t\t\tbreak\n\t\t\texcept SystemExit as e:\n\t\t\t\tpass\n\tif parser_output == []:\n\t\traise SystemExit(\"Invalid arguments\")\n\treturn tuple(parser_output)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\tsubparser_arguments_remainder = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tsubparser_arguments_remainder.extend(subparser_arguments[subparser_name].__dict__.items())\n\t\tunparsed_arguments = [arg for arg in unparsed_arguments if arg not in subparser_arguments[subparser_name].__dict__.items()]\n\n\treturn subparser_arguments, unparsed_arguments\n\n\nif __name__ == '__main__':\n\timport argparse\n\timport sys\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--repository', type=str, help='Repository path')\n\tparser.add_argument('--remote', type=str, help='Remote repository')\n\tparser.add_argument('--branch', type=str, help='Branch to check out')\n\tparser.add_argument('--commit', type=str, help='Commit to check out')\n\tparser.add_argument('--path', type=str, help='Path to check out')\n\tparser.add_argument('", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tparser_options = {}\n\tfor subparser_name, subparser in subparsers.iteritems():\n\t\toptions = subparser.parse_args(unparsed_arguments, parser_options)\n\t\tparsed_arguments[subparser_name] = options\n\t\tremaining_arguments.extend(unparsed_arguments[len(options.__dict__):])\n\t\tunparsed_arguments = remaining_arguments\n\treturn (parsed_arguments, remaining_arguments)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = unparsed_arguments\n\tall_subparsers = {}\n\tfor subparser in subparsers.values():\n\t\tparsed, remaining = parse_subparser_arguments(remaining_arguments, subparser)\n\t\tparsed_arguments.update(parsed)\n\t\tall_subparsers[subparser.name] = subparser\n\t\tremaining_arguments = remaining\n\treturn parsed_arguments, remaining_arguments, all_subparsers\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='This is a description')\n\tparser.add_argument('-v', '--verbose', action='store_true', help='Verbose')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparse_subparser = subparsers.add_parser('parse', help='Parse a file')\n\tparse_subparser.add_argument('-f', '--file', help='Input file', required=True)\n\n\tsearch_subparser = subparsers.add_parser('search', help='Search a file')\n\tsearch_subparser.add_argument('-f', '--file', help='Input file', required=True)\n\tsearch_subparser.add_argument('-m', '--method', choices=['exact', 'prefix', 'suffix', 'regex'], default='exact', help='Search method')\n\tsearch_subparser.add_argument('-i', '--input', help='Input file', required=True)\n\tsearch_subparser.add_argument('-o', '--output', help='Output file', required=True)\n\tsearch_subparser.add_argument('-s', '--search', help='Search string", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Convert between units.')\n\tparser.add_argument('--version', action='version', version='%(prog)s version 1.0.0')\n\tsubparsers = parser.add_subparsers(help='commands', dest='command')\n\tconversion_parser = subparsers.add_parser('convert')\n\tconversion_parser.add_argument('--from', type=str, help='The unit to convert from.')\n\tconversion_parser.add_argument('--to', type=str, help='The unit to convert to.')\n\tconversion_parser.add_argument('--value', type=float, help='The value to convert.')\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-d', '--debug', action='store_true',\n\t\t\t\t\t\thelp='show debugging information')\n\treturn parser, parser.add_subparsers()\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"Create a new project.\")\n\tsubparsers = parser.add_subparsers(dest = 'subparser_name')\n\n\tcreate_parser = subparsers.add_parser('create', help = 'Create a new project.')\n\tcreate_parser.add_argument('project_name', type = str, help = 'The name of the project.')\n\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest='subparser')\n\t\n\tparser_train = subparsers.add_parser('train')\n\tparser_train.add_argument('--model', type=str, default='linear', required=False, help='Model to train.')\n\tparser_train.add_argument('--test-only', action='store_true', required=False, help='Only train the model and do not test.')\n\t\n\tparser_test = subparsers.add_parser('test')\n\tparser_test.add_argument('--model', type=str, default='linear', required=False, help='Model to test.')\n\tparser_test.add_argument('--train-only', action='store_true', required=False, help='Only test the model and do not train.')\n\t\n\targs = parser.parse_args()\n\t\n\treturn parser, subparsers, args\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A command line tool for converting files to and from JSON.\"\n\t)\n\n\tparser.add_argument(\n\t\t\"action\",\n\t\thelp=\"The action to take on the input file.\",\n\t\tchoices=(\"json_to_csv\", \"csv_to_json\")\n\t)\n\n\tsubparsers = parser.add_subparsers(dest=\"input_subcommand\")\n\n\tsubparsers.add_parser(\"json\", help=\"Convert a JSON file to a CSV file.\")\n\tsubparsers.add_parser(\"csv\", help=\"Convert a CSV file to a JSON file.\")\n\tsubparsers.add_parser(\"stdin\", help=\"Convert a JSON file from standard input to CSV.\")\n\n\treturn parser, subparsers\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A simple utility for running a command in a 'subshell'.\",\n\t\tprog=\"subshell\"\n\t)\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\treturn parser, subparsers\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(\n\t\tprog=\"py-gpg\",\n\t\tdescription=\"A command line interface to the gpg-agent.\",\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdest=\"subcommand\",\n\t)\n\treturn parser, subparsers\n", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse the given input file.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparser_build = subparsers.add_parser('build', help='Build the given file.')\n\tparser_build.add_argument('file', metavar='FILE', type=argparse.FileType('r'), nargs='1', help='the input file')\n\tparser_build.add_argument('-o', '--output', type=argparse.FileType('w'), nargs='1', help='the output file')\n\tparser_build.add_argument('-b', '--base', type=argparse.FileType('r'), nargs='1', help='the base file')\n\n\tparser_extract = subparsers.add_parser('extract', help='Extract the given file.')\n\tparser_extract.add_argument('file', metavar='FILE', type=argparse.FileType('r'), nargs='1', help='the input file')\n\tparser_extract.add_argument('-o', '--output', type=argparse.FileType('w'), nargs='1', help=", "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(prog=\"Cyberpunk2000-Lite\")\n\tsubparsers = parser.add_subparsers(dest=\"subcommand\")\n\tsubparsers.required = True\n\treturn parser, subparsers"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='This is a description')\n\tparser.add_argument('-v', '--verbose', action='store_true', help='Verbose')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparse_subparser = subparsers.add_parser('parse', help='Parse a file')\n\tparse_subparser.add_argument('-f', '--file', help='Input file', required=True)\n\n\tsearch_subparser = subparsers.add_parser('search', help='Search a file')\n\tsearch_subparser.add_argument('-f', '--file', help='Input file', required=True)\n\tsearch_subparser.add_argument('-m', '--method', choices=['exact', 'prefix', 'suffix', 'regex'], default='exact', help='Search method')\n\tsearch_subparser.add_argument('-i', '--input', help='Input file', required=True)\n\tsearch_subparser.add_argument('-o', '--output', help='Output file', required=True)\n\tsearch_subparser.add_argument('-s', '--search', help='Search string", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Convert between units.')\n\tparser.add_argument('--version', action='version', version='%(prog)s version 1.0.0')\n\tsubparsers = parser.add_subparsers(help='commands', dest='command')\n\tconversion_parser = subparsers.add_parser('convert')\n\tconversion_parser.add_argument('--from', type=str, help='The unit to convert from.')\n\tconversion_parser.add_argument('--to', type=str, help='The unit to convert to.')\n\tconversion_parser.add_argument('--value', type=float, help='The value to convert.')\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-d', '--debug', action='store_true',\n\t\t\t\t\t\thelp='show debugging information')\n\treturn parser, parser.add_subparsers()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"Create a new project.\")\n\tsubparsers = parser.add_subparsers(dest = 'subparser_name')\n\n\tcreate_parser = subparsers.add_parser('create', help = 'Create a new project.')\n\tcreate_parser.add_argument('project_name', type = str, help = 'The name of the project.')\n\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest='subparser')\n\t\n\tparser_train = subparsers.add_parser('train')\n\tparser_train.add_argument('--model', type=str, default='linear', required=False, help='Model to train.')\n\tparser_train.add_argument('--test-only', action='store_true', required=False, help='Only train the model and do not test.')\n\t\n\tparser_test = subparsers.add_parser('test')\n\tparser_test.add_argument('--model', type=str, default='linear', required=False, help='Model to test.')\n\tparser_test.add_argument('--train-only', action='store_true', required=False, help='Only test the model and do not train.')\n\t\n\targs = parser.parse_args()\n\t\n\treturn parser, subparsers, args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A command line tool for converting files to and from JSON.\"\n\t)\n\n\tparser.add_argument(\n\t\t\"action\",\n\t\thelp=\"The action to take on the input file.\",\n\t\tchoices=(\"json_to_csv\", \"csv_to_json\")\n\t)\n\n\tsubparsers = parser.add_subparsers(dest=\"input_subcommand\")\n\n\tsubparsers.add_parser(\"json\", help=\"Convert a JSON file to a CSV file.\")\n\tsubparsers.add_parser(\"csv\", help=\"Convert a CSV file to a JSON file.\")\n\tsubparsers.add_parser(\"stdin\", help=\"Convert a JSON file from standard input to CSV.\")\n\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A simple utility for running a command in a 'subshell'.\",\n\t\tprog=\"subshell\"\n\t)\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(\n\t\tprog=\"py-gpg\",\n\t\tdescription=\"A command line interface to the gpg-agent.\",\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdest=\"subcommand\",\n\t)\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse the given input file.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tparser_build = subparsers.add_parser('build', help='Build the given file.')\n\tparser_build.add_argument('file', metavar='FILE', type=argparse.FileType('r'), nargs='1', help='the input file')\n\tparser_build.add_argument('-o', '--output', type=argparse.FileType('w'), nargs='1', help='the output file')\n\tparser_build.add_argument('-b', '--base', type=argparse.FileType('r'), nargs='1', help='the base file')\n\n\tparser_extract = subparsers.add_parser('extract', help='Extract the given file.')\n\tparser_extract.add_argument('file', metavar='FILE', type=argparse.FileType('r'), nargs='1', help='the input file')\n\tparser_extract.add_argument('-o', '--output', type=argparse.FileType('w'), nargs='1', help=", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(prog=\"Cyberpunk2000-Lite\")\n\tsubparsers = parser.add_subparsers(dest=\"subcommand\")\n\tsubparsers.required = True\n\treturn parser, subparsers", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor key in node.keys():\n\t\t\t\tmerged[key] = node[key]\n\t\telse:\n\t\t\tmerged[node] = node\n\treturn merged\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tdef merge_value(node1, node2):\n\t\tif isinstance(node1, MappingNode) and isinstance(node2, MappingNode):\n\t\t\tmerged_node = MappingNode()\n\t\t\tfor key, value in node1.items():\n\t\t\t\tmerged_node[key] = merge_value(value, node2[key])\n\t\t\treturn merged_node\n\t\telif isinstance(node1, MappingNode) and isinstance(node2, MappingNode):\n\t\t\treturn node2\n\t\telif isinstance(node1, MappingNode):\n\t\t\treturn node1\n\t\telif isinstance(node1, MappingNode):\n\t\t\treturn node1\n\t\telse:\n\t\t\treturn node2\n\t\n\tresult = MappingNode()\n\tfor node in nodes:\n\t\tresult = merge_value(result, node)\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key in node.keys():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = node[key]\n\t\t\telse:\n\t\t\t\tresult[key] = node[key]\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tnew_nodes = {}\n\t\n\t# merge keys and values\n\tfor node in nodes:\n\t\tfor key,value in node.items():\n\t\t\tnew_nodes[key] = value\n\t\n\t# return result\n\treturn new_nodes\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes and not isinstance(value, MappingNode):\n\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\treturn MappingNode(merged_nodes)\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, val in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(val, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], val])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = val\n\t\t\telse:\n\t\t\t\tresult[key] = val\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tnon_mapping_nodes = []\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor k, v in node.items():\n\t\t\t\tif k not in result:\n\t\t\t\t\tresult[k] = v\n\t\t\t\telse:\n\t\t\t\t\tif not isinstance(result[k], MappingNode):\n\t\t\t\t\t\tresult[k] = MappingNode(v)\n\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\telse:\n\t\t\tnon_mapping_nodes.append(node)\n\tresult = MappingNode(result)\n\tfor node in non_mapping_nodes:\n\t\tresult.append(node)\n\treturn result\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tmerged_nodes.update(node.value)\n\treturn merged_nodes\n\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\td = {}\n\tfor node in nodes:\n\t\tif node.name in d:\n\t\t\tif isinstance(d[node.name], MappingNode):\n\t\t\t\t# merge two nodes with the same key\n\t\t\t\td[node.name] = d[node.name].deep_merge(node)\n\t\t\t\t# if the new node is a leaf, replace the old one\n\t\t\t\tif d[node.name].is_leaf():\n\t\t\t\t\td[node.name] = node\n\t\t\telse:\n\t\t\t\t# replace the old node with the new one, if it's a leaf\n\t\t\t\td[node.name] = node\n\t\telse:\n\t\t\t# add the new node to the result\n\t\t\td[node.name] = node\n\treturn d.values()\n\n#src/gdax-orderbook-to-sqlite.py\n#!/usr/bin/env python3\nimport sys\nimport argparse\nimport sqlite3\nfrom gdax_orderbook import *\nfrom gdax_orderbook_to_sqlite_helpers import *\n", "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = []\n\tduplicates = {}\n\tunique_keys = []\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor key in node.keys():\n\t\t\t\tif key in duplicates:\n\t\t\t\t\tduplicates[key] = node.values[len(node.values) - 1]\n\t\t\t\telse:\n\t\t\t\t\tunique_keys.append(key)\n\t\t\t\t\tduplicates[key] = node.values[len(node.values) - 1]\n\t\telse:\n\t\t\tif node not in result:\n\t\t\t\tresult.append(node)\n\treturn (result, duplicates, unique_keys)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor key in node.keys():\n\t\t\t\tmerged[key] = node[key]\n\t\telse:\n\t\t\tmerged[node] = node\n\treturn merged\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tdef merge_value(node1, node2):\n\t\tif isinstance(node1, MappingNode) and isinstance(node2, MappingNode):\n\t\t\tmerged_node = MappingNode()\n\t\t\tfor key, value in node1.items():\n\t\t\t\tmerged_node[key] = merge_value(value, node2[key])\n\t\t\treturn merged_node\n\t\telif isinstance(node1, MappingNode) and isinstance(node2, MappingNode):\n\t\t\treturn node2\n\t\telif isinstance(node1, MappingNode):\n\t\t\treturn node1\n\t\telif isinstance(node1, MappingNode):\n\t\t\treturn node1\n\t\telse:\n\t\t\treturn node2\n\t\n\tresult = MappingNode()\n\tfor node in nodes:\n\t\tresult = merge_value(result, node)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key in node.keys():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = node[key]\n\t\t\telse:\n\t\t\t\tresult[key] = node[key]\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\t\n\tnew_nodes = {}\n\t\n\t# merge keys and values\n\tfor node in nodes:\n\t\tfor key,value in node.items():\n\t\t\tnew_nodes[key] = value\n\t\n\t# return result\n\treturn new_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes and not isinstance(value, MappingNode):\n\t\t\t\tmerged_nodes[key] = value\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\treturn MappingNode(merged_nodes)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, val in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(val, MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], val])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = val\n\t\t\telse:\n\t\t\t\tresult[key] = val\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = {}\n\tnon_mapping_nodes = []\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor k, v in node.items():\n\t\t\t\tif k not in result:\n\t\t\t\t\tresult[k] = v\n\t\t\t\telse:\n\t\t\t\t\tif not isinstance(result[k], MappingNode):\n\t\t\t\t\t\tresult[k] = MappingNode(v)\n\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\telse:\n\t\t\tnon_mapping_nodes.append(node)\n\tresult = MappingNode(result)\n\tfor node in non_mapping_nodes:\n\t\tresult.append(node)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tmerged_nodes.update(node.value)\n\treturn merged_nodes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\td = {}\n\tfor node in nodes:\n\t\tif node.name in d:\n\t\t\tif isinstance(d[node.name], MappingNode):\n\t\t\t\t# merge two nodes with the same key\n\t\t\t\td[node.name] = d[node.name].deep_merge(node)\n\t\t\t\t# if the new node is a leaf, replace the old one\n\t\t\t\tif d[node.name].is_leaf():\n\t\t\t\t\td[node.name] = node\n\t\t\telse:\n\t\t\t\t# replace the old node with the new one, if it's a leaf\n\t\t\t\td[node.name] = node\n\t\telse:\n\t\t\t# add the new node to the result\n\t\t\td[node.name] = node\n\treturn d.values()\n\n#src/gdax-orderbook-to-sqlite.py\n#!/usr/bin/env python3\nimport sys\nimport argparse\nimport sqlite3\nfrom gdax_orderbook import *\nfrom gdax_orderbook_to_sqlite_helpers import *\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t\"\"\"\n\tmerge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.\n\t\"\"\"\n\tresult = []\n\tduplicates = {}\n\tunique_keys = []\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor key in node.keys():\n\t\t\t\tif key in duplicates:\n\t\t\t\t\tduplicates[key] = node.values[len(node.values) - 1]\n\t\t\t\telse:\n\t\t\t\t\tunique_keys.append(key)\n\t\t\t\t\tduplicates[key] = node.values[len(node.values) - 1]\n\t\telse:\n\t\t\tif node not in result:\n\t\t\t\tresult.append(node)\n\treturn (result, duplicates, unique_keys)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tfrom argparse import RawTextHelpFormatter\n\n\tparser = argparse.ArgumentParser(description = \"This script will calculate the time taken by each of the methods of the GCD algorithm\",\n\t\tformatter_class = RawTextHelpFormatter)\n\tparser.add_argument(\"-n\", \"--number\", dest = \"number\", type = int, help = \"The number for which the GCD is to be calculated\")\n\tparser.add_argument(\"-f\", \"--file\", dest = \"file\", type = str, help = \"The file from which the numbers are to be read\")\n\n\tif len(arguments) == 0:\n\t\tparser.print_help()\n\t\texit(-1)\n\n\targs = parser.parse_args(arguments)\n\n\treturn args\n\n#gcd.py\nfrom utilities import parse_arguments\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Run a benchmark with the given arguments')\n\tfor argument in arguments:\n\t\tparser.add_argument('-' + argument, '--' + argument)\n\treturn parser.parse_args()\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"main\")\n\tparser.add_argument(\"-i\", \"--input\", dest=\"input_file\", required=True, help=\"Input file\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output_file\", required=False, help=\"Output file\")\n\targs = parser.parse_args(*arguments)\n\treturn args\n\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Sorts data from a text file into a binary file')\n\tparser.add_argument('-i', '--input', help='The text file to sort', required=True)\n\tparser.add_argument('-o', '--output', help='The binary file to sort', required=True)\n\tparser.add_argument('-e', '--encoding', help='The character encoding of the text file', default='utf-8')\n\tparser.add_argument('-f', '--field', help='The index of the field to sort by')\n\tparser.add_argument('-p', '--pattern', help='The pattern to match the field value against')\n\tparser.add_argument('-s', '--sort-type', help='The sorting type to use', choices=['ascending', 'descending'], default='ascending')\n\t\n\targs = parser.parse_args(*arguments)\n\n\treturn args\n\n#main.py\nfrom utils import parse_arguments\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = ArgumentParser(description=\"Create a new C++ project\")\n\tparser.add_argument(\"--project_name\", dest=\"project_name\", type=str, help=\"The name of the new project\")\n\tparser.add_argument(\"--project_path\", dest=\"project_path\", type=str, help=\"The path to create the new project in\")\n\tparser.add_argument(\"--language\", dest=\"language\", type=str, help=\"The programming language of the new project\")\n\tparser.add_argument(\"--language_version\", dest=\"language_version\", type=str, help=\"The version of the programming language of the new project\")\n\tparser.add_argument(\"--project_type\", dest=\"project_type\", type=str, help=\"The type of the new project\")\n\tparser.add_argument(\"--project_type_version\", dest=\"project_type_version\", type=str, help=\"The version of the type of the new project\")\n\tparser.add_argument(\"--project_type_name\", dest=\"project_type_name\", type=str, help=\"The name of the type of the new project\")\n\tparser.add_argument(\"--project_type_description\", dest=\"project_type_description\", type=str, help", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\tparser = ArgumentParser(description='Process command-line arguments')\n\tparser.add_argument('-c', '--config', dest='config_file', help='Path to the config file', metavar='FILE', required=True)\n\tparser.add_argument('-l', '--logfile', dest='log_file', help='Path to the logfile', metavar='FILE', required=True)\n\tparser.add_argument('-d', '--debug', action='store_true', help='Enable debug output')\n\tparser.add_argument('-s', '--skip-logfile', action='store_true', help='Skip creating a logfile, if specified')\n\t\n\targs = parser.parse_args(arguments)\n\t\n\treturn args\n\n#pyhg/__main__.py\n#!/usr/bin/env python\n\nimport getopt\nimport sys\nimport logging\nimport logging.handlers\nimport argparse\n\nfrom pyhg.config import read_config\nfrom pyhg.logfile import create_logfile\nfrom pyhg.parse_arguments import parse_arguments\n\n\"\"\"\nThis is the main script. It is invoked by the user.\n\"\"\"\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description='Download the EMBL-EBI files for the given study ID(s)')\n\tparser.add_argument('-h', '--help', action='help', help='show this help message and exit')\n\tparser.add_argument('study_id', nargs='+', help='The study ID(s) to download')\n\treturn parser.parse_args(*arguments)\n", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Downloads the latest version of the OpenLiteSpeed repository and installs it.\")\n\n\tparser.add_argument(\"-v\", \"--version\",\n\t                    type=str,\n\t                    required=False,\n\t                    default=\"latest\",\n\t                    help=\"The specific version of the OpenLiteSpeed repository to download. Defaults to the latest version. Supported values: '%(choices)s'.\")\n\n\tparser.add_argument(\"-l\", \"--local\",\n\t                    action=\"store_true\",\n\t                    required=False,\n\t                    help=\"If specified, the script will not download the repository and will instead install the local version.\")\n\n\tparser.add_argument(\"-r\", \"--remove\",\n\t                    action=\"store_true\",\n\t                    required=False,\n\t                    help=\"If specified, the script will remove the downloaded repository and all its files.\")\n\n\tparser.add_argument(\"-d\", \"--directory\",\n\t                    type=str,\n\t                    required=False,\n\t                    default=\"/tmp/\",\n\t                    help=\"The directory in which to install the OpenLiteSpeed repository. Defaults to '/tmp/'.\")\n\n\tparser.add_argument(\"-f\", \"--force\",\n\t                   ", "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\t\n\tparser = ArgumentParser()\n\t\n\tfor argument in arguments:\n\t\tparser.add_argument(*argument)\n\t\n\treturn parser.parse_args()\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser()\n\tfor argument in arguments:\n\t\tparser.add_argument(argument)\n\treturn parser.parse_args()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tfrom argparse import RawTextHelpFormatter\n\n\tparser = argparse.ArgumentParser(description = \"This script will calculate the time taken by each of the methods of the GCD algorithm\",\n\t\tformatter_class = RawTextHelpFormatter)\n\tparser.add_argument(\"-n\", \"--number\", dest = \"number\", type = int, help = \"The number for which the GCD is to be calculated\")\n\tparser.add_argument(\"-f\", \"--file\", dest = \"file\", type = str, help = \"The file from which the numbers are to be read\")\n\n\tif len(arguments) == 0:\n\t\tparser.print_help()\n\t\texit(-1)\n\n\targs = parser.parse_args(arguments)\n\n\treturn args\n\n#gcd.py\nfrom utilities import parse_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(description='Run a benchmark with the given arguments')\n\tfor argument in arguments:\n\t\tparser.add_argument('-' + argument, '--' + argument)\n\treturn parser.parse_args()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"main\")\n\tparser.add_argument(\"-i\", \"--input\", dest=\"input_file\", required=True, help=\"Input file\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output_file\", required=False, help=\"Output file\")\n\targs = parser.parse_args(*arguments)\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Sorts data from a text file into a binary file')\n\tparser.add_argument('-i', '--input', help='The text file to sort', required=True)\n\tparser.add_argument('-o', '--output', help='The binary file to sort', required=True)\n\tparser.add_argument('-e', '--encoding', help='The character encoding of the text file', default='utf-8')\n\tparser.add_argument('-f', '--field', help='The index of the field to sort by')\n\tparser.add_argument('-p', '--pattern', help='The pattern to match the field value against')\n\tparser.add_argument('-s', '--sort-type', help='The sorting type to use', choices=['ascending', 'descending'], default='ascending')\n\t\n\targs = parser.parse_args(*arguments)\n\n\treturn args\n\n#main.py\nfrom utils import parse_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = ArgumentParser(description=\"Create a new C++ project\")\n\tparser.add_argument(\"--project_name\", dest=\"project_name\", type=str, help=\"The name of the new project\")\n\tparser.add_argument(\"--project_path\", dest=\"project_path\", type=str, help=\"The path to create the new project in\")\n\tparser.add_argument(\"--language\", dest=\"language\", type=str, help=\"The programming language of the new project\")\n\tparser.add_argument(\"--language_version\", dest=\"language_version\", type=str, help=\"The version of the programming language of the new project\")\n\tparser.add_argument(\"--project_type\", dest=\"project_type\", type=str, help=\"The type of the new project\")\n\tparser.add_argument(\"--project_type_version\", dest=\"project_type_version\", type=str, help=\"The version of the type of the new project\")\n\tparser.add_argument(\"--project_type_name\", dest=\"project_type_name\", type=str, help=\"The name of the type of the new project\")\n\tparser.add_argument(\"--project_type_description\", dest=\"project_type_description\", type=str, help", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\t\n\tparser = ArgumentParser(description='Process command-line arguments')\n\tparser.add_argument('-c', '--config', dest='config_file', help='Path to the config file', metavar='FILE', required=True)\n\tparser.add_argument('-l', '--logfile', dest='log_file', help='Path to the logfile', metavar='FILE', required=True)\n\tparser.add_argument('-d', '--debug', action='store_true', help='Enable debug output')\n\tparser.add_argument('-s', '--skip-logfile', action='store_true', help='Skip creating a logfile, if specified')\n\t\n\targs = parser.parse_args(arguments)\n\t\n\treturn args\n\n#pyhg/__main__.py\n#!/usr/bin/env python\n\nimport getopt\nimport sys\nimport logging\nimport logging.handlers\nimport argparse\n\nfrom pyhg.config import read_config\nfrom pyhg.logfile import create_logfile\nfrom pyhg.parse_arguments import parse_arguments\n\n\"\"\"\nThis is the main script. It is invoked by the user.\n\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description='Download the EMBL-EBI files for the given study ID(s)')\n\tparser.add_argument('-h', '--help', action='help', help='show this help message and exit')\n\tparser.add_argument('study_id', nargs='+', help='The study ID(s) to download')\n\treturn parser.parse_args(*arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Downloads the latest version of the OpenLiteSpeed repository and installs it.\")\n\n\tparser.add_argument(\"-v\", \"--version\",\n\t                    type=str,\n\t                    required=False,\n\t                    default=\"latest\",\n\t                    help=\"The specific version of the OpenLiteSpeed repository to download. Defaults to the latest version. Supported values: '%(choices)s'.\")\n\n\tparser.add_argument(\"-l\", \"--local\",\n\t                    action=\"store_true\",\n\t                    required=False,\n\t                    help=\"If specified, the script will not download the repository and will instead install the local version.\")\n\n\tparser.add_argument(\"-r\", \"--remove\",\n\t                    action=\"store_true\",\n\t                    required=False,\n\t                    help=\"If specified, the script will remove the downloaded repository and all its files.\")\n\n\tparser.add_argument(\"-d\", \"--directory\",\n\t                    type=str,\n\t                    required=False,\n\t                    default=\"/tmp/\",\n\t                    help=\"The directory in which to install the OpenLiteSpeed repository. Defaults to '/tmp/'.\")\n\n\tparser.add_argument(\"-f\", \"--force\",\n\t                   ", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return  them as an ArgumentParser instance\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\t\n\tparser = ArgumentParser()\n\t\n\tfor argument in arguments:\n\t\tparser.add_argument(*argument)\n\t\n\treturn parser.parse_args()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tflags = []\n\t\n\tfor f in parser._actions:\n\t\tif type(f) == argparse.Action:\n\t\t\tflags.append(f.dest)\n\t\telse:\n\t\t\tflags.append(f)\n\t\t\n\treturn \" \".join(flags)\n\t", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targs = []\n\tfor flag in parser.parse_args().__dict__:\n\t\tif not flag.startswith('_'):\n\t\t\targs.append(flag)\n\treturn ' '.join(args)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.parse_args().__dict__:\n\t\tflags += f\" -{flag} \"\n\treturn flags\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser.parse_known_args()[1]:\n\t\tif flag.startswith(\"--\"):\n\t\t\tflags.append(flag)\n\t\telse:\n\t\t\tflags.append(\"--{}\".format(flag))\n\treturn \" \".join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tall_flags = parser.parse_args().__dict__.values()\n\tflags = [flag for flag in all_flags if flag is not None]\n\treturn \" \".join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._actions:\n\t\tif flag.option_strings:\n\t\t\tflags.append(\" \".join(flag.option_strings))\n\t\telse:\n\t\t\tflags.append(flag.dest)\n\treturn \" \".join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tparser_str = \"\"\n\t\n\tparser_args = parser.parse_args()\n\tfor arg in vars(parser_args).keys():\n\t\tparser_str += (\" \" + arg)\n\t\n\treturn parser_str\n\t", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._actions:\n\t\tif flag.dest == 'option':\n\t\t\tflags.append(flag.option_strings[0])\n\treturn ' '.join(flags)", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\treturn \" \".join(parser.parse_args().__dict__.keys())\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._action_groups:\n\t\tfor action in flag.actions:\n\t\t\tif action.dest in parser.defaults:\n\t\t\t\tcontinue\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tflags = []\n\t\n\tfor f in parser._actions:\n\t\tif type(f) == argparse.Action:\n\t\t\tflags.append(f.dest)\n\t\telse:\n\t\t\tflags.append(f)\n\t\t\n\treturn \" \".join(flags)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\targs = []\n\tfor flag in parser.parse_args().__dict__:\n\t\tif not flag.startswith('_'):\n\t\t\targs.append(flag)\n\treturn ' '.join(args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser.parse_args().__dict__:\n\t\tflags += f\" -{flag} \"\n\treturn flags\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser.parse_known_args()[1]:\n\t\tif flag.startswith(\"--\"):\n\t\t\tflags.append(flag)\n\t\telse:\n\t\t\tflags.append(\"--{}\".format(flag))\n\treturn \" \".join(flags)", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tall_flags = parser.parse_args().__dict__.values()\n\tflags = [flag for flag in all_flags if flag is not None]\n\treturn \" \".join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._actions:\n\t\tif flag.option_strings:\n\t\t\tflags.append(\" \".join(flag.option_strings))\n\t\telse:\n\t\t\tflags.append(flag.dest)\n\treturn \" \".join(flags)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\tparser_str = \"\"\n\t\n\tparser_args = parser.parse_args()\n\tfor arg in vars(parser_args).keys():\n\t\tparser_str += (\" \" + arg)\n\t\n\treturn parser_str\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._actions:\n\t\tif flag.dest == 'option':\n\t\t\tflags.append(flag.option_strings[0])\n\treturn ' '.join(flags)", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\t\n\treturn \" \".join(parser.parse_args().__dict__.keys())\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._action_groups:\n\t\tfor action in flag.actions:\n\t\t\tif action.dest in parser.defaults:\n\t\t\t\tcontinue\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport sys\n\timport subprocess\n\timport shlex\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"command\", nargs=argparse.REMAINDER)\n\targs, _ = parser.parse_known_args()\n\n\tcmd = [\"bash-completion\"]\n\tif args.command:\n\t\tcmd += args.command\n\n\tresult = subprocess.run(cmd, stdout=sys.stdout)\n\tos.write(sys.stdout.fileno(), result.stdout)", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport sys\n\n\tparser = argparse.ArgumentParser(description=\"borgmatic\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", description=\"valid subcommands\", help=\"additional help\")\n\n\tparser.add_argument(\"--version\", action=\"version\", version=\"borgmatic version 0.1.0\")\n\n\tinit_parser = subparsers.add_parser(\"init\", help=\"initialize a repository\")\n\tinit_parser.add_argument(\"repo\", help=\"path to the repository directory\")\n\tinit_parser.add_argument(\"--encryption-passphrase\", help=\"passphrase for the encryption key\")\n\tinit_parser.add_argument(\"--compression\", choices=[\"none\", \"lz4\", \"zstd\"], default=\"zstd\", help=\"compression algorithm\")\n\tinit_parser.add_argument(\"--exclude\", action=\"append\", help=\"exclude file or directory\")\n\tinit_parser.add_argument(\"--exclude-from\", help=\"read exclude patterns from this file\")\n\tinit_parser.add_argument(\"--exclude-regexp\",", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport inspect\n\timport argparse\n\n\t# Create the argument parser\n\tparser = argparse.ArgumentParser(prog=\"borgmatic\")\n\tparser.add_argument(\"command\", choices=[\"version\", \"help\", \"init\", \"backup\", \"prune\", \"list\", \"extract\", \"delete\", \"replicate\", \"restore\", \"find\", \"cat\", \"config\", \"create\", \"update\", \"verify\", \"verify-repo\", \"verify-key\", \"verify-checksum\", \"verify-header\", \"verify-all\", \"create-keys\", \"create-passphrase\", \"create-gpg-key\", \"create-gpg-key-from-passphrase\", \"create-gpg-key-from-passphrase-file\", \"create-gpg-key-from-passphrase-file-and-passphrase\", \"create-gpg-key-from-passphrase-file-and-passphrase-file\", \"create-gpg-key-from-passphrase-file-and-passphrase-file-and-gpg-home\", \"create-gpg-key-from-passphrase-file-and-passphrase-file-and-gpg-home-and-passphrase-file\", \"create-gpg-key-from-passphrase-file-and", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport sys\n\timport textwrap\n\t\n\timport borgmatic\n\t\n\tdef _usage(prog):\n\t\tprint(textwrap.dedent(\"\"\"\n\t\t\tUsage:\n\t\t\t\t{prog} COMMAND [ARGUMENTS]\n\t\t\t\t{prog} COMMAND -h | --help\n\t\t\t\"\"\".format(prog=prog)))\n\t\n\tdef _help(prog, command):\n\t\tprint(textwrap.dedent(\"\"\"\n\t\t\tHelp for command '{command}':\n\t\t\t    {command} [OPTIONS]\n\t\t\t    {command} -h | --help\n\t\t\t\"\"\".format(command=command)))\n\t\n\tdef _command_parser(command_name):\n\t\t\"\"\"\n\t\tReturn a parser for a command.\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tprog=command_name,\n\t\t\tdescription=\"Borgmatic {command_name} command.\".format(command_name=command_name),\n\t\t\tadd_help=False)\n\t\tparser.add_argument('-h', '--help', action='help', help='Show this help message and exit.')\n\t\treturn parser\n\t\n\tdef _subcommand_parser(command_name", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport os\n\timport sys\n\timport argparse\n\n\tscript = os.path.basename(sys.argv[0])\n\tparser = argparse.ArgumentParser(description=\"Borgmatic command-line interface\")\n\tparser.add_argument(\n\t\t\"--debug\", action=\"store_true\", default=False, help=\"Enable debug mode\"\n\t)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1.0\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"count\", default=0)\n\n\tsubparsers = parser.add_subparsers(title=\"Subcommands\", dest=\"subcommand\")\n\n\t# borgmatic init\n\tinit_parser = subparsers.add_parser(\n\t\t\"init\",\n\t\thelp=\"Initialize a new repository\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tinit_parser.add_argument(\n\t\t\"--compression\",\n\t\tdefault=None,\n\t\thelp=\"The compression algorithm to use (zstd, lz4, lzop, lz4hc)\",\n\t)\n\tinit_parser.add_argument(\n", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borgmatic.cli import main\n\tfrom borgmatic.cli.completion import generate\n\treturn generate(main.main, main.parser)\n\n", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport borgmatic\n\tparser = argparse.ArgumentParser(\n\t\tdescription=borgmatic.__doc__,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument('-v', '--verbose', action='count', default=0, help='Increase verbosity')\n\tsubparsers = parser.add_subparsers(dest='command')\n\tsubparsers.required = True\n\n\t# borgmatic --help\n\tparser_help = subparsers.add_parser('help')\n\tparser_help.add_argument('command', help='The command to get help for.')\n\n\t# borgmatic --version\n\tparser_version = subparsers.add_parser('version')\n\n\t# borgmatic --config\n\tparser_config = subparsers.add_parser('config')\n\tparser_config.add_argument('--get', '-g', action='store_true', help='Get the current configuration')\n\tparser_config.add_argument('--set', '-s', action='store_true', help='Set the current configuration')\n\tparser_config.add_argument('--unset', '-u', action='store_true', help='", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tparser, _, _, _, _ = __import__('borgmatic.command_line').get_parser()\n\tparser.print_help('borgmatic')", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_PARSER\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBSUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBSUBSUBSUBPARSERS\n\timport os\n\timport re\n\t\n\tdef get_subcommand_dict(subcommand):\n\t\t\"\"\"\n\t\tProduce a dictionary mapping the name of a subcommand to its argument parser.\n\t\t\"\"\"\n\t\tsubcommand_dict = {}\n\t\tfor parser in BORGMATIC_SUBPARSERS[subcommand]:\n\t\t\tsubcommand_dict[parser.name] = parser\n\t\treturn subcommand_dict\n\t\n\tdef get_subsubcommand_dict(subcommand,", "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\tfrom borgmatic.cli import cli\n\tfrom borgmatic.core import main\n\tfrom borgmatic.logger import logger\n\tfrom borgmatic.util import get_parser_from_subparser\n\tfrom borgmatic.util import get_subparser_from_parser\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Borgmatic is a tool for creating backups of your data',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\tdest='command',\n\t\thelp='Choose a subcommand',\n\t)\n\n\tcli(subparsers)\n\n\tsubparser = get_subparser_from_parser(subparsers, 'borgmatic')\n\tborgmatic_parser = get_parser_from_subparser(subparser)\n\n\tborgmatic_parser.print_help()\n\n\nif __name__ == '__main__':\n\tbash_completion()"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport sys\n\timport subprocess\n\timport shlex\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"command\", nargs=argparse.REMAINDER)\n\targs, _ = parser.parse_known_args()\n\n\tcmd = [\"bash-completion\"]\n\tif args.command:\n\t\tcmd += args.command\n\n\tresult = subprocess.run(cmd, stdout=sys.stdout)\n\tos.write(sys.stdout.fileno(), result.stdout)", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport sys\n\n\tparser = argparse.ArgumentParser(description=\"borgmatic\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", description=\"valid subcommands\", help=\"additional help\")\n\n\tparser.add_argument(\"--version\", action=\"version\", version=\"borgmatic version 0.1.0\")\n\n\tinit_parser = subparsers.add_parser(\"init\", help=\"initialize a repository\")\n\tinit_parser.add_argument(\"repo\", help=\"path to the repository directory\")\n\tinit_parser.add_argument(\"--encryption-passphrase\", help=\"passphrase for the encryption key\")\n\tinit_parser.add_argument(\"--compression\", choices=[\"none\", \"lz4\", \"zstd\"], default=\"zstd\", help=\"compression algorithm\")\n\tinit_parser.add_argument(\"--exclude\", action=\"append\", help=\"exclude file or directory\")\n\tinit_parser.add_argument(\"--exclude-from\", help=\"read exclude patterns from this file\")\n\tinit_parser.add_argument(\"--exclude-regexp\",", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport inspect\n\timport argparse\n\n\t# Create the argument parser\n\tparser = argparse.ArgumentParser(prog=\"borgmatic\")\n\tparser.add_argument(\"command\", choices=[\"version\", \"help\", \"init\", \"backup\", \"prune\", \"list\", \"extract\", \"delete\", \"replicate\", \"restore\", \"find\", \"cat\", \"config\", \"create\", \"update\", \"verify\", \"verify-repo\", \"verify-key\", \"verify-checksum\", \"verify-header\", \"verify-all\", \"create-keys\", \"create-passphrase\", \"create-gpg-key\", \"create-gpg-key-from-passphrase\", \"create-gpg-key-from-passphrase-file\", \"create-gpg-key-from-passphrase-file-and-passphrase\", \"create-gpg-key-from-passphrase-file-and-passphrase-file\", \"create-gpg-key-from-passphrase-file-and-passphrase-file-and-gpg-home\", \"create-gpg-key-from-passphrase-file-and-passphrase-file-and-gpg-home-and-passphrase-file\", \"create-gpg-key-from-passphrase-file-and", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport os\n\timport sys\n\timport textwrap\n\t\n\timport borgmatic\n\t\n\tdef _usage(prog):\n\t\tprint(textwrap.dedent(\"\"\"\n\t\t\tUsage:\n\t\t\t\t{prog} COMMAND [ARGUMENTS]\n\t\t\t\t{prog} COMMAND -h | --help\n\t\t\t\"\"\".format(prog=prog)))\n\t\n\tdef _help(prog, command):\n\t\tprint(textwrap.dedent(\"\"\"\n\t\t\tHelp for command '{command}':\n\t\t\t    {command} [OPTIONS]\n\t\t\t    {command} -h | --help\n\t\t\t\"\"\".format(command=command)))\n\t\n\tdef _command_parser(command_name):\n\t\t\"\"\"\n\t\tReturn a parser for a command.\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tprog=command_name,\n\t\t\tdescription=\"Borgmatic {command_name} command.\".format(command_name=command_name),\n\t\t\tadd_help=False)\n\t\tparser.add_argument('-h', '--help', action='help', help='Show this help message and exit.')\n\t\treturn parser\n\t\n\tdef _subcommand_parser(command_name", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport os\n\timport sys\n\timport argparse\n\n\tscript = os.path.basename(sys.argv[0])\n\tparser = argparse.ArgumentParser(description=\"Borgmatic command-line interface\")\n\tparser.add_argument(\n\t\t\"--debug\", action=\"store_true\", default=False, help=\"Enable debug mode\"\n\t)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1.0\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"count\", default=0)\n\n\tsubparsers = parser.add_subparsers(title=\"Subcommands\", dest=\"subcommand\")\n\n\t# borgmatic init\n\tinit_parser = subparsers.add_parser(\n\t\t\"init\",\n\t\thelp=\"Initialize a new repository\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tinit_parser.add_argument(\n\t\t\"--compression\",\n\t\tdefault=None,\n\t\thelp=\"The compression algorithm to use (zstd, lz4, lzop, lz4hc)\",\n\t)\n\tinit_parser.add_argument(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tfrom borgmatic.cli import main\n\tfrom borgmatic.cli.completion import generate\n\treturn generate(main.main, main.parser)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\timport borgmatic\n\tparser = argparse.ArgumentParser(\n\t\tdescription=borgmatic.__doc__,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument('-v', '--verbose', action='count', default=0, help='Increase verbosity')\n\tsubparsers = parser.add_subparsers(dest='command')\n\tsubparsers.required = True\n\n\t# borgmatic --help\n\tparser_help = subparsers.add_parser('help')\n\tparser_help.add_argument('command', help='The command to get help for.')\n\n\t# borgmatic --version\n\tparser_version = subparsers.add_parser('version')\n\n\t# borgmatic --config\n\tparser_config = subparsers.add_parser('config')\n\tparser_config.add_argument('--get', '-g', action='store_true', help='Get the current configuration')\n\tparser_config.add_argument('--set', '-s', action='store_true', help='Set the current configuration')\n\tparser_config.add_argument('--unset', '-u', action='store_true', help='", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\tparser, _, _, _, _ = __import__('borgmatic.command_line').get_parser()\n\tparser.print_help('borgmatic')", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_PARSER\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBSUBSUBPARSERS\n\tfrom borgmatic.borgmatic_parser import BORGMATIC_SUBSUBSUBSUBSUBSUBPARSERS\n\timport os\n\timport re\n\t\n\tdef get_subcommand_dict(subcommand):\n\t\t\"\"\"\n\t\tProduce a dictionary mapping the name of a subcommand to its argument parser.\n\t\t\"\"\"\n\t\tsubcommand_dict = {}\n\t\tfor parser in BORGMATIC_SUBPARSERS[subcommand]:\n\t\t\tsubcommand_dict[parser.name] = parser\n\t\treturn subcommand_dict\n\t\n\tdef get_subsubcommand_dict(subcommand,", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t\"\"\"\n\tProduce the borgmatic command by introspecting borgmatic's command-line argument parsers.\n\t\"\"\"\n\timport argparse\n\tfrom borgmatic.cli import cli\n\tfrom borgmatic.core import main\n\tfrom borgmatic.logger import logger\n\tfrom borgmatic.util import get_parser_from_subparser\n\tfrom borgmatic.util import get_subparser_from_parser\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Borgmatic is a tool for creating backups of your data',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\tdest='command',\n\t\thelp='Choose a subcommand',\n\t)\n\n\tcli(subparsers)\n\n\tsubparser = get_subparser_from_parser(subparsers, 'borgmatic')\n\tborgmatic_parser = get_parser_from_subparser(subparser)\n\n\tborgmatic_parser.print_help()\n\n\nif __name__ == '__main__':\n\tbash_completion()", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"Build a virtual machine from a CFG\")\n\tparser.add_argument(\n\t\t\"-c\", \"--cfg\",\n\t\tdest = \"cfg\",\n\t\ttype = argparse.FileType(\"r\"),\n\t\thelp = \"Path to CFG file\",\n\t\trequired = True\n\t)\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\",\n\t\tdest = \"verbose\",\n\t\taction = \"store_true\",\n\t\thelp = \"Print additional information to standard output\",\n\t\tdefault = False\n\t)\n\tparser.add_argument(\n\t\t\"-o\", \"--output\",\n\t\tdest = \"output\",\n\t\ttype = argparse.FileType(\"w\"),\n\t\thelp = \"Path to output file\",\n\t\trequired = False\n\t)\n\n\tsubparsers = parser.add_subparsers(dest = \"command\", help = \"Command to run\")\n\n\tbuild_parser = subparsers.add_parser(\"build\", help = \"Build a virtual machine\")\n\tbuild_parser.add_argument(\n\t\t\"--no-save\",\n\t\tdest = \"no_save\",\n\t\taction = \"store_true\",\n\t\thelp = \"Do not", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"A simple command-line tool\")\n\tparser.add_argument(\"--version\", action = \"version\", version = \"0.1\")\n\n\tsubparsers = parser.add_subparsers(title = \"subcommands\")\n\tsubparsers.required = True\n\n\t# Global parser\n\tglobal_parser = subparsers.add_parser(\"global\", help = \"A global option\")\n\tglobal_parser.add_argument(\"-v\", \"--verbose\", action = \"store_true\", default = False)\n\n\t# Subparser for \"foo\"\n\tfoo_parser = subparsers.add_parser(\"foo\", help = \"A foo option\")\n\tfoo_parser.add_argument(\"-f\", \"--foo\", action = \"store_true\", default = False)\n\n\t# Subparser for \"bar\"\n\tbar_parser = subparsers.add_parser(\"bar\", help = \"A bar option\")\n\tbar_parser.add_argument(\"-b\", \"--bar\", action = \"store_true\", default = False)\n\n\treturn parser.parse_args(*unparsed_arguments)\n\n#examples/src/foo.py", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"python3 -m \" + __package__ + \".main\")\n\tparser.add_argument(\"-h\", \"--help\", action=\"help\")\n\tsubparsers = parser.add_subparsers(help=\"choose a subparser\")\n\tsubparsers.required = True\n\n\tglobal_parser = subparsers.add_parser(\"global\")\n\tglobal_parser.add_argument(\"-v\", \"--verbose\", action=\"count\", default=0)\n\n\tparser.add_argument(\"-o\", \"--outfile\", type=str, metavar=\"NAME\", default=None)\n\tparser.add_argument(\"-s\", \"--source-dir\", type=str, metavar=\"PATH\", default=\".\")\n\tparser.add_argument(\"-t\", \"--target-dir\", type=str, metavar=\"PATH\", default=\".\")\n\n\tglobal_parser.add_argument(\"-d\", \"--debug\", action=\"store_true\")\n\tglobal_parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\")\n\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"Convert a file to a different format\")\n\t# subparsers are used to group commands\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\tparser_convert = subparsers.add_parser(\"convert\", help=\"Convert a file to a different format\")\n\tparser_convert.add_argument(\"input\", help=\"The file to convert\")\n\tparser_convert.add_argument(\"output\", help=\"The file to write to\")\n\tparser_convert.add_argument(\"--source-format\", help=\"The source format of the input file\",\n\t\t\t\t\t\t\t\t\tdest=\"source_format\", default=None)\n\tparser_convert.add_argument(\"--target-format\", help=\"The target format of the output file\",\n\t\t\t\t\t\t\t\t\tdest=\"target_format\", default=None)\n\tparser_convert.add_argument(\"--overwrite\", help=\"Overwrite the output file if it exists\",\n\t\t\t\t\t\t\t\t\taction=\"store_true\")\n\n\tparser_list = subparsers.add_parser(\"list\", help=\"List the available conversions\")\n\n\tparser_version = subparsers.add_parser(\"version\", help", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"count\", help=\"increase output verbosity\")\n\tparser.add_argument(\"-c\", \"--continue\", action=\"store_true\", help=\"continue a partially completed run\")\n\tparser.add_argument(\"-f\", \"--force\", action=\"store_true\", help=\"overwrite existing output files\")\n\tparser.add_argument(\"-t\", \"--threads\", default=1, type=int, help=\"number of threads to use\")\n\tparser.add_argument(\"-d\", \"--dry\", action=\"store_true\", help=\"don't actually run anything\")\n\tparser.add_argument(\"-s\", \"--start\", type=int, help=\"start index\")\n\tparser.add_argument(\"-e\", \"--end\", type=int, help=\"end index\")\n\tparser.add_argument(\"-i\", \"--index\", type=int, help=\"index of input file\")\n\tparser.add_argument(\"-m\", \"--model\", choices=[\"viterbi\", \"forward\", \"backward\"], help=\"model to use\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"output file\")", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='')\n\tparser.add_argument('-l', '--log', action='store_true', help='print debugging information to stdout')\n\tparser.add_argument('-v', '--verbose', action='count', help='increase verbosity level; may be repeated')\n\tparser.add_argument('-n', '--network', help='Name of network to use for subnetting and creating the \"default\" subnet')\n\tparser.add_argument('-d', '--directory', help='Directory to scan for networks')\n\tparser.add_argument('-f', '--file', help='File to read networks from')\n\tparser.add_argument('-a', '--add', help='Add a network to the \"default\" subnet')\n\tparser.add_argument('-r', '--remove', help='Remove a network from the \"default\" subnet')\n\tparser.add_argument('-p', '--print', help='Print the subnetting table to stdout')\n\tparser.add_argument('-t', '--table', help='Print the subnetting table to stdout')\n\tparser.add_argument('-w', '--write', help='Write the subnetting", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = get_parser()\n\tnamespace = parser.parse_args(*unparsed_arguments)\n\treturn {\n\t\t\"global\": namespace\n\t}\n", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run the Docker container\",\n\t\tepilog=\"Docker-py is a Python wrapper for the Docker API. \" + \\\n\t\t\"For more information, see: https://github.com/docker/docker-py.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t)\n\n\t# Global-level arguments\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\tparser.add_argument(\"--no-open\", help=\"Do not open the browser after starting the container\", action=\"store_true\")\n\tparser.add_argument(\"--no-tty\", help=\"Do not attach TTY\", action=\"store_true\")\n\tparser.add_argument(\"--no-rm\", help=\"Do not remove the container when it exits\", action=\"store_true\")\n\tparser.add_argument(\"--quiet\", help=\"Do not print container IDs\", action=\"store_true\")\n\tparser.add_argument(\"--no-compress\", help=\"Do not compress the container's filesystem\", action=\"store_true\")\n\tparser.add_argument(\"--no-cpu-shares\", help=", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Generate a new Python package with a skeleton and a testing harness\",\n\t\tformatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\n\t\t\"-i\", \"--install-requirements\",\n\t\thelp=\"Install the project's development requirements (Python, Pytest, etc.)\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-n\", \"--no-tests\",\n\t\thelp=\"Do not create a new test file\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-p\", \"--pyproject\",\n\t\thelp=\"Generate a new Python package with a pyproject.toml\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-r\", \"--requirements\",\n\t\thelp=\"Generate a new Python package with a requirements.txt\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-s\", \"--setup\",\n\t\thelp=\"Generate a new Python package with a setup.py\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-t\", \"", "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"py2rdf.py\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Be verbose\")\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\tparser.add_argument(\"-i\", \"--inverse\", action=\"store_true\", help=\"Inverse the triples\")\n\tparser.add_argument(\"-t\", \"--triples\", default=\"triples\", help=\"File to write triples to\")\n\tparser.add_argument(\"-p\", \"--properties\", default=\"properties\", help=\"File to write properties to\")\n\tparser.add_argument(\"-s\", \"--subjects\", default=\"subjects\", help=\"File to write subjects to\")\n\tparser.add_argument(\"-o\", \"--objects\", default=\"objects\", help=\"File to write objects to\")\n\tparser.add_argument(\"-d\", \"--directives\", default=\"directives\", help=\"File to write directives to\")\n\tparser.add_argument(\"-u\", \"--uris\", default=\"uris\", help=\"File to write URIs to\")\n\tparser"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"Build a virtual machine from a CFG\")\n\tparser.add_argument(\n\t\t\"-c\", \"--cfg\",\n\t\tdest = \"cfg\",\n\t\ttype = argparse.FileType(\"r\"),\n\t\thelp = \"Path to CFG file\",\n\t\trequired = True\n\t)\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\",\n\t\tdest = \"verbose\",\n\t\taction = \"store_true\",\n\t\thelp = \"Print additional information to standard output\",\n\t\tdefault = False\n\t)\n\tparser.add_argument(\n\t\t\"-o\", \"--output\",\n\t\tdest = \"output\",\n\t\ttype = argparse.FileType(\"w\"),\n\t\thelp = \"Path to output file\",\n\t\trequired = False\n\t)\n\n\tsubparsers = parser.add_subparsers(dest = \"command\", help = \"Command to run\")\n\n\tbuild_parser = subparsers.add_parser(\"build\", help = \"Build a virtual machine\")\n\tbuild_parser.add_argument(\n\t\t\"--no-save\",\n\t\tdest = \"no_save\",\n\t\taction = \"store_true\",\n\t\thelp = \"Do not", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description = \"A simple command-line tool\")\n\tparser.add_argument(\"--version\", action = \"version\", version = \"0.1\")\n\n\tsubparsers = parser.add_subparsers(title = \"subcommands\")\n\tsubparsers.required = True\n\n\t# Global parser\n\tglobal_parser = subparsers.add_parser(\"global\", help = \"A global option\")\n\tglobal_parser.add_argument(\"-v\", \"--verbose\", action = \"store_true\", default = False)\n\n\t# Subparser for \"foo\"\n\tfoo_parser = subparsers.add_parser(\"foo\", help = \"A foo option\")\n\tfoo_parser.add_argument(\"-f\", \"--foo\", action = \"store_true\", default = False)\n\n\t# Subparser for \"bar\"\n\tbar_parser = subparsers.add_parser(\"bar\", help = \"A bar option\")\n\tbar_parser.add_argument(\"-b\", \"--bar\", action = \"store_true\", default = False)\n\n\treturn parser.parse_args(*unparsed_arguments)\n\n#examples/src/foo.py", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"python3 -m \" + __package__ + \".main\")\n\tparser.add_argument(\"-h\", \"--help\", action=\"help\")\n\tsubparsers = parser.add_subparsers(help=\"choose a subparser\")\n\tsubparsers.required = True\n\n\tglobal_parser = subparsers.add_parser(\"global\")\n\tglobal_parser.add_argument(\"-v\", \"--verbose\", action=\"count\", default=0)\n\n\tparser.add_argument(\"-o\", \"--outfile\", type=str, metavar=\"NAME\", default=None)\n\tparser.add_argument(\"-s\", \"--source-dir\", type=str, metavar=\"PATH\", default=\".\")\n\tparser.add_argument(\"-t\", \"--target-dir\", type=str, metavar=\"PATH\", default=\".\")\n\n\tglobal_parser.add_argument(\"-d\", \"--debug\", action=\"store_true\")\n\tglobal_parser.add_argument(\"-q\", \"--quiet\", action=\"store_true\")\n\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tfrom argparse import ArgumentParser\n\tparser = ArgumentParser(description=\"Convert a file to a different format\")\n\t# subparsers are used to group commands\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\tsubparsers.required = True\n\n\tparser_convert = subparsers.add_parser(\"convert\", help=\"Convert a file to a different format\")\n\tparser_convert.add_argument(\"input\", help=\"The file to convert\")\n\tparser_convert.add_argument(\"output\", help=\"The file to write to\")\n\tparser_convert.add_argument(\"--source-format\", help=\"The source format of the input file\",\n\t\t\t\t\t\t\t\t\tdest=\"source_format\", default=None)\n\tparser_convert.add_argument(\"--target-format\", help=\"The target format of the output file\",\n\t\t\t\t\t\t\t\t\tdest=\"target_format\", default=None)\n\tparser_convert.add_argument(\"--overwrite\", help=\"Overwrite the output file if it exists\",\n\t\t\t\t\t\t\t\t\taction=\"store_true\")\n\n\tparser_list = subparsers.add_parser(\"list\", help=\"List the available conversions\")\n\n\tparser_version = subparsers.add_parser(\"version\", help", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"count\", help=\"increase output verbosity\")\n\tparser.add_argument(\"-c\", \"--continue\", action=\"store_true\", help=\"continue a partially completed run\")\n\tparser.add_argument(\"-f\", \"--force\", action=\"store_true\", help=\"overwrite existing output files\")\n\tparser.add_argument(\"-t\", \"--threads\", default=1, type=int, help=\"number of threads to use\")\n\tparser.add_argument(\"-d\", \"--dry\", action=\"store_true\", help=\"don't actually run anything\")\n\tparser.add_argument(\"-s\", \"--start\", type=int, help=\"start index\")\n\tparser.add_argument(\"-e\", \"--end\", type=int, help=\"end index\")\n\tparser.add_argument(\"-i\", \"--index\", type=int, help=\"index of input file\")\n\tparser.add_argument(\"-m\", \"--model\", choices=[\"viterbi\", \"forward\", \"backward\"], help=\"model to use\")\n\tparser.add_argument(\"-o\", \"--output\", help=\"output file\")", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='')\n\tparser.add_argument('-l', '--log', action='store_true', help='print debugging information to stdout')\n\tparser.add_argument('-v', '--verbose', action='count', help='increase verbosity level; may be repeated')\n\tparser.add_argument('-n', '--network', help='Name of network to use for subnetting and creating the \"default\" subnet')\n\tparser.add_argument('-d', '--directory', help='Directory to scan for networks')\n\tparser.add_argument('-f', '--file', help='File to read networks from')\n\tparser.add_argument('-a', '--add', help='Add a network to the \"default\" subnet')\n\tparser.add_argument('-r', '--remove', help='Remove a network from the \"default\" subnet')\n\tparser.add_argument('-p', '--print', help='Print the subnetting table to stdout')\n\tparser.add_argument('-t', '--table', help='Print the subnetting table to stdout')\n\tparser.add_argument('-w', '--write', help='Write the subnetting", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = get_parser()\n\tnamespace = parser.parse_args(*unparsed_arguments)\n\treturn {\n\t\t\"global\": namespace\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\t\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run the Docker container\",\n\t\tepilog=\"Docker-py is a Python wrapper for the Docker API. \" + \\\n\t\t\"For more information, see: https://github.com/docker/docker-py.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t)\n\n\t# Global-level arguments\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\tparser.add_argument(\"--no-open\", help=\"Do not open the browser after starting the container\", action=\"store_true\")\n\tparser.add_argument(\"--no-tty\", help=\"Do not attach TTY\", action=\"store_true\")\n\tparser.add_argument(\"--no-rm\", help=\"Do not remove the container when it exits\", action=\"store_true\")\n\tparser.add_argument(\"--quiet\", help=\"Do not print container IDs\", action=\"store_true\")\n\tparser.add_argument(\"--no-compress\", help=\"Do not compress the container's filesystem\", action=\"store_true\")\n\tparser.add_argument(\"--no-cpu-shares\", help=", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Generate a new Python package with a skeleton and a testing harness\",\n\t\tformatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\n\t\t\"-i\", \"--install-requirements\",\n\t\thelp=\"Install the project's development requirements (Python, Pytest, etc.)\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-n\", \"--no-tests\",\n\t\thelp=\"Do not create a new test file\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-p\", \"--pyproject\",\n\t\thelp=\"Generate a new Python package with a pyproject.toml\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-r\", \"--requirements\",\n\t\thelp=\"Generate a new Python package with a requirements.txt\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-s\", \"--setup\",\n\t\thelp=\"Generate a new Python package with a setup.py\",\n\t\taction=\"store_true\")\n\tparser.add_argument(\n\t\t\"-t\", \"", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\"\"\"\n\tGiven command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(prog=\"py2rdf.py\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Be verbose\")\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s 0.1\")\n\tparser.add_argument(\"-i\", \"--inverse\", action=\"store_true\", help=\"Inverse the triples\")\n\tparser.add_argument(\"-t\", \"--triples\", default=\"triples\", help=\"File to write triples to\")\n\tparser.add_argument(\"-p\", \"--properties\", default=\"properties\", help=\"File to write properties to\")\n\tparser.add_argument(\"-s\", \"--subjects\", default=\"subjects\", help=\"File to write subjects to\")\n\tparser.add_argument(\"-o\", \"--objects\", default=\"objects\", help=\"File to write objects to\")\n\tparser.add_argument(\"-d\", \"--directives\", default=\"directives\", help=\"File to write directives to\")\n\tparser.add_argument(\"-u\", \"--uris\", default=\"uris\", help=\"File to write URIs to\")\n\tparser", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_arguments = {}\n\tremaining_unparsed_arguments = []\n\tfor subparser in subparsers.values():\n\t\tsubparser_arguments, remaining_subparser_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\tparser_arguments[subparser.prog] = subparser_arguments\n\t\tremaining_unparsed_arguments.extend(remaining_subparser_arguments)\n\treturn parser_arguments, remaining_unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining = unparsed_arguments.copy()\n\twhile remaining:\n\t\tsubparser_name = remaining[0]\n\t\tdel remaining[0]\n\t\tparser = subparsers[subparser_name]\n\t\ttry:\n\t\t\tparsed_arguments[subparser_name], remaining = parser.parse_known_args(remaining)\n\t\texcept SystemExit:\n\t\t\traise InvalidArgumentsError(\"Failed to parse arguments for {}: {}\".format(subparser_name, remaining))\n\treturn (parsed_arguments, remaining)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\tremaining_arguments = []\n\tfor unparsed_argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tresult = subparser.parse_args(unparsed_argument)\n\t\t\t\tif result: # The subparser's action didn't take all of the arguments\n\t\t\t\t\tsubparser_arguments.update(result.__dict__)\n\t\t\t\t\tremaining_arguments.extend(unparsed_argument[unparsed_argument.index(result.__dict__[\"_prog\"])+1:])\n\t\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\treturn subparser_arguments, remaining_arguments", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = defaultdict(lambda: None)\n\tremaining_arguments = []\n\tfor unparsed_argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif not subparser.reset_required:\n\t\t\t\tcontinue\n\t\t\tsubparser.reset_required = False\n\t\t\ttry:\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_argument)\n\t\t\texcept SystemExit:\n\t\t\t\tremaining_arguments.append(unparsed_argument)\n\t\t\t\tcontinue\n\t\t\tbreak\n\treturn parsed_arguments, remaining_arguments\n\n#src/pygrep/__init__.py\n#!/usr/bin/env python\n\nimport argparse\nfrom os.path import exists\n\nfrom .core import search_file\nfrom .parse_arguments import parse_subparser_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tremaining_subparser_arguments, remaining_arguments = parse_subparser_arguments(unparsed_arguments, subparser)\n\t\tparsed_arguments[subparser_name] = remaining_subparser_arguments\n\n\t\tremaining_arguments = remaining_arguments + remaining_subparser_arguments\n\n\treturn (parsed_arguments, remaining_arguments)\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\tparsers = {}\n\tremaining_arguments = []\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith(\"--\"):\n\t\t\tsubparser_name = arg.split(\"=\")[0][2:]\n\t\t\tparser = parsers.get(subparser_name)\n\t\t\tif parser is None:\n\t\t\t\t# The parser for this subparser doesn't exist yet.\n\t\t\t\tparser = subparsers[subparser_name]\n\t\t\t\tparsers[subparser_name] = parser\n\t\t\tif parser.parse_args(unparsed_arguments[:1]) is None:\n\t\t\t\traise RuntimeError(\"Unrecognized argument: %s\" % arg)\n\t\t\tunparsed_arguments = unparsed_arguments[1:]\n\t\telse:\n\t\t\tremaining_arguments.append(arg)\n\t\n\treturn (parsers, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = []\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tresult = subparser.parse_args(unparsed_arguments)\n\t\tparsed_arguments[subparser_name] = result\n\t\tremaining_arguments += subparser.get_remaining_args()\n\treturn (parsed_arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = []\n\tsubparser_results = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparser_args = subparser.parse_args(unparsed_arguments)\n\t\tsubparser_results[subparser_name] = parser_args\n\t\tremaining_arguments = [arg for arg in unparsed_arguments if arg not in parser_args.__dict__]\n\n\treturn subparser_results, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\tremaining_arguments = []\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tfor arg in vars(parsed_arguments[subparser_name]):\n\t\t\tif arg not in ['_', '__']:\n\t\t\t\tsubparser_arguments[subparser_name][arg] = getattr(parsed_arguments[subparser_name], arg)\n\t\tremaining_arguments += [arg for arg in unparsed_arguments if arg not in vars(parsed_arguments[subparser_name])]\n\treturn (subparser_arguments, remaining_arguments)\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = list(subparsers.keys())\n\tparsed_subparsers = dict()\n\targuments_remaining = list()\n\n\t# For each subparser, parse the arguments.\n\tfor subparser_name in subparser_names:\n\t\tparser = subparsers[subparser_name]\n\t\tparsed_subparser, arguments_remaining = parse_arguments(unparsed_arguments, parser)\n\t\tparsed_subparsers[subparser_name] = parsed_subparser\n\t\tunparsed_arguments = arguments_remaining\n\n\treturn (parsed_subparsers, unparsed_arguments)\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparser_arguments = {}\n\tremaining_unparsed_arguments = []\n\tfor subparser in subparsers.values():\n\t\tsubparser_arguments, remaining_subparser_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\tparser_arguments[subparser.prog] = subparser_arguments\n\t\tremaining_unparsed_arguments.extend(remaining_subparser_arguments)\n\treturn parser_arguments, remaining_unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining = unparsed_arguments.copy()\n\twhile remaining:\n\t\tsubparser_name = remaining[0]\n\t\tdel remaining[0]\n\t\tparser = subparsers[subparser_name]\n\t\ttry:\n\t\t\tparsed_arguments[subparser_name], remaining = parser.parse_known_args(remaining)\n\t\texcept SystemExit:\n\t\t\traise InvalidArgumentsError(\"Failed to parse arguments for {}: {}\".format(subparser_name, remaining))\n\treturn (parsed_arguments, remaining)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\tremaining_arguments = []\n\tfor unparsed_argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tresult = subparser.parse_args(unparsed_argument)\n\t\t\t\tif result: # The subparser's action didn't take all of the arguments\n\t\t\t\t\tsubparser_arguments.update(result.__dict__)\n\t\t\t\t\tremaining_arguments.extend(unparsed_argument[unparsed_argument.index(result.__dict__[\"_prog\"])+1:])\n\t\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\treturn subparser_arguments, remaining_arguments", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = defaultdict(lambda: None)\n\tremaining_arguments = []\n\tfor unparsed_argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif not subparser.reset_required:\n\t\t\t\tcontinue\n\t\t\tsubparser.reset_required = False\n\t\t\ttry:\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_argument)\n\t\t\texcept SystemExit:\n\t\t\t\tremaining_arguments.append(unparsed_argument)\n\t\t\t\tcontinue\n\t\t\tbreak\n\treturn parsed_arguments, remaining_arguments\n\n#src/pygrep/__init__.py\n#!/usr/bin/env python\n\nimport argparse\nfrom os.path import exists\n\nfrom .core import search_file\nfrom .parse_arguments import parse_subparser_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tremaining_subparser_arguments, remaining_arguments = parse_subparser_arguments(unparsed_arguments, subparser)\n\t\tparsed_arguments[subparser_name] = remaining_subparser_arguments\n\n\t\tremaining_arguments = remaining_arguments + remaining_subparser_arguments\n\n\treturn (parsed_arguments, remaining_arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\t\n\tparsers = {}\n\tremaining_arguments = []\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith(\"--\"):\n\t\t\tsubparser_name = arg.split(\"=\")[0][2:]\n\t\t\tparser = parsers.get(subparser_name)\n\t\t\tif parser is None:\n\t\t\t\t# The parser for this subparser doesn't exist yet.\n\t\t\t\tparser = subparsers[subparser_name]\n\t\t\t\tparsers[subparser_name] = parser\n\t\t\tif parser.parse_args(unparsed_arguments[:1]) is None:\n\t\t\t\traise RuntimeError(\"Unrecognized argument: %s\" % arg)\n\t\t\tunparsed_arguments = unparsed_arguments[1:]\n\t\telse:\n\t\t\tremaining_arguments.append(arg)\n\t\n\treturn (parsers, remaining_arguments)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = []\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tresult = subparser.parse_args(unparsed_arguments)\n\t\tparsed_arguments[subparser_name] = result\n\t\tremaining_arguments += subparser.get_remaining_args()\n\treturn (parsed_arguments, remaining_arguments)\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tremaining_arguments = []\n\tsubparser_results = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparser_args = subparser.parse_args(unparsed_arguments)\n\t\tsubparser_results[subparser_name] = parser_args\n\t\tremaining_arguments = [arg for arg in unparsed_arguments if arg not in parser_args.__dict__]\n\n\treturn subparser_results, remaining_arguments\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_arguments = {}\n\tremaining_arguments = []\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tfor arg in vars(parsed_arguments[subparser_name]):\n\t\t\tif arg not in ['_', '__']:\n\t\t\t\tsubparser_arguments[subparser_name][arg] = getattr(parsed_arguments[subparser_name], arg)\n\t\tremaining_arguments += [arg for arg in unparsed_arguments if arg not in vars(parsed_arguments[subparser_name])]\n\treturn (subparser_arguments, remaining_arguments)\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\"\"\"\n\tGiven a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).\n\t\"\"\"\n\tsubparser_names = list(subparsers.keys())\n\tparsed_subparsers = dict()\n\targuments_remaining = list()\n\n\t# For each subparser, parse the arguments.\n\tfor subparser_name in subparser_names:\n\t\tparser = subparsers[subparser_name]\n\t\tparsed_subparser, arguments_remaining = parse_arguments(unparsed_arguments, parser)\n\t\tparsed_subparsers[subparser_name] = parsed_subparser\n\t\tunparsed_arguments = arguments_remaining\n\n\treturn (parsed_subparsers, unparsed_arguments)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='A command line tool to create a new python project.')\n\tsubparsers = parser.add_subparsers(help='Available commands', dest='command')\n\n\t# Add a subparser for each of the commands\n\tgenerate_parser = subparsers.add_parser('generate', help='Generate a new python project.')\n\tgenerate_parser.add_argument('-p', '--project', required=True, help='The name of the new project.')\n\tgenerate_parser.add_argument('-d', '--description', required=False, help='The description of the new project.')\n\tgenerate_parser.add_argument('-t', '--template', required=False, help='The template to use.')\n\n\treturn (parser, subparsers)\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser, Namespace\n\t\n\tdef top_level_parser():\n\t\t\"\"\"\n\t\tBuild a top-level parser and return it.\n\t\t\"\"\"\n\t\tparser = ArgumentParser(\n\t\t\tdescription = \"The most basic script that can be run in the terminal.\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", action = \"store_true\", help = \"Show debugging information.\")\n\t\treturn parser\n\t\n\tdef sub_parsers():\n\t\t\"\"\"\n\t\tBuild a subparser and return it.\n\t\t\"\"\"\n\t\tsubparsers = ArgumentParser(add_help = False)\n\t\tsubparsers.add_argument(\"-h\", \"--help\", action = \"help\", help = \"Display this help message and exit.\")\n\t\treturn subparsers\n\t\n\treturn (top_level_parser(), sub_parsers())\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Build a top-level parser and its subparsers')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_build = subparsers.add_parser('build', help='Build a top-level parser and its subparsers')\n\tparser_build.add_argument('--targets', help='The targets to build.', required=True,\n\t\tnargs=argparse.REMAINDER)\n\n\treturn parser, subparsers\n\nif __name__ == '__main__':\n\tparser, subparsers = make_parsers()\n\targs = parser.parse_args()\n\n\tif args.targets == None:\n\t\tprint('Missing targets')\n\t\tsys.exit(1)\n\n\tprint('Subparsers:')\n\tprint(subparsers)\n\tprint('Args:')\n\tprint(args)", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = ArgumentParser(description=\"A simple cli for managing csv files.\")\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\tparser_add = subparsers.add_parser(\n\t\t\"add\",\n\t\tdescription=\"Add a new row to a csv file.\",\n\t\thelp=\"add <csv_file_path> <row>\",\n\t\tformatter_class=RawTextHelpFormatter,\n\t)\n\tparser_add.add_argument(\n\t\t\"csv_file_path\",\n\t\ttype=str,\n\t\thelp=\"path to the csv file to add a row to.\",\n\t)\n\tparser_add.add_argument(\n\t\t\"row\",\n\t\ttype=str,\n\t\thelp=\"row to add to the csv file. row should be a string of comma separated values, e.g. 'a,b,c'.\",\n\t)\n\n\tparser_del = subparsers.add_parser(\n\t\t\"del\",\n\t\tdescription=\"Delete a row from a csv file.\",\n\t\thelp=\"del <csv_file_path> <row>\",\n\t\tformatter_class=RawTextHelpFormatter,\n\t)\n\tparser_del.add", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(add_help=False)\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tparser_list = parser_list_parser(subparsers)\n\tparser_remove = parser_remove_parser(subparsers)\n\tparser_add = parser_add_parser(subparsers)\n\tparser_set = parser_set_parser(subparsers)\n\tparser_unset = parser_unset_parser(subparsers)\n\tparser_set_default = parser_set_default_parser(subparsers)\n\tparser_unset_default = parser_unset_default_parser(subparsers)\n\tparser_set_ignore = parser_set_ignore_parser(subparsers)\n\tparser_unset_ignore = parser_unset_ignore_parser(subparsers)\n\tparser_set_default_ignore = parser_set_default_ignore_parser(subparsers)\n\tparser_unset_default_ignore = parser_unset_default_ignore_parser(subparsers)\n\tparser_set_version = parser_set_version_parser(subparsers)\n\tparser_unset_version = parser_unset_version_", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tdef main():\n\t\t\"\"\"\n\t\tMain entry point for the command-line interface.\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=\"A simple command-line interface for the Todoist CLI\",\n\t\t\tepilog=\"This program is released under the MIT License.\",\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"-v\", \"--version\", action=\"version\",\n\t\t\tversion=\"%(prog)s 1.0.0\"\n\t\t)\n\n\t\tsubparsers = parser.add_subparsers(\n\t\t\tdest=\"command\",\n\t\t\thelp=\"Valid commands\",\n\t\t\tdescription=\"Valid commands\"\n\t\t)\n\n\t\tadd_project_parser(subparsers)\n\t\tadd_task_parser(subparsers)\n\n\t\targs = parser.parse_args()\n\t\tif not args.command:\n\t\t\tparser.error(\"Please specify a command\")\n\n\t\targs.func(args)\n\n\treturn parser, subparsers\n\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='')\n\tsubparsers = parser.add_subparsers()\n\n\tdef add_subparser(subparser_name, subparser_callback, subparser_description):\n\t\tsubparser = subparsers.add_parser(subparser_name, description=subparser_description)\n\t\tsubparser.set_defaults(func=subparser_callback)\n\t\treturn subparser\n\n\tdef add_subsubparser(subparser_name, subparser_callback, subparser_description):\n\t\tsubsubparser = add_subparser(subparser_name, subparser_callback, subparser_description)\n\t\tsubsubparser.add_argument('subsub_arg', nargs='?', default='default')\n\t\treturn subsubparser\n\n\tadd_subparser('sub1', sub1_callback, 'sub1 description')\n\tadd_subparser('sub2', sub2_callback, 'sub2 description')\n\tadd_subsubparser('sub3', sub3_callback, 'sub3 description')\n\n\treturn (parser, subparsers)\n", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Python CLI for NDN\")\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\t# Run subparser\n\trun_parser = subparsers.add_parser(\"run\", help=\"Run a simulation\")\n\trun_parser.add_argument('-d', '--duration', help='Simulation duration', type=int)\n\trun_parser.add_argument('-f', '--frequency', help='Frequency of data packets', type=int)\n\trun_parser.add_argument('-d', '--data-size', help='Data size of data packets', type=int)\n\trun_parser.add_argument('-i', '--interest', help='Interest to be sent, in form of /<name>/<lifetime>')\n\trun_parser.add_argument('-a', '--attr', help='Attribute to be sent, in form of <name>=<value>')\n\trun_parser.add_argument('-n', '--num-nodes', help='Number of nodes', type=int)\n\trun_parser.add_argument('-p', '--prefix', help='Prefix to be used')\n\trun_parser.add_argument('-", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_level = argparse.ArgumentParser(description='Generate a CSV file containing a list of the available '\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'movie titles.')\n\tsubparsers = top_level.add_subparsers(help='sub-command help', dest='command')\n\t\n\tparser_list = []\n\t\n\tdef add_parser(name):\n\t\t\"\"\"\n\t\tAdd a subparser to the top level parser and return a reference to it.\n\t\t\"\"\"\n\t\tparser = subparsers.add_parser(\n\t\t\tname,\n\t\t\thelp='sub-command help',\n\t\t\tdescription='sub-command help',\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t\tparser.add_argument('--output', '-o', type=str,\n\t\t\t\t\t\t\thelp='Output file for the movies.csv file.')\n\t\tparser.add_argument('--title', '-t', type=str,\n\t\t\t\t\t\t\thelp='The title of the movies to search.')\n\t\tparser.add_argument('--year', '-y', type=int,\n\t\t\t\t\t\t\thelp='The year of the movies to search.')\n\t\tparser.add_argument('--max', '-m', type=int,\n\t\t\t\t\t\t\thelp='", "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse some files.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\tsubparsers.required = True\n\treturn parser, subparsers\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='A command line tool to create a new python project.')\n\tsubparsers = parser.add_subparsers(help='Available commands', dest='command')\n\n\t# Add a subparser for each of the commands\n\tgenerate_parser = subparsers.add_parser('generate', help='Generate a new python project.')\n\tgenerate_parser.add_argument('-p', '--project', required=True, help='The name of the new project.')\n\tgenerate_parser.add_argument('-d', '--description', required=False, help='The description of the new project.')\n\tgenerate_parser.add_argument('-t', '--template', required=False, help='The template to use.')\n\n\treturn (parser, subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tfrom argparse import ArgumentParser, Namespace\n\t\n\tdef top_level_parser():\n\t\t\"\"\"\n\t\tBuild a top-level parser and return it.\n\t\t\"\"\"\n\t\tparser = ArgumentParser(\n\t\t\tdescription = \"The most basic script that can be run in the terminal.\")\n\t\tparser.add_argument(\"-v\", \"--verbose\", action = \"store_true\", help = \"Show debugging information.\")\n\t\treturn parser\n\t\n\tdef sub_parsers():\n\t\t\"\"\"\n\t\tBuild a subparser and return it.\n\t\t\"\"\"\n\t\tsubparsers = ArgumentParser(add_help = False)\n\t\tsubparsers.add_argument(\"-h\", \"--help\", action = \"help\", help = \"Display this help message and exit.\")\n\t\treturn subparsers\n\t\n\treturn (top_level_parser(), sub_parsers())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Build a top-level parser and its subparsers')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_build = subparsers.add_parser('build', help='Build a top-level parser and its subparsers')\n\tparser_build.add_argument('--targets', help='The targets to build.', required=True,\n\t\tnargs=argparse.REMAINDER)\n\n\treturn parser, subparsers\n\nif __name__ == '__main__':\n\tparser, subparsers = make_parsers()\n\targs = parser.parse_args()\n\n\tif args.targets == None:\n\t\tprint('Missing targets')\n\t\tsys.exit(1)\n\n\tprint('Subparsers:')\n\tprint(subparsers)\n\tprint('Args:')\n\tprint(args)", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = ArgumentParser(description=\"A simple cli for managing csv files.\")\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\")\n\n\tparser_add = subparsers.add_parser(\n\t\t\"add\",\n\t\tdescription=\"Add a new row to a csv file.\",\n\t\thelp=\"add <csv_file_path> <row>\",\n\t\tformatter_class=RawTextHelpFormatter,\n\t)\n\tparser_add.add_argument(\n\t\t\"csv_file_path\",\n\t\ttype=str,\n\t\thelp=\"path to the csv file to add a row to.\",\n\t)\n\tparser_add.add_argument(\n\t\t\"row\",\n\t\ttype=str,\n\t\thelp=\"row to add to the csv file. row should be a string of comma separated values, e.g. 'a,b,c'.\",\n\t)\n\n\tparser_del = subparsers.add_parser(\n\t\t\"del\",\n\t\tdescription=\"Delete a row from a csv file.\",\n\t\thelp=\"del <csv_file_path> <row>\",\n\t\tformatter_class=RawTextHelpFormatter,\n\t)\n\tparser_del.add", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(add_help=False)\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tparser_list = parser_list_parser(subparsers)\n\tparser_remove = parser_remove_parser(subparsers)\n\tparser_add = parser_add_parser(subparsers)\n\tparser_set = parser_set_parser(subparsers)\n\tparser_unset = parser_unset_parser(subparsers)\n\tparser_set_default = parser_set_default_parser(subparsers)\n\tparser_unset_default = parser_unset_default_parser(subparsers)\n\tparser_set_ignore = parser_set_ignore_parser(subparsers)\n\tparser_unset_ignore = parser_unset_ignore_parser(subparsers)\n\tparser_set_default_ignore = parser_set_default_ignore_parser(subparsers)\n\tparser_unset_default_ignore = parser_unset_default_ignore_parser(subparsers)\n\tparser_set_version = parser_set_version_parser(subparsers)\n\tparser_unset_version = parser_unset_version_", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\timport argparse\n\n\tdef main():\n\t\t\"\"\"\n\t\tMain entry point for the command-line interface.\n\t\t\"\"\"\n\t\tparser = argparse.ArgumentParser(\n\t\t\tdescription=\"A simple command-line interface for the Todoist CLI\",\n\t\t\tepilog=\"This program is released under the MIT License.\",\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter\n\t\t)\n\t\tparser.add_argument(\n\t\t\t\"-v\", \"--version\", action=\"version\",\n\t\t\tversion=\"%(prog)s 1.0.0\"\n\t\t)\n\n\t\tsubparsers = parser.add_subparsers(\n\t\t\tdest=\"command\",\n\t\t\thelp=\"Valid commands\",\n\t\t\tdescription=\"Valid commands\"\n\t\t)\n\n\t\tadd_project_parser(subparsers)\n\t\tadd_task_parser(subparsers)\n\n\t\targs = parser.parse_args()\n\t\tif not args.command:\n\t\t\tparser.error(\"Please specify a command\")\n\n\t\targs.func(args)\n\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='')\n\tsubparsers = parser.add_subparsers()\n\n\tdef add_subparser(subparser_name, subparser_callback, subparser_description):\n\t\tsubparser = subparsers.add_parser(subparser_name, description=subparser_description)\n\t\tsubparser.set_defaults(func=subparser_callback)\n\t\treturn subparser\n\n\tdef add_subsubparser(subparser_name, subparser_callback, subparser_description):\n\t\tsubsubparser = add_subparser(subparser_name, subparser_callback, subparser_description)\n\t\tsubsubparser.add_argument('subsub_arg', nargs='?', default='default')\n\t\treturn subsubparser\n\n\tadd_subparser('sub1', sub1_callback, 'sub1 description')\n\tadd_subparser('sub2', sub2_callback, 'sub2 description')\n\tadd_subsubparser('sub3', sub3_callback, 'sub3 description')\n\n\treturn (parser, subparsers)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description=\"Python CLI for NDN\")\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\t# Run subparser\n\trun_parser = subparsers.add_parser(\"run\", help=\"Run a simulation\")\n\trun_parser.add_argument('-d', '--duration', help='Simulation duration', type=int)\n\trun_parser.add_argument('-f', '--frequency', help='Frequency of data packets', type=int)\n\trun_parser.add_argument('-d', '--data-size', help='Data size of data packets', type=int)\n\trun_parser.add_argument('-i', '--interest', help='Interest to be sent, in form of /<name>/<lifetime>')\n\trun_parser.add_argument('-a', '--attr', help='Attribute to be sent, in form of <name>=<value>')\n\trun_parser.add_argument('-n', '--num-nodes', help='Number of nodes', type=int)\n\trun_parser.add_argument('-p', '--prefix', help='Prefix to be used')\n\trun_parser.add_argument('-", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\ttop_level = argparse.ArgumentParser(description='Generate a CSV file containing a list of the available '\n\t\t\t\t\t\t\t\t\t\t\t\t\t 'movie titles.')\n\tsubparsers = top_level.add_subparsers(help='sub-command help', dest='command')\n\t\n\tparser_list = []\n\t\n\tdef add_parser(name):\n\t\t\"\"\"\n\t\tAdd a subparser to the top level parser and return a reference to it.\n\t\t\"\"\"\n\t\tparser = subparsers.add_parser(\n\t\t\tname,\n\t\t\thelp='sub-command help',\n\t\t\tdescription='sub-command help',\n\t\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t\tparser.add_argument('--output', '-o', type=str,\n\t\t\t\t\t\t\thelp='Output file for the movies.csv file.')\n\t\tparser.add_argument('--title', '-t', type=str,\n\t\t\t\t\t\t\thelp='The title of the movies to search.')\n\t\tparser.add_argument('--year', '-y', type=int,\n\t\t\t\t\t\t\thelp='The year of the movies to search.')\n\t\tparser.add_argument('--max', '-m', type=int,\n\t\t\t\t\t\t\thelp='", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\"\"\"\n\tBuild a top-level parser and its subparsers and return them as a tuple.\n\t\"\"\"\n\tparser = argparse.ArgumentParser(description='Parse some files.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\tsubparsers.required = True\n\treturn parser, subparsers\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infoblox_client/utils.py", "project": "infobloxopen/infoblox-client", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ttotal_results = len(response)\n\tif total_results > max_results:\n\t\tprint(\"You're getting too many results. Consider using another query to narrow the search.\")\n\t\ttotal_pages = int(total_results / max_results)\n\t\tprint(\"Total results: {}\".format(total_results))\n\t\tprint(\"Total pages: {}\".format(total_pages))\n\t\tprint(\"Max results: {}\".format(max_results))\n\t\tprint(\"Max results per page: {}\".format(total_results / total_pages))\n\telse:\n\t\tprint(\"Total results: {}\".format(total_results))\n\t\tprint(\"Total pages: 1\")\n\t\tprint(\"Max results: {}\".format(max_results))\n\t\tprint(\"Max results per page: {}\".format(total_results))\n\n\tpage = 1\n\twhile page <= total_pages:\n\t\tprint(\"Page {}\".format(page))\n\t\tyield response[page*max_results-max_results:page*max_results]\n\t\tpage += 1\n\n#wapi.py\nimport requests\nimport json\nfrom paging import paging\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_size = max_results\n\tpage_number = 0\n\twhile page_number * page_size < len(response):\n\t\tyield response[page_number * page_size : (page_number + 1) * page_size]\n\t\tpage_number += 1\n\n#wapi/wapi_client.py\n\"\"\"\nWSAPI client class.\n\"\"\"\n\nimport json\nimport requests\nfrom . import paging\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 1\n\twhile True:\n\t\tstart = (page - 1) * max_results\n\t\tend = start + max_results\n\t\tif end > len(response):\n\t\t\tbreak\n\t\tyield response[start:end]\n\t\tpage += 1\n\n#utils.py\nimport os\nimport sys\nimport json\nimport requests\nfrom datetime import datetime\nfrom wapi import get_all_objects, get_object, get_object_history, get_object_changes, get_object_relationship, get_object_relationships, get_object_class_relationship\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\ti = 1\n\twhile i <= len(response):\n\t\tresult = response[i-1:i+max_results-1]\n\t\ti += max_results\n\t\tyield result\n\n#utils/wapi_client.py\nfrom requests import get\nfrom utils.wapi_utils import paging\nfrom utils.wapi_exceptions import WapiError\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < len(response):\n\t\tyield response[i:i+max_results]\n\t\ti += max_results\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < len(response):\n\t\tyield response[i:i+max_results]\n\t\ti += max_results", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 0\n\ttotal_pages = response['count'] // max_results\n\twhile page < total_pages:\n\t\tpage += 1\n\t\tstart = page * max_results\n\t\tend = start + max_results\n\t\tyield response['results'][start:end]\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tmax_results = int(max_results)\n\twhile len(response) >= max_results:\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\n#wapi/__init__.py\n\"\"\"\nWAPI\n\"\"\"\nfrom .api import Wapi\n\n__version__ = '0.4.0'\n\n#tests/test_api.py\nimport os\nimport sys\nimport unittest\nfrom unittest.mock import patch\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nfrom wapi import Wapi\n\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_len = len(response)\n\tpage_count = int(response_len / max_results) + 1\n\tfor page in range(0, page_count):\n\t\tyield response[page * max_results:(page + 1) * max_results]\n\n#wapi/wapi.py\nimport requests\nimport json\nimport time\nimport datetime\nimport copy\n\nfrom .paging import paging\nfrom .exceptions import *\nfrom .constants import *\n", "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 0\n\tpage_size = max_results\n\twhile page * page_size < len(response):\n\t\tyield response[page * page_size: (page + 1) * page_size]\n\t\tpage += 1\n\n#app/__init__.py\nfrom flask import Flask, render_template, request, redirect, url_for\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_migrate import Migrate\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, SubmitField\nfrom wtforms.validators import DataRequired, ValidationError\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///catalog.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\ndb = SQLAlchemy(app)\nmigrate = Migrate(app, db)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ttotal_results = len(response)\n\tif total_results > max_results:\n\t\tprint(\"You're getting too many results. Consider using another query to narrow the search.\")\n\t\ttotal_pages = int(total_results / max_results)\n\t\tprint(\"Total results: {}\".format(total_results))\n\t\tprint(\"Total pages: {}\".format(total_pages))\n\t\tprint(\"Max results: {}\".format(max_results))\n\t\tprint(\"Max results per page: {}\".format(total_results / total_pages))\n\telse:\n\t\tprint(\"Total results: {}\".format(total_results))\n\t\tprint(\"Total pages: 1\")\n\t\tprint(\"Max results: {}\".format(max_results))\n\t\tprint(\"Max results per page: {}\".format(total_results))\n\n\tpage = 1\n\twhile page <= total_pages:\n\t\tprint(\"Page {}\".format(page))\n\t\tyield response[page*max_results-max_results:page*max_results]\n\t\tpage += 1\n\n#wapi.py\nimport requests\nimport json\nfrom paging import paging\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage_size = max_results\n\tpage_number = 0\n\twhile page_number * page_size < len(response):\n\t\tyield response[page_number * page_size : (page_number + 1) * page_size]\n\t\tpage_number += 1\n\n#wapi/wapi_client.py\n\"\"\"\nWSAPI client class.\n\"\"\"\n\nimport json\nimport requests\nfrom . import paging\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 1\n\twhile True:\n\t\tstart = (page - 1) * max_results\n\t\tend = start + max_results\n\t\tif end > len(response):\n\t\t\tbreak\n\t\tyield response[start:end]\n\t\tpage += 1\n\n#utils.py\nimport os\nimport sys\nimport json\nimport requests\nfrom datetime import datetime\nfrom wapi import get_all_objects, get_object, get_object_history, get_object_changes, get_object_relationship, get_object_relationships, get_object_class_relationship\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\t\n\ti = 1\n\twhile i <= len(response):\n\t\tresult = response[i-1:i+max_results-1]\n\t\ti += max_results\n\t\tyield result\n\n#utils/wapi_client.py\nfrom requests import get\nfrom utils.wapi_utils import paging\nfrom utils.wapi_exceptions import WapiError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < len(response):\n\t\tyield response[i:i+max_results]\n\t\ti += max_results\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\ti = 0\n\twhile i < len(response):\n\t\tyield response[i:i+max_results]\n\t\ti += max_results", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 0\n\ttotal_pages = response['count'] // max_results\n\twhile page < total_pages:\n\t\tpage += 1\n\t\tstart = page * max_results\n\t\tend = start + max_results\n\t\tyield response['results'][start:end]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tmax_results = int(max_results)\n\twhile len(response) >= max_results:\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\n#wapi/__init__.py\n\"\"\"\nWAPI\n\"\"\"\nfrom .api import Wapi\n\n__version__ = '0.4.0'\n\n#tests/test_api.py\nimport os\nimport sys\nimport unittest\nfrom unittest.mock import patch\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n\nfrom wapi import Wapi\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tresponse_len = len(response)\n\tpage_count = int(response_len / max_results) + 1\n\tfor page in range(0, page_count):\n\t\tyield response[page * max_results:(page + 1) * max_results]\n\n#wapi/wapi.py\nimport requests\nimport json\nimport time\nimport datetime\nimport copy\n\nfrom .paging import paging\nfrom .exceptions import *\nfrom .constants import *\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\t\"\"\"\n\tReturn every response with the length of max_results\n    Args:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\n    Returns:\n        Generator object with WAPI response split page by page.\n\t\"\"\"\n\tpage = 0\n\tpage_size = max_results\n\twhile page * page_size < len(response):\n\t\tyield response[page * page_size: (page + 1) * page_size]\n\t\tpage += 1\n\n#app/__init__.py\nfrom flask import Flask, render_template, request, redirect, url_for\nfrom flask_sqlalchemy import SQLAlchemy\nfrom flask_migrate import Migrate\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, SubmitField\nfrom wtforms.validators import DataRequired, ValidationError\n\napp = Flask(__name__)\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///catalog.db'\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\ndb = SQLAlchemy(app)\nmigrate = Migrate(app, db)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_unit_map = {\n\t\t\"B\": 1,\n\t\t\"KB\": 1024,\n\t\t\"MB\": 1024**2,\n\t\t\"GB\": 1024**3,\n\t\t\"TB\": 1024**4,\n\t}\n\n\tsize_in_bytes = 0\n\n\tif \"K\" in size or \"k\" in size:\n\t\tsize_in_bytes = int(size.replace(\"K\", \"\")) * size_unit_map[\"KB\"]\n\telif \"M\" in size or \"m\" in size:\n\t\tsize_in_bytes = int(size.replace(\"M\", \"\")) * size_unit_map[\"MB\"]\n\telif \"G\" in size or \"g\" in size:\n\t\tsize_in_bytes = int(size.replace(\"G\", \"\")) * size_unit_map[\"GB\"]\n\telif \"T\" in size or \"t\" in size:\n\t\tsize_in_bytes = int(size.replace(\"T\", \"\")) * size_unit_map[\"TB\"]\n\telse:\n\t\tsize_in_bytes = int(size)\n\n\treturn size_in_bytes\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']\n\tsize = size.upper()\n\tnum, suffix = size.split(' ')\n\tnum = float(num)\n\tsuffix = suffix.upper()\n\tsuffix_index = suffixes.index(suffix)\n\treturn num * (1024 ** suffix_index)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\ttry:\n\t\tsize = size.lower()\n\t\tmultiplier = 1\n\t\tif size.endswith('k'):\n\t\t\tmultiplier = 1000\n\t\telif size.endswith('m'):\n\t\t\tmultiplier = 1000000\n\t\telif size.endswith('g'):\n\t\t\tmultiplier = 1000000000\n\t\telif size.endswith('t'):\n\t\t\tmultiplier = 1000000000000\n\t\telif size.endswith('p'):\n\t\t\tmultiplier = 1000000000000000\n\t\telif size.endswith('kib'):\n\t\t\tmultiplier = 1024\n\t\telif size.endswith('mib'):\n\t\t\tmultiplier = 1024**2\n\t\telif size.endswith('gib'):\n\t\t\tmultiplier = 1024**3\n\t\telif size.endswith('tib'):\n\t\t\tmultiplier = 1024**4\n\t\telif size.endswith('pib'):\n\t\t\t", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tvalue = size[:-1]\n\tif size[-1].isdigit():\n\t\treturn int(value)\n\tif size[-1] == 'G':\n\t\treturn int(value) * 1024\n\tif size[-1] == 'M':\n\t\treturn int(value) * 1024 * 1024\n\tif size[-1] == 'K':\n\t\treturn int(value) * 1024 * 1024 * 1024\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.upper()\n\tif size.endswith('G'):\n\t\treturn int(float(size[:-1]) * 1024 * 1024 * 1024)\n\telif size.endswith('M'):\n\t\treturn int(float(size[:-1]) * 1024 * 1024)\n\telif size.endswith('K'):\n\t\treturn int(float(size[:-1]) * 1024)\n\telse:\n\t\treturn int(size)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.replace(\" \", \"\")\n\tsize = size.replace(\"KB\", \"000\")\n\tsize = size.replace(\"MB\", \"000000\")\n\tsize = size.replace(\"GB\", \"000000000\")\n\tsize = size.replace(\"TB\", \"000000000000\")\n\tsize = size.replace(\"PB\", \"000000000000000\")\n\tsize = size.replace(\"EB\", \"00000000000000000\")\n\treturn int(size)\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.upper()\n\tsize_bytes = 0\n\tif 'K' in size:\n\t\tsize_bytes = int(size.replace('K', '')) * 1024\n\telif 'M' in size:\n\t\tsize_bytes = int(size.replace('M', '')) * 1024 * 1024\n\telif 'G' in size:\n\t\tsize_bytes = int(size.replace('G', '')) * 1024 * 1024 * 1024\n\telse:\n\t\traise Exception('Incorrect file size input')\n\treturn size_bytes\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tunit = size[-1]\n\tvalue = float(size[:-1])\n\tif unit == 'K':\n\t\treturn int(value * 1024)\n\telif unit == 'M':\n\t\treturn int(value * 1024 * 1024)\n\telif unit == 'G':\n\t\treturn int(value * 1024 * 1024 * 1024)\n\telif unit == 'T':\n\t\treturn int(value * 1024 * 1024 * 1024 * 1024)\n\telse:\n\t\traise ValueError(f'size {size} is not a valid size')\n\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.upper()\n\tunit = size[-1]\n\tsize = float(size[:-1])\n\tif unit == \"K\":\n\t\treturn int(size * 1000)\n\tif unit == \"M\":\n\t\treturn int(size * 1000000)\n\tif unit == \"G\":\n\t\treturn int(size * 1000000000)\n\tif unit == \"T\":\n\t\treturn int(size * 1000000000000)\n\tif unit == \"P\":\n\t\treturn int(size * 100000000000000)\n\tif unit == \"E\":\n\t\treturn int(size * 10000000000000000)\n\treturn None\n", "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_suffixes = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024**2,\n\t\t'G': 1024**3,\n\t\t'T': 1024**4,\n\t\t'P': 1024**5,\n\t\t'E': 1024**6,\n\t\t'Z': 1024**7,\n\t\t'Y': 1024**8,\n\t}\n\n\tsize = size.upper()\n\n\tif size[-1] in size_suffixes:\n\t\tsize = size[:-1]\n\n\ttry:\n\t\treturn int(size) * size_suffixes[size[-1]]\n\texcept KeyError:\n\t\traise ValueError('Invalid file size: {}'.format(size))\n\n#src/tests/test_file_size.py\nimport unittest\nfrom file_size import size_to_bytes\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_unit_map = {\n\t\t\"B\": 1,\n\t\t\"KB\": 1024,\n\t\t\"MB\": 1024**2,\n\t\t\"GB\": 1024**3,\n\t\t\"TB\": 1024**4,\n\t}\n\n\tsize_in_bytes = 0\n\n\tif \"K\" in size or \"k\" in size:\n\t\tsize_in_bytes = int(size.replace(\"K\", \"\")) * size_unit_map[\"KB\"]\n\telif \"M\" in size or \"m\" in size:\n\t\tsize_in_bytes = int(size.replace(\"M\", \"\")) * size_unit_map[\"MB\"]\n\telif \"G\" in size or \"g\" in size:\n\t\tsize_in_bytes = int(size.replace(\"G\", \"\")) * size_unit_map[\"GB\"]\n\telif \"T\" in size or \"t\" in size:\n\t\tsize_in_bytes = int(size.replace(\"T\", \"\")) * size_unit_map[\"TB\"]\n\telse:\n\t\tsize_in_bytes = int(size)\n\n\treturn size_in_bytes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsuffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']\n\tsize = size.upper()\n\tnum, suffix = size.split(' ')\n\tnum = float(num)\n\tsuffix = suffix.upper()\n\tsuffix_index = suffixes.index(suffix)\n\treturn num * (1024 ** suffix_index)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\ttry:\n\t\tsize = size.lower()\n\t\tmultiplier = 1\n\t\tif size.endswith('k'):\n\t\t\tmultiplier = 1000\n\t\telif size.endswith('m'):\n\t\t\tmultiplier = 1000000\n\t\telif size.endswith('g'):\n\t\t\tmultiplier = 1000000000\n\t\telif size.endswith('t'):\n\t\t\tmultiplier = 1000000000000\n\t\telif size.endswith('p'):\n\t\t\tmultiplier = 1000000000000000\n\t\telif size.endswith('kib'):\n\t\t\tmultiplier = 1024\n\t\telif size.endswith('mib'):\n\t\t\tmultiplier = 1024**2\n\t\telif size.endswith('gib'):\n\t\t\tmultiplier = 1024**3\n\t\telif size.endswith('tib'):\n\t\t\tmultiplier = 1024**4\n\t\telif size.endswith('pib'):\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tvalue = size[:-1]\n\tif size[-1].isdigit():\n\t\treturn int(value)\n\tif size[-1] == 'G':\n\t\treturn int(value) * 1024\n\tif size[-1] == 'M':\n\t\treturn int(value) * 1024 * 1024\n\tif size[-1] == 'K':\n\t\treturn int(value) * 1024 * 1024 * 1024\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.upper()\n\tif size.endswith('G'):\n\t\treturn int(float(size[:-1]) * 1024 * 1024 * 1024)\n\telif size.endswith('M'):\n\t\treturn int(float(size[:-1]) * 1024 * 1024)\n\telif size.endswith('K'):\n\t\treturn int(float(size[:-1]) * 1024)\n\telse:\n\t\treturn int(size)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.replace(\" \", \"\")\n\tsize = size.replace(\"KB\", \"000\")\n\tsize = size.replace(\"MB\", \"000000\")\n\tsize = size.replace(\"GB\", \"000000000\")\n\tsize = size.replace(\"TB\", \"000000000000\")\n\tsize = size.replace(\"PB\", \"000000000000000\")\n\tsize = size.replace(\"EB\", \"00000000000000000\")\n\treturn int(size)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.upper()\n\tsize_bytes = 0\n\tif 'K' in size:\n\t\tsize_bytes = int(size.replace('K', '')) * 1024\n\telif 'M' in size:\n\t\tsize_bytes = int(size.replace('M', '')) * 1024 * 1024\n\telif 'G' in size:\n\t\tsize_bytes = int(size.replace('G', '')) * 1024 * 1024 * 1024\n\telse:\n\t\traise Exception('Incorrect file size input')\n\treturn size_bytes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tunit = size[-1]\n\tvalue = float(size[:-1])\n\tif unit == 'K':\n\t\treturn int(value * 1024)\n\telif unit == 'M':\n\t\treturn int(value * 1024 * 1024)\n\telif unit == 'G':\n\t\treturn int(value * 1024 * 1024 * 1024)\n\telif unit == 'T':\n\t\treturn int(value * 1024 * 1024 * 1024 * 1024)\n\telse:\n\t\traise ValueError(f'size {size} is not a valid size')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize = size.upper()\n\tunit = size[-1]\n\tsize = float(size[:-1])\n\tif unit == \"K\":\n\t\treturn int(size * 1000)\n\tif unit == \"M\":\n\t\treturn int(size * 1000000)\n\tif unit == \"G\":\n\t\treturn int(size * 1000000000)\n\tif unit == \"T\":\n\t\treturn int(size * 1000000000000)\n\tif unit == \"P\":\n\t\treturn int(size * 100000000000000)\n\tif unit == \"E\":\n\t\treturn int(size * 10000000000000000)\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t\"\"\"\n\tConvert human readable file size to bytes\n\n    Args:\n            size: str, a string representing a human readable file size (eg: '500K')\n    Returns:\n            int: file size in bytes\n\t\"\"\"\n\tsize_suffixes = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024**2,\n\t\t'G': 1024**3,\n\t\t'T': 1024**4,\n\t\t'P': 1024**5,\n\t\t'E': 1024**6,\n\t\t'Z': 1024**7,\n\t\t'Y': 1024**8,\n\t}\n\n\tsize = size.upper()\n\n\tif size[-1] in size_suffixes:\n\t\tsize = size[:-1]\n\n\ttry:\n\t\treturn int(size) * size_suffixes[size[-1]]\n\texcept KeyError:\n\t\traise ValueError('Invalid file size: {}'.format(size))\n\n#src/tests/test_file_size.py\nimport unittest\nfrom file_size import size_to_bytes\n", "is_pass": false, "return_code": 1}]}
{"file_path": "contrib/planb-swiftsync.py", "project": "ossobv/planb", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tresult = {}\n\tfor dic in dicts:\n\t\tfor key in dic:\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = dic[key]\n\t\t\telse:\n\t\t\t\tresult[key] += dic[key]\n\treturn result\n\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.iteritems():\n\t\t\tdic[k] = dic.get(k, 0) + v\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tres = {}\n\tfor key in dicts[0]:\n\t\tres[key] = dicts[0][key]\n\tfor dic in dicts[1:]:\n\t\tfor key in dic:\n\t\t\tres[key] += dic[key]\n\treturn res\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += v\n\t\t\telse:\n\t\t\t\tdic[k] = v\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = dict()\n\tfor d in dicts:\n\t\tfor key in d:\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += d[key]\n\t\t\telse:\n\t\t\t\tdic[key] = d[key]\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += v\n\t\t\telse:\n\t\t\t\tdic[k] = v\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k not in dic:\n\t\t\t\tdic[k] = v\n\t\t\telse:\n\t\t\t\tdic[k] += v\n\treturn dic\n", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += v\n\t\t\telse:\n\t\t\t\tdic[k] = v\n\treturn dic", "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tdic[k] = dic.get(k, 0) + v\n\treturn dic\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tresult = {}\n\tfor dic in dicts:\n\t\tfor key in dic:\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = dic[key]\n\t\t\telse:\n\t\t\t\tresult[key] += dic[key]\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.iteritems():\n\t\t\tdic[k] = dic.get(k, 0) + v\n\treturn dic\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tres = {}\n\tfor key in dicts[0]:\n\t\tres[key] = dicts[0][key]\n\tfor dic in dicts[1:]:\n\t\tfor key in dic:\n\t\t\tres[key] += dic[key]\n\treturn res\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += v\n\t\t\telse:\n\t\t\t\tdic[k] = v\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = dict()\n\tfor d in dicts:\n\t\tfor key in d:\n\t\t\tif key in dic:\n\t\t\t\tdic[key] += d[key]\n\t\t\telse:\n\t\t\t\tdic[key] = d[key]\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += v\n\t\t\telse:\n\t\t\t\tdic[k] = v\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k not in dic:\n\t\t\t\tdic[k] = v\n\t\t\telse:\n\t\t\t\tdic[k] += v\n\treturn dic\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dic:\n\t\t\t\tdic[k] += v\n\t\t\telse:\n\t\t\t\tdic[k] = v\n\treturn dic", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\t\"\"\"\n\tSum values with the same key in the dictionaries. And return a new dictionary.\n    Example:\n        given two dictionaries: dict1: {'a': 1, 'b': 2}, dict2: {'a': 5, 'b': 0}\n        return a dictionary: dic:{'a': 6, 'b': 2}\n    Returns: dict\n\t\"\"\"\n\tdic = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tdic[k] = dic.get(k, 0) + v\n\treturn dic\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pyseed/apibase.py", "project": "SEED-platform/py-seed", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\tfor key in url_args_keys:\n\t\turl = url.replace('{' + key + '}', url_args[key])\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\turl_args_keys = url_args.keys()\n\turl_args_values = url_args.values()\n\t\n\tfor key in url_args_keys:\n\t\turl = url.replace(key, url_args_values[url_args_keys.index(key)])\n\t\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\turl_args_values = url_args.values()\n\tfor i in range(len(url_args_keys)):\n\t\turl = url.replace(url_args_keys[i], url_args_values[i])\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl = url.replace(\"{\", \"\").replace(\"}\", \"\")\n\tif url_args is None:\n\t\turl_args = {}\n\tfor key, value in url_args.items():\n\t\turl = url.replace(key, value)\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl = url.replace(\"%s\", \"%s\")\n\tfor key, value in url_args.items():\n\t\turl = url.replace(\"%({0})s\".format(key), url_args[key])\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_arg_keys = url_args.keys()\n\turl_arg_args = url_args.values()\n\turl_arg_args_len = len(url_arg_args)\n\n\tfor i in range(url_arg_args_len):\n\t\turl_arg_key = url_arg_keys[i]\n\t\turl_arg_value = url_arg_args[i]\n\t\turl = url.replace(url_arg_key, url_arg_value)\n\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_keys = url_args.keys()\n\tfor key in url_keys:\n\t\tif url.find(key) != -1:\n\t\t\turl = url.replace(key, str(url_args[key]))\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args or {}\n\turl_keys = list(url_args.keys())\n\tfor k, v in url_args.items():\n\t\turl = url.replace(k, v)\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\tif url_args_keys:\n\t\tfor key in url_args_keys:\n\t\t\turl = url.replace('%s' % key, url_args[key])\n\treturn url\n", "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, value)\n\treturn url\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\tfor key in url_args_keys:\n\t\turl = url.replace('{' + key + '}', url_args[key])\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\turl_args_keys = url_args.keys()\n\turl_args_values = url_args.values()\n\t\n\tfor key in url_args_keys:\n\t\turl = url.replace(key, url_args_values[url_args_keys.index(key)])\n\t\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\turl_args_values = url_args.values()\n\tfor i in range(len(url_args_keys)):\n\t\turl = url.replace(url_args_keys[i], url_args_values[i])\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl = url.replace(\"{\", \"\").replace(\"}\", \"\")\n\tif url_args is None:\n\t\turl_args = {}\n\tfor key, value in url_args.items():\n\t\turl = url.replace(key, value)\n\treturn url\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl = url.replace(\"%s\", \"%s\")\n\tfor key, value in url_args.items():\n\t\turl = url.replace(\"%({0})s\".format(key), url_args[key])\n\treturn url\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_arg_keys = url_args.keys()\n\turl_arg_args = url_args.values()\n\turl_arg_args_len = len(url_arg_args)\n\n\tfor i in range(url_arg_args_len):\n\t\turl_arg_key = url_arg_keys[i]\n\t\turl_arg_value = url_arg_args[i]\n\t\turl = url.replace(url_arg_key, url_arg_value)\n\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_keys = url_args.keys()\n\tfor key in url_keys:\n\t\tif url.find(key) != -1:\n\t\t\turl = url.replace(key, str(url_args[key]))\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args = url_args or {}\n\turl_keys = list(url_args.keys())\n\tfor k, v in url_args.items():\n\t\turl = url.replace(k, v)\n\treturn url\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\turl_args_keys = url_args.keys()\n\tif url_args_keys:\n\t\tfor key in url_args_keys:\n\t\t\turl = url.replace('%s' % key, url_args[key])\n\treturn url\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\t\"\"\"\n\tReplace the value in url with the value in url_args\n    If url_args has a value, iterate over the keys and values from url_args.\n    Then replace the keys of the first parameter with values.\n    Returns: the modified url.\n\t\"\"\"\n\t\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, value)\n\treturn url\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cinder/api/api_utils.py", "project": "openstack/cinder", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval = str(val).lower()\n\treturn val == 'none'", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val == 'none' or (isinstance(val, str) and val.lower() == 'none')\n\texcept:\n\t\treturn False\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\tif type(val) == str:\n\t\t\treturn val.lower() == 'none'\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif isinstance(val, str):\n\t\treturn val.lower() == 'none'\n\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif type(val) == str:\n\t\treturn val == 'none'\n\t\n\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval_string = str(val)\n\tval_lowercase = val_string.lower()\n\treturn val_lowercase == 'none'\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval = str(val).lower()\n\treturn val == 'none'", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\treturn type(val) is str and val.lower() == \"none\"\n\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn isinstance(val, str) and val.lower() == 'none'\n\texcept:\n\t\treturn False\n", "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept AttributeError:\n\t\tpass\n\treturn False"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval = str(val).lower()\n\treturn val == 'none'", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val == 'none' or (isinstance(val, str) and val.lower() == 'none')\n\texcept:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\tif type(val) == str:\n\t\t\treturn val.lower() == 'none'\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif isinstance(val, str):\n\t\treturn val.lower() == 'none'\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\tif type(val) == str:\n\t\treturn val == 'none'\n\t\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval_string = str(val)\n\tval_lowercase = val_string.lower()\n\treturn val_lowercase == 'none'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\tval = str(val).lower()\n\treturn val == 'none'", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\t\n\treturn type(val) is str and val.lower() == \"none\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn isinstance(val, str) and val.lower() == 'none'\n\texcept:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t\"\"\"\n\tCheck if a string represents a None value.\n    Returns:\n        Return True if the type of val is string and the lowercase of val is equal to 'none', otherwise return False\n\t\"\"\"\n\ttry:\n\t\treturn val.lower() == 'none'\n\texcept AttributeError:\n\t\tpass\n\treturn False", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\treturn path.replace(root, \"\")\n\texcept Exception:\n\t\traise ValueError(\"Error stripping root\")", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\troot_len = len(root)\n\tif root_len > 0 and path.startswith(root):\n\t\treturn path[root_len:]\n\telse:\n\t\traise Exception(\"path %s does not start with %s\" % (path, root))\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\treturn path[len(root):]\n\texcept IndexError:\n\t\traise Exception(\"Invalid root path\")\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root):]\n\t\telif path.startswith(root + \"/\"):\n\t\t\treturn path[len(root) + 1:]\n\t\telse:\n\t\t\traise Exception(\"Root not found\")\n\texcept Exception as e:\n\t\tprint(e)\n\t\traise", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\treturn path.removeprefix(root)\n\texcept ValueError:\n\t\traise ValueError(\"Path does not start with root\")\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tname = path.lstrip(root)\n\t\tname = name.lstrip('/')\n\t\treturn name\n\texcept:\n\t\traise ValueError('Failed to strip path root')\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath = str(path)\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\treturn path\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root):]\n\t\telse:\n\t\t\traise ValueError(\"Path not in root\")\n\texcept ValueError as e:\n\t\traise e\n\n#src/utils/system.py\nimport os\nimport subprocess\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tpath = path.lower()\n\t\troot = root.lower()\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root):]\n\t\telse:\n\t\t\traise ValueError(\"Root mismatch\")\n\texcept Exception as e:\n\t\traise ValueError(f\"Error: {e}\")\n\n", "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath = path.lstrip(root)\n\tpath = path.lstrip('/')\n\tif path.startswith('/'):\n\t\traise Exception(\"Path starts with '/'\")\n\treturn path\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\treturn path.replace(root, \"\")\n\texcept Exception:\n\t\traise ValueError(\"Error stripping root\")", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\troot_len = len(root)\n\tif root_len > 0 and path.startswith(root):\n\t\treturn path[root_len:]\n\telse:\n\t\traise Exception(\"path %s does not start with %s\" % (path, root))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\treturn path[len(root):]\n\texcept IndexError:\n\t\traise Exception(\"Invalid root path\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root):]\n\t\telif path.startswith(root + \"/\"):\n\t\t\treturn path[len(root) + 1:]\n\t\telse:\n\t\t\traise Exception(\"Root not found\")\n\texcept Exception as e:\n\t\tprint(e)\n\t\traise", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\treturn path.removeprefix(root)\n\texcept ValueError:\n\t\traise ValueError(\"Path does not start with root\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tname = path.lstrip(root)\n\t\tname = name.lstrip('/')\n\t\treturn name\n\texcept:\n\t\traise ValueError('Failed to strip path root')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath = str(path)\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\treturn path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root):]\n\t\telse:\n\t\t\traise ValueError(\"Path not in root\")\n\texcept ValueError as e:\n\t\traise e\n\n#src/utils/system.py\nimport os\nimport subprocess\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\ttry:\n\t\tpath = path.lower()\n\t\troot = root.lower()\n\t\tif path.startswith(root):\n\t\t\treturn path[len(root):]\n\t\telse:\n\t\t\traise ValueError(\"Root mismatch\")\n\texcept Exception as e:\n\t\traise ValueError(f\"Error: {e}\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\t\"\"\"\n\tRemove root from path. If fails, throw exception\n\n    Returns:\n        A path without root\n\t\"\"\"\n\tpath = path.lstrip(root)\n\tpath = path.lstrip('/')\n\tif path.startswith('/'):\n\t\traise Exception(\"Path starts with '/'\")\n\treturn path\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser._action_groups:\n\t\tflags += \" \" + flag.title\n\treturn flags\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\targs = parser.parse_args()\n\tflags = [\n\t\tstr(arg) + '=' + str(getattr(args, arg))\n\t\tfor arg in dir(args)\n\t\tif arg not in ['func', 'help', 'func_name']\n\t]\n\treturn ' '.join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\targs_from_parser = [f for f in parser._actions if isinstance(f, argparse.Action)]\n\tflags = ''\n\tfor arg in args_from_parser:\n\t\tflags += f'{arg.dest} '\n\treturn flags\n\n#src/utils/__init__.py\n\"\"\"\nThis module contains a set of general utilities that are useful to other modules.\n\"\"\"\n\n# Import the modules of this package.\nfrom .parser import parser_flags\nfrom .parser import parse_flags\nfrom .parser import parse_file\nfrom .parser import argparse_wrap\nfrom .parser import argparse_wrap_file\nfrom .parser import argparse_wrap_file_args\nfrom .parser import argparse_wrap_file_kwargs\n\n#src/utils/__main__.py\n\"\"\"\nThis module contains a set of general utilities that are useful to other modules.\n\"\"\"", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = ''\n\tfor flag in parser.option_list:\n\t\tif flag.dest == 'help':\n\t\t\tcontinue\n\t\tflags += flag.dest + ' '\n\treturn flags\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\targs = parser.parse_args()\n\n\treturn ' '.join(args.__dict__.keys()).strip()\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tparser.print_help()\n\tfor action in parser._actions:\n\t\tif action.dest:\n\t\t\tflags.append('--%s' % action.dest)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n\n#src/utils/file_utils.py\nimport glob\nimport os\nimport pickle\nimport random\nimport re\n\nfrom collections import Counter\nfrom collections import defaultdict\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor arg in parser._action_groups:\n\t\tif arg.title:\n\t\t\tflags.append(arg.title)\n\t\tfor action in arg._group_actions:\n\t\t\tif not action.option_strings:\n\t\t\t\tcontinue\n\t\t\tflags.append(\" \".join(action.option_strings))\n\treturn \" \".join(flags)\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._action_groups.values():\n\t\tflags += flag.option_strings\n\treturn \" \".join(flags)\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor arg in parser.add_argument:\n\t\tflags += arg[1] + \" \"\n\treturn flags\n\n#dynastream/command_line.py\n\"\"\"\nModule for command-line interface (CLI) functionality.\n\"\"\"\n\nimport argparse\nimport os\nimport subprocess\nimport sys\n\nfrom . import parser_flags\nfrom . import util\n\n", "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser._actions:\n\t\tif flag.dest == \"help\":\n\t\t\tcontinue\n\t\tflags += \" \" + flag.name\n\treturn flags"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser._action_groups:\n\t\tflags += \" \" + flag.title\n\treturn flags\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\targs = parser.parse_args()\n\tflags = [\n\t\tstr(arg) + '=' + str(getattr(args, arg))\n\t\tfor arg in dir(args)\n\t\tif arg not in ['func', 'help', 'func_name']\n\t]\n\treturn ' '.join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\targs_from_parser = [f for f in parser._actions if isinstance(f, argparse.Action)]\n\tflags = ''\n\tfor arg in args_from_parser:\n\t\tflags += f'{arg.dest} '\n\treturn flags\n\n#src/utils/__init__.py\n\"\"\"\nThis module contains a set of general utilities that are useful to other modules.\n\"\"\"\n\n# Import the modules of this package.\nfrom .parser import parser_flags\nfrom .parser import parse_flags\nfrom .parser import parse_file\nfrom .parser import argparse_wrap\nfrom .parser import argparse_wrap_file\nfrom .parser import argparse_wrap_file_args\nfrom .parser import argparse_wrap_file_kwargs\n\n#src/utils/__main__.py\n\"\"\"\nThis module contains a set of general utilities that are useful to other modules.\n\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = ''\n\tfor flag in parser.option_list:\n\t\tif flag.dest == 'help':\n\t\t\tcontinue\n\t\tflags += flag.dest + ' '\n\treturn flags\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\targs = parser.parse_args()\n\n\treturn ' '.join(args.__dict__.keys()).strip()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tparser.print_help()\n\tfor action in parser._actions:\n\t\tif action.dest:\n\t\t\tflags.append('--%s' % action.dest)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n\n#src/utils/file_utils.py\nimport glob\nimport os\nimport pickle\nimport random\nimport re\n\nfrom collections import Counter\nfrom collections import defaultdict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor arg in parser._action_groups:\n\t\tif arg.title:\n\t\t\tflags.append(arg.title)\n\t\tfor action in arg._group_actions:\n\t\t\tif not action.option_strings:\n\t\t\t\tcontinue\n\t\t\tflags.append(\" \".join(action.option_strings))\n\treturn \" \".join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = []\n\tfor flag in parser._action_groups.values():\n\t\tflags += flag.option_strings\n\treturn \" \".join(flags)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor arg in parser.add_argument:\n\t\tflags += arg[1] + \" \"\n\treturn flags\n\n#dynastream/command_line.py\n\"\"\"\nModule for command-line interface (CLI) functionality.\n\"\"\"\n\nimport argparse\nimport os\nimport subprocess\nimport sys\n\nfrom . import parser_flags\nfrom . import util\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\t\"\"\"\n\tGiven an argparse.ArgumentParser instance, return its argument flags in a space-separated string.\n    Args:\n        script: argparse.ArgumentParser instance\n\n    Returns:\n        argument flags in a space-separated string\n\t\"\"\"\n\tflags = \"\"\n\tfor flag in parser._actions:\n\t\tif flag.dest == \"help\":\n\t\t\tcontinue\n\t\tflags += \" \" + flag.name\n\treturn flags", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath = os.path.abspath(path_name)\n\tif path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Skipping: %s\" % path_name)\n\t\treturn True\n\tprocessed.add(path)\n\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\n\tif verbose:\n\t\tprint(' - {0}'.format(path_name))\n\n\tif path_name in processed:\n\t\tprint(' * {0} already processed'.format(path_name))\n\t\treturn True\n\n\tprocessed.add(path_name)\n\treturn False\n\n# ----------------------------------------------------------------------\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tabs_path = os.path.abspath(path_name)\n\texcept OSError as err:\n\t\tprint \"Error: {0}\".format(err)\n\t\treturn False\n\telse:\n\t\tif abs_path in processed:\n\t\t\tif verbose:\n\t\t\t\tprint \"{0} is already processed, skipping\".format(abs_path)\n\t\t\treturn True\n\t\telse:\n\t\t\tprocessed.add(abs_path)\n\t\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\ttry:\n\t\tprocessed.add(path_name)\n\texcept KeyError:\n\t\tif verbose:\n\t\t\tprint('%s already processed. Skipping...' % path_name)\n\t\treturn True\n\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\timport os\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"{0} is already processed.\".format(path_name))\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tpath_name = os.path.abspath(path_name)\n\texcept:\n\t\tif verbose:\n\t\t\tprint(\"Unable to get absolute path for: %s\" % path_name)\n\t\treturn False\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"The path has already been processed: %s\" % path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif verbose:\n\t\tprint('Checking if %s has already been processed.' % path_name)\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('%s has already been processed and will not be processed again.' % path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"File or directory already processed: %s\" % path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tpath_name = os.path.abspath(path_name)\n\texcept OSError:\n\t\tpath_name = str(path_name)\n\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint \"Path '%s' has already been processed\" % path_name\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed: %s\" % path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath = os.path.abspath(path_name)\n\tif path in processed:\n\t\tif verbose:\n\t\t\tprint(\"Skipping: %s\" % path_name)\n\t\treturn True\n\tprocessed.add(path)\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\n\tif verbose:\n\t\tprint(' - {0}'.format(path_name))\n\n\tif path_name in processed:\n\t\tprint(' * {0} already processed'.format(path_name))\n\t\treturn True\n\n\tprocessed.add(path_name)\n\treturn False\n\n# ----------------------------------------------------------------------\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tabs_path = os.path.abspath(path_name)\n\texcept OSError as err:\n\t\tprint \"Error: {0}\".format(err)\n\t\treturn False\n\telse:\n\t\tif abs_path in processed:\n\t\t\tif verbose:\n\t\t\t\tprint \"{0} is already processed, skipping\".format(abs_path)\n\t\t\treturn True\n\t\telse:\n\t\t\tprocessed.add(abs_path)\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\ttry:\n\t\tprocessed.add(path_name)\n\texcept KeyError:\n\t\tif verbose:\n\t\t\tprint('%s already processed. Skipping...' % path_name)\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\timport os\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"{0} is already processed.\".format(path_name))\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tpath_name = os.path.abspath(path_name)\n\texcept:\n\t\tif verbose:\n\t\t\tprint(\"Unable to get absolute path for: %s\" % path_name)\n\t\treturn False\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"The path has already been processed: %s\" % path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif verbose:\n\t\tprint('Checking if %s has already been processed.' % path_name)\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('%s has already been processed and will not be processed again.' % path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"File or directory already processed: %s\" % path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\ttry:\n\t\tpath_name = os.path.abspath(path_name)\n\texcept OSError:\n\t\tpath_name = str(path_name)\n\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint \"Path '%s' has already been processed\" % path_name\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t\"\"\"\n\tCheck if a file or directory has already been processed.\n\n    To prevent recursion, expand the path name to an absolution path\n    call this function with a set that will store all the entries and\n    the entry to test. If the entry is already in the set, report the issue\n    and return ``True``. Otherwise, add the entry to the set and return\n    ``False`` to allow the path to be processed.\n\n    Args:\n        processed: Set to store processed pathnames\n        path_name: Path to a directory or file\n        verbose: True if verbose output is requested\n\n    Returns:\n        True if it's already in the set. False if not.\n\t\"\"\"\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed: %s\" % path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "eppy/geometry/surface.py", "project": "santoshphilip/eppy", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\tvertexTuples = []\n\t\n\tfor vertex in vertices:\n\t\tvertexTuple = []\n\t\tfor i in range(3):\n\t\t\tif i == 0:\n\t\t\t\tvertexTuple.append(vertex)\n\t\t\telif i == 1:\n\t\t\t\tvertexTuple.append(vertices[vertex - 1])\n\t\t\telif i == 2:\n\t\t\t\tvertexTuple.append(vertices[vertex - 1])\n\t\t\t\tbreak\n\t\tvertexTuples.append(tuple(vertexTuple))\n\t\n\treturn vertexTuples\n\n#src/vertex3tuple/test.py\nimport unittest\nfrom vertex3tuple import vertex3tuple\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tconverted = []\n\tfor vertex in vertices:\n\t\tif len(vertex) == 3:\n\t\t\tconverted.append((vertex[0], vertex[1], vertex[2]))\n\t\telse:\n\t\t\tconverted.append((vertex[0], vertex[1], 0))\n\treturn converted\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices = np.array(vertices)\n\tres = []\n\tfor i in range(len(vertices)):\n\t\tres.append((vertices[i], vertices[(i+1)%len(vertices)]))\n\treturn res\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex3tuples = []\n\tfor vertex in vertices:\n\t\tvertex3tuple = []\n\n\t\ttry:\n\t\t\tvertex3tuple = [vertex.get_vertex_left(), vertex.get_vertex_right(), vertex.get_vertex_left()]\n\t\texcept:\n\t\t\tpass\n\n\t\ttry:\n\t\t\tvertex3tuple = [vertex.get_vertex_right(), vertex.get_vertex_left(), vertex.get_vertex_right()]\n\t\texcept:\n\t\t\tpass\n\n\t\tvertex3tuples.append(tuple(vertex3tuple))\n\n\treturn vertex3tuples\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tpolygon = []\n\tfor i in range(len(vertices)):\n\t\tif i == 0:\n\t\t\tpolygon.append((vertices[i], vertices[i+1], vertices[0]))\n\t\telif i == len(vertices)-1:\n\t\t\tpolygon.append((vertices[i], vertices[0], vertices[i]))\n\t\telse:\n\t\t\tpolygon.append((vertices[i], vertices[i+1], vertices[i-1]))\n\treturn polygon\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_tuple = []\n\tfor v in vertices:\n\t\tvertex_tuple.append((v[0], v[1], 0))\n\t\tvertex_tuple.append((v[0], v[1], 1))\n\t\tvertex_tuple.append((v[0], v[1], 2))\n\t\tvertex_tuple.append((v[0], v[1], 3))\n\treturn vertex_tuple\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tresult = []\n\tfor item in vertices:\n\t\tif len(vertices) > item:\n\t\t\tresult.append((item, item + 1))\n\t\t\tif item + 2 < len(vertices):\n\t\t\t\tresult.append((item, item + 2))\n\t\telse:\n\t\t\tresult.append((0, 0))\n\treturn result\n\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tconverted_vertices = []\n\tverts_len = len(vertices)\n\tfor i in range(verts_len):\n\t\tif i == verts_len - 1:\n\t\t\tconverted_vertices.append(vertices[i] + vertices[0])\n\t\telse:\n\t\t\tconverted_vertices.append((vertices[i] + vertices[i + 1]))\n\treturn converted_vertices\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv3_list = []\n\tn = len(vertices)\n\tv1 = vertices[0]\n\tv2 = vertices[1]\n\n\tfor i in range(n):\n\t\tv3_list.append((v1, v2, vertices[(i+2) % n]))\n\treturn v3_list\n", "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv_list = []\n\tfor i in range(len(vertices)):\n\t\tv_list.append((vertices[i], vertices[i-1] if i-1 >= 0 else vertices[i+1] if i+1 < len(vertices) else vertices[0]))\n\treturn v_list\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\t\n\tvertexTuples = []\n\t\n\tfor vertex in vertices:\n\t\tvertexTuple = []\n\t\tfor i in range(3):\n\t\t\tif i == 0:\n\t\t\t\tvertexTuple.append(vertex)\n\t\t\telif i == 1:\n\t\t\t\tvertexTuple.append(vertices[vertex - 1])\n\t\t\telif i == 2:\n\t\t\t\tvertexTuple.append(vertices[vertex - 1])\n\t\t\t\tbreak\n\t\tvertexTuples.append(tuple(vertexTuple))\n\t\n\treturn vertexTuples\n\n#src/vertex3tuple/test.py\nimport unittest\nfrom vertex3tuple import vertex3tuple\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tconverted = []\n\tfor vertex in vertices:\n\t\tif len(vertex) == 3:\n\t\t\tconverted.append((vertex[0], vertex[1], vertex[2]))\n\t\telse:\n\t\t\tconverted.append((vertex[0], vertex[1], 0))\n\treturn converted\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertices = np.array(vertices)\n\tres = []\n\tfor i in range(len(vertices)):\n\t\tres.append((vertices[i], vertices[(i+1)%len(vertices)]))\n\treturn res\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex3tuples = []\n\tfor vertex in vertices:\n\t\tvertex3tuple = []\n\n\t\ttry:\n\t\t\tvertex3tuple = [vertex.get_vertex_left(), vertex.get_vertex_right(), vertex.get_vertex_left()]\n\t\texcept:\n\t\t\tpass\n\n\t\ttry:\n\t\t\tvertex3tuple = [vertex.get_vertex_right(), vertex.get_vertex_left(), vertex.get_vertex_right()]\n\t\texcept:\n\t\t\tpass\n\n\t\tvertex3tuples.append(tuple(vertex3tuple))\n\n\treturn vertex3tuples\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tpolygon = []\n\tfor i in range(len(vertices)):\n\t\tif i == 0:\n\t\t\tpolygon.append((vertices[i], vertices[i+1], vertices[0]))\n\t\telif i == len(vertices)-1:\n\t\t\tpolygon.append((vertices[i], vertices[0], vertices[i]))\n\t\telse:\n\t\t\tpolygon.append((vertices[i], vertices[i+1], vertices[i-1]))\n\treturn polygon\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tvertex_tuple = []\n\tfor v in vertices:\n\t\tvertex_tuple.append((v[0], v[1], 0))\n\t\tvertex_tuple.append((v[0], v[1], 1))\n\t\tvertex_tuple.append((v[0], v[1], 2))\n\t\tvertex_tuple.append((v[0], v[1], 3))\n\treturn vertex_tuple\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tresult = []\n\tfor item in vertices:\n\t\tif len(vertices) > item:\n\t\t\tresult.append((item, item + 1))\n\t\t\tif item + 2 < len(vertices):\n\t\t\t\tresult.append((item, item + 2))\n\t\telse:\n\t\t\tresult.append((0, 0))\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tconverted_vertices = []\n\tverts_len = len(vertices)\n\tfor i in range(verts_len):\n\t\tif i == verts_len - 1:\n\t\t\tconverted_vertices.append(vertices[i] + vertices[0])\n\t\telse:\n\t\t\tconverted_vertices.append((vertices[i] + vertices[i + 1]))\n\treturn converted_vertices\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv3_list = []\n\tn = len(vertices)\n\tv1 = vertices[0]\n\tv2 = vertices[1]\n\n\tfor i in range(n):\n\t\tv3_list.append((v1, v2, vertices[(i+2) % n]))\n\treturn v3_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t\"\"\"\n\tGet 3 points for each vertex of the polygon.\n    This will include the vertex and the 2 points on both sides of the vertex\n    If the subscript is out of bounds, take the value of index as 0\n    Args:\n        vertices: vertices to be converted\n\n    Returns:\n        A list where the elements of vertices represented by tuple\n\t\"\"\"\n\tv_list = []\n\tfor i in range(len(vertices)):\n\t\tv_list.append((vertices[i], vertices[i-1] if i-1 >= 0 else vertices[i+1] if i+1 < len(vertices) else vertices[0]))\n\treturn v_list\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\toutput = []\n\t\n\twhile number > 0:\n\t\toutput.append(alphabet[number % 36])\n\t\tnumber = number // 36\n\t\n\toutput = output[::-1]\n\t\n\tif padding is not None:\n\t\twhile len(output) < padding:\n\t\t\toutput.append('0')\n\t\n\treturn ''.join(output)\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tdigits = []\n\t\n\twhile number:\n\t\tdigits.append(number % 10)\n\t\tnumber //= 10\n\t\n\tif padding is not None and len(digits) < padding:\n\t\tdigits = [0] * (padding - len(digits)) + digits\n\t\n\tstring = \"\"\n\tfor d in digits:\n\t\tstring += alphabet[d]\n\t\n\treturn string\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tstring = ''\n\t\n\twhile number > 0:\n\t\tif number % len(alphabet) == 0:\n\t\t\tstring = alphabet[0] + string\n\t\t\tnumber = number // len(alphabet) - 1\n\t\telse:\n\t\t\tstring = alphabet[number % len(alphabet)] + string\n\t\t\tnumber = number // len(alphabet)\n\t\t\t\n\tif padding:\n\t\tstring = string.zfill(padding)\n\t\n\treturn string\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n    # Initialize variables\n\tstring = ''\n\t\n    # If padding is specified\n\tif padding is not None:\n\t\t# If number is shorter than padding\n\t\tif len(str(number)) < padding:\n\t\t\t# Add leading zeros to number\n\t\t\tnumber = \"0\" * (padding - len(str(number))) + str(number)\n\t\t\n\t\t# Loop through each digit in number\n\t\tfor digit in str(number):\n\t\t\t# Add digit to string\n\t\t\tstring += alphabet[int(digit)]\n\t\n\t# If padding is not specified\n\telse:\n\t\t# Loop through each digit in number\n\t\tfor digit in str(number):\n\t\t\t# Add digit to string\n\t\t\tstring += alphabet[int(digit)]\n\t\n\t# Return string\n\treturn string\n\n#src/utils/__init__.py\nfrom .int_to_string import int_to_string\n\n__all__ = [\n    \"int_to_string\"\n]", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\talphabet_size = len(alphabet)\n\tif padding is None:\n\t\tpadding = -1\n\t\n\tstring_length = len(str(number))\n\tstring = \"\"\n\t\n\tdigits = []\n\t\n\tnumber_copy = number\n\t\n\twhile number_copy >= 1:\n\t\tdigits.append(number_copy % alphabet_size)\n\t\tnumber_copy = number_copy // alphabet_size\n\t\n\tdigits.reverse()\n\t\n\t\n\tfor digit in digits:\n\t\tstring += alphabet[digit]\n\t\n\tif padding > 0:\n\t\tstring = string.ljust(padding, \"0\")\n\t\n\treturn string\n\n#util/string_to_int.py\nfrom typing import List, Optional\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tcharacters = []\n\twhile number:\n\t\tnumber, remainder = divmod(number, len(alphabet))\n\t\tcharacters.append(alphabet[remainder])\n\tif padding:\n\t\tcharacters = characters + ['0'] * (padding - len(characters))\n\treturn ''.join(reversed(characters))\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tstring = \"\"\n\n\twhile number != 0:\n\t\tstring += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\n\tstring = string[::-1]\n\tif padding is not None:\n\t\tstring += \"0\" * (padding - len(string))\n\treturn string", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\talphabet_size = len(alphabet)\n\tdigits = []\n\twhile number > 0:\n\t\tdigits.append(alphabet[number % alphabet_size])\n\t\tnumber //= alphabet_size\n\tdigits.reverse()\n\tif padding is not None:\n\t\treturn \"\".join(digits)[:padding]\n\treturn \"\".join(digits)\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tstring = ''\n\tisNegative = False\n\tif number < 0:\n\t\tisNegative = True\n\t\tnumber *= -1\n\n\twhile number > 0:\n\t\tstring = alphabet[number % len(alphabet)] + string\n\t\tnumber //= len(alphabet)\n\n\tif isNegative:\n\t\tstring = '-' + string\n\n\tif padding:\n\t\tstring = string.rjust(padding, '0')\n\n\treturn string", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = \"\"\n\t\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\t\t\n\tif padding is not None:\n\t\tresult = (\"0\" * (padding - len(result))) + result\n\t\t\n\treturn result\n"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\toutput = []\n\t\n\twhile number > 0:\n\t\toutput.append(alphabet[number % 36])\n\t\tnumber = number // 36\n\t\n\toutput = output[::-1]\n\t\n\tif padding is not None:\n\t\twhile len(output) < padding:\n\t\t\toutput.append('0')\n\t\n\treturn ''.join(output)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tdigits = []\n\t\n\twhile number:\n\t\tdigits.append(number % 10)\n\t\tnumber //= 10\n\t\n\tif padding is not None and len(digits) < padding:\n\t\tdigits = [0] * (padding - len(digits)) + digits\n\t\n\tstring = \"\"\n\tfor d in digits:\n\t\tstring += alphabet[d]\n\t\n\treturn string\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tstring = ''\n\t\n\twhile number > 0:\n\t\tif number % len(alphabet) == 0:\n\t\t\tstring = alphabet[0] + string\n\t\t\tnumber = number // len(alphabet) - 1\n\t\telse:\n\t\t\tstring = alphabet[number % len(alphabet)] + string\n\t\t\tnumber = number // len(alphabet)\n\t\t\t\n\tif padding:\n\t\tstring = string.zfill(padding)\n\t\n\treturn string\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n    # Initialize variables\n\tstring = ''\n\t\n    # If padding is specified\n\tif padding is not None:\n\t\t# If number is shorter than padding\n\t\tif len(str(number)) < padding:\n\t\t\t# Add leading zeros to number\n\t\t\tnumber = \"0\" * (padding - len(str(number))) + str(number)\n\t\t\n\t\t# Loop through each digit in number\n\t\tfor digit in str(number):\n\t\t\t# Add digit to string\n\t\t\tstring += alphabet[int(digit)]\n\t\n\t# If padding is not specified\n\telse:\n\t\t# Loop through each digit in number\n\t\tfor digit in str(number):\n\t\t\t# Add digit to string\n\t\t\tstring += alphabet[int(digit)]\n\t\n\t# Return string\n\treturn string\n\n#src/utils/__init__.py\nfrom .int_to_string import int_to_string\n\n__all__ = [\n    \"int_to_string\"\n]", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\t\n\talphabet_size = len(alphabet)\n\tif padding is None:\n\t\tpadding = -1\n\t\n\tstring_length = len(str(number))\n\tstring = \"\"\n\t\n\tdigits = []\n\t\n\tnumber_copy = number\n\t\n\twhile number_copy >= 1:\n\t\tdigits.append(number_copy % alphabet_size)\n\t\tnumber_copy = number_copy // alphabet_size\n\t\n\tdigits.reverse()\n\t\n\t\n\tfor digit in digits:\n\t\tstring += alphabet[digit]\n\t\n\tif padding > 0:\n\t\tstring = string.ljust(padding, \"0\")\n\t\n\treturn string\n\n#util/string_to_int.py\nfrom typing import List, Optional\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tcharacters = []\n\twhile number:\n\t\tnumber, remainder = divmod(number, len(alphabet))\n\t\tcharacters.append(alphabet[remainder])\n\tif padding:\n\t\tcharacters = characters + ['0'] * (padding - len(characters))\n\treturn ''.join(reversed(characters))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tstring = \"\"\n\n\twhile number != 0:\n\t\tstring += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\n\tstring = string[::-1]\n\tif padding is not None:\n\t\tstring += \"0\" * (padding - len(string))\n\treturn string", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\talphabet_size = len(alphabet)\n\tdigits = []\n\twhile number > 0:\n\t\tdigits.append(alphabet[number % alphabet_size])\n\t\tnumber //= alphabet_size\n\tdigits.reverse()\n\tif padding is not None:\n\t\treturn \"\".join(digits)[:padding]\n\treturn \"\".join(digits)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tstring = ''\n\tisNegative = False\n\tif number < 0:\n\t\tisNegative = True\n\t\tnumber *= -1\n\n\twhile number > 0:\n\t\tstring = alphabet[number % len(alphabet)] + string\n\t\tnumber //= len(alphabet)\n\n\tif isNegative:\n\t\tstring = '-' + string\n\n\tif padding:\n\t\tstring = string.rjust(padding, '0')\n\n\treturn string", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t\"\"\"\n\tConvert a number to a string, using the given alphabet.\n    The number represents a short uuid.\n    The output has the most significant digit first.\n    @param number: Int value\n    @param alphabet : List with letters\n    @param padding : Optional with int value\n    @return  string value corresponded to int\n\t\"\"\"\n\tresult = \"\"\n\t\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\t\t\n\tif padding is not None:\n\t\tresult = (\"0\" * (padding - len(result))) + result\n\t\t\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neutron_lib/agent/common/utils.py", "project": "openstack/neutron-lib", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_value] = flow_params[register_number]\n\tdel flow_params[register_number]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tregister_value = flow_params[register_value]\n\tdel flow_params[register_value]\n\tflow_params[register_number] = register_value\n\t\n\treturn flow_params\n\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number], flow_params[register_value] = flow_params[register_value], flow_params[register_number]\n\tdel flow_params[register_value]", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n", "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_value] = flow_params[register_number]\n\tdel flow_params[register_number]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tregister_value = flow_params[register_value]\n\tdel flow_params[register_value]\n\tflow_params[register_number] = register_value\n\t\n\treturn flow_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number], flow_params[register_value] = flow_params[register_value], flow_params[register_number]\n\tdel flow_params[register_value]", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\t\"\"\"\n\tReplace the value of flow_params[register_number] with flow_params[register_value] and delete flow_params[register_value]\n\t\"\"\"\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "release_dashboard/templatetags/rd_extras.py", "project": "sipwise/repoapi", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = value.replace('.', arg)\n\treturn value\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n\tr = value.replace('.', arg)\n\treturn r\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\targ = arg.replace(\".\", \"\")\n\tvalue = value.replace(\".\", arg)\n\treturn value\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\targ = arg.replace('.', '')\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = value.replace('.', arg)\n\treturn value", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = str(value)\n\tif '.' not in value:\n\t\treturn value\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = str(value).replace('.',arg)\n\treturn value\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tstr_list = value.split('.')\n\tstr_list[0] = arg + str_list[0]\n\tstr_list[-1] = arg + str_list[-1]\n\treturn '.'.join(str_list)\n\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tnew_value = value.replace('.', arg)\n\treturn new_value\n\n# tests\n", "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n\tif (value[0] == '.'):\n\t\treturn arg + value[1:]\n\telse:\n\t\treturn value\n\t\n"], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = value.replace('.', arg)\n\treturn value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n\tr = value.replace('.', arg)\n\treturn r\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\targ = arg.replace(\".\", \"\")\n\tvalue = value.replace(\".\", arg)\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\targ = arg.replace('.', '')\n\treturn value.replace('.', arg)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = value.replace('.', arg)\n\treturn value", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = str(value)\n\tif '.' not in value:\n\t\treturn value\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tvalue = str(value).replace('.',arg)\n\treturn value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tstr_list = value.split('.')\n\tstr_list[0] = arg + str_list[0]\n\tstr_list[-1] = arg + str_list[-1]\n\treturn '.'.join(str_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\tnew_value = value.replace('.', arg)\n\treturn new_value\n\n# tests\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\t\"\"\"\n\tReplaces all values of '.' to arg from the given string\n    Args:\n        value: old string\n        arg: new string to replace '.'\n    Returns:\n        str, the replaced string\n\t\"\"\"\n\t\n\tif (value[0] == '.'):\n\t\treturn arg + value[1:]\n\telse:\n\t\treturn value\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "rows/utils/__init__.py", "project": "turicas/rows", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.append(subclass)\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.append(subclass)\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tall_subclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tall_subclasses.append(subclass)\n\t\tall_subclasses.extend(subclasses(subclass))\n\treturn all_subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = set(cls.__subclasses__())\n\tif cls.__bases__:\n\t\tsubclasses.update([cls.__subclasses__() for cls in cls.__bases__])\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = [cls]\n\tfor sub in cls.__subclasses__():\n\t\tsubclasses.extend(subclasses(sub))\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = []\n\tfor sub in cls.__subclasses__():\n\t\tsubs.append(sub)\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.append(subclass)\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tall_subclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tall_subclasses.append(subclass)\n\t\tall_subclasses.extend(subclasses(subclass))\n\treturn all_subclasses\n\nimport sys\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tyield cls\n\tfor subclass in cls.__subclasses__():\n\t\tyield from subclasses(subclass)\n\n", "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = set()\n\tfor sub in cls.__subclasses__():\n\t\tsubs.add(sub)\n\t\tsubs.update(subclasses(sub))\n\treturn subs\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.append(subclass)\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.append(subclass)\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tall_subclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tall_subclasses.append(subclass)\n\t\tall_subclasses.extend(subclasses(subclass))\n\treturn all_subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = set(cls.__subclasses__())\n\tif cls.__bases__:\n\t\tsubclasses.update([cls.__subclasses__() for cls in cls.__bases__])\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = [cls]\n\tfor sub in cls.__subclasses__():\n\t\tsubclasses.extend(subclasses(sub))\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = []\n\tfor sub in cls.__subclasses__():\n\t\tsubs.append(sub)\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tsubclasses.append(subclass)\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tall_subclasses = []\n\tfor subclass in cls.__subclasses__():\n\t\tall_subclasses.append(subclass)\n\t\tall_subclasses.extend(subclasses(subclass))\n\treturn all_subclasses\n\nimport sys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tyield cls\n\tfor subclass in cls.__subclasses__():\n\t\tyield from subclasses(subclass)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t\"\"\"\n\tReturn all subclasses of a class, recursively\n\t\"\"\"\n\tsubs = set()\n\tfor sub in cls.__subclasses__():\n\t\tsubs.add(sub)\n\t\tsubs.update(subclasses(sub))\n\treturn subs\n", "is_pass": true, "return_code": 0}]}
{"file_path": "shortuuid/main.py", "project": "skorokithakis/shortuuid", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = 0\n\tfor i in range(len(string)):\n\t\tn += alphabet.index(string[i]) * 3 ** (len(string) - 1 - i)\n\treturn n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i in range(len(string)):\n\t\tnum = num * len(alphabet) + alphabet.index(string[i])\n\treturn num\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(string)\n\tres = 0\n\tfor i in range(n):\n\t\tres += alphabet.index(string[i]) * int(math.pow(len(alphabet), n - 1 - i))\n\treturn res\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(string)\n\tresult = 0\n\tfor i in range(n):\n\t\tresult = result * len(alphabet) + alphabet.index(string[n - i - 1])\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor c in string:\n\t\tnumber = number * len(alphabet) + alphabet.index(c)\n\treturn number\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor i in range(len(string)):\n\t\tresult *= len(alphabet)\n\t\tresult += alphabet.index(string[i])\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\t\n\tfor char in string:\n\t\tresult = result * len(alphabet) + alphabet.index(char)\n\t\n\treturn result\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\talphabet_dict = dict(zip(alphabet, range(len(alphabet))))\n\tfor c in string:\n\t\tresult = result * len(alphabet) + alphabet_dict[c]\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\talphabet_length = len(alphabet)\n\tresult = 0\n\tfor i in range(0, len(string)):\n\t\tresult += alphabet.index(string[i]) * (alphabet_length ** (len(string) - i - 1))\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor i, letter in enumerate(string[::-1]):\n\t\tresult += alphabet.index(letter) * 26**i\n\treturn result\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = 0\n\tfor i in range(len(string)):\n\t\tn += alphabet.index(string[i]) * 3 ** (len(string) - 1 - i)\n\treturn n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnum = 0\n\tfor i in range(len(string)):\n\t\tnum = num * len(alphabet) + alphabet.index(string[i])\n\treturn num\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(string)\n\tres = 0\n\tfor i in range(n):\n\t\tres += alphabet.index(string[i]) * int(math.pow(len(alphabet), n - 1 - i))\n\treturn res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tn = len(string)\n\tresult = 0\n\tfor i in range(n):\n\t\tresult = result * len(alphabet) + alphabet.index(string[n - i - 1])\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tnumber = 0\n\tfor c in string:\n\t\tnumber = number * len(alphabet) + alphabet.index(c)\n\treturn number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor i in range(len(string)):\n\t\tresult *= len(alphabet)\n\t\tresult += alphabet.index(string[i])\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\t\n\tfor char in string:\n\t\tresult = result * len(alphabet) + alphabet.index(char)\n\t\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\talphabet_dict = dict(zip(alphabet, range(len(alphabet))))\n\tfor c in string:\n\t\tresult = result * len(alphabet) + alphabet_dict[c]\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\talphabet_length = len(alphabet)\n\tresult = 0\n\tfor i in range(0, len(string)):\n\t\tresult += alphabet.index(string[i]) * (alphabet_length ** (len(string) - i - 1))\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t\"\"\"\n\tConvert a string to a number, using the given alphabet.\n    :param string: a string consist of the letters in alphabet\n    :param alphabet: list of letters\n    :return: int, the corresponding number of the given string using the given transition rule.\n\t\"\"\"\n\tresult = 0\n\tfor i, letter in enumerate(string[::-1]):\n\t\tresult += alphabet.index(letter) * 26**i\n\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/lister.py", "project": "SoftwareHeritage/swh-lister", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / url.split('/')[-1]\n\twith requests.get(url, stream=True) as r:\n\t\twith open(archive_path, 'wb') as f:\n\t\t\tfor chunk in r.iter_content(chunk_size=1024):\n\t\t\t\tif chunk:\n\t\t\t\t\tf.write(chunk)\n\n\twith tarfile.open(archive_path) as f:\n\t\ttry:\n\t\t\tos.mkdir(destination_path)\n\t\texcept OSError:\n\t\t\tpass\n\t\tf.extractall(destination_path)\n\tos.remove(archive_path)\n\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tresponse = requests.get(url)\n\n\twith tempfile.NamedTemporaryFile() as fp:\n\t\tfp.write(response.content)\n\t\tfp.flush()\n\t\ttar = tarfile.open(fp.name, \"r:gz\")\n\t\ttar.extractall(destination_path)\n\t\ttar.close()\n\n\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tr = requests.get(url, allow_redirects=True)\n\twith open(destination_path, 'wb') as f:\n\t\tf.write(r.content)\n\tdestination_path.with_suffix('.tar.gz').unlink()\n\n\twith tarfile.open(destination_path) as f:\n\t\tf.extractall(destination_path.parent)\n\n\tdestination_path.with_suffix('.tar.gz').unlink()\n\tdestination_path.with_suffix('.tar').unlink()\n\n\twith open(destination_path.with_suffix('.desc'), 'r') as f:\n\t\trepo_desc = json.load(f)\n\n\treturn repo_desc\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith tempfile.TemporaryDirectory() as tmpdir:\n\t\ttmpdir_path = Path(tmpdir)\n\t\ttmp_tar = Path(tmpdir_path, 'tmp.tar.gz')\n\t\twith requests.get(url, stream=True) as r:\n\t\t\tr.raise_for_status()\n\t\t\twith open(tmp_tar, 'wb') as f:\n\t\t\t\tfor chunk in r.iter_content(chunk_size=8192):\n\t\t\t\t\t# If you have chunk encoded response uncomment if\n\t\t\t\t\t# and set chunk_size parameter to None.\n\t\t\t\t\t#if chunk:\n\t\t\t\t\tf.write(chunk)\n\n\t\twith tarfile.open(tmp_tar, 'r:gz') as tf:\n\t\t\tdestination_dir = tmpdir_path.joinpath(destination_path)\n\t\t\tdestination_dir.mkdir(parents=True, exist_ok=True)\n\t\t\ttf.extractall(destination_dir)\n\n\t\treturn destination_dir\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith open(destination_path / \"desc.json\", \"w\") as f:\n\t\twith urllib.request.urlopen(url) as response:\n\t\t\tdata = response.read()\n\t\t\tresult = json.loads(data)\n\t\t\t# print(result)\n\t\t\tdescs = result[\"desc\"]\n\t\t\t# print(descs)\n\t\t\tfor package, desc in descs.items():\n\t\t\t\tf.write(f\"{package}\\n\")\n\n\t\t\treturn destination_path\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tprint(f'Downloading {url} to {destination_path}')\n\tdestination_path.parent.mkdir(parents=True, exist_ok=True)\n\tdestination_path.write_bytes(requests.get(url).content)\n\twith tarfile.open(destination_path) as tar:\n\t\tprint(f'Extracting to {destination_path}')\n\t\ttar.extractall(path=destination_path.parent)\n\treturn destination_path.parent\n\n\nif __name__ == \"__main__\":\n\timport sys\n\turl = sys.argv[1]\n\tdestination_path = Path(sys.argv[2])\n\tget_repo_archive(url, destination_path)", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith tempfile.TemporaryDirectory() as tmp_dir:\n\t\ttmp_path = Path(tmp_dir)\n\t\tprint('Downloading archive from: {} in tmp dir: {}'.format(url,tmp_dir))\n\t\tdownload_file(url,tmp_path)\n\t\tprint('Extracting archive to: {}'.format(destination_path))\n\t\textract_archive(tmp_path,destination_path)\n\t\tprint('Archive extracted to: {}'.format(destination_path))\n\t\treturn destination_path\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tdestination_path = Path(destination_path)\n\tdestination_path.mkdir(parents=True, exist_ok=True)\n\twith tempfile.TemporaryDirectory() as tmp_dir:\n\t\twith tempfile.NamedTemporaryFile() as tmp_file:\n\t\t\twith urllib.request.urlopen(url) as response, open(tmp_file.name, 'wb') as out_file:\n\t\t\t\tshutil.copyfileobj(response, out_file)\n\t\t\twith tarfile.open(tmp_file.name) as tar:\n\t\t\t\ttar.extractall(path=tmp_dir)\n\t\t\twith open(f'{tmp_dir}/desc') as f:\n\t\t\t\tdesc_file = f.readlines()\n\t\t\tfor desc in desc_file:\n\t\t\t\tif desc.startswith('Package: '):\n\t\t\t\t\tpackage_name = desc.split(' ')[1]\n\t\t\t\t\tpackage_path = Path(f'{tmp_dir}/{package_name}')\n\t\t\t\t\tdestination_path.joinpath(package_name).mkdir(parents=True, exist_ok=True)\n\t\t\t\t\tshutil.copytree(package_path, destination_path.joinpath(package_name))\n\t\treturn destination_path", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\ttemp_folder = Path('/tmp/')\n\trepo_name = url.split('/')[-1].split('.')[0]\n\trepo_folder = temp_folder / repo_name\n\trepo_folder.mkdir(parents=True, exist_ok=True)\n\tprint(f\"Downloading {url} to {destination_path}\")\n\turlretrieve(url, destination_path)\n\tprint(f\"Extracting {repo_folder}\")\n\twith tarfile.open(destination_path, 'r') as archive:\n\t\tarchive.extractall(path=repo_folder)\n\treturn repo_folder\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith tempfile.TemporaryDirectory() as tmp_dir:\n\t\twith tarfile.open(f\"{tmp_dir}/archive.tar.gz\", \"r:gz\") as archive:\n\t\t\tarchive.extractall(tmp_dir)\n\t\t\n\t\tdesc_file = glob.glob(f\"{tmp_dir}/*/desc\")\n\t\tdestination_path.mkdir(parents=True, exist_ok=True)\n\n\t\tfor file in desc_file:\n\t\t\twith open(file) as f:\n\t\t\t\tlines = f.readlines()\n\t\t\t\tpackage_name = lines[0][:-1]\n\t\t\t\tpackage_version = lines[1][:-1]\n\t\t\t\twith open(f\"{destination_path}/{package_name}.txt\", \"w\") as file:\n\t\t\t\t\tfile.write(f\"{package_name} {package_version}\")\n\n\t\treturn Path(destination_path)\n\n#src/main.py\nfrom pathlib import Path\nfrom git import Repo\nfrom utils.get_repo_archive import get_repo_archive\nfrom utils.get_repo_name import get_repo_name\n\n# get package name and version\npackage_name = get_repo_name()\nprint(f\"Found package: {package_"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tarchive_path = destination_path / url.split('/')[-1]\n\twith requests.get(url, stream=True) as r:\n\t\twith open(archive_path, 'wb') as f:\n\t\t\tfor chunk in r.iter_content(chunk_size=1024):\n\t\t\t\tif chunk:\n\t\t\t\t\tf.write(chunk)\n\n\twith tarfile.open(archive_path) as f:\n\t\ttry:\n\t\t\tos.mkdir(destination_path)\n\t\texcept OSError:\n\t\t\tpass\n\t\tf.extractall(destination_path)\n\tos.remove(archive_path)\n\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tresponse = requests.get(url)\n\n\twith tempfile.NamedTemporaryFile() as fp:\n\t\tfp.write(response.content)\n\t\tfp.flush()\n\t\ttar = tarfile.open(fp.name, \"r:gz\")\n\t\ttar.extractall(destination_path)\n\t\ttar.close()\n\n\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tr = requests.get(url, allow_redirects=True)\n\twith open(destination_path, 'wb') as f:\n\t\tf.write(r.content)\n\tdestination_path.with_suffix('.tar.gz').unlink()\n\n\twith tarfile.open(destination_path) as f:\n\t\tf.extractall(destination_path.parent)\n\n\tdestination_path.with_suffix('.tar.gz').unlink()\n\tdestination_path.with_suffix('.tar').unlink()\n\n\twith open(destination_path.with_suffix('.desc'), 'r') as f:\n\t\trepo_desc = json.load(f)\n\n\treturn repo_desc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith tempfile.TemporaryDirectory() as tmpdir:\n\t\ttmpdir_path = Path(tmpdir)\n\t\ttmp_tar = Path(tmpdir_path, 'tmp.tar.gz')\n\t\twith requests.get(url, stream=True) as r:\n\t\t\tr.raise_for_status()\n\t\t\twith open(tmp_tar, 'wb') as f:\n\t\t\t\tfor chunk in r.iter_content(chunk_size=8192):\n\t\t\t\t\t# If you have chunk encoded response uncomment if\n\t\t\t\t\t# and set chunk_size parameter to None.\n\t\t\t\t\t#if chunk:\n\t\t\t\t\tf.write(chunk)\n\n\t\twith tarfile.open(tmp_tar, 'r:gz') as tf:\n\t\t\tdestination_dir = tmpdir_path.joinpath(destination_path)\n\t\t\tdestination_dir.mkdir(parents=True, exist_ok=True)\n\t\t\ttf.extractall(destination_dir)\n\n\t\treturn destination_dir\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith open(destination_path / \"desc.json\", \"w\") as f:\n\t\twith urllib.request.urlopen(url) as response:\n\t\t\tdata = response.read()\n\t\t\tresult = json.loads(data)\n\t\t\t# print(result)\n\t\t\tdescs = result[\"desc\"]\n\t\t\t# print(descs)\n\t\t\tfor package, desc in descs.items():\n\t\t\t\tf.write(f\"{package}\\n\")\n\n\t\t\treturn destination_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tprint(f'Downloading {url} to {destination_path}')\n\tdestination_path.parent.mkdir(parents=True, exist_ok=True)\n\tdestination_path.write_bytes(requests.get(url).content)\n\twith tarfile.open(destination_path) as tar:\n\t\tprint(f'Extracting to {destination_path}')\n\t\ttar.extractall(path=destination_path.parent)\n\treturn destination_path.parent\n\n\nif __name__ == \"__main__\":\n\timport sys\n\turl = sys.argv[1]\n\tdestination_path = Path(sys.argv[2])\n\tget_repo_archive(url, destination_path)", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith tempfile.TemporaryDirectory() as tmp_dir:\n\t\ttmp_path = Path(tmp_dir)\n\t\tprint('Downloading archive from: {} in tmp dir: {}'.format(url,tmp_dir))\n\t\tdownload_file(url,tmp_path)\n\t\tprint('Extracting archive to: {}'.format(destination_path))\n\t\textract_archive(tmp_path,destination_path)\n\t\tprint('Archive extracted to: {}'.format(destination_path))\n\t\treturn destination_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\tdestination_path = Path(destination_path)\n\tdestination_path.mkdir(parents=True, exist_ok=True)\n\twith tempfile.TemporaryDirectory() as tmp_dir:\n\t\twith tempfile.NamedTemporaryFile() as tmp_file:\n\t\t\twith urllib.request.urlopen(url) as response, open(tmp_file.name, 'wb') as out_file:\n\t\t\t\tshutil.copyfileobj(response, out_file)\n\t\t\twith tarfile.open(tmp_file.name) as tar:\n\t\t\t\ttar.extractall(path=tmp_dir)\n\t\t\twith open(f'{tmp_dir}/desc') as f:\n\t\t\t\tdesc_file = f.readlines()\n\t\t\tfor desc in desc_file:\n\t\t\t\tif desc.startswith('Package: '):\n\t\t\t\t\tpackage_name = desc.split(' ')[1]\n\t\t\t\t\tpackage_path = Path(f'{tmp_dir}/{package_name}')\n\t\t\t\t\tdestination_path.joinpath(package_name).mkdir(parents=True, exist_ok=True)\n\t\t\t\t\tshutil.copytree(package_path, destination_path.joinpath(package_name))\n\t\treturn destination_path", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\ttemp_folder = Path('/tmp/')\n\trepo_name = url.split('/')[-1].split('.')[0]\n\trepo_folder = temp_folder / repo_name\n\trepo_folder.mkdir(parents=True, exist_ok=True)\n\tprint(f\"Downloading {url} to {destination_path}\")\n\turlretrieve(url, destination_path)\n\tprint(f\"Extracting {repo_folder}\")\n\twith tarfile.open(destination_path, 'r') as archive:\n\t\tarchive.extractall(path=repo_folder)\n\treturn repo_folder\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t\"\"\"\n\tGiven an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to\n\t\"\"\"\n\twith tempfile.TemporaryDirectory() as tmp_dir:\n\t\twith tarfile.open(f\"{tmp_dir}/archive.tar.gz\", \"r:gz\") as archive:\n\t\t\tarchive.extractall(tmp_dir)\n\t\t\n\t\tdesc_file = glob.glob(f\"{tmp_dir}/*/desc\")\n\t\tdestination_path.mkdir(parents=True, exist_ok=True)\n\n\t\tfor file in desc_file:\n\t\t\twith open(file) as f:\n\t\t\t\tlines = f.readlines()\n\t\t\t\tpackage_name = lines[0][:-1]\n\t\t\t\tpackage_version = lines[1][:-1]\n\t\t\t\twith open(f\"{destination_path}/{package_name}.txt\", \"w\") as file:\n\t\t\t\t\tfile.write(f\"{package_name} {package_version}\")\n\n\t\treturn Path(destination_path)\n\n#src/main.py\nfrom pathlib import Path\nfrom git import Repo\nfrom utils.get_repo_archive import get_repo_archive\nfrom utils.get_repo_name import get_repo_name\n\n# get package name and version\npackage_name = get_repo_name()\nprint(f\"Found package: {package_", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/systeminfo.py", "project": "cloudmesh/cloudmesh-common", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\treturn sys.platform.startswith('darwin')\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\treturn sys.platform.startswith(\"darwin\")\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t\n\treturn platform.system() == \"Darwin\"", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Darwin\"\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\tfrom platform import system\n\tif system() == \"Darwin\":\n\t\treturn True\n\treturn False\n", "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\treturn sys.platform.startswith('darwin')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\treturn sys.platform.startswith(\"darwin\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\t\n\treturn platform.system() == \"Darwin\"", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport sys\n\tif sys.platform == 'darwin':\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Darwin\"\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\tfrom platform import system\n\tif system() == \"Darwin\":\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\t\"\"\"\n\tChecks if the os is macOS\n\n    :return: bool, True is macOS, otherwise False.\n\t\"\"\"\n\timport platform\n\treturn platform.system() == 'Darwin'\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tnewitem = {}\n\tfor key in item:\n\t\tnewitem.update({\n\t\t\tre.sub(r'\\*', r'.*', key): item[key]\n\t\t})\n\treturn newitem\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tresult = {}\n\tfor key, value in item.items():\n\t\tresult[key] = re.compile(key)\n\treturn result\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tkeys = item.keys()\n\tkeys = [key.replace('*', '(\\\\w|\\\\W)+') for key in keys]\n\treturn dict(zip(keys, item.values()))\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret = {}\n\tfor key, value in item.items():\n\t\tret[re.compile(key).pattern] = value\n\treturn ret\n\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret = {}\n\tfor k in item.iterkeys():\n\t\tret[re.sub(r\"\\*\\*\", \"(.*)\", k)] = item[k]\n\treturn ret\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tkeys = item.keys()\n\tnew_keys = []\n\tfor k in keys:\n\t\tnew_keys.append(regex_key(k))\n\tnew_item = {}\n\tfor k, v in item.iteritems():\n\t\tnew_item[new_keys[keys.index(k)]] = v\n\treturn new_item\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tnew_dict = {}\n\tfor k, v in item.iteritems():\n\t\tnew_dict[k] = k\n\treturn new_dict\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tnew_item = {}\n\tfor key, value in item.items():\n\t\tif '*' in key:\n\t\t\tnew_key = key.replace('*', '.*')\n\t\t\tnew_item[new_key] = value\n\t\telse:\n\t\t\tnew_item[key] = value\n\treturn new_item\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\titem = item.copy()\n\tfor key in item:\n\t\titem[key] = re.sub(r\"\\W\", \"\", key)\n\treturn item\n", "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\toutput = {}\n\tfor key in item.keys():\n\t\toutput[re.sub(\"\\\\*\", \".*\", key)] = item[key]\n\treturn output"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tnewitem = {}\n\tfor key in item:\n\t\tnewitem.update({\n\t\t\tre.sub(r'\\*', r'.*', key): item[key]\n\t\t})\n\treturn newitem\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tresult = {}\n\tfor key, value in item.items():\n\t\tresult[key] = re.compile(key)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tkeys = item.keys()\n\tkeys = [key.replace('*', '(\\\\w|\\\\W)+') for key in keys]\n\treturn dict(zip(keys, item.values()))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret = {}\n\tfor key, value in item.items():\n\t\tret[re.compile(key).pattern] = value\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tret = {}\n\tfor k in item.iterkeys():\n\t\tret[re.sub(r\"\\*\\*\", \"(.*)\", k)] = item[k]\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tkeys = item.keys()\n\tnew_keys = []\n\tfor k in keys:\n\t\tnew_keys.append(regex_key(k))\n\tnew_item = {}\n\tfor k, v in item.iteritems():\n\t\tnew_item[new_keys[keys.index(k)]] = v\n\treturn new_item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tnew_dict = {}\n\tfor k, v in item.iteritems():\n\t\tnew_dict[k] = k\n\treturn new_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\tnew_item = {}\n\tfor key, value in item.items():\n\t\tif '*' in key:\n\t\t\tnew_key = key.replace('*', '.*')\n\t\t\tnew_item[new_key] = value\n\t\telse:\n\t\t\tnew_item[key] = value\n\treturn new_item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\titem = item.copy()\n\tfor key in item:\n\t\titem[key] = re.sub(r\"\\W\", \"\", key)\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\t\"\"\"\n\tConvert *.cpp keys to regex keys.\nGiven a dict where the keys are all filenames with wildcards,\nconvert only the keys into equivalent regexes and leave the values intact.\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes\n\t\"\"\"\n\toutput = {}\n\tfor key in item.keys():\n\t\toutput[re.sub(\"\\\\*\", \".*\", key)] = item[key]\n\treturn output", "is_pass": false, "return_code": 1}]}
{"file_path": "rdiffweb/core/librdiff.py", "project": "ikus060/rdiffweb", "name": "unquote", "docstring": "Remove quote from the given name.", "_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\"', '', name)\n\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = re.sub(r\"[\\\"]\", \"\", name)\n\treturn name", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tregex = re.compile(r'^\\'|\\'$')\n\tname = regex.sub('', name)\n\treturn name", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'^\"|\"$', '', name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\\'', '', name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r'^\\'|\\'$', '', name)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r'\\'(.*?)\\'', r'\\1', name)\n\treturn name\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'^\"|\"$', '', name)\n", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tquote_pattern = re.compile(r\"(\\\"|\\')\")\n\tname_without_quotes = quote_pattern.sub(\"\", name)\n\treturn name_without_quotes", "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r\"\\'(.+?)\\'\",r\"\\1\",name)\n\treturn name\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\"', '', name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\tname = re.sub(r\"[\\\"]\", \"\", name)\n\treturn name", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tregex = re.compile(r'^\\'|\\'$')\n\tname = regex.sub('', name)\n\treturn name", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'^\"|\"$', '', name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'\\'', '', name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r'^\\'|\\'$', '', name)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r'\\'(.*?)\\'', r'\\1', name)\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\timport re\n\treturn re.sub(r'^\"|\"$', '', name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tquote_pattern = re.compile(r\"(\\\"|\\')\")\n\tname_without_quotes = quote_pattern.sub(\"\", name)\n\treturn name_without_quotes", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\t\"\"\"\n\tRemove quote from the given name with regular expression.\n    Args:\n        name: input name\n    Returns:\n        name after removal\n\t\"\"\"\n\tname = re.sub(r\"\\'(.+?)\\'\",r\"\\1\",name)\n\treturn name\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/shlex.py", "project": "cloudmesh/cloudmesh-common", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform\n\n\tif platform.system() == 'Windows' and platform.system() == 'Darwin':\n\t\tsplit_str = '\\n'\n\telse:\n\t\tsplit_str = '\\r\\n'\n\n\tif platform == 'this':\n\t\tif platform.system() == 'Windows':\n\t\t\tsplit_str = '\\r\\n'\n\t\telif platform.system() == 'Darwin':\n\t\t\tsplit_str = '\\n'\n\telse:\n\t\tsplit_str = platform\n\treturn s.split(split_str)\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tres = []\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 'this':\n\t\tplatform = 'Windows' if platform == 'win32' else platform\n\tif platform == 'posix':\n\t\tres = s.split('\\\\')\n\telif platform == 'cmd':\n\t\tres = s.split('/')\n\telif platform == 'this':\n\t\tres = s.split(':')\n\telse:\n\t\traise Exception('Unknown platform: {}'.format(platform))\n\treturn res\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tif platform == 'this':\n\t\tplatform = sys.platform\n\telif platform == '1':\n\t\tplatform = 'posix'\n\telif platform == '0':\n\t\tplatform = 'cmd'\n\telif not platform:\n\t\tplatform = sys.platform\n\telif platform not in ['posix', 'cmd']:\n\t\traise ValueError('Invalid platform \"%s\"' % platform)\n\n\tif platform == 'posix':\n\t\treturn s.split('\\n')\n\telif platform == 'cmd':\n\t\treturn s.split('\\r\\n')\n\telse:\n\t\treturn s.split('\\r')\n\n\nif __name__ == '__main__':\n\tprint(split('1\\r\\n2\\r\\n3\\r\\n4'))\n\tprint(split('1\\r\\n2\\r\\n3\\n4'))\n\tprint(split('1\\n2\\n3\\n4'))\n\tprint(split('1\\n2\\n3\\r\\n4'))\n\tprint(split('1\\r\\n2\\r\\n3\\r\\n4', '0'))", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tplatform = platform.lower()\n\tif platform == 'this':\n\t\tplatform = platform.lower()\n\t\ttry:\n\t\t\tplatform = platform.lower()\n\t\t\tif platform == 'windows':\n\t\t\t\tplatform = 'win'\n\t\t\telif platform == 'linux':\n\t\t\t\tplatform = 'unix'\n\t\t\tif platform == 'win':\n\t\t\t\ts = s.replace('\\r\\n', '\\n')\n\t\t\tif platform == 'unix':\n\t\t\t\ts = s.replace('\\n', '\\r\\n')\n\t\texcept:\n\t\t\tpass\n\tif platform == 'posix':\n\t\tplatform = 'unix'\n\telif platform == 'windows':\n\t\tplatform = 'win'\n\treturn s.split('\\n')\n\nif __name__ == '__main__':\n\tprint split('a\\nb\\nc\\nd\\ne', 'this')\n\tprint split('a\\nb\\nc\\nd\\ne', 'posix')\n\tprint split('a\\nb\\nc\\nd\\ne', 'windows')", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tsplit = s.split('\\r\\n')\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == '1':\n\t\treturn split\n\telif platform == '0':\n\t\treturn split.split('\\n')\n\telse:\n\t\traise ValueError('Unsupported platform: %s. Should be 1 or 0.' % platform)\n\t\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform\n\tif platform.system() == 'Windows':\n\t\treturn s.split('\\\\')\n\telif platform.system() == 'Linux':\n\t\treturn s.split('/')\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tplatform_dict = {\n\t\t0: 'windows',\n\t\t1: 'posix',\n\t\t2: 'mac',\n\t\t3: 'linux'\n\t}\n\tif platform not in platform_dict:\n\t\traise ValueError('Unknown platform: %s' % platform)\n\telif platform == 'this':\n\t\tplatform = platform_dict[sys.platform.startswith('win')]\n\treturn s.split(platform_dict[platform])\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tif platform == 'this':\n\t\tif sys.platform.startswith('linux'):\n\t\t\tplatform = 1\n\t\telif sys.platform.startswith('win'):\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 0\n\telif platform == '1':\n\t\tplatform = 1\n\telif platform == '0':\n\t\tplatform = 0\n\n\tif platform == 1:\n\t\treturn s.split('\\\\')\n\telif platform == 0:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform\n\tif platform.system() == \"Windows\":\n\t\tif platform.python_version() >= \"2.7\":\n\t\t\tif platform.system() == \"Windows\":\n\t\t\t\tsplitter = s.split()\n\t\t\telif platform.system() == \"POSIX\":\n\t\t\t\tsplitter = s.split(' ')\n\t\telif platform.python_version() < \"2.7\":\n\t\t\tsplitter = s.split(' ')\n\telse:\n\t\tif platform.system() == \"Linux\":\n\t\t\tsplitter = s.split(' ')\n\t\telif platform.system() == \"Darwin\":\n\t\t\tsplitter = s.split(' ')\n\treturn splitter\n\n", "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tplatform = platform.lower()\n\tif platform == 'this':\n\t\tplatform = sys.platform\n\tif platform == '1':\n\t\tplatform = 'posix'\n\telif platform == '0':\n\t\tplatform = 'cmd'\n\telse:\n\t\traise ValueError('Unsupported platform: %s' % platform)\n\n\tif platform == 'posix':\n\t\tsplit_cmd = 'split -l 10000000'\n\telif platform == 'cmd':\n\t\tsplit_cmd = 'split -l 10000000 -d'\n\treturn s.split(split_cmd)\n\nif __name__ == '__main__':\n\timport sys\n\tsplit(sys.argv[1], platform=sys.argv[2])"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform\n\n\tif platform.system() == 'Windows' and platform.system() == 'Darwin':\n\t\tsplit_str = '\\n'\n\telse:\n\t\tsplit_str = '\\r\\n'\n\n\tif platform == 'this':\n\t\tif platform.system() == 'Windows':\n\t\t\tsplit_str = '\\r\\n'\n\t\telif platform.system() == 'Darwin':\n\t\t\tsplit_str = '\\n'\n\telse:\n\t\tsplit_str = platform\n\treturn s.split(split_str)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tres = []\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 'this':\n\t\tplatform = 'Windows' if platform == 'win32' else platform\n\tif platform == 'posix':\n\t\tres = s.split('\\\\')\n\telif platform == 'cmd':\n\t\tres = s.split('/')\n\telif platform == 'this':\n\t\tres = s.split(':')\n\telse:\n\t\traise Exception('Unknown platform: {}'.format(platform))\n\treturn res\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tif platform == 'this':\n\t\tplatform = sys.platform\n\telif platform == '1':\n\t\tplatform = 'posix'\n\telif platform == '0':\n\t\tplatform = 'cmd'\n\telif not platform:\n\t\tplatform = sys.platform\n\telif platform not in ['posix', 'cmd']:\n\t\traise ValueError('Invalid platform \"%s\"' % platform)\n\n\tif platform == 'posix':\n\t\treturn s.split('\\n')\n\telif platform == 'cmd':\n\t\treturn s.split('\\r\\n')\n\telse:\n\t\treturn s.split('\\r')\n\n\nif __name__ == '__main__':\n\tprint(split('1\\r\\n2\\r\\n3\\r\\n4'))\n\tprint(split('1\\r\\n2\\r\\n3\\n4'))\n\tprint(split('1\\n2\\n3\\n4'))\n\tprint(split('1\\n2\\n3\\r\\n4'))\n\tprint(split('1\\r\\n2\\r\\n3\\r\\n4', '0'))", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tplatform = platform.lower()\n\tif platform == 'this':\n\t\tplatform = platform.lower()\n\t\ttry:\n\t\t\tplatform = platform.lower()\n\t\t\tif platform == 'windows':\n\t\t\t\tplatform = 'win'\n\t\t\telif platform == 'linux':\n\t\t\t\tplatform = 'unix'\n\t\t\tif platform == 'win':\n\t\t\t\ts = s.replace('\\r\\n', '\\n')\n\t\t\tif platform == 'unix':\n\t\t\t\ts = s.replace('\\n', '\\r\\n')\n\t\texcept:\n\t\t\tpass\n\tif platform == 'posix':\n\t\tplatform = 'unix'\n\telif platform == 'windows':\n\t\tplatform = 'win'\n\treturn s.split('\\n')\n\nif __name__ == '__main__':\n\tprint split('a\\nb\\nc\\nd\\ne', 'this')\n\tprint split('a\\nb\\nc\\nd\\ne', 'posix')\n\tprint split('a\\nb\\nc\\nd\\ne', 'windows')", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tsplit = s.split('\\r\\n')\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == '1':\n\t\treturn split\n\telif platform == '0':\n\t\treturn split.split('\\n')\n\telse:\n\t\traise ValueError('Unsupported platform: %s. Should be 1 or 0.' % platform)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform\n\tif platform.system() == 'Windows':\n\t\treturn s.split('\\\\')\n\telif platform.system() == 'Linux':\n\t\treturn s.split('/')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\tplatform_dict = {\n\t\t0: 'windows',\n\t\t1: 'posix',\n\t\t2: 'mac',\n\t\t3: 'linux'\n\t}\n\tif platform not in platform_dict:\n\t\traise ValueError('Unknown platform: %s' % platform)\n\telif platform == 'this':\n\t\tplatform = platform_dict[sys.platform.startswith('win')]\n\treturn s.split(platform_dict[platform])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tif platform == 'this':\n\t\tif sys.platform.startswith('linux'):\n\t\t\tplatform = 1\n\t\telif sys.platform.startswith('win'):\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 0\n\telif platform == '1':\n\t\tplatform = 1\n\telif platform == '0':\n\t\tplatform = 0\n\n\tif platform == 1:\n\t\treturn s.split('\\\\')\n\telif platform == 0:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport platform\n\tif platform.system() == \"Windows\":\n\t\tif platform.python_version() >= \"2.7\":\n\t\t\tif platform.system() == \"Windows\":\n\t\t\t\tsplitter = s.split()\n\t\t\telif platform.system() == \"POSIX\":\n\t\t\t\tsplitter = s.split(' ')\n\t\telif platform.python_version() < \"2.7\":\n\t\t\tsplitter = s.split(' ')\n\telse:\n\t\tif platform.system() == \"Linux\":\n\t\t\tsplitter = s.split(' ')\n\t\telif platform.system() == \"Darwin\":\n\t\t\tsplitter = s.split(' ')\n\treturn splitter\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t\"\"\"\n\tSplit the input str under given platform, return the splitting result\n    If platform equals 'this', auto-detect current platform.\n    If platform equals 1, use POSIX style.\n    If platform equals 0, use Windows/CMD style.\n    Args:\n        s: input str\n        platform: 'this' = auto from current platform; 1 = POSIX; 0 = Windows/CMD\n    Returns:\n        a list of splitting str\n\t\"\"\"\n\timport sys\n\tplatform = platform.lower()\n\tif platform == 'this':\n\t\tplatform = sys.platform\n\tif platform == '1':\n\t\tplatform = 'posix'\n\telif platform == '0':\n\t\tplatform = 'cmd'\n\telse:\n\t\traise ValueError('Unsupported platform: %s' % platform)\n\n\tif platform == 'posix':\n\t\tsplit_cmd = 'split -l 10000000'\n\telif platform == 'cmd':\n\t\tsplit_cmd = 'split -l 10000000 -d'\n\treturn s.split(split_cmd)\n\nif __name__ == '__main__':\n\timport sys\n\tsplit(sys.argv[1], platform=sys.argv[2])", "is_pass": false, "return_code": 1}]}
{"file_path": "swh/lister/arch/tests/__init__.py", "project": "SoftwareHeritage/swh-lister", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport tarfile\n\timport shutil\n\timport subprocess\n\timport tempfile\n\timport logging\n\timport os\n\n\timport requests\n\tfrom pathlib import Path\n\timport gitlab\n\n\tlogger = logging.getLogger(\"prepare_repository_from_archive\")\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\twith tempfile.TemporaryDirectory(\n\t\tprefix=\"fossology_archive_\", dir=tmp_path\n\t) as tmpdir:\n\t\tlogger.info(\"Extracting archive to temp dir: %s\", tmpdir)\n\t\twith tarfile.open(archive_path, \"r:gz\") as archive_file:\n\t\t\tarchive_file.extractall(tmpdir)\n\t\t\trepo_dir = os.path.join(tmpdir, filename)\n\t\t\tlogger.info(\"Repository dir: %s\", repo_dir)\n\n\t\t\trepo_url = None\n\t\t\ttry:\n\t\t\t\trepo_url = (\n\t\t\t\t\trequests.get(\"https://git.fossology.org/index.php/?option=com_redirect&redirect=repository_manager&redirect_to=repository_manager_repositories_new\")\n\t\t\t\t\t.url\n\t\t\t\t)\n\t\t\texcept Exception:\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\trepo_url = None\n\tif filename == None:\n\t\tfilename = os.path.basename(archive_path)\n\tfilename = f\"{filename}.git\"\n\tif not os.path.exists(archive_path):\n\t\traise FileNotFoundError(f\"File {archive_path} does not exist\")\n\n\tzip_archive = ZipFile(archive_path)\n\tzip_archive.extractall(tmp_path)\n\trepo_url = f\"file://{tmp_path}/{filename}\"\n\tzip_archive.close()\n\treturn repo_url", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\ttmp_path = Path(tmp_path)\n\trepo_dir = tmp_path / \"repo\"\n\trepo_dir.mkdir(parents=True, exist_ok=True)\n\trepo_dir_path = str(repo_dir)\n\tprocess = subprocess.run(\n\t\t[\"unzip\", \"-qo\", archive_path, \"-d\", repo_dir_path],\n\t\tstderr=subprocess.PIPE,\n\t\tstdout=subprocess.PIPE,\n\t)\n\tif process.returncode != 0:\n\t\traise RuntimeError(\n\t\t\tf\"Unable to extract archive {archive_path} to {repo_dir_path}\"\n\t\t)\n\n\trepo_url = str(repo_dir_path)\n\tif filename is not None:\n\t\trepo_url = f\"file://{repo_dir_path}/{filename}\"\n\treturn repo_url\n\n#src/git_utils.py\nimport argparse\nimport logging\nimport subprocess\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nfrom git import Repo\n\nfrom src.repo_utils import prepare_repository_from_archive\n\nlogger = logging.getLogger(__name__)\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport zipfile\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\twith zipfile.ZipFile(archive_path) as z:\n\t\tz.extractall(tmp_path)\n\n\trepo_url = f\"file://{os.path.join(tmp_path, filename)}\"\n\tos.remove(archive_path)\n\treturn repo_url", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport zipfile\n\n\tarchive_path = Path(archive_path)\n\tarchive_path.resolve()\n\tfile_path = Path(tmp_path) / Path(archive_path).with_suffix(\"\").name\n\n\tif not file_path.exists():\n\t\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(file_path)\n\n\tif filename is None:\n\t\tfilename = archive_path.stem\n\n\trepo_url = f\"file://{file_path}/{filename}\"\n\n\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport tarfile\n\timport tempfile\n\timport os\n\timport shutil\n\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\t\tfilename = filename.split(\".\")[0]\n\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\twith tempfile.TemporaryDirectory(dir=tmp_path) as tmpdir:\n\t\t\ttar.extractall(tmpdir)\n\t\t\trepo_url = os.path.join(tmpdir, filename)\n\t\t\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\twith tarfile.open(f\"{tmp_path}/{filename}\", \"r:gz\") as tar:\n\t\ttar.extractall(f\"{tmp_path}\")\n\t\n\t# Get the first file from the archive\n\trepo_url = \"\"\n\tfor file in os.listdir(f\"{tmp_path}/{filename}\"):\n\t\trepo_url = f\"file://{tmp_path}/{filename}\"\n\t\tbreak\n\t\n\tos.remove(f\"{tmp_path}/{filename}\")\n\t\n\treturn repo_url\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\twith tarfile.open(archive_path, 'r:gz') as archive:\n\t\tarchive.extractall(tmp_path)\n\n\tif filename:\n\t\tfilename = filename.replace(\" \", \"_\")\n\t\trename(join(tmp_path, filename), join(tmp_path, \"repo\"))\n\n\treturn f\"file://{tmp_path}/repo\"", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\ttmp_archive_path = os.path.join(tmp_path, \"tmp_archive.tar.gz\")\n\ttmp_repo_path = os.path.join(tmp_path, \"tmp_repo\")\n\n\twith tarfile.open(archive_path, \"r:gz\") as archive:\n\t\tarchive.extractall(tmp_path)\n\n\t\trepo_url = None\n\t\tfor root, dirs, files in os.walk(tmp_repo_path):\n\t\t\tfor file in files:\n\t\t\t\tif filename is not None and file != filename:\n\t\t\t\t\tcontinue\n\t\t\t\trepo_url = os.path.join(root, file)\n\t\t\t\tbreak\n\n\t\trepo_url = os.path.realpath(repo_url)\n\n\tos.remove(tmp_archive_path)\n\tos.remove(tmp_repo_path)\n\treturn repo_url\n\n#src/migrations/python/0001_initial_migration.py\n\"\"\"\nMigration to add a table to store the source code.\n\"\"\"\n\n\n\"\"\"\nAdds a table to store the original source code.\n\nRevision ID: 0001_initial_migration\nRevises:\nCreate Date: 2020-01-01 00:", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport tarfile\n\timport shutil\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\ttmp_path = os.path.join(tmp_path, filename)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(path=tmp_path)\n\treturn f\"file://{tmp_path}\"\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport tarfile\n\timport shutil\n\timport subprocess\n\timport tempfile\n\timport logging\n\timport os\n\n\timport requests\n\tfrom pathlib import Path\n\timport gitlab\n\n\tlogger = logging.getLogger(\"prepare_repository_from_archive\")\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\twith tempfile.TemporaryDirectory(\n\t\tprefix=\"fossology_archive_\", dir=tmp_path\n\t) as tmpdir:\n\t\tlogger.info(\"Extracting archive to temp dir: %s\", tmpdir)\n\t\twith tarfile.open(archive_path, \"r:gz\") as archive_file:\n\t\t\tarchive_file.extractall(tmpdir)\n\t\t\trepo_dir = os.path.join(tmpdir, filename)\n\t\t\tlogger.info(\"Repository dir: %s\", repo_dir)\n\n\t\t\trepo_url = None\n\t\t\ttry:\n\t\t\t\trepo_url = (\n\t\t\t\t\trequests.get(\"https://git.fossology.org/index.php/?option=com_redirect&redirect=repository_manager&redirect_to=repository_manager_repositories_new\")\n\t\t\t\t\t.url\n\t\t\t\t)\n\t\t\texcept Exception:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\trepo_url = None\n\tif filename == None:\n\t\tfilename = os.path.basename(archive_path)\n\tfilename = f\"{filename}.git\"\n\tif not os.path.exists(archive_path):\n\t\traise FileNotFoundError(f\"File {archive_path} does not exist\")\n\n\tzip_archive = ZipFile(archive_path)\n\tzip_archive.extractall(tmp_path)\n\trepo_url = f\"file://{tmp_path}/{filename}\"\n\tzip_archive.close()\n\treturn repo_url", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\ttmp_path = Path(tmp_path)\n\trepo_dir = tmp_path / \"repo\"\n\trepo_dir.mkdir(parents=True, exist_ok=True)\n\trepo_dir_path = str(repo_dir)\n\tprocess = subprocess.run(\n\t\t[\"unzip\", \"-qo\", archive_path, \"-d\", repo_dir_path],\n\t\tstderr=subprocess.PIPE,\n\t\tstdout=subprocess.PIPE,\n\t)\n\tif process.returncode != 0:\n\t\traise RuntimeError(\n\t\t\tf\"Unable to extract archive {archive_path} to {repo_dir_path}\"\n\t\t)\n\n\trepo_url = str(repo_dir_path)\n\tif filename is not None:\n\t\trepo_url = f\"file://{repo_dir_path}/{filename}\"\n\treturn repo_url\n\n#src/git_utils.py\nimport argparse\nimport logging\nimport subprocess\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nfrom git import Repo\n\nfrom src.repo_utils import prepare_repository_from_archive\n\nlogger = logging.getLogger(__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport zipfile\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\twith zipfile.ZipFile(archive_path) as z:\n\t\tz.extractall(tmp_path)\n\n\trepo_url = f\"file://{os.path.join(tmp_path, filename)}\"\n\tos.remove(archive_path)\n\treturn repo_url", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport zipfile\n\n\tarchive_path = Path(archive_path)\n\tarchive_path.resolve()\n\tfile_path = Path(tmp_path) / Path(archive_path).with_suffix(\"\").name\n\n\tif not file_path.exists():\n\t\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(file_path)\n\n\tif filename is None:\n\t\tfilename = archive_path.stem\n\n\trepo_url = f\"file://{file_path}/{filename}\"\n\n\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport tarfile\n\timport tempfile\n\timport os\n\timport shutil\n\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\t\tfilename = filename.split(\".\")[0]\n\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\twith tempfile.TemporaryDirectory(dir=tmp_path) as tmpdir:\n\t\t\ttar.extractall(tmpdir)\n\t\t\trepo_url = os.path.join(tmpdir, filename)\n\t\t\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\twith tarfile.open(f\"{tmp_path}/{filename}\", \"r:gz\") as tar:\n\t\ttar.extractall(f\"{tmp_path}\")\n\t\n\t# Get the first file from the archive\n\trepo_url = \"\"\n\tfor file in os.listdir(f\"{tmp_path}/{filename}\"):\n\t\trepo_url = f\"file://{tmp_path}/{filename}\"\n\t\tbreak\n\t\n\tos.remove(f\"{tmp_path}/{filename}\")\n\t\n\treturn repo_url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\twith tarfile.open(archive_path, 'r:gz') as archive:\n\t\tarchive.extractall(tmp_path)\n\n\tif filename:\n\t\tfilename = filename.replace(\" \", \"_\")\n\t\trename(join(tmp_path, filename), join(tmp_path, \"repo\"))\n\n\treturn f\"file://{tmp_path}/repo\"", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\t\n\ttmp_archive_path = os.path.join(tmp_path, \"tmp_archive.tar.gz\")\n\ttmp_repo_path = os.path.join(tmp_path, \"tmp_repo\")\n\n\twith tarfile.open(archive_path, \"r:gz\") as archive:\n\t\tarchive.extractall(tmp_path)\n\n\t\trepo_url = None\n\t\tfor root, dirs, files in os.walk(tmp_repo_path):\n\t\t\tfor file in files:\n\t\t\t\tif filename is not None and file != filename:\n\t\t\t\t\tcontinue\n\t\t\t\trepo_url = os.path.join(root, file)\n\t\t\t\tbreak\n\n\t\trepo_url = os.path.realpath(repo_url)\n\n\tos.remove(tmp_archive_path)\n\tos.remove(tmp_repo_path)\n\treturn repo_url\n\n#src/migrations/python/0001_initial_migration.py\n\"\"\"\nMigration to add a table to store the source code.\n\"\"\"\n\n\n\"\"\"\nAdds a table to store the original source code.\n\nRevision ID: 0001_initial_migration\nRevises:\nCreate Date: 2020-01-01 00:", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t\"\"\"\n\tGiven an existing archive_path, uncompress it.\n    Returns a file repo url which can be used as origin url.\n\n    This does not deal with the case where the archive passed along does not exist.\n    @param archive_path : archive file path\n    @param filename: File name\n    @param tmp_path: Temporary file path\n    @return Repo URL\n\t\"\"\"\n\timport tarfile\n\timport shutil\n\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\n\ttmp_path = os.path.join(tmp_path, filename)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(path=tmp_path)\n\treturn f\"file://{tmp_path}\"\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/flashbake/plugins/ignored.py", "project": "commandline/flashbake", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\toutput = os.popen(f'git ls-files --others --exclude-standard {ignored} | xargs').read()\n\toutput = [x for x in output.split('\\n') if x != '']\n\n\toutput = ','.join(output)\n\n\treturn output\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = 'git ls-files --others --exclude-standard --ignored'.split()\n\tcmd.append(ignored)\n\toutput = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=False).communicate()[0]\n\tignored = output.split('\\n')\n\tignored.sort()\n\treturn ','.join(ignored)\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\tcmd = \"git ls-files -o --exclude-standard | sort -u\"\n\tproc = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE)\n\tfiles = proc.stdout.read()\n\tfiles = files.strip().split('\\n')\n\tfiles = [f for f in files if f not in ignored]\n\treturn \",\".join(files)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\tfiles = []\n\tignored_files = []\n\t\n\tgit_ignore_files = os.popen('git ls-files -i -m --exclude-standard').read()\n\t\n\tfor f in git_ignore_files.split('\\n'):\n\t\tif f:\n\t\t\tfiles.append(os.path.basename(f))\n\t\n\tfor f in files:\n\t\tif f in ignored:\n\t\t\tignored_files.append(f)\n\t\n\tignored_files = sorted(set(ignored_files))\n\t\n\treturn ','.join(ignored_files)\n\n#main.py\nimport os\nfrom flask import Flask, render_template, request\nfrom dotenv import find_dotenv, load_dotenv\nimport git\nimport yaml\nimport subprocess\nimport time\n\nload_dotenv(find_dotenv())\n\n# Define the Flask application\napp = Flask(__name__)\n\n@app.route('/')", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = \"git ls-files -o -i --exclude-standard\"\n\tcmd += \" \".join(ignored)\n\n\toutput = os.popen(cmd).read()\n\tfiles = output.split()\n\n\tfiles.sort()\n\n\treturn ','.join(files)\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored_text = \"\"\n\tignored_files = []\n\tignored_text = (ignored_text + os.popen(\"git ls-files -i --others --exclude-standard\").readlines())\n\tignored_text = (ignored_text + os.popen(\"git ls-files -o --exclude-standard\").readlines())\n\tignored_text = (ignored_text + os.popen(\"git ls-files -d --exclude-standard\").readlines())\n\tignored_text = (ignored_text + os.popen(\"git ls-files -m\").readlines())\n\tignored_files = ignored_text.split('\\n')\n\tignored_files = list(filter(None, ignored_files))\n\tignored_files = sorted(ignored_files)\n\tfor ignored_file in ignored_files:\n\t\tif ignored_file:\n\t\t\tignored.append(ignored_file)\n\tignored_text = ''\n\treturn ignored", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\toutput = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--exclude-standard\", ignored], universal_newlines=True)\n\tfiles = output.split(\"\\n\")\n\treturn \",\".join(sorted(files))\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfile_list = []\n\t\n\tcmd = 'git ls-files -i --exclude-standard'\n\t\n\t# Get the file names\n\tpiped = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n\t\n\toutput = \"\"\n\t\n\twhile True:\n\t\tline = piped.stdout.readline()\n\t\tif not line:\n\t\t\tbreak\n\t\toutput += line\n\t\tfile_list = file_list + line.split()\n\t\n\t# Sort the file names and get a single string with each filename separated by a comma\n\tfile_list = sorted(file_list)\n\t\n\tignored_files = \"\"\n\t\n\tfor i in range(len(file_list)):\n\t\tif i == len(file_list) - 1:\n\t\t\tignored_files += file_list[i]\n\t\telse:\n\t\t\tignored_files += file_list[i] + \",\"\n\t\n\treturn ignored_files\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\n\tcommand = \"git ls-files --others --exclude-standard\"\n\tprocess = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\toutput = process.communicate()[0]\n\toutput = output.strip()\n\toutput = output.split('\\n')\n\n\tignored = list(ignored)\n\tignored.append('__pycache__')\n\tfor i in range(len(output)):\n\t\toutput[i] = output[i].strip()\n\n\toutput1 = []\n\tfor i in range(len(output)):\n\t\tif output[i] not in ignored:\n\t\t\toutput1.append(output[i])\n\n\toutput1 = sorted(output1)\n\n\tignore_string = ''\n\tfor i in range(len(output1)):\n\t\tif i == len(output1) - 1:\n\t\t\tignore_string += output1[i]\n\t\telse:\n\t\t\tignore_string += output1[i] + ','\n\n\treturn ignore_string\n\n", "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_cmd = \"git ls-files -i -o -d -m --exclude-standard\"\n\tcommand = git_cmd + \" \" + ignored\n\tprocess = subprocess.Popen(command,stdout=subprocess.PIPE,shell=True)\n\tout,err = process.communicate()\n\tout = str(out).split('\\n')\n\tignored = [f for f in out if f != '']\n\tignored.sort()\n\tignored_str = ','.join(ignored)\n\treturn ignored_str\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\toutput = os.popen(f'git ls-files --others --exclude-standard {ignored} | xargs').read()\n\toutput = [x for x in output.split('\\n') if x != '']\n\n\toutput = ','.join(output)\n\n\treturn output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = 'git ls-files --others --exclude-standard --ignored'.split()\n\tcmd.append(ignored)\n\toutput = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=False).communicate()[0]\n\tignored = output.split('\\n')\n\tignored.sort()\n\treturn ','.join(ignored)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\tcmd = \"git ls-files -o --exclude-standard | sort -u\"\n\tproc = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE)\n\tfiles = proc.stdout.read()\n\tfiles = files.strip().split('\\n')\n\tfiles = [f for f in files if f not in ignored]\n\treturn \",\".join(files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\t\n\tfiles = []\n\tignored_files = []\n\t\n\tgit_ignore_files = os.popen('git ls-files -i -m --exclude-standard').read()\n\t\n\tfor f in git_ignore_files.split('\\n'):\n\t\tif f:\n\t\t\tfiles.append(os.path.basename(f))\n\t\n\tfor f in files:\n\t\tif f in ignored:\n\t\t\tignored_files.append(f)\n\t\n\tignored_files = sorted(set(ignored_files))\n\t\n\treturn ','.join(ignored_files)\n\n#main.py\nimport os\nfrom flask import Flask, render_template, request\nfrom dotenv import find_dotenv, load_dotenv\nimport git\nimport yaml\nimport subprocess\nimport time\n\nload_dotenv(find_dotenv())\n\n# Define the Flask application\napp = Flask(__name__)\n\n@app.route('/')", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tcmd = \"git ls-files -o -i --exclude-standard\"\n\tcmd += \" \".join(ignored)\n\n\toutput = os.popen(cmd).read()\n\tfiles = output.split()\n\n\tfiles.sort()\n\n\treturn ','.join(files)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tignored_text = \"\"\n\tignored_files = []\n\tignored_text = (ignored_text + os.popen(\"git ls-files -i --others --exclude-standard\").readlines())\n\tignored_text = (ignored_text + os.popen(\"git ls-files -o --exclude-standard\").readlines())\n\tignored_text = (ignored_text + os.popen(\"git ls-files -d --exclude-standard\").readlines())\n\tignored_text = (ignored_text + os.popen(\"git ls-files -m\").readlines())\n\tignored_files = ignored_text.split('\\n')\n\tignored_files = list(filter(None, ignored_files))\n\tignored_files = sorted(ignored_files)\n\tfor ignored_file in ignored_files:\n\t\tif ignored_file:\n\t\t\tignored.append(ignored_file)\n\tignored_text = ''\n\treturn ignored", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\toutput = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--exclude-standard\", ignored], universal_newlines=True)\n\tfiles = output.split(\"\\n\")\n\treturn \",\".join(sorted(files))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tfile_list = []\n\t\n\tcmd = 'git ls-files -i --exclude-standard'\n\t\n\t# Get the file names\n\tpiped = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n\t\n\toutput = \"\"\n\t\n\twhile True:\n\t\tline = piped.stdout.readline()\n\t\tif not line:\n\t\t\tbreak\n\t\toutput += line\n\t\tfile_list = file_list + line.split()\n\t\n\t# Sort the file names and get a single string with each filename separated by a comma\n\tfile_list = sorted(file_list)\n\t\n\tignored_files = \"\"\n\t\n\tfor i in range(len(file_list)):\n\t\tif i == len(file_list) - 1:\n\t\t\tignored_files += file_list[i]\n\t\telse:\n\t\t\tignored_files += file_list[i] + \",\"\n\t\n\treturn ignored_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\timport subprocess\n\n\tcommand = \"git ls-files --others --exclude-standard\"\n\tprocess = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\toutput = process.communicate()[0]\n\toutput = output.strip()\n\toutput = output.split('\\n')\n\n\tignored = list(ignored)\n\tignored.append('__pycache__')\n\tfor i in range(len(output)):\n\t\toutput[i] = output[i].strip()\n\n\toutput1 = []\n\tfor i in range(len(output)):\n\t\tif output[i] not in ignored:\n\t\t\toutput1.append(output[i])\n\n\toutput1 = sorted(output1)\n\n\tignore_string = ''\n\tfor i in range(len(output1)):\n\t\tif i == len(output1) - 1:\n\t\t\tignore_string += output1[i]\n\t\telse:\n\t\t\tignore_string += output1[i] + ','\n\n\treturn ignore_string\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\"\"\"\n\tUse the git command to obtain the file names\u3002\n    Turn the file names into a list, sort the list for only ignored files\n    return those files as a single string with each filename separated by a comma.\n\t\"\"\"\n\tgit_cmd = \"git ls-files -i -o -d -m --exclude-standard\"\n\tcommand = git_cmd + \" \" + ignored\n\tprocess = subprocess.Popen(command,stdout=subprocess.PIPE,shell=True)\n\tout,err = process.communicate()\n\tout = str(out).split('\\n')\n\tignored = [f for f in out if f != '']\n\tignored.sort()\n\tignored_str = ','.join(ignored)\n\treturn ignored_str\n", "is_pass": false, "return_code": 1}]}
{"file_path": "docopt/__init__.py", "project": "bazaar-projects/docopt-ng", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n\ttype = filename.lower()\n\tif type == 'doxyfile' or type == 'doxygen.cfg' or type == 'doxygen.cfg.in':\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename == 'doxyfile'\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\twith open(filename, 'r') as f:\n\t\tcontent = f.read()\n\tif 'doxyfile' in content.lower():\n\t\treturn True\n\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n\twith open(filename, 'r') as f:\n\t\tfor line in f:\n\t\t\tif line.strip().lower() == 'doxyfile':\n\t\t\t\treturn True\n\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ttry:\n\t\twith open(filename) as f:\n\t\t\tlines = f.readlines()\n\t\t\tfor line in lines:\n\t\t\t\tif line.strip().lower() == 'doxyfile':\n\t\t\t\t\treturn True\n\texcept Exception as e:\n\t\tprint(e)\n\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tlowerFilename = filename.lower()\n\tif lowerFilename == 'doxyfile':\n\t\treturn True\n\telse:\n\t\treturn False\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename.endswith('.doxyfile')", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n\tlowercase_filename = filename.lower()\n\n\tif lowercase_filename.endswith(\".doxyfile\"):\n\t\treturn True\n\telse:\n\t\treturn False", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tlowerFilename = filename.lower()\n\treturn lowerFilename == 'doxyfile'\n\n", "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename == 'doxyfile'\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n\ttype = filename.lower()\n\tif type == 'doxyfile' or type == 'doxygen.cfg' or type == 'doxygen.cfg.in':\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename == 'doxyfile'\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\twith open(filename, 'r') as f:\n\t\tcontent = f.read()\n\tif 'doxyfile' in content.lower():\n\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n\twith open(filename, 'r') as f:\n\t\tfor line in f:\n\t\t\tif line.strip().lower() == 'doxyfile':\n\t\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\ttry:\n\t\twith open(filename) as f:\n\t\t\tlines = f.readlines()\n\t\t\tfor line in lines:\n\t\t\t\tif line.strip().lower() == 'doxyfile':\n\t\t\t\t\treturn True\n\texcept Exception as e:\n\t\tprint(e)\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tlowerFilename = filename.lower()\n\tif lowerFilename == 'doxyfile':\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename.endswith('.doxyfile')", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\t\n\tlowercase_filename = filename.lower()\n\n\tif lowercase_filename.endswith(\".doxyfile\"):\n\t\treturn True\n\telse:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tlowerFilename = filename.lower()\n\treturn lowerFilename == 'doxyfile'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\t\"\"\"\n\tCheck if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'\n\t\"\"\"\n\tfilename = filename.lower()\n\treturn filename == 'doxyfile'\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/borg/check.py", "project": "witten/borgmatic", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\ttry:\n\t\tnumber, unit = frequency.split()\n\texcept ValueError:\n\t\treturn None\n\n\tunit_mappings = {\n\t\t\"seconds\" : 1,\n\t\t\"minutes\" : 60,\n\t\t\"hours\" : 60*60,\n\t\t\"days\" : 60*60*24,\n\t\t\"weeks\" : 60*60*24*7,\n\t\t\"months\" : 60*60*24*30,\n\t\t\"years\" : 60*60*24*365\n\t}\n\n\ttry:\n\t\treturn datetime.timedelta(int(number) * unit_mappings[unit])\n\texcept KeyError:\n\t\traise ValueError(\"Cannot parse %s as a frequency\" % frequency)\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = float(frequency.split()[0])\n\t\ttimeunit = frequency.split()[1]\n\texcept (IndexError, ValueError):\n\t\treturn None\n\t\n\tif timeunit == \"always\" or timeunit == \"always-rerun\":\n\t\treturn None\n\n\ttimeunit_mapping = {\n\t\t\"seconds\": datetime.timedelta(seconds=number),\n\t\t\"minutes\": datetime.timedelta(minutes=number),\n\t\t\"hours\": datetime.timedelta(hours=number),\n\t\t\"days\": datetime.timedelta(days=number),\n\t\t\"weeks\": datetime.timedelta(weeks=number),\n\t\t\"months\": datetime.timedelta(months=number),\n\t\t\"years\": datetime.timedelta(years=number)\n\t}\n\treturn timeunit_mapping[timeunit]\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\t# TODO: Add more time units?\n\n\ttimeunit = 1\n\tif frequency.lower() == \"always\":\n\t\ttimeunit = 0\n\telif \"minute\" in frequency.lower():\n\t\ttimeunit = 60\n\telif \"hour\" in frequency.lower():\n\t\ttimeunit = 3600\n\telif \"day\" in frequency.lower():\n\t\ttimeunit = 86400\n\telif \"week\" in frequency.lower():\n\t\ttimeunit = 604800\n\telse:\n\t\traise ValueError(\"Unknown time unit\")\n\n\tnum = int(frequency.split()[0])\n\treturn datetime.timedelta(timeunit=num)", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\ttry:\n\t\tn = int(frequency.split()[0])\n\texcept ValueError:\n\t\treturn None\n\n\tunit = frequency.split()[1].lower()\n\t\n\tif unit in [\"year\", \"years\"]:\n\t\treturn datetime.timedelta(years = n)\n\telif unit in [\"month\", \"months\"]:\n\t\treturn datetime.timedelta(months = n)\n\telif unit in [\"day\", \"days\"]:\n\t\treturn datetime.timedelta(days = n)\n\telif unit in [\"hour\", \"hours\"]:\n\t\treturn datetime.timedelta(hours = n)\n\telif unit in [\"minute\", \"minutes\"]:\n\t\treturn datetime.timedelta(minutes = n)\n\telif unit in [\"second\", \"seconds\"]:\n\t\treturn datetime.timedelta(seconds = n)\n\t\n\traise ValueError(\"Unknown frequency unit: \" + unit)\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = int(frequency)\n\texcept ValueError:\n\t\treturn None\n\n\tif frequency.endswith(\"s\"):\n\t\treturn datetime.timedelta(seconds=number)\n\telif frequency.endswith(\"m\"):\n\t\treturn datetime.timedelta(minutes=number)\n\telif frequency.endswith(\"h\"):\n\t\treturn datetime.timedelta(hours=number)\n\telif frequency.endswith(\"d\"):\n\t\treturn datetime.timedelta(days=number)\n\telse:\n\t\treturn None\n\n#src/app/views/__init__.py\nfrom .base import BaseView\nfrom .home import HomeView\nfrom .task import TaskView\nfrom .user import UserView\n\n#src/app/views/task.py\nimport logging\nimport datetime\n\nfrom flask import request, render_template, redirect, url_for, session\n\nfrom .. import app\nfrom ..forms import TaskForm\nfrom ..models import User, Task\nfrom .base import BaseView\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\tif frequency == \"always\":\n\t\treturn None\n\n\ttry:\n\t\treturn datetime.timedelta(**{frequency.split()[-1]: int(frequency.split()[0])})\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency {}\".format(frequency))\n\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = int(frequency.split()[0])\n\texcept ValueError:\n\t\traise ValueError(\"Cannot parse frequency %s\" % frequency)\n\n\tif frequency == \"always\":\n\t\treturn None\n\telif frequency == \"never\":\n\t\treturn datetime.timedelta(days=1)\n\telse:\n\t\ttimeunit = frequency.split()[1]\n\t\tif timeunit == \"s\":\n\t\t\ttimedelta = datetime.timedelta(seconds=number)\n\t\telif timeunit == \"m\":\n\t\t\ttimedelta = datetime.timedelta(minutes=number)\n\t\telif timeunit == \"h\":\n\t\t\ttimedelta = datetime.timedelta(hours=number)\n\t\telif timeunit == \"d\":\n\t\t\ttimedelta = datetime.timedelta(days=number)\n\t\telse:\n\t\t\traise ValueError(\"Unsupported time unit %s\" % timeunit)\n\n\t\treturn timedelta\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttimeunit, number = frequency.strip().split(\" \", 1)\n\n\tif timeunit == \"always\":\n\t\treturn None\n\n\ttry:\n\t\ttimeunit = timeunit.lower()\n\t\tnumber = int(number)\n\t\treturn datetime.timedelta(timeunit=number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency '{0}'\".format(frequency))\n\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber, unit = frequency.split(\" \")\n\t\tnumber = float(number)\n\t\tif unit == \"day\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif unit == \"month\":\n\t\t\treturn datetime.timedelta(days=number*30)\n\t\telif unit == \"year\":\n\t\t\treturn datetime.timedelta(days=number*365)\n\t\telse:\n\t\t\traise ValueError(\"Unknown time unit: '%s'\" % unit)\n\texcept ValueError:\n\t\treturn None\n", "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = float(frequency[:-len(frequency[-1])])\n\t\tif not number:\n\t\t\traise ValueError()\n\t\ttimeunit = frequency[-1].lower()\n\t\tif timeunit not in TIMEUNITS:\n\t\t\traise ValueError()\n\t\treturn datetime.timedelta(**{timeunit: number})\n\texcept ValueError:\n\t\tif frequency == \"always\":\n\t\t\treturn None\n\t\traise ValueError()\n\n#data_analysis/data_analysis.py\n\"\"\"\n\n@author: yang\n\"\"\"\n\nimport datetime\nimport timeutc\nimport os\n\nif __name__ == \"__main__\":\n\tinterval = timeutc.parse_frequency(\"1 timeunit\")\n\tprint(interval)\n\tprint(interval.seconds)\n\tprint(interval.days)\n\tprint(interval.total_seconds())\n\tprint(datetime.datetime.now() + interval)"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\ttry:\n\t\tnumber, unit = frequency.split()\n\texcept ValueError:\n\t\treturn None\n\n\tunit_mappings = {\n\t\t\"seconds\" : 1,\n\t\t\"minutes\" : 60,\n\t\t\"hours\" : 60*60,\n\t\t\"days\" : 60*60*24,\n\t\t\"weeks\" : 60*60*24*7,\n\t\t\"months\" : 60*60*24*30,\n\t\t\"years\" : 60*60*24*365\n\t}\n\n\ttry:\n\t\treturn datetime.timedelta(int(number) * unit_mappings[unit])\n\texcept KeyError:\n\t\traise ValueError(\"Cannot parse %s as a frequency\" % frequency)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = float(frequency.split()[0])\n\t\ttimeunit = frequency.split()[1]\n\texcept (IndexError, ValueError):\n\t\treturn None\n\t\n\tif timeunit == \"always\" or timeunit == \"always-rerun\":\n\t\treturn None\n\n\ttimeunit_mapping = {\n\t\t\"seconds\": datetime.timedelta(seconds=number),\n\t\t\"minutes\": datetime.timedelta(minutes=number),\n\t\t\"hours\": datetime.timedelta(hours=number),\n\t\t\"days\": datetime.timedelta(days=number),\n\t\t\"weeks\": datetime.timedelta(weeks=number),\n\t\t\"months\": datetime.timedelta(months=number),\n\t\t\"years\": datetime.timedelta(years=number)\n\t}\n\treturn timeunit_mapping[timeunit]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\t# TODO: Add more time units?\n\n\ttimeunit = 1\n\tif frequency.lower() == \"always\":\n\t\ttimeunit = 0\n\telif \"minute\" in frequency.lower():\n\t\ttimeunit = 60\n\telif \"hour\" in frequency.lower():\n\t\ttimeunit = 3600\n\telif \"day\" in frequency.lower():\n\t\ttimeunit = 86400\n\telif \"week\" in frequency.lower():\n\t\ttimeunit = 604800\n\telse:\n\t\traise ValueError(\"Unknown time unit\")\n\n\tnum = int(frequency.split()[0])\n\treturn datetime.timedelta(timeunit=num)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\ttry:\n\t\tn = int(frequency.split()[0])\n\texcept ValueError:\n\t\treturn None\n\n\tunit = frequency.split()[1].lower()\n\t\n\tif unit in [\"year\", \"years\"]:\n\t\treturn datetime.timedelta(years = n)\n\telif unit in [\"month\", \"months\"]:\n\t\treturn datetime.timedelta(months = n)\n\telif unit in [\"day\", \"days\"]:\n\t\treturn datetime.timedelta(days = n)\n\telif unit in [\"hour\", \"hours\"]:\n\t\treturn datetime.timedelta(hours = n)\n\telif unit in [\"minute\", \"minutes\"]:\n\t\treturn datetime.timedelta(minutes = n)\n\telif unit in [\"second\", \"seconds\"]:\n\t\treturn datetime.timedelta(seconds = n)\n\t\n\traise ValueError(\"Unknown frequency unit: \" + unit)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = int(frequency)\n\texcept ValueError:\n\t\treturn None\n\n\tif frequency.endswith(\"s\"):\n\t\treturn datetime.timedelta(seconds=number)\n\telif frequency.endswith(\"m\"):\n\t\treturn datetime.timedelta(minutes=number)\n\telif frequency.endswith(\"h\"):\n\t\treturn datetime.timedelta(hours=number)\n\telif frequency.endswith(\"d\"):\n\t\treturn datetime.timedelta(days=number)\n\telse:\n\t\treturn None\n\n#src/app/views/__init__.py\nfrom .base import BaseView\nfrom .home import HomeView\nfrom .task import TaskView\nfrom .user import UserView\n\n#src/app/views/task.py\nimport logging\nimport datetime\n\nfrom flask import request, render_template, redirect, url_for, session\n\nfrom .. import app\nfrom ..forms import TaskForm\nfrom ..models import User, Task\nfrom .base import BaseView\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\t\n\tif frequency == \"always\":\n\t\treturn None\n\n\ttry:\n\t\treturn datetime.timedelta(**{frequency.split()[-1]: int(frequency.split()[0])})\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency {}\".format(frequency))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = int(frequency.split()[0])\n\texcept ValueError:\n\t\traise ValueError(\"Cannot parse frequency %s\" % frequency)\n\n\tif frequency == \"always\":\n\t\treturn None\n\telif frequency == \"never\":\n\t\treturn datetime.timedelta(days=1)\n\telse:\n\t\ttimeunit = frequency.split()[1]\n\t\tif timeunit == \"s\":\n\t\t\ttimedelta = datetime.timedelta(seconds=number)\n\t\telif timeunit == \"m\":\n\t\t\ttimedelta = datetime.timedelta(minutes=number)\n\t\telif timeunit == \"h\":\n\t\t\ttimedelta = datetime.timedelta(hours=number)\n\t\telif timeunit == \"d\":\n\t\t\ttimedelta = datetime.timedelta(days=number)\n\t\telse:\n\t\t\traise ValueError(\"Unsupported time unit %s\" % timeunit)\n\n\t\treturn timedelta\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttimeunit, number = frequency.strip().split(\" \", 1)\n\n\tif timeunit == \"always\":\n\t\treturn None\n\n\ttry:\n\t\ttimeunit = timeunit.lower()\n\t\tnumber = int(number)\n\t\treturn datetime.timedelta(timeunit=number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency '{0}'\".format(frequency))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber, unit = frequency.split(\" \")\n\t\tnumber = float(number)\n\t\tif unit == \"day\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif unit == \"month\":\n\t\t\treturn datetime.timedelta(days=number*30)\n\t\telif unit == \"year\":\n\t\t\treturn datetime.timedelta(days=number*365)\n\t\telse:\n\t\t\traise ValueError(\"Unknown time unit: '%s'\" % unit)\n\texcept ValueError:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t\"\"\"\n\tGiven a frequency string with a number and a unit of time, return a corresponding\n     datetime.timedelta instance.\n     If the frequency is None or \"always\", return None.\n     Raise ValueError if the given frequency cannot be parsed.\n     For instance, given \"3 timeunit\", return datetime.timedelta(timeunit=3)\n\n     @param frequency :  A frequency string \"number timeunit\"\n\n    @return str, the corresponding datetime\n\t\"\"\"\n\ttry:\n\t\tnumber = float(frequency[:-len(frequency[-1])])\n\t\tif not number:\n\t\t\traise ValueError()\n\t\ttimeunit = frequency[-1].lower()\n\t\tif timeunit not in TIMEUNITS:\n\t\t\traise ValueError()\n\t\treturn datetime.timedelta(**{timeunit: number})\n\texcept ValueError:\n\t\tif frequency == \"always\":\n\t\t\treturn None\n\t\traise ValueError()\n\n#data_analysis/data_analysis.py\n\"\"\"\n\n@author: yang\n\"\"\"\n\nimport datetime\nimport timeutc\nimport os\n\nif __name__ == \"__main__\":\n\tinterval = timeutc.parse_frequency(\"1 timeunit\")\n\tprint(interval)\n\tprint(interval.seconds)\n\tprint(interval.days)\n\tprint(interval.total_seconds())\n\tprint(datetime.datetime.now() + interval)", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = str(host)\n\tif host == 'localhost' or host == '127.0.0.1' or host == 'localhost.localdomain' or host == 'localhost.localdomain.local' or host == 'localhost.local': return True\n\telse: return False\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tis_local = True\n\tif \".\" in host:\n\t\tis_local = False\n\telif host == \"localhost\":\n\t\tis_local = True\n\telif host == \"127.0.0.1\":\n\t\tis_local = True\n\treturn is_local\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_hosts = ['localhost', '127.0.0.1', 'localhost.localdomain', 'localhost4', '127.0.0.1.localdomain']\n\treturn host in local_hosts\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thosts = [\n\t\t'localhost',\n\t\t'127.0.0.1',\n\t\t'::1',\n\t\t'localhost.localdomain',\n\t\t'127.0.0.1.localdomain'\n\t]\n\tif host.lower() in hosts:\n\t\treturn True\n\treturn False\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_ip = '127.0.0.1'\n\tlocalhost = 'localhost'\n\tlocal_domain = 'localdomain'\n\n\tif re.match('^127\\\\.0\\\\.0\\\\.1$', host) or re.match('^localhost$', host) or re.match(\n\t\t'^\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}$', host) or re.match('^localhost\\\\.localdomain$', host):\n\t\treturn True\n\n\treturn False", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\ttry:\n\t\tif host == \"localhost\" or host == \"127.0.0.1\":\n\t\t\treturn True\n\t\telif host in [\"127.0.0.1\", \"localhost\", \"localhost.localdomain\"]:\n\t\t\treturn True\n\t\telif host in [\"localhost\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\"]:\n\t\t\treturn True\n\t\treturn False\n\texcept Exception as e:\n\t\treturn False\n\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = host.lower()\n\n\treturn host == 'localhost' or host == '127.0.0.1' or host == 'localhost.localdomain' or host == 'localhost.local' or host == 'localhost'\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocalhost_ip = [\"127.0.0.1\", \"127.0.0.2\", \"localhost\", \"127.0.0.3\"]\n\tlocalhost_domain = [\"localhost\", \"127.0.0.4\", \"127.0.0.5\", \"127.0.0.6\"]\n\tlocalhost_name = [\"localhost\", \"127.0.0.7\", \"127.0.0.8\", \"127.0.0.9\"]\n\n\tif host in localhost_ip or host in localhost_domain or host in localhost_name:\n\t\treturn True\n\telse:\n\t\treturn False", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_host = [\"localhost\", \"127.0.0.1\"]\n\n\thost_ip = socket.gethostbyname(host)\n\n\tif host_ip in local_host:\n\t\treturn True\n\n\treturn False\n\n", "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = str(host)\n\tif host == 'localhost':\n\t\treturn True\n\n\ttry:\n\t\tlocalhost = socket.gethostbyname('localhost')\n\texcept socket.gaierror:\n\t\treturn False\n\n\tif host == 'localhost' or host == localhost:\n\t\treturn True\n\n\tif 'localhost' in host:\n\t\treturn True\n\n\treturn False\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = str(host)\n\tif host == 'localhost' or host == '127.0.0.1' or host == 'localhost.localdomain' or host == 'localhost.localdomain.local' or host == 'localhost.local': return True\n\telse: return False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tis_local = True\n\tif \".\" in host:\n\t\tis_local = False\n\telif host == \"localhost\":\n\t\tis_local = True\n\telif host == \"127.0.0.1\":\n\t\tis_local = True\n\treturn is_local\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_hosts = ['localhost', '127.0.0.1', 'localhost.localdomain', 'localhost4', '127.0.0.1.localdomain']\n\treturn host in local_hosts\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thosts = [\n\t\t'localhost',\n\t\t'127.0.0.1',\n\t\t'::1',\n\t\t'localhost.localdomain',\n\t\t'127.0.0.1.localdomain'\n\t]\n\tif host.lower() in hosts:\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_ip = '127.0.0.1'\n\tlocalhost = 'localhost'\n\tlocal_domain = 'localdomain'\n\n\tif re.match('^127\\\\.0\\\\.0\\\\.1$', host) or re.match('^localhost$', host) or re.match(\n\t\t'^\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}\\\\.\\\\d{1,3}$', host) or re.match('^localhost\\\\.localdomain$', host):\n\t\treturn True\n\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\ttry:\n\t\tif host == \"localhost\" or host == \"127.0.0.1\":\n\t\t\treturn True\n\t\telif host in [\"127.0.0.1\", \"localhost\", \"localhost.localdomain\"]:\n\t\t\treturn True\n\t\telif host in [\"localhost\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\", \"localhost.localdomain\"]:\n\t\t\treturn True\n\t\treturn False\n\texcept Exception as e:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = host.lower()\n\n\treturn host == 'localhost' or host == '127.0.0.1' or host == 'localhost.localdomain' or host == 'localhost.local' or host == 'localhost'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocalhost_ip = [\"127.0.0.1\", \"127.0.0.2\", \"localhost\", \"127.0.0.3\"]\n\tlocalhost_domain = [\"localhost\", \"127.0.0.4\", \"127.0.0.5\", \"127.0.0.6\"]\n\tlocalhost_name = [\"localhost\", \"127.0.0.7\", \"127.0.0.8\", \"127.0.0.9\"]\n\n\tif host in localhost_ip or host in localhost_domain or host in localhost_name:\n\t\treturn True\n\telse:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\tlocal_host = [\"localhost\", \"127.0.0.1\"]\n\n\thost_ip = socket.gethostbyname(host)\n\n\tif host_ip in local_host:\n\t\treturn True\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\t\"\"\"\n\tChecks if the host is the localhost,\n    the localhost include local IP, user name, local domain name, `localhost` and `127.0.0.1`\n\n    Args:\n        host: The hostname or ip\n\n    Returns:\n        True if the host is the localhost else False\n\t\"\"\"\n\thost = str(host)\n\tif host == 'localhost':\n\t\treturn True\n\n\ttry:\n\t\tlocalhost = socket.gethostbyname('localhost')\n\texcept socket.gaierror:\n\t\treturn False\n\n\tif host == 'localhost' or host == localhost:\n\t\treturn True\n\n\tif 'localhost' in host:\n\t\treturn True\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/borg/list.py", "project": "witten/borgmatic", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [path for path in find_paths if os.path.exists(path)]\n\tfind_paths = [find_paths] if len(find_paths) == 1 else find_paths\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\t", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [p for p in find_paths]\n\tnew_find_paths = []\n\tfor p in find_paths:\n\t\tif p.endswith(\"*\"):\n\t\t\tnew_find_paths.append(p)\n\t\telse:\n\t\t\tnew_find_paths.append(p + \"*\")\n\treturn tuple(new_find_paths)\n\n#src/util/paths.py\nimport os\nimport sys\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [p.rstrip('/') for p in find_paths]\n\tfind_paths = [p for p in find_paths if p.strip()]\n\tfind_paths = [p for p in find_paths if not p.startswith('*')]\n\tfind_paths = [p for p in find_paths if not p.endswith('*')]\n\tfind_paths = [p for p in find_paths if not p.endswith('**')]\n\tfind_paths = [p for p in find_paths if not p.startswith('**')]\n\tfind_paths = [p for p in find_paths if not p.startswith('**/')]\n\tfind_paths = [p for p in find_paths if not p.endswith('**/')]\n\tfind_paths = [p for p in find_paths if not p.endswith('**')]\n\tfind_paths = [p for p in find_paths if not p.endswith('*')]\n\tfind_paths = [p for p in find_paths if not p.startswith('*')]\n\tfind_paths = [p for p in find_paths if not p.startswith('**/')]\n\tfind_paths = [p for", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tg_find_paths = []\n\tfor path in find_paths:\n\t\tfind_paths_patterns = []\n\t\tfor fp in path:\n\t\t\tfind_paths_patterns.append(fp.rglob('*'))\n\t\tg_find_paths.append(find_paths_patterns)\n\treturn tuple(g_find_paths)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tpatterns = []\n\tfor path in find_paths:\n\t\tif path.endswith(\"*\"):\n\t\t\tpatterns.append(path)\n\t\telse:\n\t\t\tpatterns.append(path + \"*\")\n\treturn tuple(patterns)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\ttransformed_paths = []\n\tfor path in find_paths:\n\t\tif path.startswith(\"*\"):\n\t\t\ttransformed_paths.append(path)\n\t\telse:\n\t\t\ttransformed_paths.append(\"*{}*\".format(path))\n\treturn tuple(transformed_paths)\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tpatterns = []\n\n\tfor path in find_paths:\n\t\tif path.startswith(\"*\"):\n\t\t\tpatterns.append(path)\n\t\telse:\n\t\t\tpatterns.append(f\"*{path}*\")\n\n\treturn tuple(patterns)\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [pathlib.Path(str(path)) for path in find_paths]\n\tfind_paths = tuple(pat.glob('**/*') for pat in find_paths)\n\treturn find_paths\n\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [p.replace('*', '*/*') for p in find_paths]\n\tfind_paths = ['{}/**/{}'.format(p, f) for p in find_paths for f in ['*', '**/*']]\n\treturn tuple(find_paths)\n", "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport re\n\tnew_find_paths = []\n\tfor find_path in find_paths:\n\t\tif re.search(\"[*?]$\", find_path):\n\t\t\t# if a path ends with * or ?, we don't need to wrap it with glob\n\t\t\tnew_find_paths.append(find_path)\n\t\telse:\n\t\t\tnew_find_paths.append(re.sub(\"\\\\*$\", \"*\", re.sub(\"\\\\?$\", \"?\", find_path)))\n\treturn tuple(new_find_paths)"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [path for path in find_paths if os.path.exists(path)]\n\tfind_paths = [find_paths] if len(find_paths) == 1 else find_paths\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\tfind_paths = [os.path.expandvars(path) for path in find_paths]\n\tfind_paths = [os.path.expanduser(path) for path in find_paths]\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [p for p in find_paths]\n\tnew_find_paths = []\n\tfor p in find_paths:\n\t\tif p.endswith(\"*\"):\n\t\t\tnew_find_paths.append(p)\n\t\telse:\n\t\t\tnew_find_paths.append(p + \"*\")\n\treturn tuple(new_find_paths)\n\n#src/util/paths.py\nimport os\nimport sys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [p.rstrip('/') for p in find_paths]\n\tfind_paths = [p for p in find_paths if p.strip()]\n\tfind_paths = [p for p in find_paths if not p.startswith('*')]\n\tfind_paths = [p for p in find_paths if not p.endswith('*')]\n\tfind_paths = [p for p in find_paths if not p.endswith('**')]\n\tfind_paths = [p for p in find_paths if not p.startswith('**')]\n\tfind_paths = [p for p in find_paths if not p.startswith('**/')]\n\tfind_paths = [p for p in find_paths if not p.endswith('**/')]\n\tfind_paths = [p for p in find_paths if not p.endswith('**')]\n\tfind_paths = [p for p in find_paths if not p.endswith('*')]\n\tfind_paths = [p for p in find_paths if not p.startswith('*')]\n\tfind_paths = [p for p in find_paths if not p.startswith('**/')]\n\tfind_paths = [p for", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tg_find_paths = []\n\tfor path in find_paths:\n\t\tfind_paths_patterns = []\n\t\tfor fp in path:\n\t\t\tfind_paths_patterns.append(fp.rglob('*'))\n\t\tg_find_paths.append(find_paths_patterns)\n\treturn tuple(g_find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tpatterns = []\n\tfor path in find_paths:\n\t\tif path.endswith(\"*\"):\n\t\t\tpatterns.append(path)\n\t\telse:\n\t\t\tpatterns.append(path + \"*\")\n\treturn tuple(patterns)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\ttransformed_paths = []\n\tfor path in find_paths:\n\t\tif path.startswith(\"*\"):\n\t\t\ttransformed_paths.append(path)\n\t\telse:\n\t\t\ttransformed_paths.append(\"*{}*\".format(path))\n\treturn tuple(transformed_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tpatterns = []\n\n\tfor path in find_paths:\n\t\tif path.startswith(\"*\"):\n\t\t\tpatterns.append(path)\n\t\telse:\n\t\t\tpatterns.append(f\"*{path}*\")\n\n\treturn tuple(patterns)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [pathlib.Path(str(path)) for path in find_paths]\n\tfind_paths = tuple(pat.glob('**/*') for pat in find_paths)\n\treturn find_paths\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\tfind_paths = [p.replace('*', '*/*') for p in find_paths]\n\tfind_paths = ['{}/**/{}'.format(p, f) for p in find_paths for f in ['*', '**/*']]\n\treturn tuple(find_paths)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t\"\"\"\n\tGiven a sequence of path, transform all path into glob patterns. Pass through existing patterns untouched.\n\n    Args:\n        find_paths: sequence of path\n    Returns:\n        tuple of transformed path\n\t\"\"\"\n\timport re\n\tnew_find_paths = []\n\tfor find_path in find_paths:\n\t\tif re.search(\"[*?]$\", find_path):\n\t\t\t# if a path ends with * or ?, we don't need to wrap it with glob\n\t\t\tnew_find_paths.append(find_path)\n\t\telse:\n\t\t\tnew_find_paths.append(re.sub(\"\\\\*$\", \"*\", re.sub(\"\\\\?$\", \"?\", find_path)))\n\treturn tuple(new_find_paths)", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/util.py", "project": "cloudmesh/cloudmesh-common", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\tcommand = \"where git\"\n\ttry:\n\t\tos.system(command)\n\t\treturn True\n\texcept:\n\t\treturn False", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\tis_windows = platform.system().lower() == \"windows\"\n\tif is_windows:\n\t\treturn True\n\treturn False\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\tif platform.system() == \"Windows\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\n\tdef _is_gitbash():\n\t\treturn os.name == 'nt' and os.environ.get('CONSOLE_SESSION') == 'gitbash'\n\n\treturn _is_gitbash()\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\ttry:\n\t\timport winreg as reg\n\texcept ImportError:\n\t\treturn False\n\n\tif reg.ConnectRegistry(None, reg.HKEY_LOCAL_MACHINE) is None:\n\t\treturn False\n\n\ttry:\n\t\ta = reg.OpenKey(reg.HKEY_LOCAL_MACHINE, r'SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment')\n\texcept WindowsError as e:\n\t\tprint(e)\n\t\treturn False\n\n\ttry:\n\t\tb = reg.OpenKey(reg.HKEY_LOCAL_MACHINE, r'SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment\\GIT_WINDOWS_EMULATOR')\n\texcept WindowsError as e:\n\t\tprint(e)\n\t\treturn False\n\n\tgitbash = reg.QueryValueEx(a, 'PATH')\n\tgitbash = gitbash[0].split(';')\n\tgitbash = [path for path in gitbash if 'git-bash.exe' in path]\n\n\tif len(gitbash) > 0:\n\t\treturn True\n\n\treturn False", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn \"GitBash\" in os.environ['CONSOLE_SESSION_ID']\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.name.lower() == \"nt\" and os.environ.get(\"CONSOLE_EMULATION\") == \"True\"", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport sys\n\n\treturn sys.platform.startswith('win')\n\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\timport sys\n\tgitbash_path = \"C:\\\\Windows\\\\System32\\\\bash.exe\"\n\treturn os.path.abspath(os.path.realpath(sys.executable)) == os.path.abspath(os.path.realpath(gitbash_path))\n", "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Windows\"\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\tcommand = \"where git\"\n\ttry:\n\t\tos.system(command)\n\t\treturn True\n\texcept:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\tis_windows = platform.system().lower() == \"windows\"\n\tif is_windows:\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\tif platform.system() == \"Windows\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\n\tdef _is_gitbash():\n\t\treturn os.name == 'nt' and os.environ.get('CONSOLE_SESSION') == 'gitbash'\n\n\treturn _is_gitbash()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\ttry:\n\t\timport winreg as reg\n\texcept ImportError:\n\t\treturn False\n\n\tif reg.ConnectRegistry(None, reg.HKEY_LOCAL_MACHINE) is None:\n\t\treturn False\n\n\ttry:\n\t\ta = reg.OpenKey(reg.HKEY_LOCAL_MACHINE, r'SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment')\n\texcept WindowsError as e:\n\t\tprint(e)\n\t\treturn False\n\n\ttry:\n\t\tb = reg.OpenKey(reg.HKEY_LOCAL_MACHINE, r'SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment\\GIT_WINDOWS_EMULATOR')\n\texcept WindowsError as e:\n\t\tprint(e)\n\t\treturn False\n\n\tgitbash = reg.QueryValueEx(a, 'PATH')\n\tgitbash = gitbash[0].split(';')\n\tgitbash = [path for path in gitbash if 'git-bash.exe' in path]\n\n\tif len(gitbash) > 0:\n\t\treturn True\n\n\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn \"GitBash\" in os.environ['CONSOLE_SESSION_ID']\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\treturn os.name.lower() == \"nt\" and os.environ.get(\"CONSOLE_EMULATION\") == \"True\"", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport sys\n\n\treturn sys.platform.startswith('win')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport os\n\timport sys\n\tgitbash_path = \"C:\\\\Windows\\\\System32\\\\bash.exe\"\n\treturn os.path.abspath(os.path.realpath(sys.executable)) == os.path.abspath(os.path.realpath(gitbash_path))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t\"\"\"\n\tCheck whether the current program is run in a Windows gitbash terminal\n\n    Returns:\n            boolean, True if gitbash else False\n\t\"\"\"\n\timport platform\n\treturn platform.system() == \"Windows\"\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/generate.py", "project": "witten/borgmatic", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'w') as fd:\n\t\t\tfd.write(rendered_config)\n\texcept FileNotFoundError:\n\t\twith open(config_filename, 'w') as fd:\n\t\t\tfd.write(rendered_config)\n\n\t\tprint(f'[INFO] {config_filename} does not exist, created it')\n\n\tfinally:\n\t\twith open(config_filename, 'r') as fd:\n\t\t\tprint(f'[INFO] {config_filename} content:')\n\t\t\tprint(fd.read())\n\n\t\tprint('[INFO] Writing configuration to {config_filename}...'.format(**locals()))\n\t\tif mode:\n\t\t\tos.chmod(config_filename, mode)\n\n#src/config/write_config.py\n#!/usr/bin/env python\n\nimport os\nimport shutil\n\nfrom . import write_configuration\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\n\tconfig_dir = os.path.dirname(config_filename)\n\tif not os.path.isdir(config_dir):\n\t\tos.makedirs(config_dir)\n\n\tif os.path.isfile(config_filename):\n\t\tif not overwrite:\n\t\t\tprint('Configuration file {} already exists, not overwriting.'.format(config_filename))\n\t\t\treturn\n\t\tos.remove(config_filename)\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\n\tos.chmod(config_filename, mode)", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\twith open(config_filename, 'w') as config_file:\n\t\tconfig_file.write(rendered_config)\n\t\tconfig_file.close()\n\tos.chmod(config_filename, mode)", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tconfig_filename = os.path.abspath(config_filename)\n\tif not os.path.exists(os.path.dirname(config_filename)):\n\t\tos.makedirs(os.path.dirname(config_filename))\n\tif os.path.exists(config_filename) and overwrite is False:\n\t\tprint(f\"Configuration file {config_filename} already exists. Cannot overwrite. Abort.\")\n\t\treturn\n\tif os.path.exists(config_filename):\n\t\tprint(f\"Removing old config file {config_filename}\")\n\t\tos.remove(config_filename)\n\tprint(f\"Writing new configuration file {config_filename}\")\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\n\tif os.path.isfile(config_filename) and not overwrite:\n\t\tprint('Config file \"%s\" already exists. Aborting...' % config_filename)\n\t\treturn\n\n\twith open(config_filename, 'w') as configfile:\n\t\tconfigfile.write(rendered_config)\n\n\tprint('Wrote config %s' % config_filename)\n\n\treturn", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tconfig_file = os.path.join(RENDERED_CONFIG_DIR, config_filename)\n\tif os.path.exists(config_file) and not overwrite:\n\t\traise Exception(f\"Configuration file {config_file} already exists. Aborting.\")\n\telif not os.path.exists(RENDERED_CONFIG_DIR):\n\t\tos.makedirs(RENDERED_CONFIG_DIR)\n\twith open(config_file, \"w\") as f:\n\t\tf.write(rendered_config)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport shutil\n\timport yaml\n\n\tconfig_dir = os.path.dirname(config_filename)\n\tos.makedirs(config_dir, exist_ok=True)\n\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise Exception(\"'{}' already exists\".format(config_filename))\n\n\n\twith open(config_filename, \"w\") as f:\n\t\tyaml.dump(rendered_config, f, default_flow_style=False)\n\t\tmode = mode | 0o600\n\t\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport fnmatch\n\timport errno\n\n\tconfig_file = open(config_filename, \"w\")\n\n\tif not overwrite:\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tos.stat(config_filename)\n\t\t\t\tbreak\n\t\t\texcept OSError as e:\n\t\t\t\tif e.errno == errno.EEXIST and os.path.isfile(config_filename):\n\t\t\t\t\traise IOError(\"Configuration file already exists\")\n\n\tconfig_file.write(rendered_config)\n\tconfig_file.close()\n\n\treturn None", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'r') as f:\n\t\t\texisting_config = f.read()\n\texcept FileNotFoundError:\n\t\texisting_config = ''\n\n\tif overwrite or existing_config == rendered_config:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tprint(\"Wrote {}\".format(config_filename))\n\telse:\n\t\tprint(\"The file {} already exists. Aborting.\".format(config_filename))\n\n\nif __name__ == '__main__':\n\timport sys\n\tfrom yaml import load, dump\n\n\twrite_configuration(sys.argv[1], load(sys.argv[2]), mode=int(sys.argv[3]), overwrite=bool(sys.argv[4]))", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tconfig_dir = os.path.dirname(config_filename)\n\tif not os.path.exists(config_dir):\n\t\tos.makedirs(config_dir)\n\n\tif not overwrite and os.path.exists(config_filename):\n\t\tprint(\"Configuration file {} already exists, and overwrite=False.\".format(config_filename))\n\t\treturn\n\n\tif os.path.exists(config_filename):\n\t\tprint(\"Overwriting existing configuration file {}\".format(config_filename))\n\n\twith open(config_filename, 'w') as config_file:\n\t\tconfig_file.write(rendered_config)\n\n\tprint(\"Wrote {}\".format(config_filename))\n\n\treturn\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'w') as fd:\n\t\t\tfd.write(rendered_config)\n\texcept FileNotFoundError:\n\t\twith open(config_filename, 'w') as fd:\n\t\t\tfd.write(rendered_config)\n\n\t\tprint(f'[INFO] {config_filename} does not exist, created it')\n\n\tfinally:\n\t\twith open(config_filename, 'r') as fd:\n\t\t\tprint(f'[INFO] {config_filename} content:')\n\t\t\tprint(fd.read())\n\n\t\tprint('[INFO] Writing configuration to {config_filename}...'.format(**locals()))\n\t\tif mode:\n\t\t\tos.chmod(config_filename, mode)\n\n#src/config/write_config.py\n#!/usr/bin/env python\n\nimport os\nimport shutil\n\nfrom . import write_configuration\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\n\tconfig_dir = os.path.dirname(config_filename)\n\tif not os.path.isdir(config_dir):\n\t\tos.makedirs(config_dir)\n\n\tif os.path.isfile(config_filename):\n\t\tif not overwrite:\n\t\t\tprint('Configuration file {} already exists, not overwriting.'.format(config_filename))\n\t\t\treturn\n\t\tos.remove(config_filename)\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\n\tos.chmod(config_filename, mode)", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\twith open(config_filename, 'w') as config_file:\n\t\tconfig_file.write(rendered_config)\n\t\tconfig_file.close()\n\tos.chmod(config_filename, mode)", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tconfig_filename = os.path.abspath(config_filename)\n\tif not os.path.exists(os.path.dirname(config_filename)):\n\t\tos.makedirs(os.path.dirname(config_filename))\n\tif os.path.exists(config_filename) and overwrite is False:\n\t\tprint(f\"Configuration file {config_filename} already exists. Cannot overwrite. Abort.\")\n\t\treturn\n\tif os.path.exists(config_filename):\n\t\tprint(f\"Removing old config file {config_filename}\")\n\t\tos.remove(config_filename)\n\tprint(f\"Writing new configuration file {config_filename}\")\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\n\tif os.path.isfile(config_filename) and not overwrite:\n\t\tprint('Config file \"%s\" already exists. Aborting...' % config_filename)\n\t\treturn\n\n\twith open(config_filename, 'w') as configfile:\n\t\tconfigfile.write(rendered_config)\n\n\tprint('Wrote config %s' % config_filename)\n\n\treturn", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tconfig_file = os.path.join(RENDERED_CONFIG_DIR, config_filename)\n\tif os.path.exists(config_file) and not overwrite:\n\t\traise Exception(f\"Configuration file {config_file} already exists. Aborting.\")\n\telif not os.path.exists(RENDERED_CONFIG_DIR):\n\t\tos.makedirs(RENDERED_CONFIG_DIR)\n\twith open(config_file, \"w\") as f:\n\t\tf.write(rendered_config)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport shutil\n\timport yaml\n\n\tconfig_dir = os.path.dirname(config_filename)\n\tos.makedirs(config_dir, exist_ok=True)\n\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise Exception(\"'{}' already exists\".format(config_filename))\n\n\n\twith open(config_filename, \"w\") as f:\n\t\tyaml.dump(rendered_config, f, default_flow_style=False)\n\t\tmode = mode | 0o600\n\t\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\timport os\n\timport fnmatch\n\timport errno\n\n\tconfig_file = open(config_filename, \"w\")\n\n\tif not overwrite:\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tos.stat(config_filename)\n\t\t\t\tbreak\n\t\t\texcept OSError as e:\n\t\t\t\tif e.errno == errno.EEXIST and os.path.isfile(config_filename):\n\t\t\t\t\traise IOError(\"Configuration file already exists\")\n\n\tconfig_file.write(rendered_config)\n\tconfig_file.close()\n\n\treturn None", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\ttry:\n\t\twith open(config_filename, 'r') as f:\n\t\t\texisting_config = f.read()\n\texcept FileNotFoundError:\n\t\texisting_config = ''\n\n\tif overwrite or existing_config == rendered_config:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tprint(\"Wrote {}\".format(config_filename))\n\telse:\n\t\tprint(\"The file {} already exists. Aborting.\".format(config_filename))\n\n\nif __name__ == '__main__':\n\timport sys\n\tfrom yaml import load, dump\n\n\twrite_configuration(sys.argv[1], load(sys.argv[2]), mode=int(sys.argv[3]), overwrite=bool(sys.argv[4]))", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t\"\"\"\n\tGiven a rendered config YAML, write it out to target file.\n    But if the file already exists and overwrite is False,\n    abort before writing anything.\n    If the file does not exist, create it.\n    Write to the file otherwise.\n\n    Returns: None\n\t\"\"\"\n\tconfig_dir = os.path.dirname(config_filename)\n\tif not os.path.exists(config_dir):\n\t\tos.makedirs(config_dir)\n\n\tif not overwrite and os.path.exists(config_filename):\n\t\tprint(\"Configuration file {} already exists, and overwrite=False.\".format(config_filename))\n\t\treturn\n\n\tif os.path.exists(config_filename):\n\t\tprint(\"Overwriting existing configuration file {}\".format(config_filename))\n\n\twith open(config_filename, 'w') as config_file:\n\t\tconfig_file.write(rendered_config)\n\n\tprint(\"Wrote {}\".format(config_filename))\n\n\treturn\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cloudmesh/common/Shell.py", "project": "cloudmesh/cloudmesh-common", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcmd = \"\"\n\ti = 0\n\twhile i < len(script):\n\t\tcmd += script[i]\n\t\tif script[i] == \";\" or script[i] == \"\\n\":\n\t\t\ti += 1\n\t\tif script[i] == \"&&\" or script[i] == \"||\":\n\t\t\ti += 1\n\t\tif script[i] == \"&\":\n\t\t\tcmd += \"\\n\"\n\t\tif script[i] == \"|\":\n\t\t\ti += 1\n\t\tif script[i] == \";\" or script[i] == \"\\n\" or script[i] == \"&\" or script[i] == \"|\":\n\t\t\tcmd += seperator\n\t\t\ti += 1\n\t\tif i == len(script):\n\t\t\tbreak\n\treturn cmd\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.replace(\"\\r\", \"\")\n\tscript = script.replace(\"\\n\", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tif script.startswith(\" \"):\n\t\tscript = script[1:]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\"\\n\"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.startswith(\" \"):\n\t\tscript = script[1:]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\"\\n\"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.replace(\"\\r\\n\", \"\")\n\tscript = script.replace(\"\\n\", \" \")\n\tscript = script.replace(\"\\t\", \" \")\n\tscript = script.replace(\" && \", \" & \")\n\tscript = script.replace(\" || \", \" | \")\n\tscript = script.replace(\" &&& \", \" && \")\n\tscript = script.replace(\" ||| \", \" || \")\n\tscript = script.replace(\" ; \", \" \")\n\tscript = script.replace(\" && \", \" & \")\n\tscript = script.replace(\" || \", \" | \")\n\tscript = script.replace(\" ; \", \" \")\n\tscript = script.replace(\" ; \", \" \")\n\tscript = script.replace(\" && \", \" & \")\n\tscript = script.replace(\" || \", \" | \")\n\tscript = script.replace(\";\", \" \")\n\tscript = script.replace(\"&&\", \" \")\n\tscript = script.replace(\"||\", \" \")\n\treturn script\n\n#src/script_helper.py\nimport os\nimport sys\nimport logging\nimport logging.config\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\timport re\n\tout = \"\"\n\tfor line in script.splitlines():\n\t\t# remove comments\n\t\tline = re.sub(\"^(#|;).*\", \"\", line)\n\t\tif line:\n\t\t\tout += line + seperator\n\treturn out.rstrip(seperator)", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.strip()\n\tscript = script.replace(\"\\r\", \"\")\n\tscript = script.replace(\"\\n\", \"\")\n\tscript = script.replace(\"&&\", seperator + \" \")\n\tscript = script.replace(\"&\", \" \" + seperator + \" \")\n\treturn script\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split(\"\\n\")\n\tlines = [line.strip() for line in lines if line]\n\treturn seperator.join(lines)\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split(\"\\n\")\n\tnew_lines = []\n\tfor line in lines:\n\t\tnew_line = line.strip()\n\t\tif new_line:\n\t\t\tnew_lines.append(new_line)\n\treturn seperator.join(new_lines)\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcommand = \"\"\n\tsplit_command = script.split(seperator)\n\tfor index in range(len(split_command)):\n\t\tif split_command[index] == \"\":\n\t\t\tcontinue\n\t\tcommand += split_command[index]\n\t\tcommand += \" \"\n\treturn command\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tnew_lines = []\n        for line in script.splitlines():\n                if len(line.strip()) == 0:\n                        continue\n                if line.endswith(seperator):\n                        new_lines.append(line[:-len(seperator)])\n                else:\n                        new_lines.append(line)\n\n\treturn seperator.join(new_lines)\n\n", "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tarr = script.split(seperator)\n\treturn \"\\n\".join(arr)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.replace(\"\\r\", \"\")\n\tscript = script.replace(\"\\n\", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tscript = script.replace(\"  \", \" \")\n\tif script.startswith(\" \"):\n\t\tscript = script[1:]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\"\\n\"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.startswith(\" \"):\n\t\tscript = script[1:]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\"\\n\"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):\n\t\tscript = script[:-1]\n\tif script.endswith(\" \"):", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.replace(\"\\r\\n\", \"\")\n\tscript = script.replace(\"\\n\", \" \")\n\tscript = script.replace(\"\\t\", \" \")\n\tscript = script.replace(\" && \", \" & \")\n\tscript = script.replace(\" || \", \" | \")\n\tscript = script.replace(\" &&& \", \" && \")\n\tscript = script.replace(\" ||| \", \" || \")\n\tscript = script.replace(\" ; \", \" \")\n\tscript = script.replace(\" && \", \" & \")\n\tscript = script.replace(\" || \", \" | \")\n\tscript = script.replace(\" ; \", \" \")\n\tscript = script.replace(\" ; \", \" \")\n\tscript = script.replace(\" && \", \" & \")\n\tscript = script.replace(\" || \", \" | \")\n\tscript = script.replace(\";\", \" \")\n\tscript = script.replace(\"&&\", \" \")\n\tscript = script.replace(\"||\", \" \")\n\treturn script\n\n#src/script_helper.py\nimport os\nimport sys\nimport logging\nimport logging.config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\timport re\n\tout = \"\"\n\tfor line in script.splitlines():\n\t\t# remove comments\n\t\tline = re.sub(\"^(#|;).*\", \"\", line)\n\t\tif line:\n\t\t\tout += line + seperator\n\treturn out.rstrip(seperator)", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tscript = script.strip()\n\tscript = script.replace(\"\\r\", \"\")\n\tscript = script.replace(\"\\n\", \"\")\n\tscript = script.replace(\"&&\", seperator + \" \")\n\tscript = script.replace(\"&\", \" \" + seperator + \" \")\n\treturn script\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split(\"\\n\")\n\tlines = [line.strip() for line in lines if line]\n\treturn seperator.join(lines)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tlines = script.split(\"\\n\")\n\tnew_lines = []\n\tfor line in lines:\n\t\tnew_line = line.strip()\n\t\tif new_line:\n\t\t\tnew_lines.append(new_line)\n\treturn seperator.join(new_lines)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tcommand = \"\"\n\tsplit_command = script.split(seperator)\n\tfor index in range(len(split_command)):\n\t\tif split_command[index] == \"\":\n\t\t\tcontinue\n\t\tcommand += split_command[index]\n\t\tcommand += \" \"\n\treturn command\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tnew_lines = []\n        for line in script.splitlines():\n                if len(line.strip()) == 0:\n                        continue\n                if line.endswith(seperator):\n                        new_lines.append(line[:-len(seperator)])\n                else:\n                        new_lines.append(line)\n\n\treturn seperator.join(new_lines)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\t\"\"\"\n\tConvert a script to one line command with the given seperator.\n\n        Args:\n                script: str\n                separator: str\n        Returns:\n                str, the one-line command.\n\t\"\"\"\n\tarr = script.split(seperator)\n\treturn \"\\n\".join(arr)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lib/matplotlib/testing/__init__.py", "project": "rougier/matplotlib", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\n\tprocess = subprocess.run([func.__name__] + list(args) + ['--timeout', str(timeout)],\n\t                          stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t                          env={'PYTHONPATH': os.environ['PYTHONPATH']}\n\t                          if extra_env is None else extra_env)\n\n\treturn process\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\ttry:\n\t\timport subprocess\n\n\t\tenv = os.environ.copy()\n\t\tif extra_env:\n\t\t\tenv.update(extra_env)\n\t\treturn subprocess.run(func.__module__ + '.' + func.__name__ + ' ' + ' '.join(args), shell=True, env=env, timeout=timeout)\n\texcept ImportError:\n\t\tprint('subprocess_run_helper could not import subprocess')\n\t\treturn None\n\n#subprocess_run.py\nimport sys\nimport os\nimport subprocess_run_helper\n\n# The subprocess_run_helper function is a wrapper for the subprocess.run()\n# function.  It takes a function, any additional command line arguments,\n# and optional environment variables.  It returns a completed process\n# object.\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\ttry:\n\t\tp = subprocess.run(func, args, env=env, timeout=timeout)\n\texcept subprocess.TimeoutExpired:\n\t\tp = subprocess.CompletedProcess(args, timeout=timeout, returncode=1)\n\treturn p\n\n#src/utils/subprocess_run.py\n#!/usr/bin/env python3\n\"\"\"\nHelper methods for running sub-processes\n\"\"\"\n\nimport subprocess\nimport logging\nimport os\nfrom utils.subprocess_run_helper import subprocess_run_helper\n\nlog = logging.getLogger(__name__)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\ttry:\n\t\tpath = os.environ['WORKSPACE']\n\texcept KeyError:\n\t\tpath = '/home/jenkins/workspace'\n\tprocess = subprocess.run([path + \"/tools/\" + func.__name__] + list(args),\n\t\tenv=os.environ,\n\t\ttimeout=timeout,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tencoding=\"utf-8\",\n\t\tcheck=True,\n\t\tenv=os.environ if extra_env is None else extra_env)\n\treturn process\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tcmd_list = [func.__module__, func.__name__] + list(args)\n\twith subprocess.Popen(cmd_list, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False) as p:\n\t\tstdout, stderr = p.communicate()\n\t\treturn p.returncode, stdout, stderr\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\ttry:\n\t\timport subprocess\n\texcept ImportError:\n\t\traise ImportError(\"subprocess module has not been found. Please install the subprocess module.\")\n\n\tenv_vars = {}\n\tif extra_env:\n\t\tenv_vars.update(extra_env)\n\n\tnew_args = [func.__module__, func.__name__]\n\tnew_args.extend(args)\n\tproc = subprocess.run(new_args, timeout=timeout, env=env_vars)\n\n\treturn proc\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tproc = subprocess.run(\n\t\t[func.__module__, func.__name__, *args],\n\t\tcwd=os.path.dirname(func.__code__.co_filename),\n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\tif proc.returncode:\n\t\traise RuntimeError(f\"{func.__module__}.{func.__name__} failed with exit code {proc.returncode}\")\n\treturn proc\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tprocess = subprocess.run(func.__name__ + \" \" + \" \".join(args), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, timeout=timeout)\n\treturn process", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\n\tenv = dict(os.environ)\n\tif extra_env is not None:\n\t\tenv.update(extra_env)\n\tproc = subprocess.run(func.__module__ + '.' + func.__name__ + ' ' + func.__qualname__ + ' ' + ' '.join(args),\n\t\t\t\t\t\t\t\t\t\t\t\t shell=True, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n\treturn proc\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\targs_str = []\n\tif args:\n\t\targs_str = list(args)\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tresult = subprocess.run(\n\t\t[func.__module__ + '.' + func.__name__] + args_str,\n\t\tcheck=True,\n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\treturn result\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\n\tprocess = subprocess.run([func.__name__] + list(args) + ['--timeout', str(timeout)],\n\t                          stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t                          env={'PYTHONPATH': os.environ['PYTHONPATH']}\n\t                          if extra_env is None else extra_env)\n\n\treturn process\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\ttry:\n\t\timport subprocess\n\n\t\tenv = os.environ.copy()\n\t\tif extra_env:\n\t\t\tenv.update(extra_env)\n\t\treturn subprocess.run(func.__module__ + '.' + func.__name__ + ' ' + ' '.join(args), shell=True, env=env, timeout=timeout)\n\texcept ImportError:\n\t\tprint('subprocess_run_helper could not import subprocess')\n\t\treturn None\n\n#subprocess_run.py\nimport sys\nimport os\nimport subprocess_run_helper\n\n# The subprocess_run_helper function is a wrapper for the subprocess.run()\n# function.  It takes a function, any additional command line arguments,\n# and optional environment variables.  It returns a completed process\n# object.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\ttry:\n\t\tp = subprocess.run(func, args, env=env, timeout=timeout)\n\texcept subprocess.TimeoutExpired:\n\t\tp = subprocess.CompletedProcess(args, timeout=timeout, returncode=1)\n\treturn p\n\n#src/utils/subprocess_run.py\n#!/usr/bin/env python3\n\"\"\"\nHelper methods for running sub-processes\n\"\"\"\n\nimport subprocess\nimport logging\nimport os\nfrom utils.subprocess_run_helper import subprocess_run_helper\n\nlog = logging.getLogger(__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\timport os\n\ttry:\n\t\tpath = os.environ['WORKSPACE']\n\texcept KeyError:\n\t\tpath = '/home/jenkins/workspace'\n\tprocess = subprocess.run([path + \"/tools/\" + func.__name__] + list(args),\n\t\tenv=os.environ,\n\t\ttimeout=timeout,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tencoding=\"utf-8\",\n\t\tcheck=True,\n\t\tenv=os.environ if extra_env is None else extra_env)\n\treturn process\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tcmd_list = [func.__module__, func.__name__] + list(args)\n\twith subprocess.Popen(cmd_list, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False) as p:\n\t\tstdout, stderr = p.communicate()\n\t\treturn p.returncode, stdout, stderr\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\ttry:\n\t\timport subprocess\n\texcept ImportError:\n\t\traise ImportError(\"subprocess module has not been found. Please install the subprocess module.\")\n\n\tenv_vars = {}\n\tif extra_env:\n\t\tenv_vars.update(extra_env)\n\n\tnew_args = [func.__module__, func.__name__]\n\tnew_args.extend(args)\n\tproc = subprocess.run(new_args, timeout=timeout, env=env_vars)\n\n\treturn proc\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tproc = subprocess.run(\n\t\t[func.__module__, func.__name__, *args],\n\t\tcwd=os.path.dirname(func.__code__.co_filename),\n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\tif proc.returncode:\n\t\traise RuntimeError(f\"{func.__module__}.{func.__name__} failed with exit code {proc.returncode}\")\n\treturn proc\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tprocess = subprocess.run(func.__name__ + \" \" + \" \".join(args), shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env, timeout=timeout)\n\treturn process", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\timport subprocess\n\n\tenv = dict(os.environ)\n\tif extra_env is not None:\n\t\tenv.update(extra_env)\n\tproc = subprocess.run(func.__module__ + '.' + func.__name__ + ' ' + func.__qualname__ + ' ' + ' '.join(args),\n\t\t\t\t\t\t\t\t\t\t\t\t shell=True, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n\treturn proc\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t\"\"\"\n\tRun a function in a sub-process\n\n    Args:\n            func: function, the function to be run. It must be in a module that is importable.\n            *args: str. Any additional command line arguments to be passed in\n                        the first argument to ``subprocess.run``.\n            extra_env: dict[str, str]. Any additional environment variables to be set for the subprocess.\n    Returns:\n            CompletedProcess instance\n\t\"\"\"\n\targs_str = []\n\tif args:\n\t\targs_str = list(args)\n\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tresult = subprocess.run(\n\t\t[func.__module__ + '.' + func.__name__] + args_str,\n\t\tcheck=True,\n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\treturn result\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/config/override.py", "project": "witten/borgmatic", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tmatch = matcher.groupdict()\n\tname = match[\"name\"]\n\tdefault = match.get(\"default\", None)\n\ttry:\n\t\tif default is None:\n\t\t\tvalue = os.environ[name]\n\t\telse:\n\t\t\tvalue = os.environ.get(name, default)\n\t\treturn value\n\texcept KeyError:\n\t\terror = \"The environment variable '{}' is not defined\".format(name)\n\t\tif default is not None:\n\t\t\terror += \" (default: {})\".format(default)\n\t\traise KeyError(error)\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tif name not in os.environ:\n\t\tif default is None:\n\t\t\traise RuntimeError(f'Environment variable {name} is not defined.')\n\t\telse:\n\t\t\treturn default\n\telse:\n\t\treturn os.environ[name]\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\treturn os.environ[matcher['name']]\n\texcept KeyError:\n\t\tif 'default' not in matcher:\n\t\t\traise ValueError('The environment variable %s is not defined' % matcher['name'])\n\t\treturn matcher['default']\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.groupdict()['name']\n\tdefault = matcher.groupdict().get('default', None)\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise EnvironmentError(f\"No environment variable {name} in the environnement\")\n\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tvalue = os.environ.get(name, default)\n\tif value is None:\n\t\traise Exception(f'Environment variable {name} not found')\n\treturn value\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tmatcher_dict = matcher.groupdict()\n\tname = matcher_dict.get('name')\n\tdefault = matcher_dict.get('default')\n\tif name not in os.environ:\n\t\tif default is None:\n\t\t\traise Exception('Environment variable %s is not defined' % name)\n\t\telse:\n\t\t\tos.environ[name] = default\n\treturn os.environ[name]\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group(1)\n\tdefault_value = matcher.group(2)\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tif default_value is None:\n\t\t\traise ValueError(f\"Environment variable {name} is not defined\")\n\t\telse:\n\t\t\tvalue = default_value\n\treturn value\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tvalue = matcher.groupdict()[\"value\"]\n\texcept KeyError:\n\t\tvalue = None\n\n\tif not value and matcher.groupdict()[\"default\"]:\n\t\tvalue = matcher.groupdict()[\"default\"]\n\n\tif not value:\n\t\traise ValueError(f\"The variable {matcher.group(1)} is not defined in environment\")\n\n\treturn value\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tval = os.environ[matcher['name']]\n\texcept KeyError:\n\t\tif 'default' not in matcher:\n\t\t\traise Exception('No default value provided for environment variable {}'.format(matcher['name']))\n\t\telse:\n\t\t\tval = matcher['default']\n\treturn val\n", "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = os.environ.get(matcher['name'], matcher.get('default', None))\n\tif value is None:\n\t\traise Exception(f'Environment variable \"{matcher[\"name\"]}\" is not defined')\n\treturn value\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tmatch = matcher.groupdict()\n\tname = match[\"name\"]\n\tdefault = match.get(\"default\", None)\n\ttry:\n\t\tif default is None:\n\t\t\tvalue = os.environ[name]\n\t\telse:\n\t\t\tvalue = os.environ.get(name, default)\n\t\treturn value\n\texcept KeyError:\n\t\terror = \"The environment variable '{}' is not defined\".format(name)\n\t\tif default is not None:\n\t\t\terror += \" (default: {})\".format(default)\n\t\traise KeyError(error)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tif name not in os.environ:\n\t\tif default is None:\n\t\t\traise RuntimeError(f'Environment variable {name} is not defined.')\n\t\telse:\n\t\t\treturn default\n\telse:\n\t\treturn os.environ[name]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\treturn os.environ[matcher['name']]\n\texcept KeyError:\n\t\tif 'default' not in matcher:\n\t\t\traise ValueError('The environment variable %s is not defined' % matcher['name'])\n\t\treturn matcher['default']\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.groupdict()['name']\n\tdefault = matcher.groupdict().get('default', None)\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise EnvironmentError(f\"No environment variable {name} in the environnement\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\tvalue = os.environ.get(name, default)\n\tif value is None:\n\t\traise Exception(f'Environment variable {name} not found')\n\treturn value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tmatcher_dict = matcher.groupdict()\n\tname = matcher_dict.get('name')\n\tdefault = matcher_dict.get('default')\n\tif name not in os.environ:\n\t\tif default is None:\n\t\t\traise Exception('Environment variable %s is not defined' % name)\n\t\telse:\n\t\t\tos.environ[name] = default\n\treturn os.environ[name]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tname = matcher.group(1)\n\tdefault_value = matcher.group(2)\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tif default_value is None:\n\t\t\traise ValueError(f\"Environment variable {name} is not defined\")\n\t\telse:\n\t\t\tvalue = default_value\n\treturn value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tvalue = matcher.groupdict()[\"value\"]\n\texcept KeyError:\n\t\tvalue = None\n\n\tif not value and matcher.groupdict()[\"default\"]:\n\t\tvalue = matcher.groupdict()[\"default\"]\n\n\tif not value:\n\t\traise ValueError(f\"The variable {matcher.group(1)} is not defined in environment\")\n\n\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\ttry:\n\t\tval = os.environ[matcher['name']]\n\texcept KeyError:\n\t\tif 'default' not in matcher:\n\t\t\traise Exception('No default value provided for environment variable {}'.format(matcher['name']))\n\t\telse:\n\t\t\tval = matcher['default']\n\treturn val\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _resolve_string(matcher):\n\t\"\"\"\n\tGet the value from environment given a matcher containing a name and an optional default value in its group dict.\n    If the variable is not defined in environment and no default value is provided, an Error is raised.\n\t\"\"\"\n\tvalue = os.environ.get(matcher['name'], matcher.get('default', None))\n\tif value is None:\n\t\traise Exception(f'Environment variable \"{matcher[\"name\"]}\" is not defined')\n\treturn value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cinder/image/glance.py", "project": "openstack/cinder", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = re.sub(r'^\\w+://', '', image_href)\n\tif ':' in image_id:\n\t\timage_id = image_id.split(':')[0]\n\timage_id = image_id.split('.')[0]\n\timage_id = image_id.split('/')[-1]\n\timage_id = image_id.split('?')[0]\n\timage_id = image_id.replace('_', '0')\n\timage_id = image_id.replace('/', '_')\n\timage_id = image_id.replace('+', 'p')\n\tif image_id.startswith('0'):\n\t\timage_id = image_id[1:]\n\ttry:\n\t\timage_id = urllib.unquote(image_id)\n\texcept UnicodeEncodeError:\n\t\traise ValueError('Invalid image id: ' + image_id)\n\timage_id = image_id.replace('-', '_')\n\timage_id = image_id.replace('_', '0')\n\timage_id = image_id.replace('0', '')\n\timage_id = image_id.replace('p', '+')\n\timage_id = image_id.replace('~',", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id, _, use_ssl = image_href.partition(\"//\")\n\tif not image_id:\n\t\traise ValueError(\"image_id is empty\")\n\treturn image_id, image_id.partition(\"/\")[0], use_ssl == \"https\"\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id, netloc, use_ssl = None, None, None\n\tuse_ssl = False\n\timage_href = image_href.strip(\"<>\")\n\tif image_href == \"\":\n\t\treturn None\n\ttry:\n\t\timage_id, netloc = image_href.split(\",\")\n\texcept ValueError:\n\t\tpass\n\tif image_id == \"\":\n\t\treturn None\n\tif netloc == \"\":\n\t\treturn None\n\tnetloc = netloc.lower()\n\ttry:\n\t\tuse_ssl = netloc.startswith(\"https\")\n\texcept TypeError:\n\t\tuse_ssl = False\n\tif use_ssl:\n\t\tnetloc = netloc[len(\"https://\"):]\n\telse:\n\t\tnetloc = netloc[len(\"http://\"):]\n\tif netloc.startswith(\"www.\"):\n\t\tnetloc = netloc[len(\"www.\"):]\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed = parse.urlparse(image_href)\n\tif not parsed.netloc:\n\t\traise ValueError(\"Image url is missing a hostname\")\n\tnetloc = parsed.netloc\n\tuse_ssl = parsed.scheme == \"https\"\n\timage_id = parsed.path[1:]\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id, _, _ = image_href.partition('/')\n\tif not image_id:\n\t\traise ValueError('Unable to parse image ID from href {href}'.format(href=image_href))\n\n\t# check if href is absolute or not\n\tuse_ssl = re.match(r'^https?://', image_href) is not None\n\tnetloc, _, _ = image_href.partition('/')\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed = urlparse(image_href)\n\tif parsed.netloc not in IMAGE_NETLOCS:\n\t\traise ValueError(f\"Image netloc {parsed.netloc} not supported.\")\n\timage_id = parsed.path.split(\"/\")[-1]\n\tuse_ssl = parsed.scheme == \"https\"\n\treturn image_id, parsed.netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = image_href.split('/')[-1]\n\tparts = image_id.split('@')\n\tif len(parts) == 2:\n\t\tnetloc, use_ssl = parts\n\t\tif use_ssl == '1':\n\t\t\tuse_ssl = True\n\t\telif use_ssl == '0':\n\t\t\tuse_ssl = False\n\t\treturn image_id, netloc, use_ssl\n\telif len(parts) == 1:\n\t\treturn image_id, '', False\n\traise ValueError('Invalid image_href: {}'.format(image_href))\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = image_href.split(\"/\")\n\timage_id = parts[-1]\n\timage_id = image_id.split(\".\")[0]\n\tnetloc = parts[2]\n\tuse_ssl = netloc.startswith(\"https:\")\n\treturn image_id, netloc, use_ssl\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tpath_parts = image_href.split('/')\n\timage_id = path_parts[-1]\n\tnetloc = path_parts[2]\n\tif netloc == 'www.flickr.com':\n\t\tuse_ssl = False\n\telif netloc == 'a.phobos.apple.com':\n\t\tuse_ssl = True\n\telse:\n\t\traise ValueError('Unsupported netloc: ' + netloc)\n\treturn (image_id, netloc, use_ssl)\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = re.match(r\"^([a-z0-9\\-]+)\\.\", image_href).group(1)\n\tnetloc = re.match(r\"^http://([^/]+)\", image_href).group(1)\n\tuse_ssl = \"https\" == image_href[:7]\n\treturn image_id, netloc, use_ssl\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = re.sub(r'^\\w+://', '', image_href)\n\tif ':' in image_id:\n\t\timage_id = image_id.split(':')[0]\n\timage_id = image_id.split('.')[0]\n\timage_id = image_id.split('/')[-1]\n\timage_id = image_id.split('?')[0]\n\timage_id = image_id.replace('_', '0')\n\timage_id = image_id.replace('/', '_')\n\timage_id = image_id.replace('+', 'p')\n\tif image_id.startswith('0'):\n\t\timage_id = image_id[1:]\n\ttry:\n\t\timage_id = urllib.unquote(image_id)\n\texcept UnicodeEncodeError:\n\t\traise ValueError('Invalid image id: ' + image_id)\n\timage_id = image_id.replace('-', '_')\n\timage_id = image_id.replace('_', '0')\n\timage_id = image_id.replace('0', '')\n\timage_id = image_id.replace('p', '+')\n\timage_id = image_id.replace('~',", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id, _, use_ssl = image_href.partition(\"//\")\n\tif not image_id:\n\t\traise ValueError(\"image_id is empty\")\n\treturn image_id, image_id.partition(\"/\")[0], use_ssl == \"https\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id, netloc, use_ssl = None, None, None\n\tuse_ssl = False\n\timage_href = image_href.strip(\"<>\")\n\tif image_href == \"\":\n\t\treturn None\n\ttry:\n\t\timage_id, netloc = image_href.split(\",\")\n\texcept ValueError:\n\t\tpass\n\tif image_id == \"\":\n\t\treturn None\n\tif netloc == \"\":\n\t\treturn None\n\tnetloc = netloc.lower()\n\ttry:\n\t\tuse_ssl = netloc.startswith(\"https\")\n\texcept TypeError:\n\t\tuse_ssl = False\n\tif use_ssl:\n\t\tnetloc = netloc[len(\"https://\"):]\n\telse:\n\t\tnetloc = netloc[len(\"http://\"):]\n\tif netloc.startswith(\"www.\"):\n\t\tnetloc = netloc[len(\"www.\"):]\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed = parse.urlparse(image_href)\n\tif not parsed.netloc:\n\t\traise ValueError(\"Image url is missing a hostname\")\n\tnetloc = parsed.netloc\n\tuse_ssl = parsed.scheme == \"https\"\n\timage_id = parsed.path[1:]\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id, _, _ = image_href.partition('/')\n\tif not image_id:\n\t\traise ValueError('Unable to parse image ID from href {href}'.format(href=image_href))\n\n\t# check if href is absolute or not\n\tuse_ssl = re.match(r'^https?://', image_href) is not None\n\tnetloc, _, _ = image_href.partition('/')\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparsed = urlparse(image_href)\n\tif parsed.netloc not in IMAGE_NETLOCS:\n\t\traise ValueError(f\"Image netloc {parsed.netloc} not supported.\")\n\timage_id = parsed.path.split(\"/\")[-1]\n\tuse_ssl = parsed.scheme == \"https\"\n\treturn image_id, parsed.netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = image_href.split('/')[-1]\n\tparts = image_id.split('@')\n\tif len(parts) == 2:\n\t\tnetloc, use_ssl = parts\n\t\tif use_ssl == '1':\n\t\t\tuse_ssl = True\n\t\telif use_ssl == '0':\n\t\t\tuse_ssl = False\n\t\treturn image_id, netloc, use_ssl\n\telif len(parts) == 1:\n\t\treturn image_id, '', False\n\traise ValueError('Invalid image_href: {}'.format(image_href))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tparts = image_href.split(\"/\")\n\timage_id = parts[-1]\n\timage_id = image_id.split(\".\")[0]\n\tnetloc = parts[2]\n\tuse_ssl = netloc.startswith(\"https:\")\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\tpath_parts = image_href.split('/')\n\timage_id = path_parts[-1]\n\tnetloc = path_parts[2]\n\tif netloc == 'www.flickr.com':\n\t\tuse_ssl = False\n\telif netloc == 'a.phobos.apple.com':\n\t\tuse_ssl = True\n\telse:\n\t\traise ValueError('Unsupported netloc: ' + netloc)\n\treturn (image_id, netloc, use_ssl)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t\"\"\"\n\tParse an image href into composite parts import urllib.\n\n    :param image_href: href of an image\n    :returns: a tuple of the form (image_id, netloc, use_ssl)\n    :raises ValueError:\n\t\"\"\"\n\timage_id = re.match(r\"^([a-z0-9\\-]+)\\.\", image_href).group(1)\n\tnetloc = re.match(r\"^http://([^/]+)\", image_href).group(1)\n\tuse_ssl = \"https\" == image_href[:7]\n\treturn image_id, netloc, use_ssl\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "makeprojects/util.py", "project": "burgerbecky/makeprojects", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list != None:\n\t\tfor item in input_list:\n\t\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\t\toutput_list.append(item[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(item)\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tresult_list = []\n\tif input_list is None:\n\t\treturn result_list\n\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tresult_list.append(string[:-1])\n\t\telse:\n\t\t\tresult_list.append(string)\n\n\treturn result_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tresult = []\n\tif input_list == None:\n\t\treturn result\n\tfor each_string in input_list:\n\t\tif len(each_string) > 1 and each_string[len(each_string) - 1] == os.sep:\n\t\t\tresult.append(each_string[:len(each_string) - 1])\n\t\telse:\n\t\t\tresult.append(each_string)\n\treturn result\n\n#sandbox/os_seperator.py\nimport os\nfrom path_seperator import remove_ending_os_sep\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\tfor i in input_list:\n\t\tif i and len(i) > 1 and i[-1] == os.sep:\n\t\t\toutput_list.append(i[:-1])\n\t\telse:\n\t\t\toutput_list.append(i)\n\t\n\treturn output_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\tfor entry in input_list:\n\t\tif len(entry) > 1 and entry[-1] == os.path.sep or entry[-1] == os.path.altsep:\n\t\t\toutput_list.append(entry[:-1])\n\t\telse:\n\t\t\toutput_list.append(entry)\n\treturn output_list", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\tfor s in input_list:\n\t\tif len(s) > 1 and s[-1] == os.sep:\n\t\t\toutput_list.append(s[:-1])\n\t\telse:\n\t\t\toutput_list.append(s)\n\treturn output_list\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\n\tfor element in input_list:\n\t\tlength = len(element)\n\n\t\tif length > 1:\n\t\t\tif element[-1] == os.path.sep:\n\t\t\t\toutput_list.append(element[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(element)\n\t\telse:\n\t\t\toutput_list.append(element)\n\n\treturn output_list\n\n", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tnew_list=[]\n\tif input_list is None:\n\t\treturn new_list\n\tfor each_element in input_list:\n\t\tif each_element != \"\":\n\t\t\tif len(each_element) > 1:\n\t\t\t\tif each_element[-1] == os.sep:\n\t\t\t\t\tnew_list.append(each_element[:-1])\n\t\t\t\telse:\n\t\t\t\t\tnew_list.append(each_element)\n\treturn new_list", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tdef _remove_ending_os_sep(input_str):\n\t\tif input_str:\n\t\t\tif len(input_str) > 1:\n\t\t\t\tif input_str[-1] == os.sep:\n\t\t\t\t\tinput_str = input_str[:-1]\n\t\t\treturn input_str\n\t\treturn []\n\n\tif input_list:\n\t\treturn [_remove_ending_os_sep(item) for item in input_list]\n\treturn []", "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == \"\\\\\" or item[-1] == \"/\":\n\t\t\titem = item[:-1]\n\t\toutput_list.append(item)\n\n\treturn output_list\n\n\nif __name__ == \"__main__\":\n\tprint remove_ending_os_sep([\"C:\\\\\", \"D:\\\\\", \"E:\\\\\"])\n\tprint remove_ending_os_sep([\"\"])\n\tprint remove_ending_os_sep([None])"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list != None:\n\t\tfor item in input_list:\n\t\t\tif len(item) > 1 and item[-1] == os.sep:\n\t\t\t\toutput_list.append(item[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(item)\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tresult_list = []\n\tif input_list is None:\n\t\treturn result_list\n\n\tfor string in input_list:\n\t\tif len(string) > 1 and string[-1] == os.sep:\n\t\t\tresult_list.append(string[:-1])\n\t\telse:\n\t\t\tresult_list.append(string)\n\n\treturn result_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tresult = []\n\tif input_list == None:\n\t\treturn result\n\tfor each_string in input_list:\n\t\tif len(each_string) > 1 and each_string[len(each_string) - 1] == os.sep:\n\t\t\tresult.append(each_string[:len(each_string) - 1])\n\t\telse:\n\t\t\tresult.append(each_string)\n\treturn result\n\n#sandbox/os_seperator.py\nimport os\nfrom path_seperator import remove_ending_os_sep\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\t\n\toutput_list = []\n\tfor i in input_list:\n\t\tif i and len(i) > 1 and i[-1] == os.sep:\n\t\t\toutput_list.append(i[:-1])\n\t\telse:\n\t\t\toutput_list.append(i)\n\t\n\treturn output_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\tfor entry in input_list:\n\t\tif len(entry) > 1 and entry[-1] == os.path.sep or entry[-1] == os.path.altsep:\n\t\t\toutput_list.append(entry[:-1])\n\t\telse:\n\t\t\toutput_list.append(entry)\n\treturn output_list", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\tfor s in input_list:\n\t\tif len(s) > 1 and s[-1] == os.sep:\n\t\t\toutput_list.append(s[:-1])\n\t\telse:\n\t\t\toutput_list.append(s)\n\treturn output_list\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\t\n\tif input_list is None:\n\t\treturn []\n\n\toutput_list = []\n\n\tfor element in input_list:\n\t\tlength = len(element)\n\n\t\tif length > 1:\n\t\t\tif element[-1] == os.path.sep:\n\t\t\t\toutput_list.append(element[:-1])\n\t\t\telse:\n\t\t\t\toutput_list.append(element)\n\t\telse:\n\t\t\toutput_list.append(element)\n\n\treturn output_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tnew_list=[]\n\tif input_list is None:\n\t\treturn new_list\n\tfor each_element in input_list:\n\t\tif each_element != \"\":\n\t\t\tif len(each_element) > 1:\n\t\t\t\tif each_element[-1] == os.sep:\n\t\t\t\t\tnew_list.append(each_element[:-1])\n\t\t\t\telse:\n\t\t\t\t\tnew_list.append(each_element)\n\treturn new_list", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\tdef _remove_ending_os_sep(input_str):\n\t\tif input_str:\n\t\t\tif len(input_str) > 1:\n\t\t\t\tif input_str[-1] == os.sep:\n\t\t\t\t\tinput_str = input_str[:-1]\n\t\t\treturn input_str\n\t\treturn []\n\n\tif input_list:\n\t\treturn [_remove_ending_os_sep(item) for item in input_list]\n\treturn []", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\"\"\"\n\tIf input list is None, return []\n    Iterate over a string list and remove trailing os seperator characters.\n    Each string is tested if its length is greater than one and if the last\n    character is the pathname seperator.\n    Returns:\n    A list after removing trailing os seperator characters.\n\t\"\"\"\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == \"\\\\\" or item[-1] == \"/\":\n\t\t\titem = item[:-1]\n\t\toutput_list.append(item)\n\n\treturn output_list\n\n\nif __name__ == \"__main__\":\n\tprint remove_ending_os_sep([\"C:\\\\\", \"D:\\\\\", \"E:\\\\\"])\n\tprint remove_ending_os_sep([\"\"])\n\tprint remove_ending_os_sep([None])", "is_pass": false, "return_code": 1}]}
{"file_path": "shconfparser/search.py", "project": "kirankotari/shconfparser", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = pattern.strip() if strip else pattern\n\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\tif not isinstance(pattern, str):\n\t\tpattern = pattern.pattern\n\n\treturn re.compile(pattern)\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = re.sub('^\\s*', '', pattern)\n\t\tpattern = re.sub('\\s*$', '', pattern)\n\treturn re.compile(pattern)\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = re.sub(r\"^\\s+|\\s+$\", \"\", pattern)\n\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\ttry:\n\t\tpattern = re.compile(pattern)\n\texcept:\n\t\tpattern = re.compile(r\"{}\".format(pattern))\n\tif strip:\n\t\tpattern = re.sub(r\"^\\s+\", \"\", pattern.pattern)\n\t\tpattern = re.sub(r\"\\s+$\", \"\", pattern.pattern)\n\treturn pattern", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tfrom re import compile as regex\n\tfrom sys import stderr\n\tif isinstance(pattern, str):\n\t\tpattern = regex(pattern.replace(' ', ''), flags=regex.IGNORECASE)\n\tif not isinstance(pattern, regex):\n\t\tstderr.write(\"Pattern must be a string or a regex object\")\n\t\texit(1)\n\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\tif pattern.startswith('^'):\n\t\tpattern = pattern[1:]\n\n\tif pattern.endswith('$'):\n\t\tpattern = pattern[:-1]\n\n\tpattern = pattern.replace(' ', '')\n\n\tpattern = re.sub(r'\\([^()]*\\)', r\"\\1\", pattern)\n\n\treturn re.compile(pattern)\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n\tif isinstance(pattern, str):\n\t\treturn re.compile(pattern, flags=re.IGNORECASE)\n\telse:\n\t\treturn pattern\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif type(pattern) is str:\n\t\treturn re.compile(pattern, re.I)\n\telif type(pattern) is re.Pattern:\n\t\treturn pattern\n\telse:\n\t\treturn re.compile(pattern, re.I)\n\n", "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = re.sub(r'^\\s*', '', pattern)\n\t\tpattern = re.sub(r'\\s*$', '', pattern)\n\treturn re.compile(pattern)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = pattern.strip() if strip else pattern\n\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\tif not isinstance(pattern, str):\n\t\tpattern = pattern.pattern\n\n\treturn re.compile(pattern)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = re.sub('^\\s*', '', pattern)\n\t\tpattern = re.sub('\\s*$', '', pattern)\n\treturn re.compile(pattern)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = re.sub(r\"^\\s+|\\s+$\", \"\", pattern)\n\tpattern = re.compile(pattern)\n\treturn pattern\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\ttry:\n\t\tpattern = re.compile(pattern)\n\texcept:\n\t\tpattern = re.compile(r\"{}\".format(pattern))\n\tif strip:\n\t\tpattern = re.sub(r\"^\\s+\", \"\", pattern.pattern)\n\t\tpattern = re.sub(r\"\\s+$\", \"\", pattern.pattern)\n\treturn pattern", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tfrom re import compile as regex\n\tfrom sys import stderr\n\tif isinstance(pattern, str):\n\t\tpattern = regex(pattern.replace(' ', ''), flags=regex.IGNORECASE)\n\tif not isinstance(pattern, regex):\n\t\tstderr.write(\"Pattern must be a string or a regex object\")\n\t\texit(1)\n\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\tif pattern.startswith('^'):\n\t\tpattern = pattern[1:]\n\n\tif pattern.endswith('$'):\n\t\tpattern = pattern[:-1]\n\n\tpattern = pattern.replace(' ', '')\n\n\tpattern = re.sub(r'\\([^()]*\\)', r\"\\1\", pattern)\n\n\treturn re.compile(pattern)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\t\n\tif isinstance(pattern, str):\n\t\treturn re.compile(pattern, flags=re.IGNORECASE)\n\telse:\n\t\treturn pattern\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\timport re\n\tif type(pattern) is str:\n\t\treturn re.compile(pattern, re.I)\n\telif type(pattern) is re.Pattern:\n\t\treturn pattern\n\telse:\n\t\treturn re.compile(pattern, re.I)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\t\"\"\"\n\tConvert a string to a regex pattern object\n\n    Args:\n            pattern: str or regex pattern object\n            strip: boolean, whether the spaces in the start/end will be stripped\n    Returns:\n            regex pattern object\n\t\"\"\"\n\tpattern = str(pattern)\n\tif strip:\n\t\tpattern = re.sub(r'^\\s*', '', pattern)\n\t\tpattern = re.sub(r'\\s*$', '', pattern)\n\treturn re.compile(pattern)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "makeprojects/core.py", "project": "burgerbecky/makeprojects", "name": "run_command", "docstring": "Call the given command(s).", "_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\tprocess = subprocess.Popen(commands, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\tstdout, stderr = process.communicate(args)\n\tretcode = process.returncode\n\n\tif verbose:\n\t\tif stdout:\n\t\t\tprint('[STDOUT]:')\n\t\t\tprint(stdout)\n\t\tif stderr:\n\t\t\tprint('[STDERR]:')\n\t\t\tprint(stderr)\n\n\tif hide_stderr:\n\t\tstderr = None\n\n\treturn stdout, stderr, retcode\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\tcmd = subprocess.Popen(commands,\n\t\t\t\t\t\t   stdin=subprocess.PIPE,\n\t\t\t\t\t\t   stdout=subprocess.PIPE,\n\t\t\t\t\t\t   stderr=subprocess.PIPE,\n\t\t\t\t\t\t   cwd=cwd,\n\t\t\t\t\t\t   env=env)\n\tout, err = cmd.communicate(input=args)\n\tretcode = cmd.returncode\n\n\tif verbose:\n\t\tprint(out)\n\t\tprint(err)\n\n\tif hide_stderr:\n\t\tif err:\n\t\t\treturn out, retcode\n\t\telse:\n\t\t\treturn out, retcode\n\telse:\n\t\treturn out, retcode\n\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tcmd = None\n\n\ttry:\n\t\tif cwd is not None:\n\t\t\tos.chdir(cwd)\n\n\t\tif verbose:\n\t\t\tprint('Running %s %s' % (commands, args))\n\n\t\tcmd = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, shell=True, env=env)\n\n\t\tif hide_stderr:\n\t\t\tcmd.stderr.close()\n\n\t\tstdout, stderr = cmd.communicate(input=args)\n\n\t\tif stderr:\n\t\t\traise Exception('Running \"%s %s\" returned stderr: %s' % (commands, args, stderr))\n\n\t\tif cmd.returncode != 0:\n\t\t\traise Exception('Running \"%s %s\" returned non-zero exit code %d' % (commands, args, cmd.returncode))\n\texcept Exception as e:\n\t\tif cmd is not None:\n\t\t\tprint('Killing process %d' % cmd.pid)\n\t\t\tcmd.kill()\n\t\traise e\n\n\treturn stdout, cmd.returncode\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\tfrom subprocess import PIPE\n\tfrom subprocess import CalledProcessError\n\timport sys\n\t\n\t\n\targs = [commands] + args\n\t\n\tpipe = subprocess.Popen(args, cwd=cwd, stdout=PIPE, stderr=subprocess.STDOUT, env=env)\n\t\n\tif not hide_stderr:\n\t\tpipe.stderr.close()\n\t\n\toutput, _ = pipe.communicate()\n\t\n\treturn (output, pipe.returncode)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport os\n\t\n\tif env is None:\n\t\tenv = os.environ.copy()\n\t\n\tcmd = subprocess.Popen(commands, args=args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tenv=env)\n\t\n\tstdout, stderr = cmd.communicate()\n\t\n\tif hide_stderr and stderr:\n\t\tstderr = \"\"\n\t\n\treturncode = cmd.returncode\n\n\tif verbose:\n\t\tprint(\"Command: %s\" % str(commands))\n\t\tprint(\"Arguments: %s\" % str(args))\n\t\tprint(\"CWD: %s\" % cwd)\n\t\tprint(\"Environment: %s\" % str(env))\n\t\tprint(\"Output: %s\" % stdout)\n\t\tprint(\"Error: %s\" % stderr)\n\n\treturn (stdout, stderr, returncode)", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tcommand = commands[0]\n\targs_str = \"\"\n\tif len(args) > 0:\n\t\targs_str = \" \" + \" \".join(args)\n\n\t# create the command\n\tcommand_str = [command]\n\tif cwd is not None:\n\t\tcommand_str.extend([\"-cwd\", cwd])\n\tcommand_str.extend([\"-stdin\", \"0\"])\n\tcommand_str.append(args_str)\n\n\t# get the command\n\tcommand_str.insert(0, \"sh\")\n\n\t# create the pipe\n\tpipe = subprocess.Popen(command_str, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n\t# run the command and get the result\n\tstdout, stderr = pipe.communicate(input=\"\".join(commands[1:]).encode())\n\tret = pipe.returncode\n\n\t# check the verbose mode\n\tif verbose:\n\t\tprint(\" \".join(command_str))\n\t\tprint(\"stdout:\")\n\t\tprint(stdout.decode(\"utf-8\"))\n\t\tprint(\"stderr:\")\n\t\tprint(stderr.decode(\"utf-8\"))\n\n\tif hide_stderr", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\targs = list(args)\n\tif env:\n\t\tenv = dict(**env)\n\n\ttry:\n\t\tresult = subprocess.run(commands, input=b\"\\n\".join(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT if hide_stderr else subprocess.PIPE, cwd=cwd, env=env)\n\t\tresult.stdout = result.stdout.decode(\"utf-8\")\n\t\tif verbose:\n\t\t\tprint(f\"{' '.join(commands)} {args}\")\n\t\t\tprint(result.stdout)\n\texcept subprocess.CalledProcessError as error:\n\t\tresult = error\n\t\tresult.stdout = result.stdout.decode(\"utf-8\")\n\t\tif verbose:\n\t\t\tprint(f\"{' '.join(commands)} {args}\")\n\t\t\tprint(result.stdout)\n\t\traise\n\n\treturn result\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\targs = [str(arg) for arg in args]\n\tif verbose:\n\t\tprint(' '.join(commands + args))\n\ttry:\n\t\tresult = subprocess.run(commands + args, cwd=cwd, check=True, capture_output=True, env=env)\n\texcept subprocess.CalledProcessError:\n\t\tresult = subprocess.run(commands + args, cwd=cwd, check=False, capture_output=False, env=env)\n\tstdout = result.stdout.decode('utf-8') if result.stdout else ''\n\tstderr = result.stderr.decode('utf-8') if result.stderr else ''\n\treturn (stdout, stderr, result.returncode)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\tif env is None:\n\t\tenv = os.environ\n\n\tprocess = subprocess.Popen(\n\t\tcommands,\n\t\tstdin=subprocess.PIPE,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\n\tstdout, stderr = process.communicate(input=args)\n\n\tif verbose:\n\t\tprint(\"stdout:\")\n\t\tprint(stdout)\n\t\tprint(\"stderr:\")\n\t\tprint(stderr)\n\n\tif stderr:\n\t\tprint(\"Command failed: \" + commands[0] + \" \" + args)\n\t\tprint(stderr)\n\n\treturn (stdout, process.returncode)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\tif env is None:\n\t\tenv = os.environ.copy()\n\t\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(command)\n\t\tif command in [\"echo\", \"pwd\"]:\n\t\t\tprint(args)\n\t\t\tprint(cwd)\n\t\n\ttry:\n\t\tresult = subprocess.run(args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, check=True, universal_newlines=True)\n\texcept subprocess.CalledProcessError as e:\n\t\tif verbose:\n\t\t\tprint(e.stderr)\n\t\traise e\n\t\n\treturn result\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\tprocess = subprocess.Popen(commands, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\tstdout, stderr = process.communicate(args)\n\tretcode = process.returncode\n\n\tif verbose:\n\t\tif stdout:\n\t\t\tprint('[STDOUT]:')\n\t\t\tprint(stdout)\n\t\tif stderr:\n\t\t\tprint('[STDERR]:')\n\t\t\tprint(stderr)\n\n\tif hide_stderr:\n\t\tstderr = None\n\n\treturn stdout, stderr, retcode\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\tcmd = subprocess.Popen(commands,\n\t\t\t\t\t\t   stdin=subprocess.PIPE,\n\t\t\t\t\t\t   stdout=subprocess.PIPE,\n\t\t\t\t\t\t   stderr=subprocess.PIPE,\n\t\t\t\t\t\t   cwd=cwd,\n\t\t\t\t\t\t   env=env)\n\tout, err = cmd.communicate(input=args)\n\tretcode = cmd.returncode\n\n\tif verbose:\n\t\tprint(out)\n\t\tprint(err)\n\n\tif hide_stderr:\n\t\tif err:\n\t\t\treturn out, retcode\n\t\telse:\n\t\t\treturn out, retcode\n\telse:\n\t\treturn out, retcode\n\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------\n\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------\n#-----------------------------------------------------------------------------------------------------------------------", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tcmd = None\n\n\ttry:\n\t\tif cwd is not None:\n\t\t\tos.chdir(cwd)\n\n\t\tif verbose:\n\t\t\tprint('Running %s %s' % (commands, args))\n\n\t\tcmd = subprocess.Popen(commands, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, shell=True, env=env)\n\n\t\tif hide_stderr:\n\t\t\tcmd.stderr.close()\n\n\t\tstdout, stderr = cmd.communicate(input=args)\n\n\t\tif stderr:\n\t\t\traise Exception('Running \"%s %s\" returned stderr: %s' % (commands, args, stderr))\n\n\t\tif cmd.returncode != 0:\n\t\t\traise Exception('Running \"%s %s\" returned non-zero exit code %d' % (commands, args, cmd.returncode))\n\texcept Exception as e:\n\t\tif cmd is not None:\n\t\t\tprint('Killing process %d' % cmd.pid)\n\t\t\tcmd.kill()\n\t\traise e\n\n\treturn stdout, cmd.returncode\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\tfrom subprocess import PIPE\n\tfrom subprocess import CalledProcessError\n\timport sys\n\t\n\t\n\targs = [commands] + args\n\t\n\tpipe = subprocess.Popen(args, cwd=cwd, stdout=PIPE, stderr=subprocess.STDOUT, env=env)\n\t\n\tif not hide_stderr:\n\t\tpipe.stderr.close()\n\t\n\toutput, _ = pipe.communicate()\n\t\n\treturn (output, pipe.returncode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport os\n\t\n\tif env is None:\n\t\tenv = os.environ.copy()\n\t\n\tcmd = subprocess.Popen(commands, args=args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tenv=env)\n\t\n\tstdout, stderr = cmd.communicate()\n\t\n\tif hide_stderr and stderr:\n\t\tstderr = \"\"\n\t\n\treturncode = cmd.returncode\n\n\tif verbose:\n\t\tprint(\"Command: %s\" % str(commands))\n\t\tprint(\"Arguments: %s\" % str(args))\n\t\tprint(\"CWD: %s\" % cwd)\n\t\tprint(\"Environment: %s\" % str(env))\n\t\tprint(\"Output: %s\" % stdout)\n\t\tprint(\"Error: %s\" % stderr)\n\n\treturn (stdout, stderr, returncode)", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\tcommand = commands[0]\n\targs_str = \"\"\n\tif len(args) > 0:\n\t\targs_str = \" \" + \" \".join(args)\n\n\t# create the command\n\tcommand_str = [command]\n\tif cwd is not None:\n\t\tcommand_str.extend([\"-cwd\", cwd])\n\tcommand_str.extend([\"-stdin\", \"0\"])\n\tcommand_str.append(args_str)\n\n\t# get the command\n\tcommand_str.insert(0, \"sh\")\n\n\t# create the pipe\n\tpipe = subprocess.Popen(command_str, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\n\t# run the command and get the result\n\tstdout, stderr = pipe.communicate(input=\"\".join(commands[1:]).encode())\n\tret = pipe.returncode\n\n\t# check the verbose mode\n\tif verbose:\n\t\tprint(\" \".join(command_str))\n\t\tprint(\"stdout:\")\n\t\tprint(stdout.decode(\"utf-8\"))\n\t\tprint(\"stderr:\")\n\t\tprint(stderr.decode(\"utf-8\"))\n\n\tif hide_stderr", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\targs = list(args)\n\tif env:\n\t\tenv = dict(**env)\n\n\ttry:\n\t\tresult = subprocess.run(commands, input=b\"\\n\".join(args), check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT if hide_stderr else subprocess.PIPE, cwd=cwd, env=env)\n\t\tresult.stdout = result.stdout.decode(\"utf-8\")\n\t\tif verbose:\n\t\t\tprint(f\"{' '.join(commands)} {args}\")\n\t\t\tprint(result.stdout)\n\texcept subprocess.CalledProcessError as error:\n\t\tresult = error\n\t\tresult.stdout = result.stdout.decode(\"utf-8\")\n\t\tif verbose:\n\t\t\tprint(f\"{' '.join(commands)} {args}\")\n\t\t\tprint(result.stdout)\n\t\traise\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\targs = [str(arg) for arg in args]\n\tif verbose:\n\t\tprint(' '.join(commands + args))\n\ttry:\n\t\tresult = subprocess.run(commands + args, cwd=cwd, check=True, capture_output=True, env=env)\n\texcept subprocess.CalledProcessError:\n\t\tresult = subprocess.run(commands + args, cwd=cwd, check=False, capture_output=False, env=env)\n\tstdout = result.stdout.decode('utf-8') if result.stdout else ''\n\tstderr = result.stderr.decode('utf-8') if result.stderr else ''\n\treturn (stdout, stderr, result.returncode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\n\tif env is None:\n\t\tenv = os.environ\n\n\tprocess = subprocess.Popen(\n\t\tcommands,\n\t\tstdin=subprocess.PIPE,\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\n\tstdout, stderr = process.communicate(input=args)\n\n\tif verbose:\n\t\tprint(\"stdout:\")\n\t\tprint(stdout)\n\t\tprint(\"stderr:\")\n\t\tprint(stderr)\n\n\tif stderr:\n\t\tprint(\"Command failed: \" + commands[0] + \" \" + args)\n\t\tprint(stderr)\n\n\treturn (stdout, process.returncode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t\"\"\"\n\tA list of command is given. Run the given command(s) with the given arguments use a subprocess. Return the running result (stdout and returncode)\n\t\"\"\"\n\timport subprocess\n\timport sys\n\timport os\n\tif env is None:\n\t\tenv = os.environ.copy()\n\t\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(command)\n\t\tif command in [\"echo\", \"pwd\"]:\n\t\t\tprint(args)\n\t\t\tprint(cwd)\n\t\n\ttry:\n\t\tresult = subprocess.run(args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, check=True, universal_newlines=True)\n\texcept subprocess.CalledProcessError as e:\n\t\tif verbose:\n\t\t\tprint(e.stderr)\n\t\traise e\n\t\n\treturn result\n", "is_pass": false, "return_code": 1}]}
{"file_path": "gopad/rest.py", "project": "gopad/gopad-python", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\timport socket\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = IP4Address(target)\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip_address = ipaddress.IPv4Address(target)\n\t\tip_address = str(ip_address)\n\t\treturn ip_address\n\texcept:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = target\n\t\tipv4 = ip.split('.')\n\t\tipv4 = [int(x) for x in ipv4]\n\n\t\tif len(ipv4) != 4:\n\t\t\treturn False\n\n\t\tfor x in ipv4:\n\t\t\tif x < 0 or x > 255:\n\t\t\t\treturn False\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = socket.inet_aton(target)\n\t\treturn ip\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\ttarget = target.split('.')\n\t\tif len(target) == 4:\n\t\t\tfor octet in target:\n\t\t\t\tif int(octet) < 0 or int(octet) > 255:\n\t\t\t\t\treturn False\n\t\t\treturn True\n\t\telse:\n\t\t\traise ValueError\n\texcept ValueError:\n\t\treturn False", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\ttarget = ipaddress.ip_address(target, version=4)\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tIPAddress(target)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip_address = ipaddress.ip_address(target)\n\t\treturn ip_address.is_global\n\texcept ipaddress.AddressValueError:\n\t\treturn False\n", "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = socket.inet_pton(socket.AF_INET, target)\n\texcept socket.error:\n\t\treturn False\n\treturn True if ip else False\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\t\n\timport socket\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = IP4Address(target)\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip_address = ipaddress.IPv4Address(target)\n\t\tip_address = str(ip_address)\n\t\treturn ip_address\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = target\n\t\tipv4 = ip.split('.')\n\t\tipv4 = [int(x) for x in ipv4]\n\n\t\tif len(ipv4) != 4:\n\t\t\treturn False\n\n\t\tfor x in ipv4:\n\t\t\tif x < 0 or x > 255:\n\t\t\t\treturn False\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = socket.inet_aton(target)\n\t\treturn ip\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\ttarget = target.split('.')\n\t\tif len(target) == 4:\n\t\t\tfor octet in target:\n\t\t\t\tif int(octet) < 0 or int(octet) > 255:\n\t\t\t\t\treturn False\n\t\t\treturn True\n\t\telse:\n\t\t\traise ValueError\n\texcept ValueError:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\ttarget = ipaddress.ip_address(target, version=4)\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tIPAddress(target)\n\t\treturn True\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip_address = ipaddress.ip_address(target)\n\t\treturn ip_address.is_global\n\texcept ipaddress.AddressValueError:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\t\"\"\"\n\tTest if IPv4 address or not.\n\n   Returns: Boolean, True if target is IPv4 address, else False.\n\t\"\"\"\n\ttry:\n\t\tip = socket.inet_pton(socket.AF_INET, target)\n\texcept socket.error:\n\t\treturn False\n\treturn True if ip else False\n", "is_pass": false, "return_code": 1}]}
{"file_path": "rdflib/util.py", "project": "mwatts15/rdflib", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set([])\n\tnodes = graph.subjects(predicate=prop)\n\troots.update(nodes)\n\tif nodes:\n\t\troots.update(find_roots(graph, prop, nodes))\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\n\tif not roots:\n\t\troots.add(graph.subject)\n\n\tfor child in graph.subject.object_iter(prop):\n\t\tif child.uri in roots:\n\t\t\tcontinue\n\t\tif graph.contains(child):\n\t\t\troots.update(find_roots(graph, prop, roots=roots))\n\t\telse:\n\t\t\troots.add(child)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\troots.add(prop)\n\tfor child in graph.subjects(predicate=prop):\n\t\troots = find_roots(graph, child, roots)\n\treturn roots\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set() if roots is None else roots\n\tfor node in graph.subjects(predicate=prop):\n\t\troots.add(node)\n\t\troots.update(find_roots(graph, prop, roots=roots))\n\treturn roots\n\n#src/data/graph/query.py\nfrom typing import Iterable, List, Optional, Set, Union\n\nfrom rdflib import Graph\nfrom rdflib.namespace import Namespace, RDF, RDFS, SKOS\n\nfrom data.graph.utils import find_roots\n\n# TODO: This is just a temporary place to keep the URIRefs\nFACETS = {\n    \"skos:inScheme\": SKOS.inScheme,\n    \"skos:broader\": SKOS.broader,\n    \"rdfs:subClassOf\": RDFS.subClassOf,\n}\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\ttry:\n\t\troots = roots or set()\n\t\troots.add(prop)\n\t\tfor child in graph.subjects(\n\t\t\tpredicate=prop, object=graph.blank_node(), obj_type=XSD.anyURI\n\t\t):\n\t\t\troots = find_roots(graph, child, roots)\n\texcept Exception as e:\n\t\tlogging.error(\"Exception in find_roots:\", e)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\tqueue = set()\n\tqueue.add(prop)\n\twhile queue:\n\t\tcurr = queue.pop()\n\t\tif curr in roots:\n\t\t\tcontinue\n\t\tif graph.has_node(curr):\n\t\t\troots.add(curr)\n\t\telse:\n\t\t\tprint(f\"Warning: Node {curr} not in graph\")\n\t\tif not graph.has_node(curr):\n\t\t\tprint(f\"Warning: Node {curr} does not exist in graph\")\n\t\tqueue.update(graph.out_neighbors(curr, prop))\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\troots.add(graph.subject)\n\troots = get_roots(graph, roots, prop)\n\treturn roots\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph.subjects(predicate=prop):\n\t\troots.add(node)\n\t\troots.update(find_roots(graph, prop, roots))\n\n\treturn roots\n\n#src/graph_util.py\nfrom typing import Callable, Dict, List, Optional, Tuple, Type, Union\n\nimport rdflib\nimport rdflib.namespace\nfrom rdflib.graph import Graph\n\nfrom .graph import Graph\nfrom .graph_node import GraphNode\nfrom .graph_node_factory import GraphNodeFactory\nfrom .graph_node_utils import GraphNodeUtils\nfrom .graph_utils import GraphUtils\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\tqueue = deque()\n\tqueue.append(prop)\n\twhile queue:\n\t\tnode = queue.popleft()\n\t\tif node not in roots:\n\t\t\t# add the node to the roots set\n\t\t\troots.add(node)\n\t\t\t# add all descendants to the queue\n\t\t\tfor descendant in graph.subjects(node, prop):\n\t\t\t\tqueue.append(descendant)\n\treturn roots\n\n#src/rdf_utils.py\nimport logging\nimport time\nfrom typing import Dict, Any, Union, Optional\nimport rdflib\nfrom rdflib import Graph, URIRef, RDF, Literal\nfrom rdflib.plugins.sparql import prepareQuery\nfrom rdflib.namespace import RDF, RDFS, SKOS, Namespace\n\nlog = logging.getLogger(__name__)\n\n# Namespaces used in the queries\nRDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\nRDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\nSKOS = Namespace(\"http://www.w3.org/2004/02/skos/", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tif prop.is_a(rdflib.RDFS.subClassOf):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.RDFS.isDefinedBy):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.RDFS.broader):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.SKOS.broader):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.SKOS.narrower):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots="], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set([])\n\tnodes = graph.subjects(predicate=prop)\n\troots.update(nodes)\n\tif nodes:\n\t\troots.update(find_roots(graph, prop, nodes))\n\treturn roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\n\tif not roots:\n\t\troots.add(graph.subject)\n\n\tfor child in graph.subject.object_iter(prop):\n\t\tif child.uri in roots:\n\t\t\tcontinue\n\t\tif graph.contains(child):\n\t\t\troots.update(find_roots(graph, prop, roots=roots))\n\t\telse:\n\t\t\troots.add(child)\n\treturn roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\troots.add(prop)\n\tfor child in graph.subjects(predicate=prop):\n\t\troots = find_roots(graph, child, roots)\n\treturn roots\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set() if roots is None else roots\n\tfor node in graph.subjects(predicate=prop):\n\t\troots.add(node)\n\t\troots.update(find_roots(graph, prop, roots=roots))\n\treturn roots\n\n#src/data/graph/query.py\nfrom typing import Iterable, List, Optional, Set, Union\n\nfrom rdflib import Graph\nfrom rdflib.namespace import Namespace, RDF, RDFS, SKOS\n\nfrom data.graph.utils import find_roots\n\n# TODO: This is just a temporary place to keep the URIRefs\nFACETS = {\n    \"skos:inScheme\": SKOS.inScheme,\n    \"skos:broader\": SKOS.broader,\n    \"rdfs:subClassOf\": RDFS.subClassOf,\n}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\ttry:\n\t\troots = roots or set()\n\t\troots.add(prop)\n\t\tfor child in graph.subjects(\n\t\t\tpredicate=prop, object=graph.blank_node(), obj_type=XSD.anyURI\n\t\t):\n\t\t\troots = find_roots(graph, child, roots)\n\texcept Exception as e:\n\t\tlogging.error(\"Exception in find_roots:\", e)\n\treturn roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\tqueue = set()\n\tqueue.add(prop)\n\twhile queue:\n\t\tcurr = queue.pop()\n\t\tif curr in roots:\n\t\t\tcontinue\n\t\tif graph.has_node(curr):\n\t\t\troots.add(curr)\n\t\telse:\n\t\t\tprint(f\"Warning: Node {curr} not in graph\")\n\t\tif not graph.has_node(curr):\n\t\t\tprint(f\"Warning: Node {curr} does not exist in graph\")\n\t\tqueue.update(graph.out_neighbors(curr, prop))\n\treturn roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\troots.add(graph.subject)\n\troots = get_roots(graph, roots, prop)\n\treturn roots\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tfor node in graph.subjects(predicate=prop):\n\t\troots.add(node)\n\t\troots.update(find_roots(graph, prop, roots))\n\n\treturn roots\n\n#src/graph_util.py\nfrom typing import Callable, Dict, List, Optional, Tuple, Type, Union\n\nimport rdflib\nimport rdflib.namespace\nfrom rdflib.graph import Graph\n\nfrom .graph import Graph\nfrom .graph_node import GraphNode\nfrom .graph_node_factory import GraphNodeFactory\nfrom .graph_node_utils import GraphNodeUtils\nfrom .graph_utils import GraphUtils\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = set()\n\tqueue = deque()\n\tqueue.append(prop)\n\twhile queue:\n\t\tnode = queue.popleft()\n\t\tif node not in roots:\n\t\t\t# add the node to the roots set\n\t\t\troots.add(node)\n\t\t\t# add all descendants to the queue\n\t\t\tfor descendant in graph.subjects(node, prop):\n\t\t\t\tqueue.append(descendant)\n\treturn roots\n\n#src/rdf_utils.py\nimport logging\nimport time\nfrom typing import Dict, Any, Union, Optional\nimport rdflib\nfrom rdflib import Graph, URIRef, RDF, Literal\nfrom rdflib.plugins.sparql import prepareQuery\nfrom rdflib.namespace import RDF, RDFS, SKOS, Namespace\n\nlog = logging.getLogger(__name__)\n\n# Namespaces used in the queries\nRDF = Namespace(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\")\nRDFS = Namespace(\"http://www.w3.org/2000/01/rdf-schema#\")\nSKOS = Namespace(\"http://www.w3.org/2004/02/skos/", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\t\"\"\"\n\t Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes\n\t\"\"\"\n\troots = roots or set()\n\tif prop.is_a(rdflib.RDFS.subClassOf):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.RDFS.isDefinedBy):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.RDFS.broader):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.SKOS.broader):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=roots)\n\t\t)\n\telif prop.is_a(rdflib.SKOS.narrower):\n\t\troots.add(prop)\n\t\troots.update(\n\t\t\tfind_roots(graph, prop, roots=", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/unit/mock/yaml_helper.py", "project": "ansible-security/ansible_collections.ibm.qradar", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresent_scalar = dumper.represent_scalar if dumper else None\n\n\tif isinstance(obj, str):\n\t\treturn represent_scalar(obj, tag='tag:yaml.org,2002:str')\n\n\tif isinstance(obj, bytes):\n\t\treturn represent_scalar(obj.decode('utf-8'), tag='tag:yaml.org,2002:binary')\n\n\treturn dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or Representer()\n\trepresenter.represent_data(obj)\n\treturn representer.doc\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or yaml.Dumper\n\trepresenter.ignore_aliases = False\n\trepresenter.width = 1000\n\treturn yaml.dump(obj, representer)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or Representer()\n\trepresenter.represent_data(obj)\n\treturn representer.doc\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tserializer = dumper\n\tif serializer is None:\n\t\tserializer = _get_serializer()\n\treturn serializer.dump(obj)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = Dumper.represent_mapping\n\tdump = representer(dumper, obj.items())\n\tdump.tag = 'tag:' + obj.__class__.__name__\n\treturn dump\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\tif dumper is None:\n\t\tdef _default(self, data):\n\t\t\treturn data.decode('utf-8') if isinstance(data, bytes) else data\n\n\t\treturn _default\n\n\treturn dumper(obj)\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresented_class = obj.__class__.__name__\n\tif dumper is None:\n\t\tif represented_class == 'int':\n\t\t\trepresented_class = 'long'\n\t\treturn yaml.dump({represented_class: obj}, default_flow_style=False)\n\telse:\n\t\treturn dumper.represent_scalar(represented_class, obj)\n\nyaml.add_representer(str, _dump_string)\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or Representer()\n\trepresenter.represent_python_object(obj)\n\treturn representer.doc\n\n", "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = yaml.representer.Representer\n\trepresenter.represent_str = yaml.representer.Representer.represent_str\n\trepresenter.represent_sequence = yaml.representer.Representer.represent_sequence\n\trepresenter.represent_mapping = yaml.representer.Representer.represent_mapping\n\n\trepresenter.represent_scalar = represent_scalar\n\n\tyaml.add_representer(type(None), represent_none, Dumper=dumper)\n\n\tyaml.add_representer(bytes, represent_bytes, Dumper=dumper)\n\n\tyaml.add_representer(str, represent_str, Dumper=dumper)\n\n\tyaml.add_representer(int, represent_int, Dumper=dumper)\n\n\tyaml.add_representer(float, represent_float, Dumper=dumper)\n\n\tyaml.add_representer(complex, represent_complex, Dumper=dumper)\n\n\tyaml.add_representer(bool, represent_bool, Dumper=dumper)\n\n\tyaml.add_representer(dict, represent_dict, Dumper=dumper)\n\n\tyaml.add_representer(list, represent_list, Dumper=d"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresent_scalar = dumper.represent_scalar if dumper else None\n\n\tif isinstance(obj, str):\n\t\treturn represent_scalar(obj, tag='tag:yaml.org,2002:str')\n\n\tif isinstance(obj, bytes):\n\t\treturn represent_scalar(obj.decode('utf-8'), tag='tag:yaml.org,2002:binary')\n\n\treturn dumper.represent_scalar('tag:yaml.org,2002:str', obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or Representer()\n\trepresenter.represent_data(obj)\n\treturn representer.doc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or yaml.Dumper\n\trepresenter.ignore_aliases = False\n\trepresenter.width = 1000\n\treturn yaml.dump(obj, representer)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or Representer()\n\trepresenter.represent_data(obj)\n\treturn representer.doc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\tserializer = dumper\n\tif serializer is None:\n\t\tserializer = _get_serializer()\n\treturn serializer.dump(obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = Dumper.represent_mapping\n\tdump = representer(dumper, obj.items())\n\tdump.tag = 'tag:' + obj.__class__.__name__\n\treturn dump\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\t\n\tif dumper is None:\n\t\tdef _default(self, data):\n\t\t\treturn data.decode('utf-8') if isinstance(data, bytes) else data\n\n\t\treturn _default\n\n\treturn dumper(obj)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresented_class = obj.__class__.__name__\n\tif dumper is None:\n\t\tif represented_class == 'int':\n\t\t\trepresented_class = 'long'\n\t\treturn yaml.dump({represented_class: obj}, default_flow_style=False)\n\telse:\n\t\treturn dumper.represent_scalar(represented_class, obj)\n\nyaml.add_representer(str, _dump_string)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = dumper or Representer()\n\trepresenter.represent_python_object(obj)\n\treturn representer.doc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t\"\"\"\n\tGiven a python object and serialize it into a YAML stream\n\n    Args:\n        obj: python object\n\n    Returns:\n        YAML stream\n\t\"\"\"\n\trepresenter = yaml.representer.Representer\n\trepresenter.represent_str = yaml.representer.Representer.represent_str\n\trepresenter.represent_sequence = yaml.representer.Representer.represent_sequence\n\trepresenter.represent_mapping = yaml.representer.Representer.represent_mapping\n\n\trepresenter.represent_scalar = represent_scalar\n\n\tyaml.add_representer(type(None), represent_none, Dumper=dumper)\n\n\tyaml.add_representer(bytes, represent_bytes, Dumper=dumper)\n\n\tyaml.add_representer(str, represent_str, Dumper=dumper)\n\n\tyaml.add_representer(int, represent_int, Dumper=dumper)\n\n\tyaml.add_representer(float, represent_float, Dumper=dumper)\n\n\tyaml.add_representer(complex, represent_complex, Dumper=dumper)\n\n\tyaml.add_representer(bool, represent_bool, Dumper=dumper)\n\n\tyaml.add_representer(dict, represent_dict, Dumper=dumper)\n\n\tyaml.add_representer(list, represent_list, Dumper=d", "is_pass": false, "return_code": 1}]}
{"file_path": "apphelpers/loggers.py", "project": "scrolltech/apphelpers", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\tif debug:\n\t\tconsole_handler = logging.StreamHandler()\n\t\tconsole_handler.setFormatter(formatter)\n\t\tlogger.addHandler(console_handler)\n\n\treturn logger\n\nif __name__ == '__main__':\n    build_app_logger().info('logging test')", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter('%(asctime)s | %(levelname)s | %(module)s.%(funcName)s | %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG)\n\tconsole_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n#logger.py\nimport logging\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\treturn logger", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tsh = logging.StreamHandler()\n\tsh.setFormatter(formatter)\n\tlogger.addHandler(sh)\n\n\tfl = logging.FileHandler(logfile)\n\tfl.setFormatter(formatter)\n\tlogger.addHandler(fl)\n\treturn logger", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlog_format = \"[%(asctime)s] %(name)s %(levelname)s: %(message)s\"\n\tlog_datefmt = \"%d-%m-%Y %H:%M:%S\"\n\tlog_level = logging.DEBUG if debug else logging.INFO\n\tformatter = logging.Formatter(log_format, datefmt=log_datefmt)\n\tfilehandler = logging.FileHandler(logfile)\n\tfilehandler.setFormatter(formatter)\n\tapp_logger = logging.getLogger(name)\n\tapp_logger.setLevel(log_level)\n\tapp_logger.addHandler(filehandler)\n\treturn app_logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.propagate = False\n\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\tif debug:\n\t\tstream_handler = logging.StreamHandler()\n\t\tstream_handler.setFormatter(formatter)\n\t\tlogger.addHandler(stream_handler)\n\n\treturn logger\n\n#src/app/config.py\nimport os\nfrom logging import DEBUG\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tformatter = logging.Formatter(fmt='%(asctime)s %(levelname)s %(name)s %(message)s')\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setFormatter(formatter)\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setFormatter(formatter)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\t\tlogger.addHandler(file_handler)\n\t\tlogger.addHandler(stream_handler)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\t\tlogger.addHandler(file_handler)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.addHandler(handler)\n\treturn logger\n\n#app/config.py\n# Standard Library\nimport os\n\n# Third Party\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\t# create file handler which logs events to debug.log\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n\tfile_handler.setFormatter(formatter)\n\tconsole_handler.setFormatter(formatter)\n\t# add the handlers to the logger\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n#main.py\nimport sys\nimport os\nimport time\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport yaml\n\nfrom logger import build_app_logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger('%s_log' % name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter(\n\t\tfmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n\t\tdatefmt='%Y-%m-%d %H:%M:%S')\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\n\tif debug:\n\t\tconsole_handler = logging.StreamHandler()\n\t\tconsole_handler.setLevel(logging.DEBUG)\n\t\tconsole_handler.setFormatter(formatter)\n\t\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\tif debug:\n\t\tconsole_handler = logging.StreamHandler()\n\t\tconsole_handler.setFormatter(formatter)\n\t\tlogger.addHandler(console_handler)\n\n\treturn logger\n\nif __name__ == '__main__':\n    build_app_logger().info('logging test')", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter('%(asctime)s | %(levelname)s | %(module)s.%(funcName)s | %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG)\n\tconsole_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n#logger.py\nimport logging\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\thandler = logging.FileHandler(logfile)\n\thandler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\treturn logger", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tsh = logging.StreamHandler()\n\tsh.setFormatter(formatter)\n\tlogger.addHandler(sh)\n\n\tfl = logging.FileHandler(logfile)\n\tfl.setFormatter(formatter)\n\tlogger.addHandler(fl)\n\treturn logger", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlog_format = \"[%(asctime)s] %(name)s %(levelname)s: %(message)s\"\n\tlog_datefmt = \"%d-%m-%Y %H:%M:%S\"\n\tlog_level = logging.DEBUG if debug else logging.INFO\n\tformatter = logging.Formatter(log_format, datefmt=log_datefmt)\n\tfilehandler = logging.FileHandler(logfile)\n\tfilehandler.setFormatter(formatter)\n\tapp_logger = logging.getLogger(name)\n\tapp_logger.setLevel(log_level)\n\tapp_logger.addHandler(filehandler)\n\treturn app_logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.propagate = False\n\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\tif debug:\n\t\tstream_handler = logging.StreamHandler()\n\t\tstream_handler.setFormatter(formatter)\n\t\tlogger.addHandler(stream_handler)\n\n\treturn logger\n\n#src/app/config.py\nimport os\nfrom logging import DEBUG\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tformatter = logging.Formatter(fmt='%(asctime)s %(levelname)s %(name)s %(message)s')\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setFormatter(formatter)\n\tstream_handler = logging.StreamHandler()\n\tstream_handler.setFormatter(formatter)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\t\tlogger.addHandler(file_handler)\n\t\tlogger.addHandler(stream_handler)\n\telse:\n\t\tlogger.setLevel(logging.INFO)\n\t\tlogger.addHandler(file_handler)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\timport logging\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.addHandler(handler)\n\treturn logger\n\n#app/config.py\n# Standard Library\nimport os\n\n# Third Party\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\t# create file handler which logs events to debug.log\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n\tfile_handler.setFormatter(formatter)\n\tconsole_handler.setFormatter(formatter)\n\t# add the handlers to the logger\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n#main.py\nimport sys\nimport os\nimport time\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport yaml\n\nfrom logger import build_app_logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t\"\"\"\n\tGeneral purpose application logger. Useful mainly for debugging\n    Args:\n        name: The name of logger\n        logfile: The log file need to save\n        debug: is need to debug\n\n    Returns:\n        Returns an instantiated logger object\n\t\"\"\"\n\tlogger = logging.getLogger('%s_log' % name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter(\n\t\tfmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n\t\tdatefmt='%Y-%m-%d %H:%M:%S')\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\n\tif debug:\n\t\tconsole_handler = logging.StreamHandler()\n\t\tconsole_handler.setLevel(logging.DEBUG)\n\t\tconsole_handler.setFormatter(formatter)\n\t\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "radiospectra/spectrogram.py", "project": "sunpy/radiospectra", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype)\n\ta.flags.writeable = False\n\treturn a\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype)\n\treturn arr\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype)\n\ta.flags.writeable = False\n\treturn a\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype)\n\treturn arr\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\ta = np.zeros(shape, dtype=dtype)\n\treturn a\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\t\"\"\"\n\tReturns a 0-filled array of the given shape and type.\n\n        Args:\n                shape : tuple\n                    shape of the array to create\n                dtype : `numpy.dtype`\n                    data-type of the array to create\n        Return: array\n\t\"\"\"\n\tarr = np.zeros(shape, dtype=dtype)\n\treturn arr\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "concert/tests/unit/devices/test_monochromator.py", "project": "ufo-kit/concert", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\tsigma = 0.1\n\treturn math.exp(-(x-u)**2/(2*sigma**2))/math.sqrt(2*math.pi*sigma**2)\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-((x-u)**2)/(2*sigma**2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-(x - u)**2/(2 * sigma**2)) / (sigma * np.sqrt(2 * np.pi))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\ty = math.exp(-((x-0.2)**2)/(2*(0.1**2)))\n\treturn y\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom math import exp, pi, sqrt\n\treturn exp(-pow(x - 0.2, 2) / (2 * 0.1**2)) / (sqrt(2 * pi) * 0.1)\n\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\ty = -0.5*((x - 0.2) / 0.1)**2\n\ty[y < 0] = 0\n\treturn y\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom scipy.stats import norm\n\treturn(norm.pdf(x,0.2,0.1))", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom numpy import exp, pi, sqrt\n\treturn 1.0 / (sqrt(2.0*pi) * 0.1) * exp(-(x - 0.2)**2 / (2.0*0.1**2))\n", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\t\n\tu, sigma = -0.2, 0.1\n\t\n\treturn math.exp(-(x-u)**2/(2*sigma**2))/math.sqrt(2*math.pi*sigma**2)\n\t", "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\ts = 0.1\n\treturn np.exp(-(x-u)**2/(2*s**2))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\tsigma = 0.1\n\treturn math.exp(-(x-u)**2/(2*sigma**2))/math.sqrt(2*math.pi*sigma**2)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-((x-u)**2)/(2*sigma**2))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-(x - u)**2/(2 * sigma**2)) / (sigma * np.sqrt(2 * np.pi))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\ty = math.exp(-((x-0.2)**2)/(2*(0.1**2)))\n\treturn y\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom math import exp, pi, sqrt\n\treturn exp(-pow(x - 0.2, 2) / (2 * 0.1**2)) / (sqrt(2 * pi) * 0.1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\ty = -0.5*((x - 0.2) / 0.1)**2\n\ty[y < 0] = 0\n\treturn y\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom scipy.stats import norm\n\treturn(norm.pdf(x,0.2,0.1))", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tfrom numpy import exp, pi, sqrt\n\treturn 1.0 / (sqrt(2.0*pi) * 0.1) * exp(-(x - 0.2)**2 / (2.0*0.1**2))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\timport math\n\t\n\tu, sigma = -0.2, 0.1\n\t\n\treturn math.exp(-(x-u)**2/(2*sigma**2))/math.sqrt(2*math.pi*sigma**2)\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t\"\"\"\n\tCalculate Gaussian centered with u is 0.2 and sigma is 0.1.\n\t\"\"\"\n\tu = 0.2\n\ts = 0.1\n\treturn np.exp(-(x-u)**2/(2*s**2))\n", "is_pass": true, "return_code": 0}]}
{"file_path": "borgmatic/commands/borgmatic.py", "project": "witten/borgmatic", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\tconfigurations[config_filename] = config\n\t\texcept Exception as e:\n\t\t\tlogging.exception(f\"Failed to load configuration file '{config_filename}' due to error: {e}\")\n\t\t\terrors.append(\"Failed to load configuration file '{}' due to error: {}\".format(config_filename, e))\n\n\treturn configurations, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlog = logging.getLogger(__name__)\n\terrors = []\n\tconfigs = {}\n\t\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = configparser.SafeConfigParser()\n\t\t\t\n\t\t\tif overrides is not None and config_filename in overrides:\n\t\t\t\t# When overriding one configuration file, make sure the overriding configuration file is parsed.\n\t\t\t\tconfig.read(config_filename)\n\t\t\t\t\n\t\t\t\t# Merge the contents of the overriding configuration file into the existing configuration file. This\n\t\t\t\t# allows for optional parameters to be specified in the overriding configuration file rather than\n\t\t\t\t# having to be specified in the main configuration file.\n\t\t\t\tconfig.read(overrides[config_filename])\n\t\t\telse:\n\t\t\t\twith open(config_filename) as fp:\n\t\t\t\t\tconfig.readfp(fp)\n\t\t\tconfigs[config_filename] = config\n\t\texcept (configparser.Error, IOError) as e:\n\t\t\tlog.error(\"Cannot read configuration file '%s': %s\" % (config_filename, e))\n\t\t\terrors.append(e)\n\t\t\t\n\tif resolve_env:\n\t\tresolve_env_variables(configs)\n\t\t\n\treturn (configs, errors)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = []\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config(config_filename, overrides)\n\t\t\tconfigs.append(config)\n\t\texcept Exception as e:\n\t\t\terrors.append(e)\n\treturn configs, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\tif filename is None or not os.path.exists(filename):\n\t\t\tcontinue\n\t\tif not os.access(filename, os.R_OK):\n\t\t\terrors.append(\"Unable to read configuration file '{}' due to insufficient permissions\".format(filename))\n\t\t\tcontinue\n\t\twith open(filename, 'r') as f:\n\t\t\ttry:\n\t\t\t\tconfigurations[filename] = configparser.ConfigParser()\n\t\t\t\tconfigurations[filename].read_file(f)\n\t\t\t\tconfigurations[filename] = _validate_configuration(configurations[filename], filename, resolve_env)\n\t\t\texcept configparser.Error as e:\n\t\t\t\terrors.append(\"Unable to parse configuration file '{}': {}\".format(filename, e.message))\n\t\tif overrides is not None:\n\t\t\tconfigurations[filename] = _merge_dictionaries(configurations[filename], overrides)\n\treturn configurations, errors\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\timport json\n\timport logging\n\timport os\n\tfrom copy import deepcopy\n\tfrom jsonschema import validate\n\tfrom jsonschema.exceptions import ValidationError\n\n\tconfigurations = {}\n\terrors = []\n\n\t# Load configuration files\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename) as f:\n\t\t\t\tconfig = json.load(f)\n\t\texcept IOError as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname='configuration.load_configurations',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tmsg='Failed to read configuration file \"%s\": %s' % (filename, e),\n\t\t\t))\n\t\t\tcontinue\n\t\texcept ValueError as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname='configuration.load_configurations',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tmsg='Failed to parse configuration file \"%s\": %s' % (filename, e),\n\t\t\t))\n\t\t\tcontinue\n\n\t\t# Validate configuration\n\t\ttry:\n\t\t\tvalidate(config, JSON_SCHEMA_CONFIGURATION)\n\t\texcept ValidationError as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname='configuration", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigurations.append((config_filename, load_configuration(config_filename)))\n\t\texcept Exception as e:\n\t\t\tlogging.exception(e)\n\n\treturn configurations\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfig_filenames = [os.path.normpath(f) for f in config_filenames]\n\tconfig_filenames = [os.path.abspath(f) for f in config_filenames]\n\tconfig_filenames = [f for f in config_filenames if os.path.isfile(f)]\n\n\tconfig_filenames = [f for f in config_filenames if f.endswith(\".py\") or f.endswith(\".ini\")]\n\n\tif not overrides:\n\t\toverrides = []\n\toverrides = [os.path.normpath(f) for f in overrides]\n\toverrides = [os.path.abspath(f) for f in overrides]\n\toverrides = [f for f in overrides if os.path.isfile(f)]\n\n\tconfig_filenames = [f for f in config_filenames if not f in overrides]\n\n\tconfig_filenames = [f for f in config_filenames if f.endswith(\".py\")]\n\n\tconfig_filenames = [f for f in config_filenames if f.endswith(\".ini\")]\n\n\tconfig_filenames = [f for f in config_fil", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogger = logging.getLogger(\"load_configurations\")\n\t\n\tresult = []\n\t\n\ttry:\n\t\tfor filename in config_filenames:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tlogger.info('Reading configuration from \"%s\".' % filename)\n\t\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\tconfig = yaml.safe_load(f.read())\n\t\t\t\texcept yaml.YAMLError as e:\n\t\t\t\t\tlogger.error('Error parsing configuration file \"%s\": %s' % (filename, e))\n\t\t\t\t\t\n\t\t\t\t\tresult.append((filename, e))\n\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tif overrides is not None:\n\t\t\t\t\tconfig = dict(config.items() + overrides.items())\n\t\t\t\tif resolve_env:\n\t\t\t\t\tresolve_env_in_config(config)\n\t\t\t\t\n\t\t\t\tresult.append((filename, config))\n\t\t\n\t\treturn tuple(result)\n\t\n\tfinally:\n\t\tlogger.info('Finished reading all configurations.')\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tresult = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_yaml_config(filename, resolve_env=resolve_env)\n\t\texcept Exception as e:\n\t\t\tlogging.exception(e)\n\t\t\tresult.append((None, [logging.exception(e)]) if isinstance(e, Exception) else (None, [e]))\n\t\t\tcontinue\n\t\tresult.append((config, []))\n\treturn tuple(result)\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\t\n\tfor configuration_filename in config_filenames:\n\t\ttry:\n\t\t\tif resolve_env:\n\t\t\t\tconfiguration_filename = os.path.expanduser(configuration_filename)\n\t\t\tconfiguration = ConfigurationLoader.load_configuration(configuration_filename)\n\t\t\tconfigurations[configuration_filename] = configuration\n\t\texcept Exception, error:\n\t\t\terrors.append(error)\n\t\n\treturn configurations, errors\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\tconfigurations[config_filename] = config\n\t\texcept Exception as e:\n\t\t\tlogging.exception(f\"Failed to load configuration file '{config_filename}' due to error: {e}\")\n\t\t\terrors.append(\"Failed to load configuration file '{}' due to error: {}\".format(config_filename, e))\n\n\treturn configurations, errors\n", "is_pass": true, "return_code": 0}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlog = logging.getLogger(__name__)\n\terrors = []\n\tconfigs = {}\n\t\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = configparser.SafeConfigParser()\n\t\t\t\n\t\t\tif overrides is not None and config_filename in overrides:\n\t\t\t\t# When overriding one configuration file, make sure the overriding configuration file is parsed.\n\t\t\t\tconfig.read(config_filename)\n\t\t\t\t\n\t\t\t\t# Merge the contents of the overriding configuration file into the existing configuration file. This\n\t\t\t\t# allows for optional parameters to be specified in the overriding configuration file rather than\n\t\t\t\t# having to be specified in the main configuration file.\n\t\t\t\tconfig.read(overrides[config_filename])\n\t\t\telse:\n\t\t\t\twith open(config_filename) as fp:\n\t\t\t\t\tconfig.readfp(fp)\n\t\t\tconfigs[config_filename] = config\n\t\texcept (configparser.Error, IOError) as e:\n\t\t\tlog.error(\"Cannot read configuration file '%s': %s\" % (config_filename, e))\n\t\t\terrors.append(e)\n\t\t\t\n\tif resolve_env:\n\t\tresolve_env_variables(configs)\n\t\t\n\treturn (configs, errors)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigs = []\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_config(config_filename, overrides)\n\t\t\tconfigs.append(config)\n\t\texcept Exception as e:\n\t\t\terrors.append(e)\n\treturn configs, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\tif filename is None or not os.path.exists(filename):\n\t\t\tcontinue\n\t\tif not os.access(filename, os.R_OK):\n\t\t\terrors.append(\"Unable to read configuration file '{}' due to insufficient permissions\".format(filename))\n\t\t\tcontinue\n\t\twith open(filename, 'r') as f:\n\t\t\ttry:\n\t\t\t\tconfigurations[filename] = configparser.ConfigParser()\n\t\t\t\tconfigurations[filename].read_file(f)\n\t\t\t\tconfigurations[filename] = _validate_configuration(configurations[filename], filename, resolve_env)\n\t\t\texcept configparser.Error as e:\n\t\t\t\terrors.append(\"Unable to parse configuration file '{}': {}\".format(filename, e.message))\n\t\tif overrides is not None:\n\t\t\tconfigurations[filename] = _merge_dictionaries(configurations[filename], overrides)\n\treturn configurations, errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\timport json\n\timport logging\n\timport os\n\tfrom copy import deepcopy\n\tfrom jsonschema import validate\n\tfrom jsonschema.exceptions import ValidationError\n\n\tconfigurations = {}\n\terrors = []\n\n\t# Load configuration files\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename) as f:\n\t\t\t\tconfig = json.load(f)\n\t\texcept IOError as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname='configuration.load_configurations',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tmsg='Failed to read configuration file \"%s\": %s' % (filename, e),\n\t\t\t))\n\t\t\tcontinue\n\t\texcept ValueError as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname='configuration.load_configurations',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tmsg='Failed to parse configuration file \"%s\": %s' % (filename, e),\n\t\t\t))\n\t\t\tcontinue\n\n\t\t# Validate configuration\n\t\ttry:\n\t\t\tvalidate(config, JSON_SCHEMA_CONFIGURATION)\n\t\texcept ValidationError as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname='configuration", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigurations.append((config_filename, load_configuration(config_filename)))\n\t\texcept Exception as e:\n\t\t\tlogging.exception(e)\n\n\treturn configurations\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfig_filenames = [os.path.normpath(f) for f in config_filenames]\n\tconfig_filenames = [os.path.abspath(f) for f in config_filenames]\n\tconfig_filenames = [f for f in config_filenames if os.path.isfile(f)]\n\n\tconfig_filenames = [f for f in config_filenames if f.endswith(\".py\") or f.endswith(\".ini\")]\n\n\tif not overrides:\n\t\toverrides = []\n\toverrides = [os.path.normpath(f) for f in overrides]\n\toverrides = [os.path.abspath(f) for f in overrides]\n\toverrides = [f for f in overrides if os.path.isfile(f)]\n\n\tconfig_filenames = [f for f in config_filenames if not f in overrides]\n\n\tconfig_filenames = [f for f in config_filenames if f.endswith(\".py\")]\n\n\tconfig_filenames = [f for f in config_filenames if f.endswith(\".ini\")]\n\n\tconfig_filenames = [f for f in config_fil", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tlogger = logging.getLogger(\"load_configurations\")\n\t\n\tresult = []\n\t\n\ttry:\n\t\tfor filename in config_filenames:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tlogger.info('Reading configuration from \"%s\".' % filename)\n\t\t\t\t\n\t\t\t\ttry:\n\t\t\t\t\tconfig = yaml.safe_load(f.read())\n\t\t\t\texcept yaml.YAMLError as e:\n\t\t\t\t\tlogger.error('Error parsing configuration file \"%s\": %s' % (filename, e))\n\t\t\t\t\t\n\t\t\t\t\tresult.append((filename, e))\n\t\t\t\t\tcontinue\n\t\t\t\t\n\t\t\t\tif overrides is not None:\n\t\t\t\t\tconfig = dict(config.items() + overrides.items())\n\t\t\t\tif resolve_env:\n\t\t\t\t\tresolve_env_in_config(config)\n\t\t\t\t\n\t\t\t\tresult.append((filename, config))\n\t\t\n\t\treturn tuple(result)\n\t\n\tfinally:\n\t\tlogger.info('Finished reading all configurations.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tresult = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_yaml_config(filename, resolve_env=resolve_env)\n\t\texcept Exception as e:\n\t\t\tlogging.exception(e)\n\t\t\tresult.append((None, [logging.exception(e)]) if isinstance(e, Exception) else (None, [e]))\n\t\t\tcontinue\n\t\tresult.append((config, []))\n\treturn tuple(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t\"\"\"\n\tGiven a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.\n\t\"\"\"\n\tconfigurations = {}\n\terrors = []\n\t\n\tfor configuration_filename in config_filenames:\n\t\ttry:\n\t\t\tif resolve_env:\n\t\t\t\tconfiguration_filename = os.path.expanduser(configuration_filename)\n\t\t\tconfiguration = ConfigurationLoader.load_configuration(configuration_filename)\n\t\t\tconfigurations[configuration_filename] = configuration\n\t\texcept Exception, error:\n\t\t\terrors.append(error)\n\t\n\treturn configurations, errors\n", "is_pass": false, "return_code": 1}]}
{"file_path": "o2sclpy/utils.py", "project": "awsteiner/o2sclpy", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, numpy.bytes_):\n\t\t\treturn obj.decode(\"utf-8\")\n\t\treturn obj\n\texcept AttributeError:\n\t\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif type(obj) == bytes:\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn str(obj)\n\texcept UnicodeDecodeError:\n\t\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\treturn obj.decode('utf-8').encode('utf-8')\n\texcept:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode(\"utf-8\")\n\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif type(obj) is np.bytes_ or type(obj) is bytes:\n\t\treturn obj.decode('utf-8')\n\treturn obj", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif (isinstance(obj, bytes) or isinstance(obj, numpy.bytes_)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\texcept:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, str):\n\t\t\treturn obj\n\t\tif isinstance(obj, np.bytes_):\n\t\t\treturn obj.decode()\n\t\treturn obj\n\texcept:\n\t\treturn obj\n\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\texcept AttributeError:\n\t\treturn obj\n", "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tobj = str(obj)\n\texcept:\n\t\tpass\n\treturn obj"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, numpy.bytes_):\n\t\t\treturn obj.decode(\"utf-8\")\n\t\treturn obj\n\texcept AttributeError:\n\t\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif type(obj) == bytes:\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn str(obj)\n\texcept UnicodeDecodeError:\n\t\treturn obj", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\treturn obj.decode('utf-8').encode('utf-8')\n\texcept:\n\t\treturn obj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\tif isinstance(obj, numpy.bytes_):\n\t\treturn obj.decode(\"utf-8\")\n\treturn obj\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif type(obj) is np.bytes_ or type(obj) is bytes:\n\t\treturn obj.decode('utf-8')\n\treturn obj", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\t\n\tif (isinstance(obj, bytes) or isinstance(obj, numpy.bytes_)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\texcept:\n\t\treturn obj\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, str):\n\t\t\treturn obj\n\t\tif isinstance(obj, np.bytes_):\n\t\t\treturn obj.decode()\n\t\treturn obj\n\texcept:\n\t\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tif isinstance(obj, bytes):\n\t\t\treturn obj.decode('utf-8')\n\t\telse:\n\t\t\treturn obj\n\texcept AttributeError:\n\t\treturn obj\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\t\"\"\"\n\tForce decoding obj with 'utf-8' if it is a numpy.bytes or bytes\n\n    Args:\n        obj: consistency config\n\n    Returns:\n        returns the bytes object corresponding to `obj` in case it is a string using UTF-8 else return `obj`\n\t\"\"\"\n\ttry:\n\t\tobj = str(obj)\n\texcept:\n\t\tpass\n\treturn obj", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tns = ticks * 10 ** 9\n\tdt = datetime.fromtimestamp(ns / 10 ** 9, tz)\n\treturn cls(dt)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tdt = datetime.datetime.utcfromtimestamp(ticks/1e9)\n\texcept OverflowError:\n\t\traise ValueError('ticks is out of bounds')\n\n\tif tz is None:\n\t\treturn cls(dt)\n\telse:\n\t\treturn cls(dt, tz)", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom tzlocal import get_localzone\n\n\tif not ticks:\n\t\treturn cls.from_utc(0)\n\telif ticks > 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\telse:\n\t\tdt = datetime.utcfromtimestamp(ticks / 1000000000)\n\t\tif tz:\n\t\t\tdt = dt.replace(tzinfo=tz)\n\t\treturn cls.from_utc(dt)\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tfrom math import floor\n\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be >= 0\")\n\n\tn = ticks / 1e9\n\td = timedelta(seconds=n)\n\tdt = datetime(1970, 1, 1, tzinfo=tz) + d\n\treturn cls(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second,\n\t\tdt.microsecond, dt.tzinfo)\n\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\traise NotImplementedError()\n\n# py:method Time.from_ticks_datetime(cls, ticks, tz=None)\n\n\n# py:method Time.from_ticks_utc(cls, ticks)\n\n\n# py:method Time.from_utc(cls, tz=None)\n\n\n# py:method Time.from_utc_datetime(cls, tz=None)\n\n\n# py:method Time.from_utc_timestamp(cls, tz=None)\n\n\n# py:method Time.to_ticks(self)\n\n\n# py:method Time.to_ticks_datetime(self)\n\n\n# py:method Time.to_ticks_utc(self)\n\n\n# py:method Time.to_utc(self)\n\n\n# py:method Time.to_utc_datetime(self)\n\n\n# py:method Time.to_utc_timestamp(self)\n\n\n# py:method Time.from_datetime(cls, dt, tz=None)\n\n\n# py:method Time.from_naive(cls, dt, tz=None)\n\n\n# py:method Time.from_timestamp(cls, ts, tz=None)\n\n\n# py:method", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\ttime = cls.fromtimestamp(ticks / 1e9, tz)\n\texcept ValueError:\n\t\traise ValueError(\"ticks out of bounds\")\n\treturn time\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tticks = int(ticks)\n\n\tif not 0 <= ticks < 86400000000000:\n\t\traise ValueError(\"ticks is out of bounds\")\n\n\treturn cls.from_datetime(datetime.datetime.utcfromtimestamp(ticks / 1000000000), tz=tz)", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\treturn cls(ticks / 1000)\n\texcept OverflowError:\n\t\traise ValueError(\"ticks out of bounds\")\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tdt = datetime.datetime.utcfromtimestamp(ticks / 1e9)\n\tif not tz:\n\t\treturn cls(dt)\n\treturn cls(dt, tz)\n", "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tns = ticks % 1000000000\n\t\tsecs = ticks // 1000000000\n\t\ttz = tz or datetime.timezone.utc\n\t\tdt = datetime.datetime.fromtimestamp(secs, tz)\n\t\tdt = dt.replace(microsecond=ns)\n\t\tdt = cls(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond, dt.tzinfo)\n\t\treturn dt\n\texcept OverflowError:\n\t\traise ValueError('ticks should be an integer less than 86400000000000')\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tns = ticks * 10 ** 9\n\tdt = datetime.fromtimestamp(ns / 10 ** 9, tz)\n\treturn cls(dt)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tdt = datetime.datetime.utcfromtimestamp(ticks/1e9)\n\texcept OverflowError:\n\t\traise ValueError('ticks is out of bounds')\n\n\tif tz is None:\n\t\treturn cls(dt)\n\telse:\n\t\treturn cls(dt, tz)", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tfrom datetime import datetime\n\tfrom tzlocal import get_localzone\n\n\tif not ticks:\n\t\treturn cls.from_utc(0)\n\telif ticks > 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\telse:\n\t\tdt = datetime.utcfromtimestamp(ticks / 1000000000)\n\t\tif tz:\n\t\t\tdt = dt.replace(tzinfo=tz)\n\t\treturn cls.from_utc(dt)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tfrom datetime import datetime, timedelta\n\tfrom math import floor\n\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be >= 0\")\n\n\tn = ticks / 1e9\n\td = timedelta(seconds=n)\n\tdt = datetime(1970, 1, 1, tzinfo=tz) + d\n\treturn cls(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second,\n\t\tdt.microsecond, dt.tzinfo)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\traise NotImplementedError()\n\n# py:method Time.from_ticks_datetime(cls, ticks, tz=None)\n\n\n# py:method Time.from_ticks_utc(cls, ticks)\n\n\n# py:method Time.from_utc(cls, tz=None)\n\n\n# py:method Time.from_utc_datetime(cls, tz=None)\n\n\n# py:method Time.from_utc_timestamp(cls, tz=None)\n\n\n# py:method Time.to_ticks(self)\n\n\n# py:method Time.to_ticks_datetime(self)\n\n\n# py:method Time.to_ticks_utc(self)\n\n\n# py:method Time.to_utc(self)\n\n\n# py:method Time.to_utc_datetime(self)\n\n\n# py:method Time.to_utc_timestamp(self)\n\n\n# py:method Time.from_datetime(cls, dt, tz=None)\n\n\n# py:method Time.from_naive(cls, dt, tz=None)\n\n\n# py:method Time.from_timestamp(cls, ts, tz=None)\n\n\n# py:method", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\ttime = cls.fromtimestamp(ticks / 1e9, tz)\n\texcept ValueError:\n\t\traise ValueError(\"ticks out of bounds\")\n\treturn time\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tticks = int(ticks)\n\n\tif not 0 <= ticks < 86400000000000:\n\t\traise ValueError(\"ticks is out of bounds\")\n\n\treturn cls.from_datetime(datetime.datetime.utcfromtimestamp(ticks / 1000000000), tz=tz)", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\treturn cls(ticks / 1000)\n\texcept OverflowError:\n\t\traise ValueError(\"ticks out of bounds\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\tdt = datetime.datetime.utcfromtimestamp(ticks / 1e9)\n\tif not tz:\n\t\treturn cls(dt)\n\treturn cls(dt, tz)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\t\"\"\"\n\tCreate a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)\n\t\"\"\"\n\ttry:\n\t\tns = ticks % 1000000000\n\t\tsecs = ticks // 1000000000\n\t\ttz = tz or datetime.timezone.utc\n\t\tdt = datetime.datetime.fromtimestamp(secs, tz)\n\t\tdt = dt.replace(microsecond=ns)\n\t\tdt = cls(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond, dt.tzinfo)\n\t\treturn dt\n\texcept OverflowError:\n\t\traise ValueError('ticks should be an integer less than 86400000000000')\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\timport sys\n\tfrom . import Protocol\n\n\tprotocol_handlers = {}\n\n\t# get all available protocol versions\n\tversions = Protocol.get_available_versions()\n\n\t# get the requested protocol version, or None if not specified\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be specified as a tuple\")\n\t\tif len(protocol_version) != 2:\n\t\t\traise ValueError(\"Protocol version must be specified as a tuple of len 2\")\n\n\t\t# if version is not supported, raise an error\n\t\tif not protocol_version in versions:\n\t\t\traise ValueError(\"Protocol version {} is not supported\".format(protocol_version))\n\n\t\t# add the handler class for the requested version\n\t\tprotocol_handlers[protocol_version] = cls\n\n\telse:\n\t\t# add all handlers for all versions\n\t\tfor version in versions:\n\t\t\tprotocol_handlers[version] = cls\n\n\treturn protocol_handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = {}\n\n\tif protocol_version is not None:\n\t\tprotocol_handlers[protocol_version] = cls\n\telse:\n\t\tfor version in cls.PROTOCOL_VERS:\n\t\t\tprotocol_handlers[version] = cls\n\n\treturn protocol_handlers\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tdef get_handler(version):\n\t\tif version in cls.PROTOCOL_HANDLERS:\n\t\t\treturn cls.PROTOCOL_HANDLERS[version]\n\t\telse:\n\t\t\treturn None\n\n\tdef get_supported_versions():\n\t\tsupported_versions = set()\n\t\tfor version in cls.PROTOCOL_HANDLERS:\n\t\t\tsupported_versions.add(version)\n\t\treturn supported_versions\n\n\tdef get_version_handlers():\n\t\tversion_handlers = {}\n\t\tfor version in get_supported_versions():\n\t\t\thandler = get_handler(version)\n\t\t\tif handler is not None:\n\t\t\t\tversion_handlers[version] = handler\n\t\treturn version_handlers\n\n\tif protocol_version is None:\n\t\treturn get_version_handlers()\n\telse:\n\t\tversion_handlers = get_version_handlers()\n\t\tif protocol_version in version_handlers:\n\t\t\treturn {protocol_version: version_handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tdef _get_version_handler(version):\n\t\t\"\"\"\n\t\tReturn a function that will return the handler class\n\t\tfor the given version tuple.\n\t\t\"\"\"\n\t\tdef _get_handler(version):\n\t\t\t\"\"\"\n\t\t\tReturn the handler class for the given version tuple.\n\t\t\t\"\"\"\n\t\t\tif version in handlers:\n\t\t\t\treturn handlers[version]\n\t\t\telse:\n\t\t\t\treturn None\n\n\t\t_get_handler.__name__ = 'handler_for_%s' % version\n\t\treturn _get_handler\n\n\tif protocol_version is None:\n\t\t# If no version is given, return all available handlers\n\t\treturn {version: handlers[version] for version in handlers}\n\telse:\n\t\t# Otherwise, return one handler for the given version\n\t\tif isinstance(protocol_version, tuple):\n\t\t\tif protocol_version in handlers:\n\t\t\t\treturn handlers[protocol_version]\n\t\telse:\n\t\t\traise TypeError('protocol_version must be a tuple')\n\t\traise KeyError('%s is not a supported protocol version' % protocol_version)\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandler_dict = {}\n\n\tdef add_handler(handler):\n\t\thandler_dict[(handler.protocol_version, handler.version)] = handler\n\n\tdef is_supported(protocol_version):\n\t\tif not protocol_version:\n\t\t\treturn True\n\t\ttry:\n\t\t\treturn (protocol_version[0], protocol_version[1]) in handler_dict\n\t\texcept:\n\t\t\treturn False\n\n\tfor handler in cls._protocol_handlers:\n\t\tif is_supported(handler.protocol_version):\n\t\t\tadd_handler(handler)\n\n\tif protocol_version is None:\n\t\treturn handler_dict\n\telse:\n\t\treturn {protocol_version: handler_dict.get(protocol_version, None)}\n\n#py-bolt/exceptions.py", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tfrom bolt.protocol.v3.handlers import HandlerV3\n\tfrom bolt.protocol.v4.handlers import HandlerV4\n\tfrom bolt.protocol.v5.handlers import HandlerV5\n\n\t# TODO: Implement new protocol handlers here\n\tversion_handlers = {\n\t\t(3, 5): HandlerV3,\n\t\t(4, 0): HandlerV4,\n\t\t(5, 0): HandlerV5,\n\t}\n\n\tif protocol_version is not None:\n\t\tif type(protocol_version) is not tuple:\n\t\t\traise TypeError('Protocol version must be a tuple')\n\n\t\t# TODO: Add support for protocol versions greater than 5.0\n\n\t\ttry:\n\t\t\treturn version_handlers[protocol_version]\n\t\texcept KeyError:\n\t\t\treturn None\n\telse:\n\t\treturn version_handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandler_map = {}\n\tfor handler in cls.__subclasses__():\n\t\thandler_map.update(handler.protocol_handlers(protocol_version))\n\treturn handler_map\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = dict()\n\tfor version in cls.protocol_versions:\n\t\tif protocol_version:\n\t\t\tif version == protocol_version:\n\t\t\t\tprotocol_handlers[version] = cls\n\t\t\t\tbreak\n\t\telse:\n\t\t\tprotocol_handlers[version] = cls\n\treturn protocol_handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\ttry:\n\t\tprotocol_version = tuple(protocol_version)\n\texcept TypeError:\n\t\traise TypeError('protocol_version must be a tuple')\n\n\thandlers = {}\n\n\tfor ver in _PROTOCOL_VERSIONS:\n\t\t# skip if not supported by the given version\n\t\tif ver > protocol_version:\n\t\t\tcontinue\n\n\t\t# skip if not supported by any previous versions\n\t\tif protocol_version is not None and ver < protocol_version:\n\t\t\tcontinue\n\n\t\t# if we're here, we need to return the handler for this protocol version\n\t\thandlers[ver] = cls\n\n\treturn handlers\n\n", "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = {\n\t\t(0, 0): Bolt00ProtocolHandler,\n\t\t(1, 0): Bolt10ProtocolHandler,\n\t\t(2, 0): Bolt20ProtocolHandler,\n\t\t(3, 0): Bolt30ProtocolHandler,\n\t\t(3, 1): Bolt31ProtocolHandler,\n\t\t(3, 2): Bolt32ProtocolHandler,\n\t\t(3, 3): Bolt33ProtocolHandler,\n\t\t(3, 4): Bolt34ProtocolHandler\n\t}\n\n\tif protocol_version is None:\n\t\treturn protocol_handlers\n\n\telif isinstance(protocol_version, int):\n\t\tversion_tuple = (protocol_version // 100, protocol_version % 100)\n\n\t\tif version_tuple in protocol_handlers:\n\t\t\treturn {version_tuple: protocol_handlers[version_tuple]}\n\n\t\telif version_tuple[0] not in protocol_handlers:\n\t\t\treturn {}\n\n\t\telif version_tuple[1] not in protocol_handlers[version_tuple[0]]:\n\t\t\treturn {}\n\n\t\telse:\n\t\t\treturn {version_tuple: protocol_handlers[version_tuple]}\n\n\t"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\timport sys\n\tfrom . import Protocol\n\n\tprotocol_handlers = {}\n\n\t# get all available protocol versions\n\tversions = Protocol.get_available_versions()\n\n\t# get the requested protocol version, or None if not specified\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be specified as a tuple\")\n\t\tif len(protocol_version) != 2:\n\t\t\traise ValueError(\"Protocol version must be specified as a tuple of len 2\")\n\n\t\t# if version is not supported, raise an error\n\t\tif not protocol_version in versions:\n\t\t\traise ValueError(\"Protocol version {} is not supported\".format(protocol_version))\n\n\t\t# add the handler class for the requested version\n\t\tprotocol_handlers[protocol_version] = cls\n\n\telse:\n\t\t# add all handlers for all versions\n\t\tfor version in versions:\n\t\t\tprotocol_handlers[version] = cls\n\n\treturn protocol_handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = {}\n\n\tif protocol_version is not None:\n\t\tprotocol_handlers[protocol_version] = cls\n\telse:\n\t\tfor version in cls.PROTOCOL_VERS:\n\t\t\tprotocol_handlers[version] = cls\n\n\treturn protocol_handlers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tdef get_handler(version):\n\t\tif version in cls.PROTOCOL_HANDLERS:\n\t\t\treturn cls.PROTOCOL_HANDLERS[version]\n\t\telse:\n\t\t\treturn None\n\n\tdef get_supported_versions():\n\t\tsupported_versions = set()\n\t\tfor version in cls.PROTOCOL_HANDLERS:\n\t\t\tsupported_versions.add(version)\n\t\treturn supported_versions\n\n\tdef get_version_handlers():\n\t\tversion_handlers = {}\n\t\tfor version in get_supported_versions():\n\t\t\thandler = get_handler(version)\n\t\t\tif handler is not None:\n\t\t\t\tversion_handlers[version] = handler\n\t\treturn version_handlers\n\n\tif protocol_version is None:\n\t\treturn get_version_handlers()\n\telse:\n\t\tversion_handlers = get_version_handlers()\n\t\tif protocol_version in version_handlers:\n\t\t\treturn {protocol_version: version_handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tdef _get_version_handler(version):\n\t\t\"\"\"\n\t\tReturn a function that will return the handler class\n\t\tfor the given version tuple.\n\t\t\"\"\"\n\t\tdef _get_handler(version):\n\t\t\t\"\"\"\n\t\t\tReturn the handler class for the given version tuple.\n\t\t\t\"\"\"\n\t\t\tif version in handlers:\n\t\t\t\treturn handlers[version]\n\t\t\telse:\n\t\t\t\treturn None\n\n\t\t_get_handler.__name__ = 'handler_for_%s' % version\n\t\treturn _get_handler\n\n\tif protocol_version is None:\n\t\t# If no version is given, return all available handlers\n\t\treturn {version: handlers[version] for version in handlers}\n\telse:\n\t\t# Otherwise, return one handler for the given version\n\t\tif isinstance(protocol_version, tuple):\n\t\t\tif protocol_version in handlers:\n\t\t\t\treturn handlers[protocol_version]\n\t\telse:\n\t\t\traise TypeError('protocol_version must be a tuple')\n\t\traise KeyError('%s is not a supported protocol version' % protocol_version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandler_dict = {}\n\n\tdef add_handler(handler):\n\t\thandler_dict[(handler.protocol_version, handler.version)] = handler\n\n\tdef is_supported(protocol_version):\n\t\tif not protocol_version:\n\t\t\treturn True\n\t\ttry:\n\t\t\treturn (protocol_version[0], protocol_version[1]) in handler_dict\n\t\texcept:\n\t\t\treturn False\n\n\tfor handler in cls._protocol_handlers:\n\t\tif is_supported(handler.protocol_version):\n\t\t\tadd_handler(handler)\n\n\tif protocol_version is None:\n\t\treturn handler_dict\n\telse:\n\t\treturn {protocol_version: handler_dict.get(protocol_version, None)}\n\n#py-bolt/exceptions.py", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tfrom bolt.protocol.v3.handlers import HandlerV3\n\tfrom bolt.protocol.v4.handlers import HandlerV4\n\tfrom bolt.protocol.v5.handlers import HandlerV5\n\n\t# TODO: Implement new protocol handlers here\n\tversion_handlers = {\n\t\t(3, 5): HandlerV3,\n\t\t(4, 0): HandlerV4,\n\t\t(5, 0): HandlerV5,\n\t}\n\n\tif protocol_version is not None:\n\t\tif type(protocol_version) is not tuple:\n\t\t\traise TypeError('Protocol version must be a tuple')\n\n\t\t# TODO: Add support for protocol versions greater than 5.0\n\n\t\ttry:\n\t\t\treturn version_handlers[protocol_version]\n\t\texcept KeyError:\n\t\t\treturn None\n\telse:\n\t\treturn version_handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\thandler_map = {}\n\tfor handler in cls.__subclasses__():\n\t\thandler_map.update(handler.protocol_handlers(protocol_version))\n\treturn handler_map\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = dict()\n\tfor version in cls.protocol_versions:\n\t\tif protocol_version:\n\t\t\tif version == protocol_version:\n\t\t\t\tprotocol_handlers[version] = cls\n\t\t\t\tbreak\n\t\telse:\n\t\t\tprotocol_handlers[version] = cls\n\treturn protocol_handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\ttry:\n\t\tprotocol_version = tuple(protocol_version)\n\texcept TypeError:\n\t\traise TypeError('protocol_version must be a tuple')\n\n\thandlers = {}\n\n\tfor ver in _PROTOCOL_VERSIONS:\n\t\t# skip if not supported by the given version\n\t\tif ver > protocol_version:\n\t\t\tcontinue\n\n\t\t# skip if not supported by any previous versions\n\t\tif protocol_version is not None and ver < protocol_version:\n\t\t\tcontinue\n\n\t\t# if we're here, we need to return the handler for this protocol version\n\t\thandlers[ver] = cls\n\n\treturn handlers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t\"\"\"\n\tReturn a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple\n\t\"\"\"\n\tprotocol_handlers = {\n\t\t(0, 0): Bolt00ProtocolHandler,\n\t\t(1, 0): Bolt10ProtocolHandler,\n\t\t(2, 0): Bolt20ProtocolHandler,\n\t\t(3, 0): Bolt30ProtocolHandler,\n\t\t(3, 1): Bolt31ProtocolHandler,\n\t\t(3, 2): Bolt32ProtocolHandler,\n\t\t(3, 3): Bolt33ProtocolHandler,\n\t\t(3, 4): Bolt34ProtocolHandler\n\t}\n\n\tif protocol_version is None:\n\t\treturn protocol_handlers\n\n\telif isinstance(protocol_version, int):\n\t\tversion_tuple = (protocol_version // 100, protocol_version % 100)\n\n\t\tif version_tuple in protocol_handlers:\n\t\t\treturn {version_tuple: protocol_handlers[version_tuple]}\n\n\t\telif version_tuple[0] not in protocol_handlers:\n\t\t\treturn {}\n\n\t\telif version_tuple[1] not in protocol_handlers[version_tuple[0]]:\n\t\t\treturn {}\n\n\t\telse:\n\t\t\treturn {version_tuple: protocol_handlers[version_tuple]}\n\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.values = list(values)\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\ttry:\n\t\tvalues = values[0].split(',')\n\texcept AttributeError:\n\t\traise ValueError('Could not load bookmarks from %s' % values)\n\t\n\tif len(values) != cls.num_bookmarks:\n\t\traise ValueError('Expected %d bookmarks but got %d' % (cls.num_bookmarks, len(values)))\n\t\n\ttry:\n\t\tbookmarks = [float(v) for v in values]\n\texcept ValueError:\n\t\traise ValueError('Bookmarks could not be parsed')\n\t\n\treturn cls(bookmarks)\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tresult = cls()\n\tresult.from_raw_values(values)\n\treturn result\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tname = values[0]\n\turl = values[1]\n\ttitle = values[2]\n\tdescription = values[3]\n\tcreated_at = values[4]\n\tupdated_at = values[5]\n\tbookmark_id = values[6]\n\treturn cls(name, url, title, description, created_at, updated_at, bookmark_id)\n\n#app/models/bookmarks.py\nfrom .base import Base\nfrom .bookmark_values import from_raw_values\nfrom .bookmark_id import BookmarkId\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.add(value)\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = Bookmarks()\n\tfor value in values:\n\t\tbookmarks.add(value)\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\t\n\tfor value in values:\n\t\tbookmarks.append(cls.from_raw_value(value))\n\t\n\treturn cls(bookmarks)\n\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.from_raw_value(value)\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\n\tbookmarks = []\n\tfor value in values:\n\t\tbookmarks.append(cls(value))\n\t\n\treturn bookmarks\n", "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.add_bookmark(value)\n\treturn bookmarks\n"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tbookmarks.values = list(values)\n\treturn bookmarks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\ttry:\n\t\tvalues = values[0].split(',')\n\texcept AttributeError:\n\t\traise ValueError('Could not load bookmarks from %s' % values)\n\t\n\tif len(values) != cls.num_bookmarks:\n\t\traise ValueError('Expected %d bookmarks but got %d' % (cls.num_bookmarks, len(values)))\n\t\n\ttry:\n\t\tbookmarks = [float(v) for v in values]\n\texcept ValueError:\n\t\traise ValueError('Bookmarks could not be parsed')\n\t\n\treturn cls(bookmarks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tresult = cls()\n\tresult.from_raw_values(values)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tname = values[0]\n\turl = values[1]\n\ttitle = values[2]\n\tdescription = values[3]\n\tcreated_at = values[4]\n\tupdated_at = values[5]\n\tbookmark_id = values[6]\n\treturn cls(name, url, title, description, created_at, updated_at, bookmark_id)\n\n#app/models/bookmarks.py\nfrom .base import Base\nfrom .bookmark_values import from_raw_values\nfrom .bookmark_id import BookmarkId\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.add(value)\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = Bookmarks()\n\tfor value in values:\n\t\tbookmarks.add(value)\n\treturn bookmarks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = []\n\t\n\tfor value in values:\n\t\tbookmarks.append(cls.from_raw_value(value))\n\t\n\treturn cls(bookmarks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.from_raw_value(value)\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\t\n\tbookmarks = []\n\tfor value in values:\n\t\tbookmarks.append(cls(value))\n\t\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\t\"\"\"\n\tCreate a Bookmarks object from a list of raw bookmark string values.\n\t\"\"\"\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.add_bookmark(value)\n\treturn bookmarks\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = seq if isinstance(seq, (tuple, list, str)) else (seq,)\n\tif bufsize is not None:\n\t\tseq = [seq] * bufsize\n\treturn seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = seq if isinstance(seq, (tuple, list, set, frozenset)) else (seq,)\n\n\tif bufsize is None:\n\t\tbufsize = max(len(seq), 100)\n\n\ttype_ = type(seq)\n\tseq = tuple(seq)\n\n\tif len(seq) > bufsize:\n\t\traise ValueError(\"sequence too large\")\n\n\treturn seq, type_\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = seq if isinstance(seq, (tuple, list)) else (seq,)\n\ttype_ = type(seq)\n\tif isinstance(seq, str):\n\t\tseq = seq.encode()\n\tif isinstance(seq, bytes):\n\t\tif bufsize is not None:\n\t\t\tseq = seq.ljust(bufsize)\n\treturn (seq, type_)\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tseq = seq if isinstance(seq, (tuple, list)) else (seq,)\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\n\tif len(seq) <= bufsize:\n\t\treturn (seq, 'seq')\n\t\n\tseqs, types = zip(*[_get_seq_with_type(s, bufsize) for s in seq])\n\treturn (seqs, types)\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = iter(seq)\n\texcept TypeError:\n\t\tseq = (seq,)\n\n\tif bufsize is not None:\n\t\tseq = itertools.islice(seq, bufsize)\n\n\ttry:\n\t\treturn next(seq), type(seq)\n\texcept StopIteration:\n\t\treturn seq, type(seq)\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq, type = seq, type(seq)\n\texcept (ValueError, TypeError):\n\t\tpass\n\n\tif not isinstance(seq, str):\n\t\tseq = str(seq)\n\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\tseq = _get_seq_buffer(seq, bufsize)\n\treturn seq, type\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tis_seq = not isinstance(seq, basestring)\n\texcept NameError:\n\t\tis_seq = False\n\tif is_seq:\n\t\tif bufsize is None:\n\t\t\tbufsize = sys.maxint\n\t\treturn seq, \"seq\"\n\telse:\n\t\tif bufsize is None:\n\t\t\tbufsize = sys.maxint\n\t\treturn \"\", \"str\"\n", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = _resolve_object(seq, seq.__class__)\n\tif _is_sequence(seq):\n\t\tseq = seq.seq\n\telif _is_string(seq):\n\t\tseq = seq.seq\n\telif _is_iterable(seq):\n\t\t# Assume all other iterable types\n\t\tseq = list(seq)\n\telse:\n\t\traise TypeError('Cannot determine sequence type')\n\t\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\n\treturn (seq[:bufsize], type(seq))", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tseq = seq if isinstance(seq, tuple) else (seq,)\n\t\n\tif isinstance(seq, int):\n\t\treturn (seq,)\n\telif isinstance(seq, float):\n\t\treturn (seq,)\n\telif isinstance(seq, str):\n\t\treturn (seq,)\n\telif isinstance(seq, bytes):\n\t\treturn (seq,)\n\telif isinstance(seq, (list, tuple)):\n\t\tif isinstance(seq[0], (int, float, str, bytes)):\n\t\t\treturn (seq,)\n\t\telse:\n\t\t\treturn (tuple(seq),)\n\telif hasattr(seq, \"__iter__\"):\n\t\tif isinstance(seq, (set, frozenset)):\n\t\t\treturn (tuple(seq),)\n\t\telif isinstance(seq, dict):\n\t\t\treturn (tuple(seq.items()),)\n\t\telif bufsize is not None:\n\t\t\tseq = list(seq)\n\t\t\tif len(seq) > bufsize:\n\t\t\t\tseq = seq[:bufsize]\n\t\treturn (tuple(seq),)\n\t\n\treturn (seq, type(seq))\n\t", "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = seq[:bufsize]\n\texcept TypeError:\n\t\tpass\n\treturn seq, type(seq)\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = seq if isinstance(seq, (tuple, list, str)) else (seq,)\n\tif bufsize is not None:\n\t\tseq = [seq] * bufsize\n\treturn seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = seq if isinstance(seq, (tuple, list, set, frozenset)) else (seq,)\n\n\tif bufsize is None:\n\t\tbufsize = max(len(seq), 100)\n\n\ttype_ = type(seq)\n\tseq = tuple(seq)\n\n\tif len(seq) > bufsize:\n\t\traise ValueError(\"sequence too large\")\n\n\treturn seq, type_\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = seq if isinstance(seq, (tuple, list)) else (seq,)\n\ttype_ = type(seq)\n\tif isinstance(seq, str):\n\t\tseq = seq.encode()\n\tif isinstance(seq, bytes):\n\t\tif bufsize is not None:\n\t\t\tseq = seq.ljust(bufsize)\n\treturn (seq, type_)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tseq = seq if isinstance(seq, (tuple, list)) else (seq,)\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\n\tif len(seq) <= bufsize:\n\t\treturn (seq, 'seq')\n\t\n\tseqs, types = zip(*[_get_seq_with_type(s, bufsize) for s in seq])\n\treturn (seqs, types)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = iter(seq)\n\texcept TypeError:\n\t\tseq = (seq,)\n\n\tif bufsize is not None:\n\t\tseq = itertools.islice(seq, bufsize)\n\n\ttry:\n\t\treturn next(seq), type(seq)\n\texcept StopIteration:\n\t\treturn seq, type(seq)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq, type = seq, type(seq)\n\texcept (ValueError, TypeError):\n\t\tpass\n\n\tif not isinstance(seq, str):\n\t\tseq = str(seq)\n\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\tseq = _get_seq_buffer(seq, bufsize)\n\treturn seq, type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tis_seq = not isinstance(seq, basestring)\n\texcept NameError:\n\t\tis_seq = False\n\tif is_seq:\n\t\tif bufsize is None:\n\t\t\tbufsize = sys.maxint\n\t\treturn seq, \"seq\"\n\telse:\n\t\tif bufsize is None:\n\t\t\tbufsize = sys.maxint\n\t\treturn \"\", \"str\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\tseq = _resolve_object(seq, seq.__class__)\n\tif _is_sequence(seq):\n\t\tseq = seq.seq\n\telif _is_string(seq):\n\t\tseq = seq.seq\n\telif _is_iterable(seq):\n\t\t# Assume all other iterable types\n\t\tseq = list(seq)\n\telse:\n\t\traise TypeError('Cannot determine sequence type')\n\t\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\t\n\treturn (seq[:bufsize], type(seq))", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\t\n\tseq = seq if isinstance(seq, tuple) else (seq,)\n\t\n\tif isinstance(seq, int):\n\t\treturn (seq,)\n\telif isinstance(seq, float):\n\t\treturn (seq,)\n\telif isinstance(seq, str):\n\t\treturn (seq,)\n\telif isinstance(seq, bytes):\n\t\treturn (seq,)\n\telif isinstance(seq, (list, tuple)):\n\t\tif isinstance(seq[0], (int, float, str, bytes)):\n\t\t\treturn (seq,)\n\t\telse:\n\t\t\treturn (tuple(seq),)\n\telif hasattr(seq, \"__iter__\"):\n\t\tif isinstance(seq, (set, frozenset)):\n\t\t\treturn (tuple(seq),)\n\t\telif isinstance(seq, dict):\n\t\t\treturn (tuple(seq.items()),)\n\t\telif bufsize is not None:\n\t\t\tseq = list(seq)\n\t\t\tif len(seq) > bufsize:\n\t\t\t\tseq = seq[:bufsize]\n\t\treturn (tuple(seq),)\n\t\n\treturn (seq, type(seq))\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\t\"\"\"\n\tReturn a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).\n\t\"\"\"\n\ttry:\n\t\tseq = seq[:bufsize]\n\texcept TypeError:\n\t\tpass\n\treturn seq, type(seq)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\ttry:\n\t\treturn self.scale\n\texcept AttributeError:\n\t\tif other is None:\n\t\t\treturn 0\n\t\telif not isinstance(other, (int, float)):\n\t\t\traise TypeError(\"scale() takes an optional float argument\")\n\t\telif other == 0:\n\t\t\traise ValueError(\"scale cannot be 0\")\n\t\telse:\n\t\t\tself.scale = other\n\t\t\treturn other\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\ttry:\n\t\treturn self._scale\n\texcept AttributeError:\n\t\ttry:\n\t\t\tself._scale = self._integral * self.scale_factor\n\t\texcept Exception:\n\t\t\traise LenaValueError('Histogram scale cannot be zero')\n\t\treturn self._scale\n\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tscale = self._scale\n\tif other is None:\n\t\treturn scale\n\tif recompute:\n\t\tscale = self._rescale(other)\n\telse:\n\t\tif scale == 0:\n\t\t\traise .LenaValueError(\"Can't rescale histogram with zero scale.\")\n\t\tscale = self._rescale(other, scale)\n\tself._scale = scale\n\treturn scale\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tself.recompute(recompute)\n\tif other is None:\n\t\treturn self.scale\n\tif other == 0:\n\t\traise LenaValueError('Cannot scale a histogram with scale 0')\n\tscale = self.scale * other / self.size\n\tself.scale = scale\n\tself.recompute()\n\treturn self\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tother = self.other if other is None else other\n\tif recompute or self.scale is None:\n\t\tself.scale = self.integral()\n\t\tself.recompute = False\n\treturn other / self.scale if other else self.scale\n\n\tdef rescale(self, other=None, recompute=False):\n\t\t\"\"\"\n\t\tRescale (fill) self to *other*.\n\nIf *other* is ``None``, return rescaled histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\t\"\"\"\n\t\tother = self.other if other is None else other\n\t\tif recompute or self.scale is None:\n\t\t\tself.scale = self.integral()\n\t\t\tself.recompute = False\n\t\tself.other = other / self.scale\n\t\treturn self\n\n\tdef normalize(self, recompute=", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tdef __init__(self, other=None):\n\t\tfrom ._lena_lib import _lena_histogram_scale\n\t\tself._lena_histogram_scale = _lena_histogram_scale\n\t\tself._lena_histogram_scale(self, other)\n\n\tdef __repr__(self):\n\t\tfrom ._lena_lib import _lena_histogram_scale_repr\n\t\treturn _lena_histogram_scale_repr(self)\n\n\tdef __str__(self):\n\t\tfrom ._lena_lib import _lena_histogram_scale_str\n\t\treturn _lena_histogram_scale_str(self)\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\t:type key: tuple of int\n\t\t\"\"\"\n\t\tfrom ._lena_lib import _lena_histogram_scale_getitem\n\t\treturn _lena_histogram_scale_getitem(self, key)\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\t:type key: tuple of int\n\t\t:type value: float\n\t\t\"\"\"\n\t\tfrom ._lena_lib import _lena_histogram_scale_setitem\n\t\t_lena", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tother = self.other(other)\n\tif self.is_empty or (self.scale == 0.0 and recompute):\n\t\tself.scale = self.integral()\n\tif other is not None:\n\t\tself.scale *= other\n\treturn self.scale\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tother = other if other else self.scale\n\tif not recompute and self.scale == other:\n\t\treturn\n\n\tself.scale = other\n\n\tself._compute_bins()\n\tself._compute_density()\n\tself._compute_scale()\n\n\tself._compute_cumulative_density()\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tself.check_not_empty()\n\tself.check_not_empty_scale()\n\n\tif other is None:\n\t\treturn self.scale\n\n\tif recompute:\n\t\tself.scale = self.compute_scale()\n\n\telse:\n\t\tself.scale = other\n\n\treturn self.scale\n", "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tscale = self.get_scale(recompute)\n\tif other is None:\n\t\treturn scale\n\telif scale == 0:\n\t\traise LenaValueError(\"histogram with scale 0 can't be rescaled\")\n\treturn other / scale\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\ttry:\n\t\treturn self.scale\n\texcept AttributeError:\n\t\tif other is None:\n\t\t\treturn 0\n\t\telif not isinstance(other, (int, float)):\n\t\t\traise TypeError(\"scale() takes an optional float argument\")\n\t\telif other == 0:\n\t\t\traise ValueError(\"scale cannot be 0\")\n\t\telse:\n\t\t\tself.scale = other\n\t\t\treturn other\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\ttry:\n\t\treturn self._scale\n\texcept AttributeError:\n\t\ttry:\n\t\t\tself._scale = self._integral * self.scale_factor\n\t\texcept Exception:\n\t\t\traise LenaValueError('Histogram scale cannot be zero')\n\t\treturn self._scale\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tscale = self._scale\n\tif other is None:\n\t\treturn scale\n\tif recompute:\n\t\tscale = self._rescale(other)\n\telse:\n\t\tif scale == 0:\n\t\t\traise .LenaValueError(\"Can't rescale histogram with zero scale.\")\n\t\tscale = self._rescale(other, scale)\n\tself._scale = scale\n\treturn scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tself.recompute(recompute)\n\tif other is None:\n\t\treturn self.scale\n\tif other == 0:\n\t\traise LenaValueError('Cannot scale a histogram with scale 0')\n\tscale = self.scale * other / self.size\n\tself.scale = scale\n\tself.recompute()\n\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tother = self.other if other is None else other\n\tif recompute or self.scale is None:\n\t\tself.scale = self.integral()\n\t\tself.recompute = False\n\treturn other / self.scale if other else self.scale\n\n\tdef rescale(self, other=None, recompute=False):\n\t\t\"\"\"\n\t\tRescale (fill) self to *other*.\n\nIf *other* is ``None``, return rescaled histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\t\"\"\"\n\t\tother = self.other if other is None else other\n\t\tif recompute or self.scale is None:\n\t\t\tself.scale = self.integral()\n\t\t\tself.recompute = False\n\t\tself.other = other / self.scale\n\t\treturn self\n\n\tdef normalize(self, recompute=", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tdef __init__(self, other=None):\n\t\tfrom ._lena_lib import _lena_histogram_scale\n\t\tself._lena_histogram_scale = _lena_histogram_scale\n\t\tself._lena_histogram_scale(self, other)\n\n\tdef __repr__(self):\n\t\tfrom ._lena_lib import _lena_histogram_scale_repr\n\t\treturn _lena_histogram_scale_repr(self)\n\n\tdef __str__(self):\n\t\tfrom ._lena_lib import _lena_histogram_scale_str\n\t\treturn _lena_histogram_scale_str(self)\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\t:type key: tuple of int\n\t\t\"\"\"\n\t\tfrom ._lena_lib import _lena_histogram_scale_getitem\n\t\treturn _lena_histogram_scale_getitem(self, key)\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\t:type key: tuple of int\n\t\t:type value: float\n\t\t\"\"\"\n\t\tfrom ._lena_lib import _lena_histogram_scale_setitem\n\t\t_lena", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tother = self.other(other)\n\tif self.is_empty or (self.scale == 0.0 and recompute):\n\t\tself.scale = self.integral()\n\tif other is not None:\n\t\tself.scale *= other\n\treturn self.scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tother = other if other else self.scale\n\tif not recompute and self.scale == other:\n\t\treturn\n\n\tself.scale = other\n\n\tself._compute_bins()\n\tself._compute_density()\n\tself._compute_scale()\n\n\tself._compute_cumulative_density()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tself.check_not_empty()\n\tself.check_not_empty_scale()\n\n\tif other is None:\n\t\treturn self.scale\n\n\tif recompute:\n\t\tself.scale = self.compute_scale()\n\n\telse:\n\t\tself.scale = other\n\n\treturn self.scale\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.\n\t\"\"\"\n\tscale = self.get_scale(recompute)\n\tif other is None:\n\t\treturn scale\n\telif scale == 0:\n\t\traise LenaValueError(\"histogram with scale 0 can't be rescaled\")\n\treturn other / scale\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom . import LenaValueError\n\n\tif other is None:\n\t\treturn self.scale\n\n\ttry:\n\t\treturn self.set(scale=other)\n\texcept LenaValueError as e:\n\t\traise LenaValueError(str(e))\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tself.set_scale(self.get_scale(other))\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tif other is None:\n\t\treturn self._scale\n\n\tif not (isinstance(other, (int, float)) or other is None):\n\t\traise LenaValueError('Scale must be a number or None.')\n\n\tself._scale = other\n\n\treturn other\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tfrom .. import LenaError\n\t\n\tif other is None:\n\t\treturn self._scale\n\telse:\n\t\tif not isinstance(other, (int, float)):\n\t\t\traise LenaError('expected numeric value')\n\t\tif other is 0.0:\n\t\t\traise LenaError('cannot rescale to zero')\n\t\tself.x, self.y = self.last_element()\n\t\tself.z = self.z or 0.0\n\t\tif self.z is 0.0:\n\t\t\traise LenaError('cannot rescale to zero')\n\t\tself._scale = other\n\t\treturn self._scale", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tx = self['x']\n\ty = self['y']\n\tif other is None:\n\t\tif 'x' in self:\n\t\t\ttry:\n\t\t\t\treturn np.sqrt(x**2 + y**2)\n\t\t\texcept TypeError:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"Cannot rescale %s to %s\" % (x, other))\n\t\telse:\n\t\t\treturn self['scale']\n\telse:\n\t\t# rescale to other\n\t\ttry:\n\t\t\tother = float(other)\n\t\texcept ValueError:\n\t\t\traise LenaValueError(\"Invalid scale value: %s\" % other)\n\t\tif not hasattr(other, '__len__'):\n\t\t\ttry:\n\t\t\t\tother = [other] * len(x)\n\t\t\texcept TypeError:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"Invalid scale value: %s\" % other)\n\t\ttry:\n\t\t\treturn np.sqrt(np.absolute(x)**2 + np.absolute(y)**2) / other\n\t\texcept TypeError:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Cannot rescale %s to %s\" % (x, other))\n\n# lena/__init__.py\nfrom .core import Lena\nfrom .errors import LenaValue", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom . import exceptions as lev\n\n\tif other is None:\n\t\treturn self.get_scale()\n\n\ttry:\n\t\tret = self.get_scale(other)\n\texcept lev.LenaValueError as e:\n\t\traise lev.LenaValueError('%s for %s' % (e, self))\n\n\tself.set_scale(other)\n\treturn ret\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tx = self.x\n\ty = self.y\n\tz = self.z\n\tif other is None:\n\t\treturn x.scale if x.scale is not None else y.scale if y.scale is not None else z.scale\n\tx.scale = other\n\ty.scale = other\n\tz.scale = other\n\tself.x = x\n\tself.y = y\n\tself.z = z\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tother = other if other is not None else self\n\tself.scale = other\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom . import _internal\n\tscale = self._get_field(\"scale\")\n\n\tif other is None:\n\t\treturn scale\n\n\ttry:\n\t\treturn _internal.scale(scale, other)\n\texcept ValueError as e:\n\t\traise LenaValueError(e) from e\n\n", "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\ttry:\n\t\tif other is None:\n\t\t\treturn self.scale\n\t\telse:\n\t\t\tself.scale = other\n\texcept LenaValueError:\n\t\traise LenaValueError('Cannot rescale to unknown or zero scale.')\n\n\tx = self.last(self.x)\n\ty = self.last(self.y)\n\tz = self.last(self.z)\n\n\tfor i in range(len(x)):\n\t\tself.x[i] = x[i] / self.scale\n\t\tself.y[i] = y[i] / self.scale\n\t\tself.z[i] = z[i] / self.scale"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom . import LenaValueError\n\n\tif other is None:\n\t\treturn self.scale\n\n\ttry:\n\t\treturn self.set(scale=other)\n\texcept LenaValueError as e:\n\t\traise LenaValueError(str(e))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tself.set_scale(self.get_scale(other))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tif other is None:\n\t\treturn self._scale\n\n\tif not (isinstance(other, (int, float)) or other is None):\n\t\traise LenaValueError('Scale must be a number or None.')\n\n\tself._scale = other\n\n\treturn other\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\t\n\tfrom .. import LenaError\n\t\n\tif other is None:\n\t\treturn self._scale\n\telse:\n\t\tif not isinstance(other, (int, float)):\n\t\t\traise LenaError('expected numeric value')\n\t\tif other is 0.0:\n\t\t\traise LenaError('cannot rescale to zero')\n\t\tself.x, self.y = self.last_element()\n\t\tself.z = self.z or 0.0\n\t\tif self.z is 0.0:\n\t\t\traise LenaError('cannot rescale to zero')\n\t\tself._scale = other\n\t\treturn self._scale", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tx = self['x']\n\ty = self['y']\n\tif other is None:\n\t\tif 'x' in self:\n\t\t\ttry:\n\t\t\t\treturn np.sqrt(x**2 + y**2)\n\t\t\texcept TypeError:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"Cannot rescale %s to %s\" % (x, other))\n\t\telse:\n\t\t\treturn self['scale']\n\telse:\n\t\t# rescale to other\n\t\ttry:\n\t\t\tother = float(other)\n\t\texcept ValueError:\n\t\t\traise LenaValueError(\"Invalid scale value: %s\" % other)\n\t\tif not hasattr(other, '__len__'):\n\t\t\ttry:\n\t\t\t\tother = [other] * len(x)\n\t\t\texcept TypeError:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"Invalid scale value: %s\" % other)\n\t\ttry:\n\t\t\treturn np.sqrt(np.absolute(x)**2 + np.absolute(y)**2) / other\n\t\texcept TypeError:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Cannot rescale %s to %s\" % (x, other))\n\n# lena/__init__.py\nfrom .core import Lena\nfrom .errors import LenaValue", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom . import exceptions as lev\n\n\tif other is None:\n\t\treturn self.get_scale()\n\n\ttry:\n\t\tret = self.get_scale(other)\n\texcept lev.LenaValueError as e:\n\t\traise lev.LenaValueError('%s for %s' % (e, self))\n\n\tself.set_scale(other)\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tx = self.x\n\ty = self.y\n\tz = self.z\n\tif other is None:\n\t\treturn x.scale if x.scale is not None else y.scale if y.scale is not None else z.scale\n\tx.scale = other\n\ty.scale = other\n\tz.scale = other\n\tself.x = x\n\tself.y = y\n\tself.z = z\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tother = other if other is not None else self\n\tself.scale = other\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\tfrom . import _internal\n\tscale = self._get_field(\"scale\")\n\n\tif other is None:\n\t\treturn scale\n\n\ttry:\n\t\treturn _internal.scale(scale, other)\n\texcept ValueError as e:\n\t\traise LenaValueError(e) from e\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\"\"\"\n\tGet or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.\n\t\"\"\"\n\ttry:\n\t\tif other is None:\n\t\t\treturn self.scale\n\t\telse:\n\t\t\tself.scale = other\n\texcept LenaValueError:\n\t\traise LenaValueError('Cannot rescale to unknown or zero scale.')\n\n\tx = self.last(self.x)\n\ty = self.last(self.y)\n\tz = self.last(self.z)\n\n\tfor i in range(len(x)):\n\t\tself.x[i] = x[i] / self.scale\n\t\tself.y[i] = y[i] / self.scale\n\t\tself.z[i] = z[i] / self.scale", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tfrom .util import split_bins, get_scale\n\n\tnum_fields = len(field_names)\n\tdef make_point(bin_):\n\t\tif callable(make_value):\n\t\t\tvalue = make_value(bin_)\n\t\t\treturn tuple(map(value, field_names))\n\t\telse:\n\t\t\tvalue = get_field(bin_, num_fields) if num_fields > 0 else bin_\n\t\t\treturn (value,) * num_fields\n\n\tgraph = Graph()\n\tgraph.scale = get_scale(hist) if scale is None else scale\n\tfor bin_, context in split_bins(hist):\n\t\tgraph.add_point(make_point(bin_), get_coordinate(hist, context))\n\treturn graph\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .. import graph\n\tfrom .. import utils\n\tfrom .. import bin\n\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Invalid get_coordinate: \" + get_coordinate)\n\n\tif make_value and callable(make_value):\n\t\tmake_value = utils.make_value_by_function(make_value)\n\n\tdef make_point(bin_):\n\t\t\"\"\"\n\t\tMake a graph point from a histogram bin.\n\t\t\"\"\"\n\t\tif callable(make_value):\n\t\t\tx, y = make_value(bin_)\n\t\telse:\n\t\t\tx = getattr(bin_, field_names[0])\n\t\t\ty = getattr(bin_, field_names[1])\n\t\treturn (x, y)\n\n\tif scale is True:\n\t\tscale = hist.scale\n\n\tgraph_ = graph.graph(field_names, scale=scale)\n\tfor bin_ in hist:\n\t\tgraph_.add_point(make_point(bin_))\n\n\treturn graph_\n\n#pyutils/misc/__init__.py\n\"\"\"\nSome miscellaneous utilities.\n\"\"\"\n\nfrom .hist_to_graph import hist_to_graph\n\n__all__ =", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tgraph = Graph()\n\tfield_names = list(field_names)\n\tif make_value is not None:\n\t\tfield_names.append(\"value\")\n\tif get_coordinate == \"left\":\n\t\tfor (bin_, field) in zip(hist, field_names):\n\t\t\tgraph.add_point(field, bin_.x, bin_.y)\n\telif get_coordinate == \"right\":\n\t\tfor (bin_, field) in zip(hist, field_names):\n\t\t\tgraph.add_point(field, bin_.x, bin_.y)\n\t\t\tgraph.add_point(field, bin_.x, bin_.y + bin_.bin_width)\n\telif get_coordinate == \"middle\":\n\t\tfor (bin_, field) in zip(hist, field_names):\n\t\t\tgraph.add_point(field, bin_.x - bin_.bin_width / 2, bin_.y)\n\telse:\n\t\traise ValueError(\"get_coordinate must be left, right or middle\")\n\tif scale:\n\t\tgraph.scale = hist.scale\n\treturn graph", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tfrom .histogram import Histogram\n\tfrom .bin import Bin\n\tif not isinstance(hist, Histogram):\n\t\thist = Histogram(hist)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.value\n\ttry:\n\t\tcoordinate = get_coordinate\n\t\tif coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError\n\texcept ValueError:\n\t\tcoordinate = \"left\"\n\tvalue_list = hist.bins_to_list(make_value)\n\tdim = len(field_names)\n\tdimension = hist.dimension\n\tgraph = Graph(value_list, field_names, dimension, coordinate, scale)\n\treturn graph\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\timport numpy as np\n\tfrom . import graph\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.mean\n\n\th = hist.as_array()\n\tx, y = np.meshgrid(range(h.shape[1]), h.T, indexing=\"ij\")\n\tx, y = x.ravel(), y.ravel()\n\n\tgraph = graph.Graph(\n\t\tx=x,\n\t\ty=y,\n\t\tfield_names=field_names,\n\t\tscale=scale,\n\t)\n\n\tfor i, bin_ in enumerate(h):\n\t\tgraph.points.add(\n\t\t\t**{\n\t\t\t\tfield_names[0]: i,\n\t\t\t\tfield_names[1]: make_value(bin_),\n\t\t\t}\n\t\t)\n\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\timport numpy\n\n\tfrom .base import make, bin_tuple\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\n\tbin_tuple.check(hist)\n\n\tcoordinate = get_coordinate\n\tif coordinate == \"right\":\n\t\tcoordinate = \"left\"\n\telif coordinate == \"middle\":\n\t\tcoordinate = \"right\"\n\n\tx, y = numpy.array(hist).T\n\tif coordinate == \"left\":\n\t\tx = numpy.array(x)\n\telse:\n\t\tx = numpy.array(y)\n\n\ty = numpy.array(y)\n\n\tif scale is None:\n\t\tscale = hist.scale\n\n\treturn make(x, y, make_value, scale=scale, field_names=field_names)\n\n#pythia/histogram/bin.py", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .histogram import histogram\n\tfrom .graph import graph\n\n\thist = histogram(hist)\n\tgraph = graph()\n\n\tgraph.set_field_names(field_names)\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.mean\n\n\tdef get_value(bin_):\n\t\treturn (make_value(bin_),)\n\n\tif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.center\n\n\tfor bin_ in hist:\n\t\tgraph.append(get_value(bin_))\n\n\tif scale is not None:\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\tgraph.set_scale(scale)\n\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph\n\tfrom .bin import bin\n\timport operator\n\tif scale is None: scale = getattr(hist, \"scale\", True)\n\tgraph_ = graph()\n\tgraph_.set_x_field_names(field_names[0], field_names[1])\n\tif scale: graph_.set_y_scale(getattr(hist, \"scale\", True))\n\tcoordinate = getattr(hist, \"coordinate\", get_coordinate)\n\tbins = list(hist)\n\tbins.sort(key=operator.attrgetter(\"x\"))\n\tfor bin_ in bins:\n\t\tgraph_.add_point(bin_.x, make_value(bin_))\n\treturn graph_", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\thist_data = hist.histogram()\n\tgraph = graph.graph()\n\tgraph.set_scale(scale)\n\tfor i in range(len(field_names)):\n\t\tgraph.set_field(i, hist_data[i])\n\tif make_value is None:\n\t\tgraph.set_field(len(field_names), hist_data[-1])\n\telse:\n\t\tgraph.set_field(len(field_names), make_value(hist_data[-1]))\n\tgraph.set_coordinate(get_coordinate)\n\treturn graph", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.mean\n\telif callable(make_value):\n\t\tpass\n\telse:\n\t\traise TypeError(\"make_value must be callable or None\")\n\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.mid\n\telse:\n\t\traise ValueError(\"get_coordinate must be left, right or middle\")\n\n\tgraph = Graph(field_names=field_names)\n\tfor field_name in field_names:\n\t\tgraph.add_field_value(field_name, get_coordinate)\n\n\tfor bin_ in hist.bins:\n\t\tgraph.add_point(make_value(bin_))\n\n\tif scale is True:\n\t\tgraph.scale = hist.scale\n\n\treturn graph\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tfrom .util import split_bins, get_scale\n\n\tnum_fields = len(field_names)\n\tdef make_point(bin_):\n\t\tif callable(make_value):\n\t\t\tvalue = make_value(bin_)\n\t\t\treturn tuple(map(value, field_names))\n\t\telse:\n\t\t\tvalue = get_field(bin_, num_fields) if num_fields > 0 else bin_\n\t\t\treturn (value,) * num_fields\n\n\tgraph = Graph()\n\tgraph.scale = get_scale(hist) if scale is None else scale\n\tfor bin_, context in split_bins(hist):\n\t\tgraph.add_point(make_point(bin_), get_coordinate(hist, context))\n\treturn graph\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .. import graph\n\tfrom .. import utils\n\tfrom .. import bin\n\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Invalid get_coordinate: \" + get_coordinate)\n\n\tif make_value and callable(make_value):\n\t\tmake_value = utils.make_value_by_function(make_value)\n\n\tdef make_point(bin_):\n\t\t\"\"\"\n\t\tMake a graph point from a histogram bin.\n\t\t\"\"\"\n\t\tif callable(make_value):\n\t\t\tx, y = make_value(bin_)\n\t\telse:\n\t\t\tx = getattr(bin_, field_names[0])\n\t\t\ty = getattr(bin_, field_names[1])\n\t\treturn (x, y)\n\n\tif scale is True:\n\t\tscale = hist.scale\n\n\tgraph_ = graph.graph(field_names, scale=scale)\n\tfor bin_ in hist:\n\t\tgraph_.add_point(make_point(bin_))\n\n\treturn graph_\n\n#pyutils/misc/__init__.py\n\"\"\"\nSome miscellaneous utilities.\n\"\"\"\n\nfrom .hist_to_graph import hist_to_graph\n\n__all__ =", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tgraph = Graph()\n\tfield_names = list(field_names)\n\tif make_value is not None:\n\t\tfield_names.append(\"value\")\n\tif get_coordinate == \"left\":\n\t\tfor (bin_, field) in zip(hist, field_names):\n\t\t\tgraph.add_point(field, bin_.x, bin_.y)\n\telif get_coordinate == \"right\":\n\t\tfor (bin_, field) in zip(hist, field_names):\n\t\t\tgraph.add_point(field, bin_.x, bin_.y)\n\t\t\tgraph.add_point(field, bin_.x, bin_.y + bin_.bin_width)\n\telif get_coordinate == \"middle\":\n\t\tfor (bin_, field) in zip(hist, field_names):\n\t\t\tgraph.add_point(field, bin_.x - bin_.bin_width / 2, bin_.y)\n\telse:\n\t\traise ValueError(\"get_coordinate must be left, right or middle\")\n\tif scale:\n\t\tgraph.scale = hist.scale\n\treturn graph", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tfrom .histogram import Histogram\n\tfrom .bin import Bin\n\tif not isinstance(hist, Histogram):\n\t\thist = Histogram(hist)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.value\n\ttry:\n\t\tcoordinate = get_coordinate\n\t\tif coordinate not in (\"left\", \"right\", \"middle\"):\n\t\t\traise ValueError\n\texcept ValueError:\n\t\tcoordinate = \"left\"\n\tvalue_list = hist.bins_to_list(make_value)\n\tdim = len(field_names)\n\tdimension = hist.dimension\n\tgraph = Graph(value_list, field_names, dimension, coordinate, scale)\n\treturn graph\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\timport numpy as np\n\tfrom . import graph\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.mean\n\n\th = hist.as_array()\n\tx, y = np.meshgrid(range(h.shape[1]), h.T, indexing=\"ij\")\n\tx, y = x.ravel(), y.ravel()\n\n\tgraph = graph.Graph(\n\t\tx=x,\n\t\ty=y,\n\t\tfield_names=field_names,\n\t\tscale=scale,\n\t)\n\n\tfor i, bin_ in enumerate(h):\n\t\tgraph.points.add(\n\t\t\t**{\n\t\t\t\tfield_names[0]: i,\n\t\t\t\tfield_names[1]: make_value(bin_),\n\t\t\t}\n\t\t)\n\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\timport numpy\n\n\tfrom .base import make, bin_tuple\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\n\tbin_tuple.check(hist)\n\n\tcoordinate = get_coordinate\n\tif coordinate == \"right\":\n\t\tcoordinate = \"left\"\n\telif coordinate == \"middle\":\n\t\tcoordinate = \"right\"\n\n\tx, y = numpy.array(hist).T\n\tif coordinate == \"left\":\n\t\tx = numpy.array(x)\n\telse:\n\t\tx = numpy.array(y)\n\n\ty = numpy.array(y)\n\n\tif scale is None:\n\t\tscale = hist.scale\n\n\treturn make(x, y, make_value, scale=scale, field_names=field_names)\n\n#pythia/histogram/bin.py", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .histogram import histogram\n\tfrom .graph import graph\n\n\thist = histogram(hist)\n\tgraph = graph()\n\n\tgraph.set_field_names(field_names)\n\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.mean\n\n\tdef get_value(bin_):\n\t\treturn (make_value(bin_),)\n\n\tif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.center\n\n\tfor bin_ in hist:\n\t\tgraph.append(get_value(bin_))\n\n\tif scale is not None:\n\t\tif scale is True:\n\t\t\tscale = hist.scale\n\t\tgraph.set_scale(scale)\n\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import graph\n\tfrom .bin import bin\n\timport operator\n\tif scale is None: scale = getattr(hist, \"scale\", True)\n\tgraph_ = graph()\n\tgraph_.set_x_field_names(field_names[0], field_names[1])\n\tif scale: graph_.set_y_scale(getattr(hist, \"scale\", True))\n\tcoordinate = getattr(hist, \"coordinate\", get_coordinate)\n\tbins = list(hist)\n\tbins.sort(key=operator.attrgetter(\"x\"))\n\tfor bin_ in bins:\n\t\tgraph_.add_point(bin_.x, make_value(bin_))\n\treturn graph_", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\thist_data = hist.histogram()\n\tgraph = graph.graph()\n\tgraph.set_scale(scale)\n\tfor i in range(len(field_names)):\n\t\tgraph.set_field(i, hist_data[i])\n\tif make_value is None:\n\t\tgraph.set_field(len(field_names), hist_data[-1])\n\telse:\n\t\tgraph.set_field(len(field_names), make_value(hist_data[-1]))\n\tgraph.set_coordinate(get_coordinate)\n\treturn graph", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t\"\"\"\n\tConvert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.\n\t\"\"\"\n\tfrom .graph import Graph\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.mean\n\telif callable(make_value):\n\t\tpass\n\telse:\n\t\traise TypeError(\"make_value must be callable or None\")\n\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: bin_.mid\n\telse:\n\t\traise ValueError(\"get_coordinate must be left, right or middle\")\n\n\tgraph = Graph(field_names=field_names)\n\tfor field_name in field_names:\n\t\tgraph.add_field_value(field_name, get_coordinate)\n\n\tfor bin_ in hist.bins:\n\t\tgraph.add_point(make_value(bin_))\n\n\tif scale is True:\n\t\tgraph.scale = hist.scale\n\n\treturn graph\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\timplementers = iface.providedBy(candidate)\n\tif not implementers:\n\t\traise Invalid(\n\t\t\t'%s does not provide %s' % (candidate, iface))\n\tif tentative:\n\t\treturn True\n\n\tmethods = iface.methods()\n\tfor name, method in methods.items():\n\t\tif not _verify(method, candidate, tentative=True):\n\t\t\traise Invalid(\n\t\t\t\t'%s.%s does not implement %s' % (\n\t\t\t\t\tcandidate.__class__.__name__,\n\t\t\t\t\tname,\n\t\t\t\t\tiface))\n\n\tattributes = iface.attributes()\n\tfor name, attr in attributes.items():\n\t\tif not _verify(attr, candidate, tentative=True):\n\t\t\traise Invalid(\n\t\t\t\t'%s does not have an attribute named %r' % (\n\t\t\t\t\tcandidate.__class__.__name__,\n\t\t\t\t\tname))\n\n\timplementers = iface.providedBy(candidate, vtype)\n\tif not implementers:\n\t\traise Invalid(\n\t\t\t'%s does not provide %s' % (candidate, iface))\n\tif tentative:\n\t\treturn True\n\n\tmethods = iface.methods(vtype)\n\tfor name, method in methods.items():\n\t\tif not _verify(", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Invalid\n\n\tif candidate.providedBy(iface):\n\t\treturn True\n\n\tif not tentative:\n\t\terrors = []\n\t\tfor method in candidate.providedBy(iface, False):\n\t\t\tif not method.verify(iface):\n\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\"%s.%s does not implement %s\" % (\n\t\t\t\t\t\tcandidate.__name__, method.__name__,\n\t\t\t\t\t\tiface\n\t\t\t\t\t),\n\t\t\t\t\tprovides=iface,\n\t\t\t\t))\n\t\tif not errors:\n\t\t\treturn True\n\t\traise errors\n\n\treturn False\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tret = True\n\ttry:\n\t\tif iface.providedBy(candidate) or candidate.providedBy(iface):\n\t\t\tret = _verify_methods(iface, candidate)\n\texcept zope.interface.Invalid:\n\t\tret = False\n\n\tif not ret:\n\t\traise\n\n\tif not tentative:\n\t\ttry:\n\t\t\tret = _verify_attributes(iface, candidate)\n\t\texcept zope.interface.Invalid:\n\t\t\tret = False\n\n\tif not ret:\n\t\traise\n\n\tif not tentative:\n\t\ttry:\n\t\t\tret = _verify_implementation(iface, candidate, vtype)\n\t\texcept zope.interface.Invalid:\n\t\t\tret = False\n\n\tif not ret:\n\t\traise\n\n\treturn ret\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tverifier = interfaces.verify(iface, candidate, vtype)\n\texcept interfaces.Invalid as err:\n\t\traise err\n\texcept interfaces.NotImplemented as err:\n\t\t# XXX: I'm not sure if this is desirable.\n\t\t# XXX: We don't want to use it when the candidate\n\t\t# doesn't provide the interface, because it's not\n\t\t# clear whether the candidate has any means of\n\t\t# providing said interface. We may consider\n\t\t# using a custom exception for this.\n\t\traise err\n\texcept interfaces.NotProvides as err:\n\t\t# XXX: I'm not sure if this is desirable.\n\t\t# XXX: We don't want to use it when the candidate\n\t\t# doesn't provide the interface, because it's not\n\t\t# clear whether the candidate has any means of\n\t\t# providing said interface. We may consider\n\t\t# using a custom exception for this.\n\t\traise err\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tprovides = iface.provides\n\timplements = iface.implements\n\tmethods = iface.getMethods()\n\tattributes = iface.getAttributes()\n\terrors = []\n\n\tif implements and candidate.provides not in implements:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Provides %r not implemented by %r\" %\n\t\t\t\t(candidate.provides, candidate)\n\t\t\t)\n\t\t)\n\n\tif provides and candidate.implements not in provides:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Implements %r not provided by %r\" %\n\t\t\t\t(candidate.implements, candidate)\n\t\t\t)\n\t\t)\n\n\tfor method in methods:\n\t\tif method.provides not in iface.provides:\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Method %r not provided by %r\" %\n\t\t\t\t\t(method.provides, candidate)\n\t\t\t\t)\n\t\t\t)\n\t\telif method.name != method.getMethodName():\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Method %r has an invalid name %r\" %\n\t\t\t\t\t(method.provides, candidate)\n\t\t\t\t)\n\t\t\t)\n\n\t\tif method.name != method.getMethodName():\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef check_method(name, method):\n\t\t\"\"\"\n\t\tVerify that *name* is a valid method on *iface*.\n\n\t\t:param str name: The method to verify\n\t\t:param callable method: A callable to verify\n\t\t:return: A valid method\n\t\t:raises zope.interface.Invalid: If *method* is not a valid method\n\t\t\"\"\"\n\t\tif not method:\n\t\t\traise Invalid(name, iface, 'No such method')\n\t\telif not method.implementation is iface:\n\t\t\traise Invalid(name, iface, 'Method %s is not implemented by %s' % (name, iface))\n\t\treturn method\n\n\tdef check_attr(name, method):\n\t\t\"\"\"\n\t\tVerify that *name* is a valid attribute on *iface*.\n\n\t\t:param str name: The attribute to verify\n\t\t:param callable method: A callable to verify\n\t\t:return: A valid attribute\n\t\t:raises zope.interface.Invalid: If *method* is not a valid attribute\n\t\t\"\"\"\n\t\tif not method:\n\t\t\traise Invalid(name, iface, 'No such attribute')\n\t\telif not method.implementation is iface:\n\t\t\traise Invalid(name,", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef verify_one_method(method):\n\t\tif hasattr(iface, method):\n\t\t\tprovided = iface.providedBy(candidate)\n\t\t\tif tentative or provided:\n\t\t\t\tvalid = isinstance(candidate, iface)\n\t\t\t\tif not valid:\n\t\t\t\t\traise Invalid(\n\t\t\t\t\t\t'%s does not implement %s' % (candidate, iface),\n\t\t\t\t\t\tprovided)\n\t\t\t\treturn valid\n\t\t\telse:\n\t\t\t\tvalid = True\n\t\t\t\ttry:\n\t\t\t\t\tvalid = iface.provides(candidate)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tpass\n\t\t\t\tif not valid or not iface.providedBy(candidate):\n\t\t\t\t\traise Invalid(\n\t\t\t\t\t\t'%s does not provide %s' % (candidate, iface),\n\t\t\t\t\t\tprovided)\n\t\t\t\treturn valid\n\t\telse:\n\t\t\traise Invalid(\n\t\t\t\t'%s does not provide %s' % (candidate, iface))\n\n\tdef verify_one_attribute(attr):\n\t\tif hasattr(iface, attr):\n\t\t\tprovided = iface.providedBy(candidate)\n\t\t\tif tentative or provided:\n\t\t\t\tvalid = isinstance(getattr(candidate, attr), iface)\n\t\t\t\tif not valid:\n\t\t\t\t\traise Invalid(\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef check_method(meth):\n\t\t\"\"\"\n\t\tVerify that a method is valid.\n\t\t\"\"\"\n\t\tmethod = getattr(iface, meth)\n\t\tif method is None:\n\t\t\treturn False\n\t\tmethod_name = getattr(method, '__name__', meth.__name__)\n\t\tif not isinstance(method_name, str):\n\t\t\t# Python < 2.7.9 is missing this attribute, and\n\t\t\t# PyPy 2.7 does not.\n\t\t\traise zope.interface.Invalid('%s does not have a name' % meth)\n\t\treturn True\n\n\tdef check_attribute(attr):\n\t\t\"\"\"\n\t\tVerify that an attribute is valid.\n\t\t\"\"\"\n\t\tattr_value = getattr(iface, attr)\n\t\tif attr_value is None:\n\t\t\treturn False\n\t\tattr_name = getattr(attr_value, '__name__', attr)\n\t\tif not isinstance(attr_name, str):\n\t\t\t# Python < 2.7.9 is missing this attribute, and\n\t\t\t# PyPy 2.7 does not.\n\t\t\traise zope.interface.Invalid('%s does not have a name' % attr)\n\t\treturn True\n\n\tdef check_class(cls):\n\t\t\"\"\"\n\t\tVerify", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\timplementers = getImplementers(iface, candidate)\n\t# make sure the candidate claims that it provides\n\t# the interface\n\tif not candidate.providedBy(iface):\n\t\traise Invalid(\n\t\t\t'%s does not provide %s' % (candidate.__name__, iface.__name__))\n\n\t# make sure that the candidate claims that it implements\n\t# the interface\n\tif not any(implementer.isAssigned() for implementer in implementers):\n\t\traise Invalid(\n\t\t\t'%s does not implement %s' % (candidate.__name__, iface.__name__))\n\n\t# make sure the candidate defines all the necessary methods\n\tmethods = iface.getMethods(candidate)\n\tif (not methods) and (not tentative):\n\t\traise Invalid('%s has no methods' % candidate.__name__)\n\tfor name, method in methods.items():\n\t\tif method.checker is not None:\n\t\t\tif isinstance(method.checker, Invalid):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'%s.%s has an invalid signature'\n\t\t\t\t\t% (candidate.__name__, name))\n\t\t\telif isinstance(method.checker, NotImplementedError):\n\t\t\t\traise NotImplementedError(\n\t\t\t\t\t'%s.%s has not been implemented'", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tprovides = iface.provides\n\timplements = iface.implements\n\n\tif not provides and not implements:\n\t\t# No provider and no implementer, it can't provide\n\t\treturn False\n\n\tif tentative and not candidate.provides(iface):\n\t\t# Tentative interface, but candidate doesn't provide it\n\t\treturn False\n\n\tmethods = []\n\tattributes = []\n\tfor method in iface.providedBy(candidate):\n\t\tif method.name not in methods:\n\t\t\tmethods.append(method.name)\n\n\tfor attribute in iface.providedBy(candidate):\n\t\tif attribute.name not in attributes:\n\t\t\tattributes.append(attribute.name)\n\n\tif not methods:\n\t\t# No methods, can't provide\n\t\treturn False\n\tif not attributes:\n\t\t# No attributes, can't provide\n\t\treturn False\n\n\tif vtype:\n\t\tfor method in iface.providedBy(candidate):\n\t\t\tif not isinstance(method, vtype):\n\t\t\t\treturn False\n\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\timplementers = iface.providedBy(candidate)\n\tif not implementers:\n\t\traise Invalid(\n\t\t\t'%s does not provide %s' % (candidate, iface))\n\tif tentative:\n\t\treturn True\n\n\tmethods = iface.methods()\n\tfor name, method in methods.items():\n\t\tif not _verify(method, candidate, tentative=True):\n\t\t\traise Invalid(\n\t\t\t\t'%s.%s does not implement %s' % (\n\t\t\t\t\tcandidate.__class__.__name__,\n\t\t\t\t\tname,\n\t\t\t\t\tiface))\n\n\tattributes = iface.attributes()\n\tfor name, attr in attributes.items():\n\t\tif not _verify(attr, candidate, tentative=True):\n\t\t\traise Invalid(\n\t\t\t\t'%s does not have an attribute named %r' % (\n\t\t\t\t\tcandidate.__class__.__name__,\n\t\t\t\t\tname))\n\n\timplementers = iface.providedBy(candidate, vtype)\n\tif not implementers:\n\t\traise Invalid(\n\t\t\t'%s does not provide %s' % (candidate, iface))\n\tif tentative:\n\t\treturn True\n\n\tmethods = iface.methods(vtype)\n\tfor name, method in methods.items():\n\t\tif not _verify(", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Invalid\n\n\tif candidate.providedBy(iface):\n\t\treturn True\n\n\tif not tentative:\n\t\terrors = []\n\t\tfor method in candidate.providedBy(iface, False):\n\t\t\tif not method.verify(iface):\n\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\"%s.%s does not implement %s\" % (\n\t\t\t\t\t\tcandidate.__name__, method.__name__,\n\t\t\t\t\t\tiface\n\t\t\t\t\t),\n\t\t\t\t\tprovides=iface,\n\t\t\t\t))\n\t\tif not errors:\n\t\t\treturn True\n\t\traise errors\n\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tret = True\n\ttry:\n\t\tif iface.providedBy(candidate) or candidate.providedBy(iface):\n\t\t\tret = _verify_methods(iface, candidate)\n\texcept zope.interface.Invalid:\n\t\tret = False\n\n\tif not ret:\n\t\traise\n\n\tif not tentative:\n\t\ttry:\n\t\t\tret = _verify_attributes(iface, candidate)\n\t\texcept zope.interface.Invalid:\n\t\t\tret = False\n\n\tif not ret:\n\t\traise\n\n\tif not tentative:\n\t\ttry:\n\t\t\tret = _verify_implementation(iface, candidate, vtype)\n\t\texcept zope.interface.Invalid:\n\t\t\tret = False\n\n\tif not ret:\n\t\traise\n\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tverifier = interfaces.verify(iface, candidate, vtype)\n\texcept interfaces.Invalid as err:\n\t\traise err\n\texcept interfaces.NotImplemented as err:\n\t\t# XXX: I'm not sure if this is desirable.\n\t\t# XXX: We don't want to use it when the candidate\n\t\t# doesn't provide the interface, because it's not\n\t\t# clear whether the candidate has any means of\n\t\t# providing said interface. We may consider\n\t\t# using a custom exception for this.\n\t\traise err\n\texcept interfaces.NotProvides as err:\n\t\t# XXX: I'm not sure if this is desirable.\n\t\t# XXX: We don't want to use it when the candidate\n\t\t# doesn't provide the interface, because it's not\n\t\t# clear whether the candidate has any means of\n\t\t# providing said interface. We may consider\n\t\t# using a custom exception for this.\n\t\traise err\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tprovides = iface.provides\n\timplements = iface.implements\n\tmethods = iface.getMethods()\n\tattributes = iface.getAttributes()\n\terrors = []\n\n\tif implements and candidate.provides not in implements:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Provides %r not implemented by %r\" %\n\t\t\t\t(candidate.provides, candidate)\n\t\t\t)\n\t\t)\n\n\tif provides and candidate.implements not in provides:\n\t\terrors.append(\n\t\t\tInvalid(\n\t\t\t\t\"Implements %r not provided by %r\" %\n\t\t\t\t(candidate.implements, candidate)\n\t\t\t)\n\t\t)\n\n\tfor method in methods:\n\t\tif method.provides not in iface.provides:\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Method %r not provided by %r\" %\n\t\t\t\t\t(method.provides, candidate)\n\t\t\t\t)\n\t\t\t)\n\t\telif method.name != method.getMethodName():\n\t\t\terrors.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"Method %r has an invalid name %r\" %\n\t\t\t\t\t(method.provides, candidate)\n\t\t\t\t)\n\t\t\t)\n\n\t\tif method.name != method.getMethodName():\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef check_method(name, method):\n\t\t\"\"\"\n\t\tVerify that *name* is a valid method on *iface*.\n\n\t\t:param str name: The method to verify\n\t\t:param callable method: A callable to verify\n\t\t:return: A valid method\n\t\t:raises zope.interface.Invalid: If *method* is not a valid method\n\t\t\"\"\"\n\t\tif not method:\n\t\t\traise Invalid(name, iface, 'No such method')\n\t\telif not method.implementation is iface:\n\t\t\traise Invalid(name, iface, 'Method %s is not implemented by %s' % (name, iface))\n\t\treturn method\n\n\tdef check_attr(name, method):\n\t\t\"\"\"\n\t\tVerify that *name* is a valid attribute on *iface*.\n\n\t\t:param str name: The attribute to verify\n\t\t:param callable method: A callable to verify\n\t\t:return: A valid attribute\n\t\t:raises zope.interface.Invalid: If *method* is not a valid attribute\n\t\t\"\"\"\n\t\tif not method:\n\t\t\traise Invalid(name, iface, 'No such attribute')\n\t\telif not method.implementation is iface:\n\t\t\traise Invalid(name,", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef verify_one_method(method):\n\t\tif hasattr(iface, method):\n\t\t\tprovided = iface.providedBy(candidate)\n\t\t\tif tentative or provided:\n\t\t\t\tvalid = isinstance(candidate, iface)\n\t\t\t\tif not valid:\n\t\t\t\t\traise Invalid(\n\t\t\t\t\t\t'%s does not implement %s' % (candidate, iface),\n\t\t\t\t\t\tprovided)\n\t\t\t\treturn valid\n\t\t\telse:\n\t\t\t\tvalid = True\n\t\t\t\ttry:\n\t\t\t\t\tvalid = iface.provides(candidate)\n\t\t\t\texcept AttributeError:\n\t\t\t\t\tpass\n\t\t\t\tif not valid or not iface.providedBy(candidate):\n\t\t\t\t\traise Invalid(\n\t\t\t\t\t\t'%s does not provide %s' % (candidate, iface),\n\t\t\t\t\t\tprovided)\n\t\t\t\treturn valid\n\t\telse:\n\t\t\traise Invalid(\n\t\t\t\t'%s does not provide %s' % (candidate, iface))\n\n\tdef verify_one_attribute(attr):\n\t\tif hasattr(iface, attr):\n\t\t\tprovided = iface.providedBy(candidate)\n\t\t\tif tentative or provided:\n\t\t\t\tvalid = isinstance(getattr(candidate, attr), iface)\n\t\t\t\tif not valid:\n\t\t\t\t\traise Invalid(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef check_method(meth):\n\t\t\"\"\"\n\t\tVerify that a method is valid.\n\t\t\"\"\"\n\t\tmethod = getattr(iface, meth)\n\t\tif method is None:\n\t\t\treturn False\n\t\tmethod_name = getattr(method, '__name__', meth.__name__)\n\t\tif not isinstance(method_name, str):\n\t\t\t# Python < 2.7.9 is missing this attribute, and\n\t\t\t# PyPy 2.7 does not.\n\t\t\traise zope.interface.Invalid('%s does not have a name' % meth)\n\t\treturn True\n\n\tdef check_attribute(attr):\n\t\t\"\"\"\n\t\tVerify that an attribute is valid.\n\t\t\"\"\"\n\t\tattr_value = getattr(iface, attr)\n\t\tif attr_value is None:\n\t\t\treturn False\n\t\tattr_name = getattr(attr_value, '__name__', attr)\n\t\tif not isinstance(attr_name, str):\n\t\t\t# Python < 2.7.9 is missing this attribute, and\n\t\t\t# PyPy 2.7 does not.\n\t\t\traise zope.interface.Invalid('%s does not have a name' % attr)\n\t\treturn True\n\n\tdef check_class(cls):\n\t\t\"\"\"\n\t\tVerify", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\timplementers = getImplementers(iface, candidate)\n\t# make sure the candidate claims that it provides\n\t# the interface\n\tif not candidate.providedBy(iface):\n\t\traise Invalid(\n\t\t\t'%s does not provide %s' % (candidate.__name__, iface.__name__))\n\n\t# make sure that the candidate claims that it implements\n\t# the interface\n\tif not any(implementer.isAssigned() for implementer in implementers):\n\t\traise Invalid(\n\t\t\t'%s does not implement %s' % (candidate.__name__, iface.__name__))\n\n\t# make sure the candidate defines all the necessary methods\n\tmethods = iface.getMethods(candidate)\n\tif (not methods) and (not tentative):\n\t\traise Invalid('%s has no methods' % candidate.__name__)\n\tfor name, method in methods.items():\n\t\tif method.checker is not None:\n\t\t\tif isinstance(method.checker, Invalid):\n\t\t\t\traise Invalid(\n\t\t\t\t\t'%s.%s has an invalid signature'\n\t\t\t\t\t% (candidate.__name__, name))\n\t\t\telif isinstance(method.checker, NotImplementedError):\n\t\t\t\traise NotImplementedError(\n\t\t\t\t\t'%s.%s has not been implemented'", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tprovides = iface.provides\n\timplements = iface.implements\n\n\tif not provides and not implements:\n\t\t# No provider and no implementer, it can't provide\n\t\treturn False\n\n\tif tentative and not candidate.provides(iface):\n\t\t# Tentative interface, but candidate doesn't provide it\n\t\treturn False\n\n\tmethods = []\n\tattributes = []\n\tfor method in iface.providedBy(candidate):\n\t\tif method.name not in methods:\n\t\t\tmethods.append(method.name)\n\n\tfor attribute in iface.providedBy(candidate):\n\t\tif attribute.name not in attributes:\n\t\t\tattributes.append(attribute.name)\n\n\tif not methods:\n\t\t# No methods, can't provide\n\t\treturn False\n\tif not attributes:\n\t\t# No attributes, can't provide\n\t\treturn False\n\n\tif vtype:\n\t\tfor method in iface.providedBy(candidate):\n\t\t\tif not isinstance(method, vtype):\n\t\t\t\treturn False\n\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tprovides = iface.providedBy(candidate)\n\texcept zope.interface.Invalid:\n\t\traise Invalid(iface, candidate)\n\tif not provides:\n\t\treturn False\n\tif tentative:\n\t\treturn True\n\t# This must be a real interface, so let's look for the\n\t# provider methods.\n\tfor name, method in candidate.__dict__.items():\n\t\tif not name.startswith('_'):\n\t\t\ttry:\n\t\t\t\tmethod.__is__ = iface\n\t\t\texcept AttributeError:\n\t\t\t\t# The method is not a valid method.\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tmethod.provides(iface)\n\t\t\texcept zope.interface.Invalid:\n\t\t\t\traise Invalid(iface, candidate, method)\n\treturn True\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef check(obj, interface, iface, tentative):\n\t\t\"\"\"\n\t\tMake sure the candidate can provide the interface.\n\t\t\"\"\"\n\t\tif tentative:\n\t\t\treturn True\n\t\tiface.providedBy(obj)\n\t\tfor name in iface.names():\n\t\t\tmethod = getattr(obj, name)\n\t\t\tiface.provides(method)\n\n\tdef checkAttributes(obj, iface):\n\t\t\"\"\"\n\t\tMake sure the candidate defines all the necessary attributes.\n\t\t\"\"\"\n\t\tfor name in iface.names():\n\t\t\ttry:\n\t\t\t\tgetattr(obj, name)\n\t\t\texcept AttributeError:\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"%s does not provide %s.%s\" % (obj, iface, name))\n\n\tdef checkMethods(obj, iface):\n\t\t\"\"\"\n\t\tMake sure the candidate defines all the necessary methods.\n\t\t\"\"\"\n\t\tfor method in iface.names():\n\t\t\tif not hasattr(obj, method):\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"%s does not provide %s.%s\" % (obj, iface, method))\n\n\t\t\tmethod = getattr(obj, method)\n\t\t\tinterfaceMethod = getattr(iface, method", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Invalid\n\tfrom zope.interface.verify import verifyClass\n\tfrom zope.interface.verify import verifyImplementer\n\tfrom zope.interface.verify import verifyProvider\n\n\timplementer = verifyImplementer(iface, candidate)\n\tprovider = verifyProvider(iface, candidate, tentative)\n\n\tverifyClass(iface, candidate)\n\n\tif not tentative:\n\t\tifaceMethods = set(iface.__dict__.keys())\n\t\tcandidateMethods = set(candidate.__dict__.keys())\n\t\tduplicateMethods = ifaceMethods & candidateMethods\n\t\tif duplicateMethods:\n\t\t\traise Invalid(\n\t\t\t\t'%s has %s duplicate methods: %s' % (\n\t\t\t\t\tcandidate.__name__,\n\t\t\t\t\tlen(duplicateMethods),\n\t\t\t\t\t', '.join(duplicateMethods)))\n\n\t\tconflictingMethods = ifaceMethods - candidateMethods\n\t\tif conflictingMethods:\n\t\t\traise Invalid(\n\t\t\t\t'%s has conflicting methods: %s' % (\n\t\t\t\t\tcandidate.__name__,\n\t\t\t\t\t', '.join(conflictingMethods)))\n\n\tif hasattr(candidate, iface.getName()):\n\t\t# Check that the candidate defines the interface methods.\n\t\tcandidateMethods = set(candidate", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tif iface not in candidate.providedBy(candidate):\n\t\t\treturn False\n\t\tfor method in iface:\n\t\t\tif method not in candidate:\n\t\t\t\treturn False\n\t\t\tif not method.check(candidate):\n\t\t\t\treturn False\n\t\tfor attr in iface:\n\t\t\tif not candidate.hasAttribute(attr):\n\t\t\t\treturn False\n\t\treturn True\n\texcept (zope.interface.Invalid, zope.interface.NoSuchMethodError):\n\t\traise Invalid(iface, candidate)\n\n# Copyright (c) 2015-2016 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef checkAttr(name, value):\n\t\ttry:\n\t\t\tvalue.__name__\n\t\texcept AttributeError:\n\t\t\traise zope.interface.Invalid(name, \"not a valid attribute\")\n\n\tdef checkMethod(name, value):\n\t\ttry:\n\t\t\tvalue.__name__\n\t\texcept AttributeError:\n\t\t\traise zope.interface.Invalid(name, \"not a valid method\")\n\n\tif not candidate.providedBy(iface):\n\t\traise zope.interface.Invalid(iface, \"does not provide interface\")\n\n\tmethods = [x for x in iface.interfaces if x != iface]\n\tmethods.extend(iface.interfaces)\n\tfor name in iface.interfaces:\n\t\ttry:\n\t\t\tfor method in methods:\n\t\t\t\tif not method.providedBy(iface):\n\t\t\t\t\traise zope.interface.Invalid(name, \"does not provide interface\")\n\t\texcept zope.interface.Invalid:\n\t\t\traise\n\n\tfor name, value in iface.methods.items():\n\t\ttry:\n\t\t\tcheckMethod(name, value)\n\t\texcept zope.interface.Invalid:\n\t\t\traise\n\n\tfor name, value in iface.attributes.items():\n\t\ttry:\n\t\t\tcheck", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfailures = []\n\n\tif not candidate.providedBy(iface):\n\t\tfailures.append(\n\t\t\tInvalid(\n\t\t\t\t'%s does not provide %s' % (candidate, iface),\n\t\t\t\tcandidate=candidate,\n\t\t\t\tinterface=iface,\n\t\t\t\t))\n\n\tif any(not m.implements(iface) for m in candidate.__dict__.values()):\n\t\tfailures.append(\n\t\t\tInvalid(\n\t\t\t\t'%s does not declare that it implements %s' % (candidate, iface),\n\t\t\t\tcandidate=candidate,\n\t\t\t\tinterface=iface,\n\t\t\t\t))\n\n\tif any(not m.provides(iface) for m in candidate.__dict__.values()):\n\t\tfailures.append(\n\t\t\tInvalid(\n\t\t\t\t'%s does not declare that it provides %s' % (candidate, iface),\n\t\t\t\tcandidate=candidate,\n\t\t\t\tinterface=iface,\n\t\t\t\t))\n\n\tfor method in candidate.__dict__.values():\n\t\tif not hasattr(method, '__call__'):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tmethod.__call__()\n\t\texcept Invalid:\n\t\t\tfailures.append(Invalid(\n\t\t\t\t'%s.%s does not comply", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef doit(method):\n\t\tif not method.providedBy(iface):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s does not provide %r' % (candidate, iface))\n\t\timplementer = iface.implementer(candidate)\n\t\tif implementer is not None:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s does not implement %r' % (candidate, implementer))\n\t\tproviders = iface.providers(candidate)\n\t\tif providers is not None:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s provides multiple instances of %r' % (\n\t\t\t\t\tcandidate, providers))\n\t\tattrs = iface.attributes(candidate)\n\t\tif attrs is not None:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s provides multiple attributes' % candidate)\n\t\tif not hasattr(candidate, method.name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s does not provide %s' % (candidate, method.name))\n\t\tmissing = method.missing(candidate)\n\t\tif missing:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s provides %s but it is not required by %s' % (", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Invalid\n\tfrom zope.interface.interfaces import IImplementerProvider\n\n\tiface = iface.providedBy(candidate)\n\tif not iface:\n\t\tif tentative:\n\t\t\treturn True\n\n\t\traise Invalid(\"{0} does not provide {1}\".format(candidate, iface))\n\n\tif not iface.implements(iface):\n\t\traise Invalid(\"{0} does not implement {1}\".format(candidate, iface))\n\n\tiface = iface.providedBy(candidate)\n\tif iface.provides(iface):\n\t\traise Invalid(\"{0} does not provide {1}\".format(candidate, iface))\n\n\tfrom zope.interface.common.mapping import getMethods, getAttributes\n\n\tmethods = getMethods(iface)\n\tattrs = getAttributes(iface)\n\n\tfor method in methods:\n\t\tverifyMethod(iface, candidate, method)\n\n\tfor name, attr, value in attrs:\n\t\tverifyAttribute(iface, candidate, name, attr, value)\n\n\treturn True\n", "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tiface.providedBy(candidate)\n\texcept zope.interface.Invalid:\n\t\traise\n\n\tfor name, method in iteritems(iface):\n\t\ttry:\n\t\t\tcandidate.provides(method)\n\t\texcept zope.interface.Invalid:\n\t\t\traise\n\n\t\tif not method.implementedBy(candidate):\n\t\t\traise\n\n\t\ttry:\n\t\t\tsignature = inspect.signature(method)\n\t\texcept TypeError:\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tcandidate.validate(signature)\n\t\texcept ValueError:\n\t\t\traise\n\n\t\ttry:\n\t\t\tattributes = method.attributes\n\t\texcept AttributeError:\n\t\t\tcontinue\n\n\t\tfor name, value in iteritems(attributes):\n\t\t\ttry:\n\t\t\t\tcandidate.validate(name, value)\n\t\t\texcept ValueError:\n\t\t\t\traise\n\n\treturn True"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tprovides = iface.providedBy(candidate)\n\texcept zope.interface.Invalid:\n\t\traise Invalid(iface, candidate)\n\tif not provides:\n\t\treturn False\n\tif tentative:\n\t\treturn True\n\t# This must be a real interface, so let's look for the\n\t# provider methods.\n\tfor name, method in candidate.__dict__.items():\n\t\tif not name.startswith('_'):\n\t\t\ttry:\n\t\t\t\tmethod.__is__ = iface\n\t\t\texcept AttributeError:\n\t\t\t\t# The method is not a valid method.\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tmethod.provides(iface)\n\t\t\texcept zope.interface.Invalid:\n\t\t\t\traise Invalid(iface, candidate, method)\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef check(obj, interface, iface, tentative):\n\t\t\"\"\"\n\t\tMake sure the candidate can provide the interface.\n\t\t\"\"\"\n\t\tif tentative:\n\t\t\treturn True\n\t\tiface.providedBy(obj)\n\t\tfor name in iface.names():\n\t\t\tmethod = getattr(obj, name)\n\t\t\tiface.provides(method)\n\n\tdef checkAttributes(obj, iface):\n\t\t\"\"\"\n\t\tMake sure the candidate defines all the necessary attributes.\n\t\t\"\"\"\n\t\tfor name in iface.names():\n\t\t\ttry:\n\t\t\t\tgetattr(obj, name)\n\t\t\texcept AttributeError:\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"%s does not provide %s.%s\" % (obj, iface, name))\n\n\tdef checkMethods(obj, iface):\n\t\t\"\"\"\n\t\tMake sure the candidate defines all the necessary methods.\n\t\t\"\"\"\n\t\tfor method in iface.names():\n\t\t\tif not hasattr(obj, method):\n\t\t\t\traise zope.interface.Invalid(\n\t\t\t\t\t\"%s does not provide %s.%s\" % (obj, iface, method))\n\n\t\t\tmethod = getattr(obj, method)\n\t\t\tinterfaceMethod = getattr(iface, method", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tpass", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Invalid\n\tfrom zope.interface.verify import verifyClass\n\tfrom zope.interface.verify import verifyImplementer\n\tfrom zope.interface.verify import verifyProvider\n\n\timplementer = verifyImplementer(iface, candidate)\n\tprovider = verifyProvider(iface, candidate, tentative)\n\n\tverifyClass(iface, candidate)\n\n\tif not tentative:\n\t\tifaceMethods = set(iface.__dict__.keys())\n\t\tcandidateMethods = set(candidate.__dict__.keys())\n\t\tduplicateMethods = ifaceMethods & candidateMethods\n\t\tif duplicateMethods:\n\t\t\traise Invalid(\n\t\t\t\t'%s has %s duplicate methods: %s' % (\n\t\t\t\t\tcandidate.__name__,\n\t\t\t\t\tlen(duplicateMethods),\n\t\t\t\t\t', '.join(duplicateMethods)))\n\n\t\tconflictingMethods = ifaceMethods - candidateMethods\n\t\tif conflictingMethods:\n\t\t\traise Invalid(\n\t\t\t\t'%s has conflicting methods: %s' % (\n\t\t\t\t\tcandidate.__name__,\n\t\t\t\t\t', '.join(conflictingMethods)))\n\n\tif hasattr(candidate, iface.getName()):\n\t\t# Check that the candidate defines the interface methods.\n\t\tcandidateMethods = set(candidate", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tif iface not in candidate.providedBy(candidate):\n\t\t\treturn False\n\t\tfor method in iface:\n\t\t\tif method not in candidate:\n\t\t\t\treturn False\n\t\t\tif not method.check(candidate):\n\t\t\t\treturn False\n\t\tfor attr in iface:\n\t\t\tif not candidate.hasAttribute(attr):\n\t\t\t\treturn False\n\t\treturn True\n\texcept (zope.interface.Invalid, zope.interface.NoSuchMethodError):\n\t\traise Invalid(iface, candidate)\n\n# Copyright (c) 2015-2016 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef checkAttr(name, value):\n\t\ttry:\n\t\t\tvalue.__name__\n\t\texcept AttributeError:\n\t\t\traise zope.interface.Invalid(name, \"not a valid attribute\")\n\n\tdef checkMethod(name, value):\n\t\ttry:\n\t\t\tvalue.__name__\n\t\texcept AttributeError:\n\t\t\traise zope.interface.Invalid(name, \"not a valid method\")\n\n\tif not candidate.providedBy(iface):\n\t\traise zope.interface.Invalid(iface, \"does not provide interface\")\n\n\tmethods = [x for x in iface.interfaces if x != iface]\n\tmethods.extend(iface.interfaces)\n\tfor name in iface.interfaces:\n\t\ttry:\n\t\t\tfor method in methods:\n\t\t\t\tif not method.providedBy(iface):\n\t\t\t\t\traise zope.interface.Invalid(name, \"does not provide interface\")\n\t\texcept zope.interface.Invalid:\n\t\t\traise\n\n\tfor name, value in iface.methods.items():\n\t\ttry:\n\t\t\tcheckMethod(name, value)\n\t\texcept zope.interface.Invalid:\n\t\t\traise\n\n\tfor name, value in iface.attributes.items():\n\t\ttry:\n\t\t\tcheck", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfailures = []\n\n\tif not candidate.providedBy(iface):\n\t\tfailures.append(\n\t\t\tInvalid(\n\t\t\t\t'%s does not provide %s' % (candidate, iface),\n\t\t\t\tcandidate=candidate,\n\t\t\t\tinterface=iface,\n\t\t\t\t))\n\n\tif any(not m.implements(iface) for m in candidate.__dict__.values()):\n\t\tfailures.append(\n\t\t\tInvalid(\n\t\t\t\t'%s does not declare that it implements %s' % (candidate, iface),\n\t\t\t\tcandidate=candidate,\n\t\t\t\tinterface=iface,\n\t\t\t\t))\n\n\tif any(not m.provides(iface) for m in candidate.__dict__.values()):\n\t\tfailures.append(\n\t\t\tInvalid(\n\t\t\t\t'%s does not declare that it provides %s' % (candidate, iface),\n\t\t\t\tcandidate=candidate,\n\t\t\t\tinterface=iface,\n\t\t\t\t))\n\n\tfor method in candidate.__dict__.values():\n\t\tif not hasattr(method, '__call__'):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tmethod.__call__()\n\t\texcept Invalid:\n\t\t\tfailures.append(Invalid(\n\t\t\t\t'%s.%s does not comply", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tdef doit(method):\n\t\tif not method.providedBy(iface):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s does not provide %r' % (candidate, iface))\n\t\timplementer = iface.implementer(candidate)\n\t\tif implementer is not None:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s does not implement %r' % (candidate, implementer))\n\t\tproviders = iface.providers(candidate)\n\t\tif providers is not None:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s provides multiple instances of %r' % (\n\t\t\t\t\tcandidate, providers))\n\t\tattrs = iface.attributes(candidate)\n\t\tif attrs is not None:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s provides multiple attributes' % candidate)\n\t\tif not hasattr(candidate, method.name):\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s does not provide %s' % (candidate, method.name))\n\t\tmissing = method.missing(candidate)\n\t\tif missing:\n\t\t\traise zope.interface.Invalid(\n\t\t\t\t'%s provides %s but it is not required by %s' % (", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\tfrom zope.interface import Invalid\n\tfrom zope.interface.interfaces import IImplementerProvider\n\n\tiface = iface.providedBy(candidate)\n\tif not iface:\n\t\tif tentative:\n\t\t\treturn True\n\n\t\traise Invalid(\"{0} does not provide {1}\".format(candidate, iface))\n\n\tif not iface.implements(iface):\n\t\traise Invalid(\"{0} does not implement {1}\".format(candidate, iface))\n\n\tiface = iface.providedBy(candidate)\n\tif iface.provides(iface):\n\t\traise Invalid(\"{0} does not provide {1}\".format(candidate, iface))\n\n\tfrom zope.interface.common.mapping import getMethods, getAttributes\n\n\tmethods = getMethods(iface)\n\tattrs = getAttributes(iface)\n\n\tfor method in methods:\n\t\tverifyMethod(iface, candidate, method)\n\n\tfor name, attr, value in attrs:\n\t\tverifyAttribute(iface, candidate, name, attr, value)\n\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.\n\t\"\"\"\n\ttry:\n\t\tiface.providedBy(candidate)\n\texcept zope.interface.Invalid:\n\t\traise\n\n\tfor name, method in iteritems(iface):\n\t\ttry:\n\t\t\tcandidate.provides(method)\n\t\texcept zope.interface.Invalid:\n\t\t\traise\n\n\t\tif not method.implementedBy(candidate):\n\t\t\traise\n\n\t\ttry:\n\t\t\tsignature = inspect.signature(method)\n\t\texcept TypeError:\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tcandidate.validate(signature)\n\t\texcept ValueError:\n\t\t\traise\n\n\t\ttry:\n\t\t\tattributes = method.attributes\n\t\texcept AttributeError:\n\t\t\tcontinue\n\n\t\tfor name, value in iteritems(attributes):\n\t\t\ttry:\n\t\t\t\tcandidate.validate(name, value)\n\t\t\texcept ValueError:\n\t\t\t\traise\n\n\treturn True", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom os import remove, path\n\tfrom subprocess import Popen, PIPE\n\tfrom sys import exit\n\n\tif tentative:\n\t\tprint \"Tentative class verification: \" + candidate\n\n\ttry:\n\t\t# Verify that the class can be instantiated\n\t\ttry:\n\t\t\tcandidate.name\n\t\texcept AttributeError:\n\t\t\tprint \"Could not instantiate class %s\" % candidate\n\t\t\treturn False\n\n\t\t# Verify that the class has a *name* attribute\n\t\tif not hasattr(candidate, 'name'):\n\t\t\tprint \"Class %s is missing a 'name' attribute\" % candidate\n\t\t\treturn False\n\n\t\t# Verify that the class has a *run* method\n\t\tif not hasattr(candidate, 'run'):\n\t\t\tprint \"Class %s is missing a 'run' method\" % candidate\n\t\t\treturn False\n\n\t\t# Verify that it can be run\n\t\ttry:\n\t\t\tcandidate.run()\n\t\texcept Exception, e:\n\t\t\tprint \"Failed to run %s: %s\" % (candidate.name, e)\n\t\t\treturn False\n\n\t\t# Verify that the class has a *dependencies* attribute\n\t\tif not hasattr(candidate, 'dependencies'):\n\t\t\tprint \"Class %s is missing", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tret = False\n\tif candidate is not None:\n\t\tret = candidate.provides(iface)\n\t\tif ret:\n\t\t\tret = ret.query(iface)\n\t\tif not ret:\n\t\t\tret = candidate.query(iface)\n\t\tif not ret:\n\t\t\tret = candidate.provides(iface)\n\tif ret and not tentative:\n\t\tret = False\n\treturn ret\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\timport os\n\timport subprocess\n\tfrom lib.util import printError, printOk, printWarning\n\t\n\tdef runProbe(cmd):\n\t\t\"\"\"\n\t\tRun the *cmd* as a subprocess.\n\t\t\"\"\"\n\t\tresult = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\tstdout, stderr = result.communicate()\n\t\tresult.wait()\n\t\tif result.returncode != 0:\n\t\t\tprintError(\"Error running: \" + cmd)\n\t\t\tprintError(stderr)\n\t\t\treturn False\n\t\treturn True\n\n\tdef run(cmd):\n\t\t\"\"\"\n\t\tRun the *cmd* as a subprocess.\n\t\t\"\"\"\n\t\tresult = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\tstdout, stderr = result.communicate()\n\t\tresult.wait()\n\t\tif result.returncode != 0:\n\t\t\tprintError(\"Error running: \" + cmd)\n\t\t\tprintError(stderr)\n\t\t\treturn False\n\t\treturn True\n\t\t\n\t# Run the verification procedure as a subprocess.\n\t\n\t# The actual verification procedure.\n\tprobe = \"ifconfig -a", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tdef verify(iface, candidate):\n\t\t\"\"\" Verify that *candidate* provides *iface *\"\"\"\n\t\tif not candidate.provides(iface):\n\t\t\traise Exception(\"{} does not provide {}\".format(candidate, iface))\n\t\tif candidate.getCapability(\"{}.{}\".format(iface, 'service.name')) is None:\n\t\t\traise Exception(\"{} does not provide {}\".format(candidate, iface))\n\t\tif candidate.getCapability(\"{}.{}\".format(iface, 'service.type')) is None:\n\t\t\traise Exception(\"{} does not provide {}\".format(candidate, iface))\n\t\treturn candidate\n\t\n\tif tentative:\n\t\ttry:\n\t\t\treturn verify(iface, candidate)\n\t\texcept:\n\t\t\treturn None\n\telse:\n\t\treturn verify(iface, candidate)", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprint \"Verifying interface: \" + candidate + \" against: \" + iface\n\tif not tentative:\n\t\tprint \"Not tentative\"\n\t\tresult = raw_input(\"Is this correct? (y/N) \")\n\t\tif not result.lower() in (\"y\", \"yes\"):\n\t\t\treturn False\n\t(status, output) = ifconfig(candidate)\n\tif output != \"\":\n\t\tprint \"Got output: \" + output\n\tprint \"Got status: \" + str(status)\n\tif status == 0:\n\t\treturn True\n\telse:\n\t\treturn False\n\nif __name__ == '__main__':\n\tfrom sys import argv\n\tif len(argv) == 2:\n\t\tprint \"Verifying \" + argv[1]\n\t\tverifyClass(argv[1], argv[1])\n\telse:\n\t\tprint \"You need to supply an interface name.\"", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tfrom . import Interface\n\tfrom . import InterfaceError\n\t\n\tif candidate is None:\n\t\traise InterfaceError(\"no interface candidate\")\n\t\n\ttry:\n\t\tinterface = Interface(candidate)\n\t\n\texcept InterfaceError:\n\t\traise InterfaceError(\"interface candidate does not appear to be a network interface\")\n\t\n\texcept ValueError:\n\t\traise InterfaceError(\"interface candidate appears to be a device name\")\n\t\n\texcept OSError:\n\t\traise InterfaceError(\"interface candidate appears to be a device name, but appears not to be a valid network interface\")\n\t\n\tif tentative:\n\t\tif interface.isValid():\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\t\n\telse:\n\t\tif interface.isValid(iface):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ttry:\n\t\tclassProvider = iface.providedBy(candidate)\n\texcept AttributeError:\n\t\tclassProvider = False\n\texcept TypeError:\n\t\tclassProvider = False\n\texcept ValueError:\n\t\tclassProvider = False\n\t\n\tif tentative:\n\t\treturn classProvider\n\telse:\n\t\treturn classProvider and candidate is not None\n", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprint \"Verifying %s\" % candidate\n\ttry:\n\t\timport importlib\n\t\tmodule = importlib.import_module(candidate)\n\t\tiface = getattr(module, iface)\n\t\tprint \"Found %s\" % module\n\texcept ImportError:\n\t\tprint \"Could not import %s\" % candidate\n\t\tif tentative:\n\t\t\tprint \"Trying again...\"\n\t\t\treturn verifyClass(iface, candidate, tentative=False)\n\t\telse:\n\t\t\tprint \"Did not find %s\" % candidate\n\t\t\treturn False\n\treturn True\n\nif __name__ == \"__main__\":\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--iface\", default=\"Sensor\", help=\"interface to verify\")\n\tparser.add_argument(\"--candidate\", default=\"sensors\", help=\"module to verify\")\n\targs = parser.parse_args()\n\tverifyClass(args.iface, args.candidate)", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprint(\"Verifying candidate:\", candidate)\n\tpy = ctypes.cdll.LoadLibrary(candidate)\n\tpy.verify.argtypes = [ctypes.c_void_p, ctypes.c_char_p]\n\tpy.verify.restype = ctypes.c_int\n\tif py.verify(iface, candidate) == 0:\n\t\tif tentative:\n\t\t\tprint(\"Tentative verification successful.\")\n\t\telse:\n\t\t\tprint(\"Verification successful.\")\n\telse:\n\t\tprint(\"Verification failed.\")\n\n\tpy.free.argtypes = [ctypes.c_void_p]\n\tpy.free(py)\n\nif __name__ == '__main__':\n\timport sys\n\tverifyClass(sys.argv[1], sys.argv[2])", "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tpass\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tfrom os import remove, path\n\tfrom subprocess import Popen, PIPE\n\tfrom sys import exit\n\n\tif tentative:\n\t\tprint \"Tentative class verification: \" + candidate\n\n\ttry:\n\t\t# Verify that the class can be instantiated\n\t\ttry:\n\t\t\tcandidate.name\n\t\texcept AttributeError:\n\t\t\tprint \"Could not instantiate class %s\" % candidate\n\t\t\treturn False\n\n\t\t# Verify that the class has a *name* attribute\n\t\tif not hasattr(candidate, 'name'):\n\t\t\tprint \"Class %s is missing a 'name' attribute\" % candidate\n\t\t\treturn False\n\n\t\t# Verify that the class has a *run* method\n\t\tif not hasattr(candidate, 'run'):\n\t\t\tprint \"Class %s is missing a 'run' method\" % candidate\n\t\t\treturn False\n\n\t\t# Verify that it can be run\n\t\ttry:\n\t\t\tcandidate.run()\n\t\texcept Exception, e:\n\t\t\tprint \"Failed to run %s: %s\" % (candidate.name, e)\n\t\t\treturn False\n\n\t\t# Verify that the class has a *dependencies* attribute\n\t\tif not hasattr(candidate, 'dependencies'):\n\t\t\tprint \"Class %s is missing", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tret = False\n\tif candidate is not None:\n\t\tret = candidate.provides(iface)\n\t\tif ret:\n\t\t\tret = ret.query(iface)\n\t\tif not ret:\n\t\t\tret = candidate.query(iface)\n\t\tif not ret:\n\t\t\tret = candidate.provides(iface)\n\tif ret and not tentative:\n\t\tret = False\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\timport os\n\timport subprocess\n\tfrom lib.util import printError, printOk, printWarning\n\t\n\tdef runProbe(cmd):\n\t\t\"\"\"\n\t\tRun the *cmd* as a subprocess.\n\t\t\"\"\"\n\t\tresult = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\tstdout, stderr = result.communicate()\n\t\tresult.wait()\n\t\tif result.returncode != 0:\n\t\t\tprintError(\"Error running: \" + cmd)\n\t\t\tprintError(stderr)\n\t\t\treturn False\n\t\treturn True\n\n\tdef run(cmd):\n\t\t\"\"\"\n\t\tRun the *cmd* as a subprocess.\n\t\t\"\"\"\n\t\tresult = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\tstdout, stderr = result.communicate()\n\t\tresult.wait()\n\t\tif result.returncode != 0:\n\t\t\tprintError(\"Error running: \" + cmd)\n\t\t\tprintError(stderr)\n\t\t\treturn False\n\t\treturn True\n\t\t\n\t# Run the verification procedure as a subprocess.\n\t\n\t# The actual verification procedure.\n\tprobe = \"ifconfig -a", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tdef verify(iface, candidate):\n\t\t\"\"\" Verify that *candidate* provides *iface *\"\"\"\n\t\tif not candidate.provides(iface):\n\t\t\traise Exception(\"{} does not provide {}\".format(candidate, iface))\n\t\tif candidate.getCapability(\"{}.{}\".format(iface, 'service.name')) is None:\n\t\t\traise Exception(\"{} does not provide {}\".format(candidate, iface))\n\t\tif candidate.getCapability(\"{}.{}\".format(iface, 'service.type')) is None:\n\t\t\traise Exception(\"{} does not provide {}\".format(candidate, iface))\n\t\treturn candidate\n\t\n\tif tentative:\n\t\ttry:\n\t\t\treturn verify(iface, candidate)\n\t\texcept:\n\t\t\treturn None\n\telse:\n\t\treturn verify(iface, candidate)", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprint \"Verifying interface: \" + candidate + \" against: \" + iface\n\tif not tentative:\n\t\tprint \"Not tentative\"\n\t\tresult = raw_input(\"Is this correct? (y/N) \")\n\t\tif not result.lower() in (\"y\", \"yes\"):\n\t\t\treturn False\n\t(status, output) = ifconfig(candidate)\n\tif output != \"\":\n\t\tprint \"Got output: \" + output\n\tprint \"Got status: \" + str(status)\n\tif status == 0:\n\t\treturn True\n\telse:\n\t\treturn False\n\nif __name__ == '__main__':\n\tfrom sys import argv\n\tif len(argv) == 2:\n\t\tprint \"Verifying \" + argv[1]\n\t\tverifyClass(argv[1], argv[1])\n\telse:\n\t\tprint \"You need to supply an interface name.\"", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\t\n\tfrom . import Interface\n\tfrom . import InterfaceError\n\t\n\tif candidate is None:\n\t\traise InterfaceError(\"no interface candidate\")\n\t\n\ttry:\n\t\tinterface = Interface(candidate)\n\t\n\texcept InterfaceError:\n\t\traise InterfaceError(\"interface candidate does not appear to be a network interface\")\n\t\n\texcept ValueError:\n\t\traise InterfaceError(\"interface candidate appears to be a device name\")\n\t\n\texcept OSError:\n\t\traise InterfaceError(\"interface candidate appears to be a device name, but appears not to be a valid network interface\")\n\t\n\tif tentative:\n\t\tif interface.isValid():\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\t\n\telse:\n\t\tif interface.isValid(iface):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\ttry:\n\t\tclassProvider = iface.providedBy(candidate)\n\texcept AttributeError:\n\t\tclassProvider = False\n\texcept TypeError:\n\t\tclassProvider = False\n\texcept ValueError:\n\t\tclassProvider = False\n\t\n\tif tentative:\n\t\treturn classProvider\n\telse:\n\t\treturn classProvider and candidate is not None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprint \"Verifying %s\" % candidate\n\ttry:\n\t\timport importlib\n\t\tmodule = importlib.import_module(candidate)\n\t\tiface = getattr(module, iface)\n\t\tprint \"Found %s\" % module\n\texcept ImportError:\n\t\tprint \"Could not import %s\" % candidate\n\t\tif tentative:\n\t\t\tprint \"Trying again...\"\n\t\t\treturn verifyClass(iface, candidate, tentative=False)\n\t\telse:\n\t\t\tprint \"Did not find %s\" % candidate\n\t\t\treturn False\n\treturn True\n\nif __name__ == \"__main__\":\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--iface\", default=\"Sensor\", help=\"interface to verify\")\n\tparser.add_argument(\"--candidate\", default=\"sensors\", help=\"module to verify\")\n\targs = parser.parse_args()\n\tverifyClass(args.iface, args.candidate)", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tprint(\"Verifying candidate:\", candidate)\n\tpy = ctypes.cdll.LoadLibrary(candidate)\n\tpy.verify.argtypes = [ctypes.c_void_p, ctypes.c_char_p]\n\tpy.verify.restype = ctypes.c_int\n\tif py.verify(iface, candidate) == 0:\n\t\tif tentative:\n\t\t\tprint(\"Tentative verification successful.\")\n\t\telse:\n\t\t\tprint(\"Verification successful.\")\n\telse:\n\t\tprint(\"Verification failed.\")\n\n\tpy.free.argtypes = [ctypes.c_void_p]\n\tpy.free(py)\n\nif __name__ == '__main__':\n\timport sys\n\tverifyClass(sys.argv[1], sys.argv[2])", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t\"\"\"\n\tVerify that the *candidate* might correctly provide *iface*.\n\t\"\"\"\n\tpass\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = explicit_mc or _determineMetaclassFromBases(bases)\n\tif not metaclass:\n\t\traise TypeError(\"No metaclass provided for module %s\" % __name__)\n\treturn metaclass\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = None\n\tfor base in bases:\n\t\tif base == object:\n\t\t\tmc = base\n\t\t\tbreak\n\t\telif issubclass(base, metaclass=explicit_mc):\n\t\t\tmc = base\n\t\t\tbreak\n\t\telse:\n\t\t\tmc = base.__metaclass__\n\treturn mc\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tclass_ = None\n\twhile not class_ and bases:\n\t\tclass_ = bases.pop()\n\n\tif explicit_mc:\n\t\tclass_ = explicit_mc\n\n\treturn class_\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t\t\n\tfor base in bases:\n\t\tif base is not None:\n\t\t\treturn base.__metaclass__\n\t\t\t\n\traise RuntimeError(\"No metaclass found, neither explicit nor bases specified\")\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = None\n\tfor base in bases:\n\t\tif base is object:\n\t\t\tif explicit_mc is None:\n\t\t\t\tmc = type\n\t\t\t\tbreak\n\t\t\telif explicit_mc is not None:\n\t\t\t\tmc = explicit_mc\n\t\t\t\tbreak\n\telse:\n\t\tif explicit_mc is None:\n\t\t\tmc = type\n\t\telif explicit_mc is not None:\n\t\t\tmc = explicit_mc\n\treturn mc\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tclass_name = '__Meta__'\n\tmc = None\n\tmc_name = None\n\n\twhile bases:\n\t\tbase = bases.pop()\n\t\tif hasattr(base, class_name) and getattr(base, class_name) is not None:\n\t\t\tmc = getattr(base, class_name)\n\t\t\tif mc is not None:\n\t\t\t\tmc_name = mc.__name__\n\t\t\t\tbreak\n\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\t\tmc_name = explicit_mc.__name__\n\n\tif mc is None:\n\t\traise TypeError(\n\t\t\t\"__metaclass__ found in %s, but no metaclass found\" % ', '.join(map(str, bases))\n\t\t)\n\n\tbases.insert(0, mc)\n\tmc_name = mc.__name__\n\n\treturn mc_name, mc\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = bases[0] if bases else object\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\treturn mc\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tmc = type\n\t\n\tif explicit_mc:\n\t\tmc = explicit_mc\n\n\tfor base in bases:\n\t\tif isinstance(base, type):\n\t\t\tif base.__name__ == 'type':\n\t\t\t\traise RuntimeError('Cyclic metaclass inheritance detected')\n\t\t\telif base.mro().index(mc) != 0:\n\t\t\t\t# If the metaclass is not on the top of the mro, it\n\t\t\t\t# is a subclass of a metaclass, so we're done\n\t\t\t\treturn base\n\t\t\telif base.__metaclass__ == mc:\n\t\t\t\t# If a metaclass is explicitly specified, we're done\n\t\t\t\treturn base\n\t\t\telif base.__metaclass__ is None:\n\t\t\t\t# If a metaclass is explicitly not specified, we're\n\t\t\t\t# done\n\t\t\t\treturn mc\n\t\t\telse:\n\t\t\t\t# If a metaclass is explicitly specified, and it's not\n\t\t\t\t# the metaclass itself, we'll recurse\n\t\t\t\treturn determineMetaclass(bases, base.__metaclass__)\n\t\telif isinstance(base, type):\n\t\t\t# If a base is a type, we'll recurse\n\t\t\treturn determineMetaclass(bases, base)\n\t\telif isinstance(base", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tself.args = args\n\t\tself.kwargs = kwargs\n\tdef __new__(cls, *args, **kwargs):\n\t\tif explicit_mc:\n\t\t\tself = super(cls, explicit_mc).__new__(explicit_mc, *args, **kwargs)\n\t\telse:\n\t\t\tself = super(cls, cls).__new__(cls, *args, **kwargs)\n\t\treturn self\n\n\tclass Meta:\n\t\t__bases__ = bases\n\t\t__call__ = __new__\n\n\treturn Meta\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\t# Return explicit metaclass if specified\n\tif explicit_mc:\n\t\treturn explicit_mc\n\t\n\t# Return first metaclass if no explicit metaclass is specified\n\ttry:\n\t\treturn bases[0]\n\texcept TypeError:\n\t\treturn None\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmetaclass = explicit_mc or _determineMetaclassFromBases(bases)\n\tif not metaclass:\n\t\traise TypeError(\"No metaclass provided for module %s\" % __name__)\n\treturn metaclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = None\n\tfor base in bases:\n\t\tif base == object:\n\t\t\tmc = base\n\t\t\tbreak\n\t\telif issubclass(base, metaclass=explicit_mc):\n\t\t\tmc = base\n\t\t\tbreak\n\t\telse:\n\t\t\tmc = base.__metaclass__\n\treturn mc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tclass_ = None\n\twhile not class_ and bases:\n\t\tclass_ = bases.pop()\n\n\tif explicit_mc:\n\t\tclass_ = explicit_mc\n\n\treturn class_\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t\t\n\tfor base in bases:\n\t\tif base is not None:\n\t\t\treturn base.__metaclass__\n\t\t\t\n\traise RuntimeError(\"No metaclass found, neither explicit nor bases specified\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = None\n\tfor base in bases:\n\t\tif base is object:\n\t\t\tif explicit_mc is None:\n\t\t\t\tmc = type\n\t\t\t\tbreak\n\t\t\telif explicit_mc is not None:\n\t\t\t\tmc = explicit_mc\n\t\t\t\tbreak\n\telse:\n\t\tif explicit_mc is None:\n\t\t\tmc = type\n\t\telif explicit_mc is not None:\n\t\t\tmc = explicit_mc\n\treturn mc\n", "is_pass": true, "return_code": 0}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tclass_name = '__Meta__'\n\tmc = None\n\tmc_name = None\n\n\twhile bases:\n\t\tbase = bases.pop()\n\t\tif hasattr(base, class_name) and getattr(base, class_name) is not None:\n\t\t\tmc = getattr(base, class_name)\n\t\t\tif mc is not None:\n\t\t\t\tmc_name = mc.__name__\n\t\t\t\tbreak\n\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\t\tmc_name = explicit_mc.__name__\n\n\tif mc is None:\n\t\traise TypeError(\n\t\t\t\"__metaclass__ found in %s, but no metaclass found\" % ', '.join(map(str, bases))\n\t\t)\n\n\tbases.insert(0, mc)\n\tmc_name = mc.__name__\n\n\treturn mc_name, mc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tmc = bases[0] if bases else object\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\treturn mc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\tmc = type\n\t\n\tif explicit_mc:\n\t\tmc = explicit_mc\n\n\tfor base in bases:\n\t\tif isinstance(base, type):\n\t\t\tif base.__name__ == 'type':\n\t\t\t\traise RuntimeError('Cyclic metaclass inheritance detected')\n\t\t\telif base.mro().index(mc) != 0:\n\t\t\t\t# If the metaclass is not on the top of the mro, it\n\t\t\t\t# is a subclass of a metaclass, so we're done\n\t\t\t\treturn base\n\t\t\telif base.__metaclass__ == mc:\n\t\t\t\t# If a metaclass is explicitly specified, we're done\n\t\t\t\treturn base\n\t\t\telif base.__metaclass__ is None:\n\t\t\t\t# If a metaclass is explicitly not specified, we're\n\t\t\t\t# done\n\t\t\t\treturn mc\n\t\t\telse:\n\t\t\t\t# If a metaclass is explicitly specified, and it's not\n\t\t\t\t# the metaclass itself, we'll recurse\n\t\t\t\treturn determineMetaclass(bases, base.__metaclass__)\n\t\telif isinstance(base, type):\n\t\t\t# If a base is a type, we'll recurse\n\t\t\treturn determineMetaclass(bases, base)\n\t\telif isinstance(base", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\tdef __init__(self, *args, **kwargs):\n\t\tself.args = args\n\t\tself.kwargs = kwargs\n\tdef __new__(cls, *args, **kwargs):\n\t\tif explicit_mc:\n\t\t\tself = super(cls, explicit_mc).__new__(explicit_mc, *args, **kwargs)\n\t\telse:\n\t\t\tself = super(cls, cls).__new__(cls, *args, **kwargs)\n\t\treturn self\n\n\tclass Meta:\n\t\t__bases__ = bases\n\t\t__call__ = __new__\n\n\treturn Meta\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t\"\"\"\n\tDetermine metaclass from 1+ bases and optional explicit __metaclass__\n\t\"\"\"\n\t\n\t# Return explicit metaclass if specified\n\tif explicit_mc:\n\t\treturn explicit_mc\n\t\n\t# Return first metaclass if no explicit metaclass is specified\n\ttry:\n\t\treturn bases[0]\n\texcept TypeError:\n\t\treturn None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tself.pop(key, default)\n\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.popitem()[1]\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n\n# 4.7.3.2", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n\n\n\n# D.setdefault(k[,d]) -> d.  Return the value of the item with the given key.\n# If the key does not exist: insert the key with the specified value\n# and return the new value.", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key, default)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn value\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\treturn value\n\n# 2.", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.popitem()[1]\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\n# 10.6.4.3\nfrom collections import OrderedDict\n\n# 10.6.4.4\nfrom collections import deque\n\n# 10.6.4.5\nfrom collections import Counter\n\n# 10.6.4.6\nfrom collections import defaultdict\n\n# 10.6.4.7\nfrom collections import namedtuple\n\n# 10.6.4.8\nfrom collections import ChainMap\n\n# 10.6.4.9\nfrom collections import Mapping\n\n# 10.6.4.10\nfrom collections import UserDict\n\n# 10.6.4.11\nfrom collections import UserList\n\n# 10.6.4.12\nfrom collections import UserString\n\n# 10.6.4.13\nfrom collections import UserDict\n\n# 10.6.4.14\nfrom collections import UserList\n\n# 10.6.4.15\nfrom collections import UserString\n\n# 10.6.4.16\nfrom collections import de", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tself.storage.pop(key, default)\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tdef pop_impl(self, key, default):\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\treturn pop_impl(self, key, default)\n", "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key)\n\texcept KeyError as e:\n\t\tif default is __marker:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tself.pop(key, default)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.popitem()[1]\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n\n# 4.7.3.2", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n\n\n\n# D.setdefault(k[,d]) -> d.  Return the value of the item with the given key.\n# If the key does not exist: insert the key with the specified value\n# and return the new value.", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key, default)\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn value\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\treturn value\n\n# 2.", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.popitem()[1]\n\texcept KeyError:\n\t\tif default is __marker:\n\t\t\traise\n\t\treturn default\n\n# 10.6.4.3\nfrom collections import OrderedDict\n\n# 10.6.4.4\nfrom collections import deque\n\n# 10.6.4.5\nfrom collections import Counter\n\n# 10.6.4.6\nfrom collections import defaultdict\n\n# 10.6.4.7\nfrom collections import namedtuple\n\n# 10.6.4.8\nfrom collections import ChainMap\n\n# 10.6.4.9\nfrom collections import Mapping\n\n# 10.6.4.10\nfrom collections import UserDict\n\n# 10.6.4.11\nfrom collections import UserList\n\n# 10.6.4.12\nfrom collections import UserString\n\n# 10.6.4.13\nfrom collections import UserDict\n\n# 10.6.4.14\nfrom collections import UserList\n\n# 10.6.4.15\nfrom collections import UserString\n\n# 10.6.4.16\nfrom collections import de", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tself.storage.pop(key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\tdef pop_impl(self, key, default):\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\treturn pop_impl(self, key, default)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\t\"\"\"\n\tD.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.\n\t\"\"\"\n\ttry:\n\t\treturn self.pop(key)\n\texcept KeyError as e:\n\t\tif default is __marker:\n\t\t\traise\n\t\telse:\n\t\t\treturn default\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile self.items:\n\t\titem = self.items.popitem()\n\t\tif not item[1].pop():\n\t\t\tdel item[0]\n\t\telse:\n\t\t\tself.items[item] = item[1]\n\treturn item\n\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\th = self.hits\n\tif not h:\n\t\traise KeyError(\"dictionary is empty\")\n\t\n\tmin_key, min_value = None, float(\"-inf\")\n\t\n\tfor key, value in h.items():\n\t\tif value > min_value:\n\t\t\tmin_value = value\n\t\t\tmin_key = key\n\t\n\tdel h[min_key]\n\treturn min_key, min_value\n\n#src/lib/utils.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\titem = self[-1]\n\tself.remove(item)\n\treturn item", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\n\t\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\td = self._data\n\th = d.hash\n\tif not h:\n\t\traise KeyError('dictionary is empty')\n\telif not h.table:\n\t\traise KeyError('dictionary is empty')\n\telif not h.table[0]:\n\t\traise KeyError('dictionary is empty')\n\tkey, value = h.table[0]\n\th.table[0] = None\n\tdel d[key]\n\th.update_frequency(key)\n\treturn key, value\n\n#src/python/collections/__init__.py\nfrom .deque import deque\nfrom .defaultdict import defaultdict\nfrom .ordereddict import OrderedDict\nfrom .counter import Counter\nfrom .namedtuple import namedtuple\nfrom .counter import Counter\nfrom .namedtuple import namedtuple\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tself.remove(self.most_frequent)\n\treturn self.most_frequent\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\ttry:\n\t\tkey, value, freq = self.get_min()\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\telse:\n\t\tdel self.items[key]\n\t\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t...\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tpass", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t...\n\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\twhile self.items:\n\t\titem = self.items.popitem()\n\t\tif not item[1].pop():\n\t\t\tdel item[0]\n\t\telse:\n\t\t\tself.items[item] = item[1]\n\treturn item\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\th = self.hits\n\tif not h:\n\t\traise KeyError(\"dictionary is empty\")\n\t\n\tmin_key, min_value = None, float(\"-inf\")\n\t\n\tfor key, value in h.items():\n\t\tif value > min_value:\n\t\t\tmin_value = value\n\t\t\tmin_key = key\n\t\n\tdel h[min_key]\n\treturn min_key, min_value\n\n#src/lib/utils.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\titem = self[-1]\n\tself.remove(item)\n\treturn item", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t\n\t\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\td = self._data\n\th = d.hash\n\tif not h:\n\t\traise KeyError('dictionary is empty')\n\telif not h.table:\n\t\traise KeyError('dictionary is empty')\n\telif not h.table[0]:\n\t\traise KeyError('dictionary is empty')\n\tkey, value = h.table[0]\n\th.table[0] = None\n\tdel d[key]\n\th.update_frequency(key)\n\treturn key, value\n\n#src/python/collections/__init__.py\nfrom .deque import deque\nfrom .defaultdict import defaultdict\nfrom .ordereddict import OrderedDict\nfrom .counter import Counter\nfrom .namedtuple import namedtuple\nfrom .counter import Counter\nfrom .namedtuple import namedtuple\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .counter import Counter\nfrom .", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tself.remove(self.most_frequent)\n\treturn self.most_frequent\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\ttry:\n\t\tkey, value, freq = self.get_min()\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\telse:\n\t\tdel self.items[key]\n\t\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least frequently used.\n\t\"\"\"\n\t...\n\t", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.most_recently_used.popitem()\n\tself.recently_used[key] = value\n\tdel self.recently_used[key]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tlast = self.items[-1]\n\tdel self.items[-1]\n\tself.keys.pop()\n\treturn last\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself.heap.pop()\n\tkey, value = self.heap.pop(), self.values.pop()\n\treturn (key, value)", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.storage.popitem()\n\treturn (key, value)\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\titem = self.most_recent.popitem()\n\tself.access_count[item[0]] -= 1\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\twhile self.length > 0:\n\t\tlast = self.lru_list[-1]\n\t\tself.lru_list.remove(last)\n\t\tdel self.lru_values[last]\n\t\tself.length -= 1\n\treturn last\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.most_recently_used.popitem()\n\tself.recently_used[key] = value\n\tdel self.recently_used[key]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tlast = self.items[-1]\n\tdel self.items[-1]\n\tself.keys.pop()\n\treturn last\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tself.heap.pop()\n\tkey, value = self.heap.pop(), self.values.pop()\n\treturn (key, value)", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tkey, value = self.storage.popitem()\n\treturn (key, value)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\titem = self.most_recent.popitem()\n\tself.access_count[item[0]] -= 1\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tRemove and return the `(key, value)` pair least recently used.\n\t\"\"\"\n\twhile self.length > 0:\n\t\tlast = self.lru_list[-1]\n\t\tself.lru_list.remove(last)\n\t\tdel self.lru_values[last]\n\t\tself.length -= 1\n\treturn last\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tlastitem = self.items.pop()\n\tdel self.keys[lastitem[0]]\n\treturn lastitem\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\titem = self.tail.prev\n\tself._remove(item)\n\treturn (item.key, item.val)\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\"\"\"\n\tReturns:\n\t\tA (key, value) pair.  Raises KeyError if the list is empty.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError(\"pop from empty list\")\n\t\n\titem = self[-1]\n\tdel self[-1]\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\titem = self._last()\n\tif item is not None:\n\t\tself._remove(item[0])\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tself.move_to_end(self.end)\n\tself.remove(self.end)\n\treturn self.popitem()", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\titem = self.queue.pop()\n\tself.size -= 1\n\treturn item\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tl = len(self)\n\tif l == 0:\n\t\traise KeyError('pop from an empty stack')\n\tkey, value = self[-1]\n\tdel self[-1]\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tself._check_not_empty()\n\tkey, value = self._remove_last()\n\tself._len -= 1\n\treturn key, value\n", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\timport heapq\n\t\n\twhile True:\n\t\ttry:\n\t\t\treturn self.popitem(last=False)\n\t\texcept IndexError:\n\t\t\tbreak\n\n\tdef pop(self, last=True):\n\t\t\"\"\"\n\t\tRemove and return the (key, value) pair most recently used.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.popitem(last=last)\n\t\texcept IndexError:\n\t\t\traise KeyError('dictionary is empty')\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet the value for the given key.\n\t\t\"\"\"\n\t\timport heapq\n\t\tself._rebalance()\n\t\theapq.heappush(self._heap, (self._counter[key], key, value))\n\t\tself._counter[key] += 1\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemove the item with the given key.\n\t\t\"\"\"\n\t\timport heapq\n\t\tself._rebalance()\n\t\ttry:\n\t\t\theapq.heappop(self._heap)\n\t\texcept IndexError:\n\t\t\traise KeyError('key not in dictionary')\n\t\tdel self._counter[key]\n\t\n\n\tdef __str__(self", "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tself._check_stack()\n\titem = self._items[-1]\n\tdel self._items[-1]\n\tself._keys_to_items[item[0]] = item[1]\n\treturn item\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tlastitem = self.items.pop()\n\tdel self.keys[lastitem[0]]\n\treturn lastitem\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\titem = self.tail.prev\n\tself._remove(item)\n\treturn (item.key, item.val)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\t\"\"\"\n\tReturns:\n\t\tA (key, value) pair.  Raises KeyError if the list is empty.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError(\"pop from empty list\")\n\t\n\titem = self[-1]\n\tdel self[-1]\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\titem = self._last()\n\tif item is not None:\n\t\tself._remove(item[0])\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tself.move_to_end(self.end)\n\tself.remove(self.end)\n\treturn self.popitem()", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\titem = self.queue.pop()\n\tself.size -= 1\n\treturn item\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tl = len(self)\n\tif l == 0:\n\t\traise KeyError('pop from an empty stack')\n\tkey, value = self[-1]\n\tdel self[-1]\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tself._check_not_empty()\n\tkey, value = self._remove_last()\n\tself._len -= 1\n\treturn key, value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\timport heapq\n\t\n\twhile True:\n\t\ttry:\n\t\t\treturn self.popitem(last=False)\n\t\texcept IndexError:\n\t\t\tbreak\n\n\tdef pop(self, last=True):\n\t\t\"\"\"\n\t\tRemove and return the (key, value) pair most recently used.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.popitem(last=last)\n\t\texcept IndexError:\n\t\t\traise KeyError('dictionary is empty')\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet the value for the given key.\n\t\t\"\"\"\n\t\timport heapq\n\t\tself._rebalance()\n\t\theapq.heappush(self._heap, (self._counter[key], key, value))\n\t\tself._counter[key] += 1\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemove the item with the given key.\n\t\t\"\"\"\n\t\timport heapq\n\t\tself._rebalance()\n\t\ttry:\n\t\t\theapq.heappop(self._heap)\n\t\texcept IndexError:\n\t\t\traise KeyError('key not in dictionary')\n\t\tdel self._counter[key]\n\t\n\n\tdef __str__(self", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\t\"Remove and return the (key, value) pair most recently used.\n\t\"\"\"\n\tself._check_stack()\n\titem = self._items[-1]\n\tdel self._items[-1]\n\tself._keys_to_items[item[0]] = item[1]\n\treturn item\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\ttry:\n\t\tpopped = self._data.popitem()\n\texcept KeyError:\n\t\traise KeyError(\"pop from an empty stack\")\n\texcept IndexError:\n\t\traise IndexError(\"pop from an empty stack\")\n\t\n\tself._size -= 1\n\tself._modified = True\n\t\n\treturn popped\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tself._pop()\n\tkey, value = self.popitem()\n\treturn key, value\n\n\n# Insertion sort\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ti = self._rand.randrange(0, len(self._data))\n\tk, v = self._data[i]\n\tdel self._data[i]\n\treturn k, v\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkeys = [x for x in self.keys()]\n\tif not keys:\n\t\traise KeyError('set is empty')\n\tkey = choice(keys)\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey, value = self.__choice()\n\tself.remove(key)\n\treturn key, value\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf there is no `(key, value)` pairs, return `None`\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf there is only one `(key, value)` pair, return that pair\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf there are more than one `(key, value)` pairs, remove and return a random `(key, value)` pair\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf the `(key, value)` pair was removed, return the deleted `(key, value)` pair\n\t\"\"\"\n\n# Problem:\n\n# 1. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 2. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 3. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 4. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 5. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 6. Write a function that takes in a `dict` and returns a `dict", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\twhile True:\n\t\ttry:\n\t\t\trandom_key = random.choice(self.keys())\n\t\t\trandom_value = random.choice(self[random_key])\n\t\t\tself[random_key].remove(random_value)\n\t\t\tself.pop(random_key)\n\t\t\treturn random_key, random_value\n\t\texcept ValueError:\n\t\t\tcontinue", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\tdef __choice(self):\n\t\t\"\"\"\n\t\tSelect a random key, value pair from the dictionary\n\t\t\"\"\"\n\t\tkeys = list(self.keys())\n\t\tvalue = keys[random.randint(0, len(keys) - 1)]\n\t\tdel self[value]\n\t\treturn (value, self[value])\n\n\tkey, value = self.__choice()\n\treturn (key, value)", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\timport random\n\t\n\tkeys = list(self.keys())\n\t\n\tif len(keys) == 0:\n\t\traise KeyError('pop from empty dictionary')\n\t\n\ti = random.randrange(0, len(keys))\n\t\n\tkey = keys[i]\n\t\n\tself.remove(key)\n\t\n\treturn key, self[key]\n\n", "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ttry:\n\t\ti = random.randint(0, len(self) - 1)\n\t\tval = list(self.items())[i]\n\t\tdel self[val[0]]\n\t\treturn val\n\texcept Exception:\n\t\traise KeyError('dictionary is empty')"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\ttry:\n\t\tpopped = self._data.popitem()\n\texcept KeyError:\n\t\traise KeyError(\"pop from an empty stack\")\n\texcept IndexError:\n\t\traise IndexError(\"pop from an empty stack\")\n\t\n\tself._size -= 1\n\tself._modified = True\n\t\n\treturn popped\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tself._pop()\n\tkey, value = self.popitem()\n\treturn key, value\n\n\n# Insertion sort\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ti = self._rand.randrange(0, len(self._data))\n\tk, v = self._data[i]\n\tdel self._data[i]\n\treturn k, v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkeys = [x for x in self.keys()]\n\tif not keys:\n\t\traise KeyError('set is empty')\n\tkey = choice(keys)\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\tkey, value = self.__choice()\n\tself.remove(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf there is no `(key, value)` pairs, return `None`\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf there is only one `(key, value)` pair, return that pair\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf there are more than one `(key, value)` pairs, remove and return a random `(key, value)` pair\n\t\"\"\"\n\t\n\t\"\"\"\n\tIf the `(key, value)` pair was removed, return the deleted `(key, value)` pair\n\t\"\"\"\n\n# Problem:\n\n# 1. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 2. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 3. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 4. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 5. Write a function that takes in a `dict` and returns a `dict` with all values set to `None`\n\n# 6. Write a function that takes in a `dict` and returns a `dict", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\twhile True:\n\t\ttry:\n\t\t\trandom_key = random.choice(self.keys())\n\t\t\trandom_value = random.choice(self[random_key])\n\t\t\tself[random_key].remove(random_value)\n\t\t\tself.pop(random_key)\n\t\t\treturn random_key, random_value\n\t\texcept ValueError:\n\t\t\tcontinue", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\t\n\tdef __choice(self):\n\t\t\"\"\"\n\t\tSelect a random key, value pair from the dictionary\n\t\t\"\"\"\n\t\tkeys = list(self.keys())\n\t\tvalue = keys[random.randint(0, len(keys) - 1)]\n\t\tdel self[value]\n\t\treturn (value, self[value])\n\n\tkey, value = self.__choice()\n\treturn (key, value)", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\timport random\n\t\n\tkeys = list(self.keys())\n\t\n\tif len(keys) == 0:\n\t\traise KeyError('pop from empty dictionary')\n\t\n\ti = random.randrange(0, len(keys))\n\t\n\tkey = keys[i]\n\t\n\tself.remove(key)\n\t\n\treturn key, self[key]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\"\"\"\n\tFind, remove and return a random `(key, value)` pair via __choice in the class\n\t\"\"\"\n\ttry:\n\t\ti = random.randint(0, len(self) - 1)\n\t\tval = list(self.items())[i]\n\t\tdel self[val[0]]\n\t\treturn val\n\texcept Exception:\n\t\traise KeyError('dictionary is empty')", "is_pass": true, "return_code": 0}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\tin_re = self.in_re\n\n\tif self.in_re_type == 'regex':\n\t\tin_re = self.in_re\n\telif self.in_re_type == 'str':\n\t\tin_re = re.compile(r'^' + self.in_re + '$', flags=re.IGNORECASE)\n\telif self.in_re_type == 'list':\n\t\tin_re = re.compile(r'^' + '|'.join(self.in_re) + '$', flags=re.IGNORECASE)\n\telse:\n\t\traise ValueError('Unknown in-style parameter regex type: ' + self.in_re_type)\n\n\treturn in_re\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpattern = r\"^\\s*(?P<in>((?:[^\\s]|[^\\s]+)(?:[^\\s]|[^\\s]+)*))\"\n\treturn Pattern(pattern)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tre_in = '(?:%s)' % '|'.join(self.param_names)\n\treturn re.compile(re_in, re.I)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = self.in_regex\n\tif not in_regex:\n\t\tin_regex = re.compile(r'^\\s*in\\s+([^:\\s]+)\\s*$')\n\t\treturn in_regex\n\telse:\n\t\treturn in_regex\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tself.in_pattern = re.compile(\n\t\tr\"^(?:in\\s+)?(?:[^\\s=]+)(?:\\s*=\\s*(?:(?:[^\\s=]+)|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = self._in_regex\n\treturn in_regex\n\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = r\"([^\\s]+) in \\{([^\\}]+)\\}\"\n\treturn re.compile(in_regex)\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpattern = self._in_pattern_template.format(\n\t\tself._pattern_type,\n\t\tself._pattern_params,\n\t\tself._pattern_flags,\n\t)\n\treturn re.compile(pattern, self._pattern_flags)\n\n", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tr = r'^\\s*in\\s+'\n\tr = r'^\\s*in\\s+(\\S+)\\s*$'\n\treturn re.compile(r, re.MULTILINE)\n\n# ============================================================================", "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r'(?:in )?'\n\tif self.in_value:\n\t\tregex += self.in_value\n\telif self.in_values:\n\t\tregex += f'({\"|\".join(self.in_values)})'\n\treturn re.compile(regex)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\t\n\tin_re = self.in_re\n\n\tif self.in_re_type == 'regex':\n\t\tin_re = self.in_re\n\telif self.in_re_type == 'str':\n\t\tin_re = re.compile(r'^' + self.in_re + '$', flags=re.IGNORECASE)\n\telif self.in_re_type == 'list':\n\t\tin_re = re.compile(r'^' + '|'.join(self.in_re) + '$', flags=re.IGNORECASE)\n\telse:\n\t\traise ValueError('Unknown in-style parameter regex type: ' + self.in_re_type)\n\n\treturn in_re\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpattern = r\"^\\s*(?P<in>((?:[^\\s]|[^\\s]+)(?:[^\\s]|[^\\s]+)*))\"\n\treturn Pattern(pattern)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tre_in = '(?:%s)' % '|'.join(self.param_names)\n\treturn re.compile(re_in, re.I)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = self.in_regex\n\tif not in_regex:\n\t\tin_regex = re.compile(r'^\\s*in\\s+([^:\\s]+)\\s*$')\n\t\treturn in_regex\n\telse:\n\t\treturn in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tself.in_pattern = re.compile(\n\t\tr\"^(?:in\\s+)?(?:[^\\s=]+)(?:\\s*=\\s*(?:(?:[^\\s=]+)|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]+|(?:\"\n\t\t\"(?:'[^']*'|\\\"[^\\\"]*\\\")|\\((?:[^()]", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = self._in_regex\n\treturn in_regex\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tin_regex = r\"([^\\s]+) in \\{([^\\}]+)\\}\"\n\treturn re.compile(in_regex)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tpattern = self._in_pattern_template.format(\n\t\tself._pattern_type,\n\t\tself._pattern_params,\n\t\tself._pattern_flags,\n\t)\n\treturn re.compile(pattern, self._pattern_flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tr = r'^\\s*in\\s+'\n\tr = r'^\\s*in\\s+(\\S+)\\s*$'\n\treturn re.compile(r, re.MULTILINE)\n\n# ============================================================================", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\"\"\"\n\tregex = r'(?:in )?'\n\tif self.in_value:\n\t\tregex += self.in_value\n\telif self.in_values:\n\t\tregex += f'({\"|\".join(self.in_values)})'\n\treturn re.compile(regex)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(self)\n\treturn converter\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tstyle = self._styler.style()\n\treturn _converting.Converter(self._styler, style)\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself._converter = _converting._Converter(\n\t\tself._param_style, self._param_style_name_list,\n\t\tself._param_style_name_map, self._param_style_list,\n\t\tself._param_style_map)\n\treturn self._converter\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(\n\t\tself._get_parameter_style_name(),\n\t\tself._get_parameter_style_type()\n\t)\n\tself._add_converter(converter)\n\treturn converter\n\n#src/pyqtgraph/parametertree/converters/_converter.py\n# -*- coding: utf-8 -*-\n# Copyright (c) 2015-2023 PyQt6 development team and the QGLViewer development team.\n# (see the `project trace <https://github.com/cs01/pyqtgraph/wiki/project-trace>`_ for\n# further information).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting.Converter(self)\n\treturn converter\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.log.debug('%s.%s', self.__class__.__name__, self._create_converter.__name__)\n\tconverter = self._create_converter()\n\treturn converter\n\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.converter = _converting._Converter(self.parameter_style)\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(self._base_style.converter)\n\treturn converter", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself._converter = _converting._Converter(self.style)\n\treturn self._converter\n", "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.logger.info(\"Creating parameter style converter\")\n\treturn _converting._Converter(self)"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(self)\n\treturn converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tstyle = self._styler.style()\n\treturn _converting.Converter(self._styler, style)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself._converter = _converting._Converter(\n\t\tself._param_style, self._param_style_name_list,\n\t\tself._param_style_name_map, self._param_style_list,\n\t\tself._param_style_map)\n\treturn self._converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(\n\t\tself._get_parameter_style_name(),\n\t\tself._get_parameter_style_type()\n\t)\n\tself._add_converter(converter)\n\treturn converter\n\n#src/pyqtgraph/parametertree/converters/_converter.py\n# -*- coding: utf-8 -*-\n# Copyright (c) 2015-2023 PyQt6 development team and the QGLViewer development team.\n# (see the `project trace <https://github.com/cs01/pyqtgraph/wiki/project-trace>`_ for\n# further information).\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting.Converter(self)\n\treturn converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.log.debug('%s.%s', self.__class__.__name__, self._create_converter.__name__)\n\tconverter = self._create_converter()\n\treturn converter\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.converter = _converting._Converter(self.parameter_style)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tconverter = _converting._Converter(self._base_style.converter)\n\treturn converter", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself._converter = _converting._Converter(self.style)\n\treturn self._converter\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\t\"\"\"\n\tCreate the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).\n\t\"\"\"\n\tself.logger.info(\"Creating parameter style converter\")\n\treturn _converting._Converter(self)", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tclass _DateTimeParseError(Exception):\n\t\tpass\n\n\tdef _parse_datetime_string(self, dt_str):\n\t\t# Re-implemented parsing algorithm from Python (see\n\t\t# https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior).\n\t\ttry:\n\t\t\tdt_str = dt_str.strip()\n\t\texcept AttributeError:\n\t\t\traise _DateTimeParseError(\"Invalid date string\")\n\t\tif not dt_str:\n\t\t\traise _DateTimeParseError(\"Empty date string\")\n\t\tif len(dt_str) == 19:\n\t\t\tif dt_str[4] == dt_str[7] == '-':\n\t\t\t\tdt_str = dt_str[:4] + '0' + dt_str[5:7] + '0' + dt_str[8:10]\n\t\t\telif dt_str[4] == dt_str[6] == '-':\n\t\t\t\tdt_str = dt_str[:4] + '0' + dt_str[5:7] + dt_str[8:10]\n\t\t\telse:\n\t\t\t\traise _DateTimeParseError(\"Invalid date string\")\n\t\telif len(dt_str) == 16:\n\t\t\tif dt_", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\treturn self._parse_iso8601(dt_str)\n\texcept (ValueError, OverflowError):\n\t\traise ValueError('%s is not a valid ISO-8601 datetime string' % dt_str)\n\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt_str = dt_str.strip()\n\n\tif dt_str == '':\n\t\treturn None\n\n\t# 1. Parse the date portion\n\tdate_str = self.parse_date(dt_str)\n\tif date_str is None:\n\t\treturn None\n\n\tdt = self.parse_date_str(date_str)\n\tif dt is None:\n\t\treturn None\n\n\t# 2. Parse the time portion\n\tdt_str = dt_str.replace(date_str, '')\n\tdt_str = dt_str.strip()\n\n\tif dt_str == '':\n\t\treturn dt\n\n\ttry:\n\t\ttime_str = self.parse_time(dt_str)\n\t\tif time_str is None:\n\t\t\treturn None\n\texcept ValueError:\n\t\ttime_str = None\n\n\t# 3. Handle the time portion\n\tif time_str is not None:\n\t\tdt = dt.replace(time=time_str)\n\telse:\n\t\tdt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n\n\treturn dt\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt_str = str(dt_str)\n\n\ttry:\n\t\tdt = parse(dt_str)\n\texcept ValueError:\n\t\traise ValueError(\"Unable to parse date\")\n\n\treturn dt\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tformat_string = self.FORMAT\n\tif not dt_str:\n\t\treturn None\n\n\tdate_str, time_str = dt_str.split('T')\n\n\ttz = self.tz\n\ttzinfo = tz or None\n\n\ttry:\n\t\tdate = self.parse(date_str)\n\texcept ValueError:\n\t\t# No time portion\n\t\tif time_str:\n\t\t\traise ValueError(\"No time portion in ISO 8601 datetime string\")\n\t\tformat_string = format_string.replace('T', ' ')\n\n\ttry:\n\t\ttime = self.parse(time_str)\n\texcept ValueError:\n\t\t# No date portion\n\t\tif date_str:\n\t\t\traise ValueError(\"No date portion in ISO 8601 datetime string\")\n\t\tformat_string = format_string.replace(' ', 'T')\n\n\ttry:\n\t\tif tz:\n\t\t\ttzinfo = tz\n\t\telif time_str:\n\t\t\t# Time portion is not specified, but date portion is\n\t\t\ttzinfo = time.tzinfo if time else None\n\t\telif date_str:\n\t\t\t# Date portion is not specified, but time portion is\n\t\t\ttzinfo = date.tzinfo if date else None\n\texcept ValueError:", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\tformat = '%Y-%m-%dT%H:%M:%S'\n\t\tif dt_str.endswith('Z'):\n\t\t\tformat += ':%z'\n\t\telif dt_str.endswith('+00:00'):\n\t\t\tformat += ':%z'\n\t\telif dt_str.endswith('-00:00'):\n\t\t\tformat += ':%z'\n\t\telif dt_str.endswith(':00'):\n\t\t\tformat += ':'\n\t\telif dt_str.endswith('+00'):\n\t\t\tformat += '%z'\n\t\telif dt_str.endswith('-00'):\n\t\t\tformat += '%z'\n\t\telif dt_str.endswith(':00:00'):\n\t\t\tformat += ':%S'\n\t\telif dt_str.endswith('+00:00:00'):\n\t\t\tformat += ':%S'\n\t\telif dt_str.endswith('-00:00:00'):\n\t\t\tformat += ':%S'\n\t\telif dt_str.endswith(':00:0", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\treturn iso8601.parse_date(dt_str)\n\texcept iso8601.ParseError:\n\t\treturn iso8601.parse_date(dt_str.rstrip(':'))\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\tdt = self.parse(dt_str)\n\texcept ValueError as exc:\n\t\traise ValueError(\n\t\t\t\"Unable to parse ISO-8601 string %r: %s\" % (dt_str, exc)\n\t\t) from exc\n\n\t# Convert to UTC if necessary\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=dateutil.tz.tzutc())\n\n\treturn dt\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt = parse(dt_str, default=date(1,1,1))\n\tdt.replace(tzinfo=dateutil.tz.tzutc())\n\treturn dt\n", "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt = None\n\tdt_str = dt_str.strip()\n\n\t# Parse the date\n\tif len(dt_str) > 10:\n\t\tdt_str = dt_str[:10]\n\tif len(dt_str) > 8:\n\t\tdt_str = dt_str[:8]\n\tif len(dt_str) > 6:\n\t\tdt_str = dt_str[:6]\n\tdt = dateparser.parse(dt_str)\n\n\t# Parse the time\n\tif dt is not None:\n\t\tdt_str = dt_str[10:]\n\t\tif len(dt_str) > 0:\n\t\t\tdt_str = dt_str.replace(':', '')\n\t\t\tdt_str = dt_str.replace(' ', '')\n\t\t\tdt_str = dt_str.replace('-', '')\n\t\t\tdt_str = dt_str.replace(':', '')\n\t\t\tdt_str = dt_str.replace('+', '')\n\t\t\tdt_str = dt_str.replace(':', '')\n\t\t\tdt_str = dt_str.replace(' ', '')\n\t\t\tdt_str = dt_str.replace('Z', '')\n\t\t\tdt_str = dt_str.replace('+', '')"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tclass _DateTimeParseError(Exception):\n\t\tpass\n\n\tdef _parse_datetime_string(self, dt_str):\n\t\t# Re-implemented parsing algorithm from Python (see\n\t\t# https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior).\n\t\ttry:\n\t\t\tdt_str = dt_str.strip()\n\t\texcept AttributeError:\n\t\t\traise _DateTimeParseError(\"Invalid date string\")\n\t\tif not dt_str:\n\t\t\traise _DateTimeParseError(\"Empty date string\")\n\t\tif len(dt_str) == 19:\n\t\t\tif dt_str[4] == dt_str[7] == '-':\n\t\t\t\tdt_str = dt_str[:4] + '0' + dt_str[5:7] + '0' + dt_str[8:10]\n\t\t\telif dt_str[4] == dt_str[6] == '-':\n\t\t\t\tdt_str = dt_str[:4] + '0' + dt_str[5:7] + dt_str[8:10]\n\t\t\telse:\n\t\t\t\traise _DateTimeParseError(\"Invalid date string\")\n\t\telif len(dt_str) == 16:\n\t\t\tif dt_", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\treturn self._parse_iso8601(dt_str)\n\texcept (ValueError, OverflowError):\n\t\traise ValueError('%s is not a valid ISO-8601 datetime string' % dt_str)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt_str = dt_str.strip()\n\n\tif dt_str == '':\n\t\treturn None\n\n\t# 1. Parse the date portion\n\tdate_str = self.parse_date(dt_str)\n\tif date_str is None:\n\t\treturn None\n\n\tdt = self.parse_date_str(date_str)\n\tif dt is None:\n\t\treturn None\n\n\t# 2. Parse the time portion\n\tdt_str = dt_str.replace(date_str, '')\n\tdt_str = dt_str.strip()\n\n\tif dt_str == '':\n\t\treturn dt\n\n\ttry:\n\t\ttime_str = self.parse_time(dt_str)\n\t\tif time_str is None:\n\t\t\treturn None\n\texcept ValueError:\n\t\ttime_str = None\n\n\t# 3. Handle the time portion\n\tif time_str is not None:\n\t\tdt = dt.replace(time=time_str)\n\telse:\n\t\tdt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n\n\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt_str = str(dt_str)\n\n\ttry:\n\t\tdt = parse(dt_str)\n\texcept ValueError:\n\t\traise ValueError(\"Unable to parse date\")\n\n\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tformat_string = self.FORMAT\n\tif not dt_str:\n\t\treturn None\n\n\tdate_str, time_str = dt_str.split('T')\n\n\ttz = self.tz\n\ttzinfo = tz or None\n\n\ttry:\n\t\tdate = self.parse(date_str)\n\texcept ValueError:\n\t\t# No time portion\n\t\tif time_str:\n\t\t\traise ValueError(\"No time portion in ISO 8601 datetime string\")\n\t\tformat_string = format_string.replace('T', ' ')\n\n\ttry:\n\t\ttime = self.parse(time_str)\n\texcept ValueError:\n\t\t# No date portion\n\t\tif date_str:\n\t\t\traise ValueError(\"No date portion in ISO 8601 datetime string\")\n\t\tformat_string = format_string.replace(' ', 'T')\n\n\ttry:\n\t\tif tz:\n\t\t\ttzinfo = tz\n\t\telif time_str:\n\t\t\t# Time portion is not specified, but date portion is\n\t\t\ttzinfo = time.tzinfo if time else None\n\t\telif date_str:\n\t\t\t# Date portion is not specified, but time portion is\n\t\t\ttzinfo = date.tzinfo if date else None\n\texcept ValueError:", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\tformat = '%Y-%m-%dT%H:%M:%S'\n\t\tif dt_str.endswith('Z'):\n\t\t\tformat += ':%z'\n\t\telif dt_str.endswith('+00:00'):\n\t\t\tformat += ':%z'\n\t\telif dt_str.endswith('-00:00'):\n\t\t\tformat += ':%z'\n\t\telif dt_str.endswith(':00'):\n\t\t\tformat += ':'\n\t\telif dt_str.endswith('+00'):\n\t\t\tformat += '%z'\n\t\telif dt_str.endswith('-00'):\n\t\t\tformat += '%z'\n\t\telif dt_str.endswith(':00:00'):\n\t\t\tformat += ':%S'\n\t\telif dt_str.endswith('+00:00:00'):\n\t\t\tformat += ':%S'\n\t\telif dt_str.endswith('-00:00:00'):\n\t\t\tformat += ':%S'\n\t\telif dt_str.endswith(':00:0", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\treturn iso8601.parse_date(dt_str)\n\texcept iso8601.ParseError:\n\t\treturn iso8601.parse_date(dt_str.rstrip(':'))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\ttry:\n\t\tdt = self.parse(dt_str)\n\texcept ValueError as exc:\n\t\traise ValueError(\n\t\t\t\"Unable to parse ISO-8601 string %r: %s\" % (dt_str, exc)\n\t\t) from exc\n\n\t# Convert to UTC if necessary\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=dateutil.tz.tzutc())\n\n\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt = parse(dt_str, default=date(1,1,1))\n\tdt.replace(tzinfo=dateutil.tz.tzutc())\n\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t\"\"\"\n\tParse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0\n\t\"\"\"\n\tdt = None\n\tdt_str = dt_str.strip()\n\n\t# Parse the date\n\tif len(dt_str) > 10:\n\t\tdt_str = dt_str[:10]\n\tif len(dt_str) > 8:\n\t\tdt_str = dt_str[:8]\n\tif len(dt_str) > 6:\n\t\tdt_str = dt_str[:6]\n\tdt = dateparser.parse(dt_str)\n\n\t# Parse the time\n\tif dt is not None:\n\t\tdt_str = dt_str[10:]\n\t\tif len(dt_str) > 0:\n\t\t\tdt_str = dt_str.replace(':', '')\n\t\t\tdt_str = dt_str.replace(' ', '')\n\t\t\tdt_str = dt_str.replace('-', '')\n\t\t\tdt_str = dt_str.replace(':', '')\n\t\t\tdt_str = dt_str.replace('+', '')\n\t\t\tdt_str = dt_str.replace(':', '')\n\t\t\tdt_str = dt_str.replace(' ', '')\n\t\t\tdt_str = dt_str.replace('Z', '')\n\t\t\tdt_str = dt_str.replace('+', '')", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t...\n\n#src/dateutil/_parser.py", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tresult = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\texcept OverflowError:\n\t\traise OverflowError(\n\t\t\t\"Date/time string %r too large to convert to a datetime object\" % (timestr,))\n\texcept ValueError:\n\t\traise ValueError(\"Invalid date/time string\")\n\treturn result", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tdefault = default or datetime.datetime.min\n\ttzinfos = tzinfos or {}\n\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tself._validate_tzinfos(tzinfos, ignoretz)\n\tdt, tokens = self._parse(timestr, default, ignoretz,\n\t                         tzinfos=tzinfos, **kwargs)\n\tif self.fuzzy_with_tokens:\n\t\treturn dt, tokens\n\telse:\n\t\treturn dt\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t...\n\n#dateutil/tz.py\nfrom calendar import monthrange\nfrom dateutil.rrule import rrule\nfrom datetime import datetime, date, time, timedelta\nfrom .tzfile import TzFile\nfrom .tzlocal import getlocalzone\nfrom .tzutil import MAXYEAR, MINYEAR, localize\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tif tzinfos is None:\n\t\t\ttzinfos = {}\n\t\tif not isinstance(tzinfos, dict):\n\t\t\ttzinfos = {tzinfos: tzinfos}\n\t\tif default is None:\n\t\t\tdefault = datetime.datetime.now()\n\t\tkwargs['default'] = default\n\t\tkwargs['ignoretz'] = ignoretz\n\t\tkwargs['tzinfos'] = tzinfos\n\t\treturn _parse(timestr, **kwargs)\n\texcept Exception, e:\n\t\traise ParserError(e)\n\n\n\n\n\n###\n#\n# ParserError\n#\n###", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tkwargs['fuzzy_with_tokens'] = True\n\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\tfinally:\n\t\tdel kwargs['fuzzy_with_tokens']\n\n#src/dateutil/_parser/_parser.py\nimport sys\nimport calendar\nfrom . import parserinfo\nfrom ._parserinfo import ParserInfo\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tparsed = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\tif isinstance(parsed, tuple):\n\t\t\tparsed = parsed[0]\n\texcept OverflowError:\n\t\traise OverflowError(\"Date overflows Python's integer type\")\n\treturn parsed\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t...\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t...\n\n#src/dateutil/_parser.py", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tresult = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\texcept OverflowError:\n\t\traise OverflowError(\n\t\t\t\"Date/time string %r too large to convert to a datetime object\" % (timestr,))\n\texcept ValueError:\n\t\traise ValueError(\"Invalid date/time string\")\n\treturn result", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tdefault = default or datetime.datetime.min\n\ttzinfos = tzinfos or {}\n\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\tself._validate_tzinfos(tzinfos, ignoretz)\n\tdt, tokens = self._parse(timestr, default, ignoretz,\n\t                         tzinfos=tzinfos, **kwargs)\n\tif self.fuzzy_with_tokens:\n\t\treturn dt, tokens\n\telse:\n\t\treturn dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t...\n\n#dateutil/tz.py\nfrom calendar import monthrange\nfrom dateutil.rrule import rrule\nfrom datetime import datetime, date, time, timedelta\nfrom .tzfile import TzFile\nfrom .tzlocal import getlocalzone\nfrom .tzutil import MAXYEAR, MINYEAR, localize\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tif tzinfos is None:\n\t\t\ttzinfos = {}\n\t\tif not isinstance(tzinfos, dict):\n\t\t\ttzinfos = {tzinfos: tzinfos}\n\t\tif default is None:\n\t\t\tdefault = datetime.datetime.now()\n\t\tkwargs['default'] = default\n\t\tkwargs['ignoretz'] = ignoretz\n\t\tkwargs['tzinfos'] = tzinfos\n\t\treturn _parse(timestr, **kwargs)\n\texcept Exception, e:\n\t\traise ParserError(e)\n\n\n\n\n\n###\n#\n# ParserError\n#\n###", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tkwargs['fuzzy_with_tokens'] = True\n\t\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\tfinally:\n\t\tdel kwargs['fuzzy_with_tokens']\n\n#src/dateutil/_parser/_parser.py\nimport sys\nimport calendar\nfrom . import parserinfo\nfrom ._parserinfo import ParserInfo\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\ttry:\n\t\tparsed = self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\t\tif isinstance(parsed, tuple):\n\t\t\tparsed = parsed[0]\n\texcept OverflowError:\n\t\traise OverflowError(\"Date overflows Python's integer type\")\n\treturn parsed\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t\"\"\"\n\tParse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.\n\t\"\"\"\n\t...\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt.tzinfo is None:\n\t\traise ValueError('Cannot convert non-aware datetime to another timezone')\n\n\tfrom_tz = dt.tzinfo\n\n\tif from_tz.utcoffset(dt) is None:\n\t\traise ValueError('Cannot convert non-aware datetime to another timezone')\n\n\tif dt.tzinfo.utcoffset(dt).seconds < 0:\n\t\t# This datetime is in a \"forward\" timezone.\n\t\tdt = dt + dt.tzinfo.utcoffset(dt)\n\telse:\n\t\t# This datetime is in a \"backward\" timezone.\n\t\tdt = dt - dt.tzinfo.utcoffset(dt)\n\n\t# We now have a timezone-aware datetime in the desired timezone.\n\tdt = dt.replace(tzinfo=None)\n\treturn dt", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz = dt.tzinfo\n\tif tz is None:\n\t\ttz = pytz.UTC\n\n\tif dt.tzname() == \"UTC\":\n\t\tdt = dt.astimezone(tz)\n\n\tif dt.tzname() != tz.tzname():\n\t\tdt = dt.astimezone(tz)\n\n\treturn dt\n\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfrom pytz import AmbiguousTimeError\n\tfrom pytz.exceptions import AmbiguousTimeError\n\tfrom pytz.exceptions import InvalidTimeError\n\n\t# If this is the first occurrence in the chain, we assume that it's\n\t# not ambiguous and we can just return it.\n\n\tif dt.tzinfo and dt.tzinfo.utcoffset(dt) is None:\n\t\treturn dt\n\n\t# If it's not a valid timezone, this is a bad datetime.\n\t# InvalidTimeError is raised when the user passes in an invalid\n\t# timezone to a timezone-aware datetime.\n\n\ttry:\n\t\tdt.astimezone(dt.tzinfo)\n\texcept InvalidTimeError:\n\t\traise\n\n\t# If we get here, then we know that this datetime is not\n\t# ambiguous.\n\n\ttry:\n\t\tdt.utcoffset()\n\texcept AmbiguousTimeError:\n\t\traise AmbiguousTimeError(\n\t\t\t\"ambiguous timezone: %s\" % (dt,)\n\t\t)", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt.tzname() != 'UTC':\n\t\traise ValueError('fromutc(): %s is not in UTC timezone' % dt)\n\t\n\tdt = dt.astimezone(pytz.utc)\n\t\n\tif dt.tzinfo is not None:\n\t\tdt = dt.astimezone(dt.tzinfo)\n\t\t\n\tdt = dt.replace(tzinfo=None)\n\t\n\treturn dt\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = dt.astimezone(tz=self.timezone_utc)\n\n\tdt_folded = self.fold(dt)\n\n\tif dt_folded is None:\n\t\tdt_folded = dt\n\n\treturn self.to_utc(dt_folded)", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz = dt.tzinfo\n\td = dt - datetime.timedelta(hours=tz.utcoffset(dt).seconds / 3600)\n\tdt = tz.localize(d)\n\tu = dt.utcoffset()\n\tif u == datetime.timedelta(0):\n\t\tdt = dt.replace(tzinfo=None)\n\telse:\n\t\tdt = dt.replace(tzinfo=tz)\n\treturn dt", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfold = False\n\tif dt.tzinfo is None:\n\t\tfold = True\n\telif dt.tzinfo.utcoffset(dt) is None:\n\t\tfold = True\n\telif dt.tzinfo.dst(dt) is not None:\n\t\tfold = True\n\treturn dt.astimezone(self.utcoffset(dt)) if not fold else dt\n", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt_tz = self.astimezone(dt.tzinfo or self.tzinfo)\n\t\n\tif dt_tz.tzinfo != dt.tzinfo:\n\t\tif dt_tz.tzinfo is None:\n\t\t\tdt_tz = dt_tz.replace(tzinfo=dt.tzinfo)\n\t\telse:\n\t\t\tdt_tz = dt_tz.astimezone(dt.tzinfo)\n\t\n\tdt_tz = dt_tz.replace(tzinfo=None)\n\t\n\tif dt_tz == dt:\n\t\treturn dt\n\telse:\n\t\treturn dt_tz\n\t\n\ndatetime.datetime.fromutc = classmethod(fromutc)", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz_now = datetime.datetime.now(tz_a)\n\tutc = dt.astimezone(pytz.UTC)\n\n\ttz_origin = dt.tzinfo\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tz_now.tzinfo)\n\n\tdt_now = dt.astimezone(tz_now)\n\n\tdt_origin = dt.replace(tzinfo=tz_origin)\n\n\tif dt_now.year == tz_now.year:\n\t\treturn dt_now\n\n\t# If this is a fold, then it's the first occurrence of the\n\t# ambiguous datetime.\n\tif dt_origin.year == tz_origin.year:\n\t\treturn dt_origin\n\n\t# If this is a fold, we can re-calculate the year\n\tif dt_origin.year == tz_origin.year + 1:\n\t\tdt_origin = dt_origin.replace(year=tz_origin.year)\n\t\treturn dt_origin\n\n\t# If this is a fold, we can re-calculate the year\n\tif dt_origin.year == tz_origin.year - 1:\n\t\tdt_origin = dt_origin.replace(year=tz_origin.year)\n\t\treturn dt_origin\n\n\t# If", "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz = dt.tzinfo\n\tif tz.utcoffset(dt) is None:\n\t\traise ValueError('datetime should be timezone-aware')\n\n\tdt = dt.astimezone(tz)\n\n\tif dt.tzinfo is not tz:\n\t\traise ValueError('datetime should be in %s' % tz)\n\n\tdt = dt.replace(tzinfo=None)\n\n\tif dt.tzinfo is None:\n\t\traise ValueError('datetime should be in %s' % tz)\n\n\treturn dt"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt.tzinfo is None:\n\t\traise ValueError('Cannot convert non-aware datetime to another timezone')\n\n\tfrom_tz = dt.tzinfo\n\n\tif from_tz.utcoffset(dt) is None:\n\t\traise ValueError('Cannot convert non-aware datetime to another timezone')\n\n\tif dt.tzinfo.utcoffset(dt).seconds < 0:\n\t\t# This datetime is in a \"forward\" timezone.\n\t\tdt = dt + dt.tzinfo.utcoffset(dt)\n\telse:\n\t\t# This datetime is in a \"backward\" timezone.\n\t\tdt = dt - dt.tzinfo.utcoffset(dt)\n\n\t# We now have a timezone-aware datetime in the desired timezone.\n\tdt = dt.replace(tzinfo=None)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz = dt.tzinfo\n\tif tz is None:\n\t\ttz = pytz.UTC\n\n\tif dt.tzname() == \"UTC\":\n\t\tdt = dt.astimezone(tz)\n\n\tif dt.tzname() != tz.tzname():\n\t\tdt = dt.astimezone(tz)\n\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfrom pytz import AmbiguousTimeError\n\tfrom pytz.exceptions import AmbiguousTimeError\n\tfrom pytz.exceptions import InvalidTimeError\n\n\t# If this is the first occurrence in the chain, we assume that it's\n\t# not ambiguous and we can just return it.\n\n\tif dt.tzinfo and dt.tzinfo.utcoffset(dt) is None:\n\t\treturn dt\n\n\t# If it's not a valid timezone, this is a bad datetime.\n\t# InvalidTimeError is raised when the user passes in an invalid\n\t# timezone to a timezone-aware datetime.\n\n\ttry:\n\t\tdt.astimezone(dt.tzinfo)\n\texcept InvalidTimeError:\n\t\traise\n\n\t# If we get here, then we know that this datetime is not\n\t# ambiguous.\n\n\ttry:\n\t\tdt.utcoffset()\n\texcept AmbiguousTimeError:\n\t\traise AmbiguousTimeError(\n\t\t\t\"ambiguous timezone: %s\" % (dt,)\n\t\t)", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\t\n\tif dt.tzname() != 'UTC':\n\t\traise ValueError('fromutc(): %s is not in UTC timezone' % dt)\n\t\n\tdt = dt.astimezone(pytz.utc)\n\t\n\tif dt.tzinfo is not None:\n\t\tdt = dt.astimezone(dt.tzinfo)\n\t\t\n\tdt = dt.replace(tzinfo=None)\n\t\n\treturn dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt = dt.astimezone(tz=self.timezone_utc)\n\n\tdt_folded = self.fold(dt)\n\n\tif dt_folded is None:\n\t\tdt_folded = dt\n\n\treturn self.to_utc(dt_folded)", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz = dt.tzinfo\n\td = dt - datetime.timedelta(hours=tz.utcoffset(dt).seconds / 3600)\n\tdt = tz.localize(d)\n\tu = dt.utcoffset()\n\tif u == datetime.timedelta(0):\n\t\tdt = dt.replace(tzinfo=None)\n\telse:\n\t\tdt = dt.replace(tzinfo=tz)\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tfold = False\n\tif dt.tzinfo is None:\n\t\tfold = True\n\telif dt.tzinfo.utcoffset(dt) is None:\n\t\tfold = True\n\telif dt.tzinfo.dst(dt) is not None:\n\t\tfold = True\n\treturn dt.astimezone(self.utcoffset(dt)) if not fold else dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\tdt_tz = self.astimezone(dt.tzinfo or self.tzinfo)\n\t\n\tif dt_tz.tzinfo != dt.tzinfo:\n\t\tif dt_tz.tzinfo is None:\n\t\t\tdt_tz = dt_tz.replace(tzinfo=dt.tzinfo)\n\t\telse:\n\t\t\tdt_tz = dt_tz.astimezone(dt.tzinfo)\n\t\n\tdt_tz = dt_tz.replace(tzinfo=None)\n\t\n\tif dt_tz == dt:\n\t\treturn dt\n\telse:\n\t\treturn dt_tz\n\t\n\ndatetime.datetime.fromutc = classmethod(fromutc)", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz_now = datetime.datetime.now(tz_a)\n\tutc = dt.astimezone(pytz.UTC)\n\n\ttz_origin = dt.tzinfo\n\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tz_now.tzinfo)\n\n\tdt_now = dt.astimezone(tz_now)\n\n\tdt_origin = dt.replace(tzinfo=tz_origin)\n\n\tif dt_now.year == tz_now.year:\n\t\treturn dt_now\n\n\t# If this is a fold, then it's the first occurrence of the\n\t# ambiguous datetime.\n\tif dt_origin.year == tz_origin.year:\n\t\treturn dt_origin\n\n\t# If this is a fold, we can re-calculate the year\n\tif dt_origin.year == tz_origin.year + 1:\n\t\tdt_origin = dt_origin.replace(year=tz_origin.year)\n\t\treturn dt_origin\n\n\t# If this is a fold, we can re-calculate the year\n\tif dt_origin.year == tz_origin.year - 1:\n\t\tdt_origin = dt_origin.replace(year=tz_origin.year)\n\t\treturn dt_origin\n\n\t# If", "is_pass": true, "return_code": 0}, {"generate_code": "def fromutc(self, dt):\n\t\"\"\"\n\tGiven a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.\n\t\"\"\"\n\ttz = dt.tzinfo\n\tif tz.utcoffset(dt) is None:\n\t\traise ValueError('datetime should be timezone-aware')\n\n\tdt = dt.astimezone(tz)\n\n\tif dt.tzinfo is not tz:\n\t\traise ValueError('datetime should be in %s' % tz)\n\n\tdt = dt.replace(tzinfo=None)\n\n\tif dt.tzinfo is None:\n\t\traise ValueError('datetime should be in %s' % tz)\n\n\treturn dt", "is_pass": true, "return_code": 0}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tis_naive = dt.tzinfo is None\n\tif is_naive:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\t# If dt is already aware, return it unchanged\n\t\tif dt.tzinfo is tzinfo:\n\t\t\treturn dt\n\n\t\t# If dt is naive and tzinfo is None, return a naive\n\t\t# datetime with the same time zone as dt\n\t\tif tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=dt.tzinfo)\n\n\t\t# If tzinfo is not None, return a datetime with the\n\t\t# same tzinfo as dt, but with the given offset\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt\n\treturn dt", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\ttry:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\texcept AttributeError:\n\t\treturn dt\n\n#dateutil/utils/pytz/__init__.py\n# Copyright (c) 2008-2015 by pytz developers.\n# All rights reserved.\n\n\"\"\"\npytz\n====\n\nPython port of IANA Time Zone Database.\n\nSee http://pytz.sourceforge.net for details.\n\nThis module provides a simple, easy to use interface to the full pytz\ndatabase.\n\nSee the :py:mod:`pytz.tzinfo` module for details on how pytz works.\n\"\"\"\n\nfrom ._pytz import (\n    UTC,\n    timezone,\n    UnknownTimeZoneError,\n    AmbiguousTimeError,\n    NonExistentTimeError,\n    NotFoundError,\n    NonExistentTZInfoError,\n)\n\nfrom ._pytzinfo import _get_tzinfo_for_timezone\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n#python/dateutil/utils/pytz.py\n# -*- coding: utf-8 -*-\n\"\"\"\nPython timezone utilities\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements Python timezone utilities.\n\n:copyright: Copyright 2006-2016 by the Sphinx team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport pytz\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tif dt.tzinfo is None:\n\t\treturn datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute,\n\t\t\tdt.second, dt.microsecond, dt.tzinfo)\n\telse:\n\t\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo)\n\tif dt.utcoffset() is None:\n\t\tdt = dt.replace(tzinfo=None)\n\treturn dt\n\n#dateutil/parser/_parser.py\nfrom dateutil import tz\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\treturn dt.replace(tzinfo=tzinfo)", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdef tz_aware(dt, tzinfo):\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n\n\treturn tz_aware(dt, tzinfo)\n\n", "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt_tz = dt.replace(tzinfo=tzinfo)\n\tif dt_tz.utcoffset() is None:\n\t\treturn dt_tz.replace(tzinfo=None)\n\treturn dt_tz\n\n#dateutil/utils/datetime_rounder.py\nimport datetime\n\nfrom dateutil.utils import default_tzinfo\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tis_naive = dt.tzinfo is None\n\tif is_naive:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\t# If dt is already aware, return it unchanged\n\t\tif dt.tzinfo is tzinfo:\n\t\t\treturn dt\n\n\t\t# If dt is naive and tzinfo is None, return a naive\n\t\t# datetime with the same time zone as dt\n\t\tif tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=dt.tzinfo)\n\n\t\t# If tzinfo is not None, return a datetime with the\n\t\t# same tzinfo as dt, but with the given offset\n\t\treturn dt.replace(tzinfo=tzinfo)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt\n\treturn dt", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\ttry:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\texcept AttributeError:\n\t\treturn dt\n\n#dateutil/utils/pytz/__init__.py\n# Copyright (c) 2008-2015 by pytz developers.\n# All rights reserved.\n\n\"\"\"\npytz\n====\n\nPython port of IANA Time Zone Database.\n\nSee http://pytz.sourceforge.net for details.\n\nThis module provides a simple, easy to use interface to the full pytz\ndatabase.\n\nSee the :py:mod:`pytz.tzinfo` module for details on how pytz works.\n\"\"\"\n\nfrom ._pytz import (\n    UTC,\n    timezone,\n    UnknownTimeZoneError,\n    AmbiguousTimeError,\n    NonExistentTimeError,\n    NotFoundError,\n    NonExistentTZInfoError,\n)\n\nfrom ._pytzinfo import _get_tzinfo_for_timezone\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n#python/dateutil/utils/pytz.py\n# -*- coding: utf-8 -*-\n\"\"\"\nPython timezone utilities\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis module implements Python timezone utilities.\n\n:copyright: Copyright 2006-2016 by the Sphinx team, see AUTHORS.\n:license: BSD, see LICENSE for details.\n\"\"\"\n\nimport pytz\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tif dt.tzinfo is None:\n\t\treturn datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute,\n\t\t\tdt.second, dt.microsecond, dt.tzinfo)\n\telse:\n\t\treturn dt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt = dt.replace(tzinfo=tzinfo)\n\tif dt.utcoffset() is None:\n\t\tdt = dt.replace(tzinfo=None)\n\treturn dt\n\n#dateutil/parser/_parser.py\nfrom dateutil import tz\n", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\t\n\treturn dt.replace(tzinfo=tzinfo)", "is_pass": false, "return_code": 1}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdef tz_aware(dt, tzinfo):\n\t\tif dt.tzinfo is None:\n\t\t\treturn dt.replace(tzinfo=tzinfo)\n\t\treturn dt\n\n\treturn tz_aware(dt, tzinfo)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\t\"\"\"\n\tSets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.\n\t\"\"\"\n\tdt_tz = dt.replace(tzinfo=tzinfo)\n\tif dt_tz.utcoffset() is None:\n\t\treturn dt_tz.replace(tzinfo=None)\n\treturn dt_tz\n\n#dateutil/utils/datetime_rounder.py\nimport datetime\n\nfrom dateutil.utils import default_tzinfo\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.__before = before\n\tself.__after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = bytes(before)\n\tself._after = bytes(after)\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n#src/py_split.py\nfrom typing import Optional\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.__before = before\n\tself.__after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = bytes(before)\n\tself._after = bytes(after)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself._before = before\n\tself._after = after\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n\n#src/py_split.py\nfrom typing import Optional\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.\n\t\"\"\"\n\tself.before = before\n\tself.after = after\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\tif 'events' in request.json:\n\t\treturn True\n\t\n\tif 'tag' in request.json and request.json['tag'] == Magic_ENV_TAG:\n\t\treturn True\n\t\n\treturn False", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = request.json\n\t\tevents = request_body[\"events\"]\n\texcept Exception as e:\n\t\tlogging.error(f\"Request body JSON parsing error: {e}\")\n\t\treturn False\n\n\ttry:\n\t\ttag = request_body[\"tag\"]\n\t\tif tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept Exception as e:\n\t\tlogging.error(f\"Request body JSON parsing error: {e}\")\n\t\treturn False\n\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = request.get_json()\n\texcept (TypeError, json.JSONDecodeError):\n\t\trequest_body = None\n\t\n\tif request_body is None:\n\t\treturn False\n\t\n\tif isinstance(request_body, dict):\n\t\tif \"events\" in request_body:\n\t\t\treturn True\n\t\n\t\telif \"tag\" in request_body:\n\t\t\treturn request_body[\"tag\"] == Magic_ENV_TAG\n\t\n\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\t# Check whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\tif request.content_type == \"application/json\" and request.json:\n\t\t\treturn True\n\t\telif request.content_type == \"application/xml\" and request.xml:\n\t\t\treturn True\n\t\telif request.json and request.xml:\n\t\t\traise ValueError(\"XML and JSON body cannot be loaded in the same time.\")\n\t\telse:\n\t\t\treturn False\n\texcept Exception as e:\n\t\traise Exception(f\"Error occurred while loading request body: {str(e)}\") from e\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = request.json()\n\texcept ValueError:\n\t\tpass\n\telse:\n\t\tfor event in request_body:\n\t\t\tif isinstance(event, dict):\n\t\t\t\tif event.get(\"event\") == \"error\":\n\t\t\t\t\traise MagicEnvError\n\t\t\t\telse:\n\t\t\t\t\treturn True\n\t\tif request.headers.get(\"Content-Type\") == \"application/xml\":\n\t\t\tif request.headers.get(\"Magic-Env-Tag\") == Magic_ENV_TAG:\n\t\t\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\tif request.data.get('events', None) is not None:\n\t\treturn True\n\telif request.data.get('tag', None) == Magic_ENV_TAG:\n\t\treturn True\n\telse:\n\t\treturn False", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_json = request.json\n\t\tif \"events\" in request_json:\n\t\t\treturn True\n\t\telif \"<\" + Magic_ENV_TAG + \">\" in request_json:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tif request.json:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\ttry:\n\t\tif request.headers['X-Magic-Env'] == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\treturn False\n", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest.json\n\t\treturn True\n\texcept:\n\t\ttry:\n\t\t\trequest.xml\n\t\t\treturn True\n\t\texcept:\n\t\t\ttry:\n\t\t\t\trequest.form\n\t\t\t\treturn True\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\trequest.headers[\"tag\"]\n\t\t\t\t\treturn True\n\t\t\t\texcept:\n\t\t\t\t\treturn False", "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tdata = json.loads(request.get_data())\n\t\tif \"events\" in data:\n\t\t\treturn True\n\t\telif \"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\" in str(request.get_data()):\n\t\t\tif request.headers.get(\"Magic_ENV_TAG\") == Magic_ENV_TAG:\n\t\t\t\treturn True\n\texcept:\n\t\treturn False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\tif 'events' in request.json:\n\t\treturn True\n\t\n\tif 'tag' in request.json and request.json['tag'] == Magic_ENV_TAG:\n\t\treturn True\n\t\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = request.json\n\t\tevents = request_body[\"events\"]\n\texcept Exception as e:\n\t\tlogging.error(f\"Request body JSON parsing error: {e}\")\n\t\treturn False\n\n\ttry:\n\t\ttag = request_body[\"tag\"]\n\t\tif tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept Exception as e:\n\t\tlogging.error(f\"Request body JSON parsing error: {e}\")\n\t\treturn False\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = request.get_json()\n\texcept (TypeError, json.JSONDecodeError):\n\t\trequest_body = None\n\t\n\tif request_body is None:\n\t\treturn False\n\t\n\tif isinstance(request_body, dict):\n\t\tif \"events\" in request_body:\n\t\t\treturn True\n\t\n\t\telif \"tag\" in request_body:\n\t\t\treturn request_body[\"tag\"] == Magic_ENV_TAG\n\t\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\t# Check whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\tif request.content_type == \"application/json\" and request.json:\n\t\t\treturn True\n\t\telif request.content_type == \"application/xml\" and request.xml:\n\t\t\treturn True\n\t\telif request.json and request.xml:\n\t\t\traise ValueError(\"XML and JSON body cannot be loaded in the same time.\")\n\t\telse:\n\t\t\treturn False\n\texcept Exception as e:\n\t\traise Exception(f\"Error occurred while loading request body: {str(e)}\") from e\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_body = request.json()\n\texcept ValueError:\n\t\tpass\n\telse:\n\t\tfor event in request_body:\n\t\t\tif isinstance(event, dict):\n\t\t\t\tif event.get(\"event\") == \"error\":\n\t\t\t\t\traise MagicEnvError\n\t\t\t\telse:\n\t\t\t\t\treturn True\n\t\tif request.headers.get(\"Content-Type\") == \"application/xml\":\n\t\t\tif request.headers.get(\"Magic-Env-Tag\") == Magic_ENV_TAG:\n\t\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\t\n\tif request.data.get('events', None) is not None:\n\t\treturn True\n\telif request.data.get('tag', None) == Magic_ENV_TAG:\n\t\treturn True\n\telse:\n\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest_json = request.json\n\t\tif \"events\" in request_json:\n\t\t\treturn True\n\t\telif \"<\" + Magic_ENV_TAG + \">\" in request_json:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tif request.json:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\ttry:\n\t\tif request.headers['X-Magic-Env'] == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\trequest.json\n\t\treturn True\n\texcept:\n\t\ttry:\n\t\t\trequest.xml\n\t\t\treturn True\n\t\texcept:\n\t\t\ttry:\n\t\t\t\trequest.form\n\t\t\t\treturn True\n\t\t\texcept:\n\t\t\t\ttry:\n\t\t\t\t\trequest.headers[\"tag\"]\n\t\t\t\t\treturn True\n\t\t\t\texcept:\n\t\t\t\t\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.\n\t\"\"\"\n\ttry:\n\t\tdata = json.loads(request.get_data())\n\t\tif \"events\" in data:\n\t\t\treturn True\n\t\telif \"<?xml version=\\\"1.0\\\" encoding=\\\"utf-8\\\"?>\" in str(request.get_data()):\n\t\t\tif request.headers.get(\"Magic_ENV_TAG\") == Magic_ENV_TAG:\n\t\t\t\treturn True\n\texcept:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tevents = request.event\n\tif events is None:\n\t\treturn False\n\tif not isinstance(events, list):\n\t\treturn False\n\tif len(events) == 0:\n\t\treturn False\n\telif len(events) == 1:\n\t\tevent = events[0]\n\t\tif not isinstance(event, dict):\n\t\t\treturn False\n\t\tif 'event' not in event:\n\t\t\treturn False\n\t\treturn True\n\telse:\n\t\t# multiple events are present\n\t\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tdata = request.get_json()\n\t\tif data['events'] is not None:\n\t\t\treturn True\n\texcept Exception as err:\n\t\tpass\n\treturn False", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tdata = request.get_json()\n\tif data is not None:\n\t\tif data.get(\"events\") is not None:\n\t\t\treturn True\n\n\treturn False", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest.json()\n\t\tif \"events\" in request.json():\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tcontent = request.json\n\t\tif \"events\" in content:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\treturn False\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\trequest_body = request.get_json()\n\n\tif request_body is None:\n\t\treturn False\n\n\tif not isinstance(request_body, dict):\n\t\treturn False\n\n\tevent = request_body.get(\"event\")\n\n\tif event is None:\n\t\treturn False\n\n\treturn True\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tbody = request.body\n\tif body:\n\t\ttry:\n\t\t\tbody = json.loads(body)\n\t\texcept json.JSONDecodeError:\n\t\t\treturn False\n\t\tif not isinstance(body, list):\n\t\t\treturn False\n\t\tfor event in body:\n\t\t\tif not isinstance(event, dict):\n\t\t\t\treturn False\n\t\t\tif not all(key in event for key in ('type', 'data')):\n\t\t\t\treturn False\n\treturn True\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tget_json(request)\n\t\treturn True\n\texcept (JSONDecodeError, TypeError):\n\t\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_json = request.json\n\texcept TypeError:\n\t\treturn False\n\t\n\ttry:\n\t\tif request_json[\"events\"]:\n\t\t\treturn True\n\texcept KeyError:\n\t\treturn False\n\n\treturn False", "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest.json\n\texcept json.decoder.JSONDecodeError:\n\t\treturn False\n\treturn True\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tevents = request.event\n\tif events is None:\n\t\treturn False\n\tif not isinstance(events, list):\n\t\treturn False\n\tif len(events) == 0:\n\t\treturn False\n\telif len(events) == 1:\n\t\tevent = events[0]\n\t\tif not isinstance(event, dict):\n\t\t\treturn False\n\t\tif 'event' not in event:\n\t\t\treturn False\n\t\treturn True\n\telse:\n\t\t# multiple events are present\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tdata = request.get_json()\n\t\tif data['events'] is not None:\n\t\t\treturn True\n\texcept Exception as err:\n\t\tpass\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tdata = request.get_json()\n\tif data is not None:\n\t\tif data.get(\"events\") is not None:\n\t\t\treturn True\n\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest.json()\n\t\tif \"events\" in request.json():\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tcontent = request.json\n\t\tif \"events\" in content:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\trequest_body = request.get_json()\n\n\tif request_body is None:\n\t\treturn False\n\n\tif not isinstance(request_body, dict):\n\t\treturn False\n\n\tevent = request_body.get(\"event\")\n\n\tif event is None:\n\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\tbody = request.body\n\tif body:\n\t\ttry:\n\t\t\tbody = json.loads(body)\n\t\texcept json.JSONDecodeError:\n\t\t\treturn False\n\t\tif not isinstance(body, list):\n\t\t\treturn False\n\t\tfor event in body:\n\t\t\tif not isinstance(event, dict):\n\t\t\t\treturn False\n\t\t\tif not all(key in event for key in ('type', 'data')):\n\t\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\tget_json(request)\n\t\treturn True\n\texcept (JSONDecodeError, TypeError):\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest_json = request.json\n\texcept TypeError:\n\t\treturn False\n\t\n\ttry:\n\t\tif request_json[\"events\"]:\n\t\t\treturn True\n\texcept KeyError:\n\t\treturn False\n\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t\"\"\"\n\tCheck whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.\n\t\"\"\"\n\ttry:\n\t\trequest.json\n\texcept json.decoder.JSONDecodeError:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\ttime = ensure_timezone(dt)\n\treturn time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\timport datetime\n\timport pytz\n\tfrom ensure_timezone import ensure_timezone\n\t\n\tdt = ensure_timezone(dt)\n\t\n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\tdt = ensure_timezone(dt)\n\t\n\t# If the time is in the future, return the time in a human-readable format.\n\tif dt.time() > datetime.time(0,0,0):\n\t\treturn dt\n\t\n\t\n\t# Return the date in a human-readable format.\n\treturn dt.strftime(\"%d %b %Y\")\n\n#app/blog/models.py\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom django.urls import reverse\nfrom django.utils.html import strip_tags\nfrom markdownx.models import MarkdownxField\nfrom markdownx.utils import markdownify\nfrom django.utils import timezone\n\nfrom .utils import format_dt\n\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tlocal_dt = ensure_timezone(dt, local_tz=True)\n\treturn local_dt.strftime('%d/%m/%Y %H:%M:%S')\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tdt = datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%SZ\")\n\treturn format_time(dt)\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tfrom datetime import datetime\n\timport pytz\n\tfrom . import ensure_timezone\n\tdt = ensure_timezone(dt)\n\treturn datetime.strftime(dt, '%b %d %Y %H:%M:%S %Z')\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tensure_timezone(dt)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%A, %B %d, %Y at %I:%M %p\")\n", "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\ttimezone_dt = ensure_timezone(dt)\n\treturn timezone_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\ttime = ensure_timezone(dt)\n\treturn time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\timport datetime\n\timport pytz\n\tfrom ensure_timezone import ensure_timezone\n\t\n\tdt = ensure_timezone(dt)\n\t\n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\t\n\tdt = ensure_timezone(dt)\n\t\n\t# If the time is in the future, return the time in a human-readable format.\n\tif dt.time() > datetime.time(0,0,0):\n\t\treturn dt\n\t\n\t\n\t# Return the date in a human-readable format.\n\treturn dt.strftime(\"%d %b %Y\")\n\n#app/blog/models.py\nfrom django.db import models\nfrom django.contrib.auth.models import User\nfrom django.urls import reverse\nfrom django.utils.html import strip_tags\nfrom markdownx.models import MarkdownxField\nfrom markdownx.utils import markdownify\nfrom django.utils import timezone\n\nfrom .utils import format_dt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tlocal_dt = ensure_timezone(dt, local_tz=True)\n\treturn local_dt.strftime('%d/%m/%Y %H:%M:%S')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tfrom datetime import datetime\n\n\tdt = datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%SZ\")\n\treturn format_time(dt)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tfrom datetime import datetime\n\timport pytz\n\tfrom . import ensure_timezone\n\tdt = ensure_timezone(dt)\n\treturn datetime.strftime(dt, '%b %d %Y %H:%M:%S %Z')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tensure_timezone(dt)\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\tdt = ensure_timezone(dt)\n\treturn dt.strftime(\"%A, %B %d, %Y at %I:%M %p\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format_dt(dt):\n\t\"\"\"\n\tUse the ensure_timezone function to format the time of dt and return the time.\n\t\"\"\"\n\ttimezone_dt = ensure_timezone(dt)\n\treturn timezone_dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_re = re.compile(r'<([^>]+)>')\n\ttags = set()\n\tout_text = text\n\tif replacer is not None:\n\t\tout_text = tag_re.sub(lambda match: replacer(match.group(1)), text)\n\telse:\n\t\tout_text = tag_re.sub(lambda match: \"\", text)\n\tout_text = out_text.strip()\n\tfor tag in re.findall(r'<([^>]+)>', text):\n\t\ttags.add(tag)\n\treturn tags, out_text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = text.lower()\n\ttext = text.replace('<', '</')\n\ttext = text.replace('>', '/>')\n\ttext = text.replace('&', '&amp;')\n\ttext = text.replace('\"', '&quot;')\n\ttext = text.replace(\"'\", '&#39;')\n\ttext = text.replace('`', '&#96;')\n\ttext = text.replace('=', '&#61;')\n\ttext = text.replace('-', '&#45;')\n\ttext = text.replace('+', '&#43;')\n\ttext = text.replace('|', '&#124;')\n\ttext = text.replace('^', '&#124;')\n\ttext = text.replace('*', '&#42;')\n\ttext = text.replace('~', '&#42;')\n\ttext = text.replace('?', '&#63;')\n\ttext = text.replace(':', '&#58;')\n\ttext = text.replace('$', '&#36;')\n\ttext = text.replace('%', '&#37;')\n\ttext = text.replace('&lt;', '&lt;/')\n\ttext = text.replace('&gt", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = \"\".join(text.split(\"\\n\"))\n\n\ttags = tag_regex.findall(text)\n\ttags = set(tags)\n\n\ttext = text.replace(\"<\", \"&lt;\")\n\ttext = text.replace(\">\", \"&gt;\")\n\n\tif replacer:\n\t\ttext = replacer(text)\n\n\treturn tags, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tdef replace(tag: str) -> str:\n\t\t\"\"\"\n\t\tReplace the tag word.\n\t\t\"\"\"\n\t\tif replacer:\n\t\t\treturn replacer(tag)\n\t\telse:\n\t\t\treturn tag\n\n\tmatches = re.findall(r'<\\w+(?=\\s*=)>', text)\n\tmatches += re.findall(r'<\\w+(?=\\s+\\/)>', text)\n\tresult = text\n\tfor match in matches:\n\t\t# Remove tag from result\n\t\tresult = result.replace(match, '')\n\t\t# Replace tag with result of replacement function\n\t\tresult = result.replace(match, replace(match))\n\n\treturn set(re.findall(r'<\\w+(?=\\s*=)>', result)), result\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_re = r\"<[^>]+>\"\n\ttag_matches = re.findall(tag_re, text)\n\tif len(tag_matches) == 0:\n\t\treturn set(), text\n\n\ttag_words = set([re.sub(r\"<[^>]+>\", \"\", tag) for tag in tag_matches])\n\n\tif replacer:\n\t\ttext = replacer(text, tag_words)\n\n\treturn tag_words, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tmatch = re.search(r\"<[a-zA-Z0-9_]+>([^<]*)</\\1>\", text)\n\tif not match:\n\t\treturn set(), text\n\ttags = set(match.group(1).split())\n\ttext = text.replace(match.group(), \"\")\n\tif replacer:\n\t\ttext = replacer(text, tags)\n\treturn tags, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_start = re.compile(r'<([a-zA-Z0-9_-]+)(?:.*?>|>)')\n\ttag_end = re.compile(r'</(.+)>')\n\n\ttag_stack = []\n\toriginal_text = text\n\ttext = text.strip()\n\n\tfor match in tag_start.finditer(text):\n\t\ttag = match.group(1)\n\t\tif tag == \"code\":\n\t\t\ttag_stack.append(tag)\n\t\telif tag == \"pre\":\n\t\t\ttag_stack.append(tag)\n\t\telse:\n\t\t\ttag_stack.append(tag)\n\n\t\ttag_stack.append(match.group(0))\n\n\twhile tag_stack:\n\t\ttext = tag_stack.pop()\n\t\tif text.startswith(\"</\"):\n\t\t\ttag_end = tag_end.match(text)\n\t\t\tif tag_end is None:\n\t\t\t\traise Exception(\"Unmatched closing tag: \" + text)\n\n\t\t\ttag = tag_end.group(1)\n\t\t\tif tag == \"code\":\n\t\t\t\tdel tag_stack[-1]\n\t\t\telif tag == \"pre\":\n\t\t\t\tdel tag_stack[-1]\n\t\t\telse:\n\t\t\t", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_regex = r\"(\\w+)\"\n\ttags = set()\n\ttext = text.replace(\"<\", \"\")\n\ttext = text.replace(\">\", \"\")\n\twhile True:\n\t\tm = re.search(tag_regex, text)\n\t\tif not m:\n\t\t\tbreak\n\t\ttag = m[1]\n\t\ttags.add(tag)\n\t\ttext = text.replace(tag, \"\")\n\t\tif replacer:\n\t\t\ttext = replacer(tag) + text\n\ttext = text.replace(\"<\", \"\")\n\ttext = text.replace(\">\", \"\")\n\treturn tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tdef replace(match: Match) -> str:\n\t\ttag = match.group(1)\n\t\tcontent = match.group(2)\n\n\t\tif replacer is not None:\n\t\t\ttry:\n\t\t\t\treturn replacer(tag, content)\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\treturn content\n\n\tpattern = r\"\\[(.*?)\\]\\((.*?)\\)\"\n\ttext = re.sub(pattern, replace, text)\n\n\tmatch = re.search(r\"\\[(.*?)\\]\", text)\n\ttags = set()\n\twhile match:\n\t\ttags.add(match.group(1))\n\t\tmatch = re.search(r\"\\[(.*?)\\]\", text)\n\n\treturn tags, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tin_tag = False\n\ttag_set = set()\n\ttag_start = -1\n\tout_text = \"\"\n\treplacer_cache = {}\n\n\tfor i, char in enumerate(text):\n\t\tif char == \"<\":\n\t\t\tin_tag = True\n\t\t\ttag_start = i\n\t\telif char == \">\":\n\t\t\tin_tag = False\n\t\t\ttag = text[tag_start + 1:i]\n\t\t\tif in_tag:\n\t\t\t\ttag_set.add(tag)\n\t\t\t\tif replacer is None:\n\t\t\t\t\tout_text += f\"<{tag}>\"\n\t\t\t\telse:\n\t\t\t\t\tout_text += f\"<{replacer(tag)}>\"\n\t\t\telse:\n\t\t\t\tout_text += f\"</{tag}>\"\n\n\tif replacer is not None:\n\t\tfor tag in tag_set:\n\t\t\treplacer_cache[tag] = replacer(tag)\n\n\treturn tag_set, out_text\n\n#src/python/replacer.py\nimport re\nimport os\nimport json\nimport subprocess\n\nfrom . import utils\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_re = re.compile(r'<([^>]+)>')\n\ttags = set()\n\tout_text = text\n\tif replacer is not None:\n\t\tout_text = tag_re.sub(lambda match: replacer(match.group(1)), text)\n\telse:\n\t\tout_text = tag_re.sub(lambda match: \"\", text)\n\tout_text = out_text.strip()\n\tfor tag in re.findall(r'<([^>]+)>', text):\n\t\ttags.add(tag)\n\treturn tags, out_text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = text.lower()\n\ttext = text.replace('<', '</')\n\ttext = text.replace('>', '/>')\n\ttext = text.replace('&', '&amp;')\n\ttext = text.replace('\"', '&quot;')\n\ttext = text.replace(\"'\", '&#39;')\n\ttext = text.replace('`', '&#96;')\n\ttext = text.replace('=', '&#61;')\n\ttext = text.replace('-', '&#45;')\n\ttext = text.replace('+', '&#43;')\n\ttext = text.replace('|', '&#124;')\n\ttext = text.replace('^', '&#124;')\n\ttext = text.replace('*', '&#42;')\n\ttext = text.replace('~', '&#42;')\n\ttext = text.replace('?', '&#63;')\n\ttext = text.replace(':', '&#58;')\n\ttext = text.replace('$', '&#36;')\n\ttext = text.replace('%', '&#37;')\n\ttext = text.replace('&lt;', '&lt;/')\n\ttext = text.replace('&gt", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttext = \"\".join(text.split(\"\\n\"))\n\n\ttags = tag_regex.findall(text)\n\ttags = set(tags)\n\n\ttext = text.replace(\"<\", \"&lt;\")\n\ttext = text.replace(\">\", \"&gt;\")\n\n\tif replacer:\n\t\ttext = replacer(text)\n\n\treturn tags, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tdef replace(tag: str) -> str:\n\t\t\"\"\"\n\t\tReplace the tag word.\n\t\t\"\"\"\n\t\tif replacer:\n\t\t\treturn replacer(tag)\n\t\telse:\n\t\t\treturn tag\n\n\tmatches = re.findall(r'<\\w+(?=\\s*=)>', text)\n\tmatches += re.findall(r'<\\w+(?=\\s+\\/)>', text)\n\tresult = text\n\tfor match in matches:\n\t\t# Remove tag from result\n\t\tresult = result.replace(match, '')\n\t\t# Replace tag with result of replacement function\n\t\tresult = result.replace(match, replace(match))\n\n\treturn set(re.findall(r'<\\w+(?=\\s*=)>', result)), result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_re = r\"<[^>]+>\"\n\ttag_matches = re.findall(tag_re, text)\n\tif len(tag_matches) == 0:\n\t\treturn set(), text\n\n\ttag_words = set([re.sub(r\"<[^>]+>\", \"\", tag) for tag in tag_matches])\n\n\tif replacer:\n\t\ttext = replacer(text, tag_words)\n\n\treturn tag_words, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tmatch = re.search(r\"<[a-zA-Z0-9_]+>([^<]*)</\\1>\", text)\n\tif not match:\n\t\treturn set(), text\n\ttags = set(match.group(1).split())\n\ttext = text.replace(match.group(), \"\")\n\tif replacer:\n\t\ttext = replacer(text, tags)\n\treturn tags, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\ttag_start = re.compile(r'<([a-zA-Z0-9_-]+)(?:.*?>|>)')\n\ttag_end = re.compile(r'</(.+)>')\n\n\ttag_stack = []\n\toriginal_text = text\n\ttext = text.strip()\n\n\tfor match in tag_start.finditer(text):\n\t\ttag = match.group(1)\n\t\tif tag == \"code\":\n\t\t\ttag_stack.append(tag)\n\t\telif tag == \"pre\":\n\t\t\ttag_stack.append(tag)\n\t\telse:\n\t\t\ttag_stack.append(tag)\n\n\t\ttag_stack.append(match.group(0))\n\n\twhile tag_stack:\n\t\ttext = tag_stack.pop()\n\t\tif text.startswith(\"</\"):\n\t\t\ttag_end = tag_end.match(text)\n\t\t\tif tag_end is None:\n\t\t\t\traise Exception(\"Unmatched closing tag: \" + text)\n\n\t\t\ttag = tag_end.group(1)\n\t\t\tif tag == \"code\":\n\t\t\t\tdel tag_stack[-1]\n\t\t\telif tag == \"pre\":\n\t\t\t\tdel tag_stack[-1]\n\t\t\telse:\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tdef replace(match: Match) -> str:\n\t\ttag = match.group(1)\n\t\tcontent = match.group(2)\n\n\t\tif replacer is not None:\n\t\t\ttry:\n\t\t\t\treturn replacer(tag, content)\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\treturn content\n\n\tpattern = r\"\\[(.*?)\\]\\((.*?)\\)\"\n\ttext = re.sub(pattern, replace, text)\n\n\tmatch = re.search(r\"\\[(.*?)\\]\", text)\n\ttags = set()\n\twhile match:\n\t\ttags.add(match.group(1))\n\t\tmatch = re.search(r\"\\[(.*?)\\]\", text)\n\n\treturn tags, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t\"\"\"\n\tFind tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.\n\t\"\"\"\n\tin_tag = False\n\ttag_set = set()\n\ttag_start = -1\n\tout_text = \"\"\n\treplacer_cache = {}\n\n\tfor i, char in enumerate(text):\n\t\tif char == \"<\":\n\t\t\tin_tag = True\n\t\t\ttag_start = i\n\t\telif char == \">\":\n\t\t\tin_tag = False\n\t\t\ttag = text[tag_start + 1:i]\n\t\t\tif in_tag:\n\t\t\t\ttag_set.add(tag)\n\t\t\t\tif replacer is None:\n\t\t\t\t\tout_text += f\"<{tag}>\"\n\t\t\t\telse:\n\t\t\t\t\tout_text += f\"<{replacer(tag)}>\"\n\t\t\telse:\n\t\t\t\tout_text += f\"</{tag}>\"\n\n\tif replacer is not None:\n\t\tfor tag in tag_set:\n\t\t\treplacer_cache[tag] = replacer(tag)\n\n\treturn tag_set, out_text\n\n#src/python/replacer.py\nimport re\nimport os\nimport json\nimport subprocess\n\nfrom . import utils\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\timport re\n\t\n\t# Find all links\n\tall_links = re.findall(r'<a href=\"(.*?)\">(.*?)</a>', text, re.IGNORECASE)\n\tif not all_links:\n\t\treturn text\n\t\n\t# Prepend base_url to links\n\tlinks = []\n\tbase_url = 'http://www.vox.com'\n\tfor link in all_links:\n\t\tlink = re.sub(r'^http://www.vox.com', base_url, link[0], 1)\n\t\tlinks.append(link)\n\t\n\t# Put back links\n\ttext = re.sub(r'<a href=\"([^\"]+)\">([^<]+)</a>', r'\\1', text)\n\tfor link in links:\n\t\ttext = text.replace(link, '<a href=\"' + link + '\">' + link + '</a>')\n\treturn text\n\n#vox/scrape_blogs.py", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\timport urllib2\n\ttry:\n\t\timport BeautifulSoup as bs\n\texcept:\n\t\timport bs4 as bs\n\timport htmlentitydefs\n\tfrom nltk import sent_tokenize, word_tokenize\n\tfrom nltk.corpus import stopwords\n\tfrom nltk.stem.porter import PorterStemmer\n\tfrom nltk.tokenize import TweetTokenizer\n\tfrom nltk.stem import WordNetLemmatizer\n\tfrom nltk.corpus import wordnet\n\n\tdef clean_html_tags(text):\n\t\t\"\"\"\n\t\tRemove html tags from text.\n\t\t\"\"\"\n\t\tcleanr = re.compile('<.*?>')\n\t\treturn re.sub(cleanr, '', text)\n\n\tdef clean_url(text):\n\t\t\"\"\"\n\t\tRemove URLs and their associated links from text.\n\t\t\"\"\"\n\t\turlregex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[#]|[!]|[\\(]|[)\\]|[;]|[:]|[@]|[,]|[?]|[=]|", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = linkify_text(text)\n\ttext = add_link_attributes(text)\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(r\"\\[\\S+\\]\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">%s</a>\" % (x.group(0)[1:-1], x.group(0)[1:-1]), text)\n\ttext = re.sub(r\"\\(http://www.google.com/search?q=\\S+\\)\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">%s</a>\" % (x.group(0)[1:-1], x.group(0)[1:-1]), text)\n\ttext = re.sub(r\"\\(http://www.google.com/search?q=\\S+\\)\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">%s</a>\" % (x.group(0)[1:-1], x.group(0)[1:-1]), text)\n\ttext = re.sub(r\"\\(http://www.google.com/search?q=\\S+\\)\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tdef handle_link(match):\n\t\tlink = match.group(0)\n\t\ttitle = link\n\t\turl = link\n\t\thtml = '<a href=\"%s\" target=\"_blank\" title=\"%s\">%s</a>' % (url, title, link)\n\t\treturn html\n\ttext = re.sub(r'\\[(.*?)\\]\\((.*?)\\)', handle_link, text)\n\treturn text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom bs4 import BeautifulSoup\n\tsoup = BeautifulSoup(text, \"lxml\")\n\tfor link in soup.find_all('a'):\n\t\tlink['data-original-title'] = link.get('title', '')\n\t\tlink['title'] = ''\n\t\tlink['href'] = link.get('href', '')\n\t\ta = soup.new_tag(\"a\")\n\t\ta['href'] = link['href']\n\t\ta['title'] = link['data-original-title']\n\t\tlink.replace_with(a)\n\treturn str(soup)\n\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(r\"\\s+\", \" \", text)\n\n\t# Add some attributes\n\ttext = re.sub(r\"\\(([^\\)]+)\\)\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\[([^\\]]+)\\]\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\{([^\\}]+)\\}\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\[([^\\]]+)\\]\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\(([^\\)]+)\\)\", r'<span class=\"linkified\">\\1</span>', text)\n\n\t# Linkify links\n\ttext = re.sub(r\"\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\timport urllib\n\tfrom BeautifulSoup import BeautifulSoup\n\t\n\ttry:\n\t\thtml = BeautifulSoup(text)\n\texcept:\n\t\treturn text\n\n\t# Get all links in text\n\tlinks = []\n\tfor link in html.findAll('a'):\n\t\tlinks.append(link.get('href'))\n\n\t# Find all textual links\n\tlinks = [link for link in links if link.find('http') == 0]\n\n\t# Remove HTML fragments\n\tlinks = [link for link in links if link.find('<') == -1]\n\n\t# Process links\n\tfor i,link in enumerate(links):\n\t\tif link.find('@') > 0:\n\t\t\tlinks[i] = 'mailto:' + link\n\t\telif link.find('http') == 0:\n\t\t\tlinks[i] = link\n\t\telse:\n\t\t\tlinks[i] = 'http://' + link\n\n\t# Replace links\n\tnew_text = text\n\tfor link in links:\n\t\tnew_text = re.sub(link, '<a href=\"' + link + '\">' + link + '</a>', new_text)\n\n\treturn new_text\n", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace(\"\\n\", \" \")\n\ttext = text.replace(\"\\t\", \" \")\n\ttext = text.replace(\"\\r\", \" \")\n\n\t# strip off excess white space\n\tstart = 0\n\twhile start < len(text) and text[start].isspace():\n\t\tstart += 1\n\tend = len(text)\n\twhile end > 0 and text[end - 1].isspace():\n\t\tend -= 1\n\ttext = text[start:end]\n\n\t# split on newlines / tabs\n\ttext = text.split(\"\\n\")\n\ttext = [x for x in text if x]\n\n\tfor i in xrange(len(text)):\n\t\t# replace links\n\t\ttext[i] = text[i].replace(\"www.youtube.com\", \"youtube.com\")\n\t\ttext[i] = text[i].replace(\"youtu.be\", \"youtube.com\")\n\n\t\t# add attributes\n\t\ttext[i] = text[i].replace(\"watch?v=\", \"v/\")\n\t\ttext[i] = text[i].replace(\"watch?feature=player_embedded\", \"v/embed/\")\n\t\ttext[i] = text[i].replace(\"youtu.be/\", \"youtube.com/", "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\tfrom HTMLParser import HTMLParser\n\tfrom BeautifulSoup import BeautifulSoup\n\t\n\t# remove html tags\n\ttext = re.sub(r'<.*?>', '', text)\n\t# replace any newlines with a space\n\ttext = re.sub(r'\\n', ' ', text)\n\t# remove links\n\ttext = re.sub(r'(\\s|^)(?:(?:https?|ftp|file)://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`()<>]*)', '', text)\n\t# replace image links with image tags\n\ttext = re.sub(r'<img.*?src=[\\'\"]?([^\\'\" >]+)', '<img src=\"\\1\"', text)\n\t# replace links with anchor tags\n\ttext = re.sub(r'<a.*?href=[\\'\"]?([^\\'\" >]+)', '<a href=\"\\"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\t\n\timport re\n\t\n\t# Find all links\n\tall_links = re.findall(r'<a href=\"(.*?)\">(.*?)</a>', text, re.IGNORECASE)\n\tif not all_links:\n\t\treturn text\n\t\n\t# Prepend base_url to links\n\tlinks = []\n\tbase_url = 'http://www.vox.com'\n\tfor link in all_links:\n\t\tlink = re.sub(r'^http://www.vox.com', base_url, link[0], 1)\n\t\tlinks.append(link)\n\t\n\t# Put back links\n\ttext = re.sub(r'<a href=\"([^\"]+)\">([^<]+)</a>', r'\\1', text)\n\tfor link in links:\n\t\ttext = text.replace(link, '<a href=\"' + link + '\">' + link + '</a>')\n\treturn text\n\n#vox/scrape_blogs.py", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\timport urllib2\n\ttry:\n\t\timport BeautifulSoup as bs\n\texcept:\n\t\timport bs4 as bs\n\timport htmlentitydefs\n\tfrom nltk import sent_tokenize, word_tokenize\n\tfrom nltk.corpus import stopwords\n\tfrom nltk.stem.porter import PorterStemmer\n\tfrom nltk.tokenize import TweetTokenizer\n\tfrom nltk.stem import WordNetLemmatizer\n\tfrom nltk.corpus import wordnet\n\n\tdef clean_html_tags(text):\n\t\t\"\"\"\n\t\tRemove html tags from text.\n\t\t\"\"\"\n\t\tcleanr = re.compile('<.*?>')\n\t\treturn re.sub(cleanr, '', text)\n\n\tdef clean_url(text):\n\t\t\"\"\"\n\t\tRemove URLs and their associated links from text.\n\t\t\"\"\"\n\t\turlregex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[#]|[!]|[\\(]|[)\\]|[;]|[:]|[@]|[,]|[?]|[=]|", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = linkify_text(text)\n\ttext = add_link_attributes(text)\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(r\"\\[\\S+\\]\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">%s</a>\" % (x.group(0)[1:-1], x.group(0)[1:-1]), text)\n\ttext = re.sub(r\"\\(http://www.google.com/search?q=\\S+\\)\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">%s</a>\" % (x.group(0)[1:-1], x.group(0)[1:-1]), text)\n\ttext = re.sub(r\"\\(http://www.google.com/search?q=\\S+\\)\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">%s</a>\" % (x.group(0)[1:-1], x.group(0)[1:-1]), text)\n\ttext = re.sub(r\"\\(http://www.google.com/search?q=\\S+\\)\", lambda x: \"<a href=\\\"http://www.google.com/search?q=%s\\\">", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tdef handle_link(match):\n\t\tlink = match.group(0)\n\t\ttitle = link\n\t\turl = link\n\t\thtml = '<a href=\"%s\" target=\"_blank\" title=\"%s\">%s</a>' % (url, title, link)\n\t\treturn html\n\ttext = re.sub(r'\\[(.*?)\\]\\((.*?)\\)', handle_link, text)\n\treturn text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\tfrom bs4 import BeautifulSoup\n\tsoup = BeautifulSoup(text, \"lxml\")\n\tfor link in soup.find_all('a'):\n\t\tlink['data-original-title'] = link.get('title', '')\n\t\tlink['title'] = ''\n\t\tlink['href'] = link.get('href', '')\n\t\ta = soup.new_tag(\"a\")\n\t\ta['href'] = link['href']\n\t\ta['title'] = link['data-original-title']\n\t\tlink.replace_with(a)\n\treturn str(soup)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = re.sub(r\"\\s+\", \" \", text)\n\n\t# Add some attributes\n\ttext = re.sub(r\"\\(([^\\)]+)\\)\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\[([^\\]]+)\\]\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\{([^\\}]+)\\}\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\[([^\\]]+)\\]\", r'<span class=\"linkified\">\\1</span>', text)\n\ttext = re.sub(r\"\\(([^\\)]+)\\)\", r'<span class=\"linkified\">\\1</span>', text)\n\n\t# Linkify links\n\ttext = re.sub(r\"\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\timport urllib\n\tfrom BeautifulSoup import BeautifulSoup\n\t\n\ttry:\n\t\thtml = BeautifulSoup(text)\n\texcept:\n\t\treturn text\n\n\t# Get all links in text\n\tlinks = []\n\tfor link in html.findAll('a'):\n\t\tlinks.append(link.get('href'))\n\n\t# Find all textual links\n\tlinks = [link for link in links if link.find('http') == 0]\n\n\t# Remove HTML fragments\n\tlinks = [link for link in links if link.find('<') == -1]\n\n\t# Process links\n\tfor i,link in enumerate(links):\n\t\tif link.find('@') > 0:\n\t\t\tlinks[i] = 'mailto:' + link\n\t\telif link.find('http') == 0:\n\t\t\tlinks[i] = link\n\t\telse:\n\t\t\tlinks[i] = 'http://' + link\n\n\t# Replace links\n\tnew_text = text\n\tfor link in links:\n\t\tnew_text = re.sub(link, '<a href=\"' + link + '\">' + link + '</a>', new_text)\n\n\treturn new_text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\ttext = text.replace(\"\\n\", \" \")\n\ttext = text.replace(\"\\t\", \" \")\n\ttext = text.replace(\"\\r\", \" \")\n\n\t# strip off excess white space\n\tstart = 0\n\twhile start < len(text) and text[start].isspace():\n\t\tstart += 1\n\tend = len(text)\n\twhile end > 0 and text[end - 1].isspace():\n\t\tend -= 1\n\ttext = text[start:end]\n\n\t# split on newlines / tabs\n\ttext = text.split(\"\\n\")\n\ttext = [x for x in text if x]\n\n\tfor i in xrange(len(text)):\n\t\t# replace links\n\t\ttext[i] = text[i].replace(\"www.youtube.com\", \"youtube.com\")\n\t\ttext[i] = text[i].replace(\"youtu.be\", \"youtube.com\")\n\n\t\t# add attributes\n\t\ttext[i] = text[i].replace(\"watch?v=\", \"v/\")\n\t\ttext[i] = text[i].replace(\"watch?feature=player_embedded\", \"v/embed/\")\n\t\ttext[i] = text[i].replace(\"youtu.be/\", \"youtube.com/", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t\"\"\"\n\tProcess links in text, adding some attributes and linkifying textual links.\n\t\"\"\"\n\timport re\n\tfrom HTMLParser import HTMLParser\n\tfrom BeautifulSoup import BeautifulSoup\n\t\n\t# remove html tags\n\ttext = re.sub(r'<.*?>', '', text)\n\t# replace any newlines with a space\n\ttext = re.sub(r'\\n', ' ', text)\n\t# remove links\n\ttext = re.sub(r'(\\s|^)(?:(?:https?|ftp|file)://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`()<>]*)', '', text)\n\t# replace image links with image tags\n\ttext = re.sub(r'<img.*?src=[\\'\"]?([^\\'\" >]+)', '<img src=\"\\1\"', text)\n\t# replace links with anchor tags\n\ttext = re.sub(r'<a.*?href=[\\'\"]?([^\\'\" >]+)', '<a href=\"\\", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest: Request = Request(url, headers={\"User-Agent\": USER_AGENT})\n\t\tresponse: Response = urlopen(request)\n\t\treturn response.headers.get_content_type()\n\texcept:\n\t\treturn None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' }\n\trequest = Request(url, headers=headers)\n\ttry:\n\t\tresponse = urlopen(request, timeout=30)\n\t\treturn response.headers.get('Content-Type')\n\texcept:\n\t\tprint('Error, fetching content type')\n\t\treturn None\n\n#crawler/__init__.py\nfrom .fetch_content_type import fetch_content_type\nfrom .fetch_response import fetch_response\n\n#crawler/crawl.py\nimport os\nimport pickle\n\nfrom crawler import fetch_content_type, fetch_response\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = {\n\t\t'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36',\n\t\t'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n\t}\n\t\n\ttry:\n\t\tr = requests.get(url, headers=headers)\n\t\tcontent_type = r.headers['Content-Type']\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\treturn content_type\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\trequest = Request(url, headers={'User-Agent': USER_AGENT})\n\twith urlopen(request) as response:\n\t\treturn response.headers.get('content-type')", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = urllib.request.Request(url, method='HEAD')\n\t\trequest.add_header('User-Agent', user_agent)\n\t\twith urllib.request.urlopen(request) as f:\n\t\t\tresponse = f.info()\n\t\t\treturn response.get('Content-Type')\n\texcept urllib.error.HTTPError as e:\n\t\tlogging.debug(f'{e.code} - {e.reason}')\n\t\treturn None\n\n#src/main.py\n#!/usr/bin/env python\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\n\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\n\nimport config\nfrom utils import fetch_content_type\n\nif __name__ == '__main__':\n\tload_dotenv()\n\n\t# Logging\n\tlogging.basicConfig(level=logging.INFO)\n\tlogging.getLogger('urllib3').setLevel(logging.ERROR)\n\tlogging.getLogger('urllib3.connectionpool').setLevel(logging.ERROR)\n\n\t# Config\n\tuser_agent = config.user_agent\n\turl = config.url\n\tdownload_directory = config.download_directory\n\n\tlogging.info(f", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = { \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\" }\n\tresponse = requests.head(url, headers=headers)\n\tcontent_type = response.headers.get(\"Content-Type\")\n\treturn content_type if content_type is not None else None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = {'User-Agent': USER_AGENT}\n\trequest = Request(url, headers=headers)\n\ttry:\n\t\tres = urlopen(request)\n\t\theaders = res.headers\n\texcept:\n\t\treturn None\n\n\tcontent_type = headers.get('Content-Type', None)\n\t#print(content_type)\n\treturn content_type\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tuser_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.7 Safari/605.1.15'\n\theaders = {'User-Agent': user_agent}\n\ttry:\n\t\tresponse = requests.get(url, headers=headers)\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(e)\n\t\treturn None\n\treturn response.headers.get('Content-Type', None)\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\treq = requests.get(url, headers={'USER-AGENT': DEFAULT_USER_AGENT})\n\tif req.headers.get('content-type') is None:\n\t\treturn None\n\treturn req.headers.get('content-type')\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treq = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n\t\tresponse = urllib2.urlopen(req)\n\t\treturn response.info().get('Content-Type', None)\n\texcept Exception:\n\t\treturn None"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest: Request = Request(url, headers={\"User-Agent\": USER_AGENT})\n\t\tresponse: Response = urlopen(request)\n\t\treturn response.headers.get_content_type()\n\texcept:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = { 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36' }\n\trequest = Request(url, headers=headers)\n\ttry:\n\t\tresponse = urlopen(request, timeout=30)\n\t\treturn response.headers.get('Content-Type')\n\texcept:\n\t\tprint('Error, fetching content type')\n\t\treturn None\n\n#crawler/__init__.py\nfrom .fetch_content_type import fetch_content_type\nfrom .fetch_response import fetch_response\n\n#crawler/crawl.py\nimport os\nimport pickle\n\nfrom crawler import fetch_content_type, fetch_response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = {\n\t\t'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.71 Safari/537.36',\n\t\t'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n\t}\n\t\n\ttry:\n\t\tr = requests.get(url, headers=headers)\n\t\tcontent_type = r.headers['Content-Type']\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\treturn content_type\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\trequest = Request(url, headers={'User-Agent': USER_AGENT})\n\twith urlopen(request) as response:\n\t\treturn response.headers.get('content-type')", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\trequest = urllib.request.Request(url, method='HEAD')\n\t\trequest.add_header('User-Agent', user_agent)\n\t\twith urllib.request.urlopen(request) as f:\n\t\t\tresponse = f.info()\n\t\t\treturn response.get('Content-Type')\n\texcept urllib.error.HTTPError as e:\n\t\tlogging.debug(f'{e.code} - {e.reason}')\n\t\treturn None\n\n#src/main.py\n#!/usr/bin/env python\n\nimport datetime\nimport logging\nimport os\nimport sys\nimport time\n\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\n\nimport config\nfrom utils import fetch_content_type\n\nif __name__ == '__main__':\n\tload_dotenv()\n\n\t# Logging\n\tlogging.basicConfig(level=logging.INFO)\n\tlogging.getLogger('urllib3').setLevel(logging.ERROR)\n\tlogging.getLogger('urllib3.connectionpool').setLevel(logging.ERROR)\n\n\t# Config\n\tuser_agent = config.user_agent\n\turl = config.url\n\tdownload_directory = config.download_directory\n\n\tlogging.info(f", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = { \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\" }\n\tresponse = requests.head(url, headers=headers)\n\tcontent_type = response.headers.get(\"Content-Type\")\n\treturn content_type if content_type is not None else None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\theaders = {'User-Agent': USER_AGENT}\n\trequest = Request(url, headers=headers)\n\ttry:\n\t\tres = urlopen(request)\n\t\theaders = res.headers\n\texcept:\n\t\treturn None\n\n\tcontent_type = headers.get('Content-Type', None)\n\t#print(content_type)\n\treturn content_type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\tuser_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.7 Safari/605.1.15'\n\theaders = {'User-Agent': user_agent}\n\ttry:\n\t\tresponse = requests.get(url, headers=headers)\n\texcept requests.exceptions.RequestException as e:\n\t\tprint(e)\n\t\treturn None\n\treturn response.headers.get('Content-Type', None)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\treq = requests.get(url, headers={'USER-AGENT': DEFAULT_USER_AGENT})\n\tif req.headers.get('content-type') is None:\n\t\treturn None\n\treturn req.headers.get('content-type')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\t\"\"\"\n\tSet the head of the request through the URL and USER_AGENT.\n\t\"\"\"\n\ttry:\n\t\treq = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n\t\tresponse = urllib2.urlopen(req)\n\t\treturn response.info().get('Content-Type', None)\n\texcept Exception:\n\t\treturn None", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tcharacters = set(tag)\n\tcharacters.discard('_')\n\t\n\tif not characters.issubset(LEEGAL_TAG_CHARS):\n\t\treturn True\n\t\n\treturn False\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tLEEGAL_TAG_CHARS = \"0123456789_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\t\n\treturn any(char in LEGAL_TAG_CHARS for char in tag)\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t# the LEEGAL_TAG_CHARS is a string containing all the ASCII characters that are part of a tag.\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_\"\n\n\t# we iterate through each character in the LEEGAL_TAG_CHARS\n\tfor char in LEEGAL_TAG_CHARS:\n\t\t# if the current character is in the tag, return False\n\t\tif char in tag:\n\t\t\treturn False\n\t# if we haven't returned False yet, return True\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tLEEGAL_TAG_CHARS = 'abcdefghijklmnopqrstuvwxyz'\n\tif tag.lower() not in LEEGAL_TAG_CHARS:\n\t\treturn False\n\treturn True", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tLEEGAL_TAG_CHARS = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_\"\n\t\n\ttag_characters = list(tag)\n\tfor c in range(len(tag_characters)):\n\t\tif tag_characters[c] not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\t\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t# check whether the tag is within the allowed tags\n\tif tag not in LEEGAL_TAG_CHARS:\n\t\treturn False\n\t\n\t# check whether the tag is the only character in the tag\n\tif tag.count(tag[0]) == len(tag):\n\t\treturn False\n\t\n\treturn True\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_.~'\n\tif tag[0] not in LEEGAL_TAG_CHARS:\n\t\treturn False\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\t\n\treturn True\n\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t# Creates a set with the characters that belong to a tag.\n\ttag_set = set(LEEGAL_TAG_CHARS)\n\t\n\t# Checks whether each character belongs to a tag.\n\treturn not any(map(lambda c: c not in tag_set, tag))\n\n", "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\ttag_char_chars = [\n\t\tchar\n\t\tfor char in LEEGAL_TAG_CHARS\n\t]\n\treturn tag not in tag_char_chars\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tcharacters = set(tag)\n\tcharacters.discard('_')\n\t\n\tif not characters.issubset(LEEGAL_TAG_CHARS):\n\t\treturn True\n\t\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tLEEGAL_TAG_CHARS = \"0123456789_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\t\n\treturn any(char in LEGAL_TAG_CHARS for char in tag)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t# the LEEGAL_TAG_CHARS is a string containing all the ASCII characters that are part of a tag.\n\tLEEGAL_TAG_CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_\"\n\n\t# we iterate through each character in the LEEGAL_TAG_CHARS\n\tfor char in LEEGAL_TAG_CHARS:\n\t\t# if the current character is in the tag, return False\n\t\tif char in tag:\n\t\t\treturn False\n\t# if we haven't returned False yet, return True\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tLEEGAL_TAG_CHARS = 'abcdefghijklmnopqrstuvwxyz'\n\tif tag.lower() not in LEEGAL_TAG_CHARS:\n\t\treturn False\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\tLEEGAL_TAG_CHARS = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ-_\"\n\t\n\ttag_characters = list(tag)\n\tfor c in range(len(tag_characters)):\n\t\tif tag_characters[c] not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t# check whether the tag is within the allowed tags\n\tif tag not in LEEGAL_TAG_CHARS:\n\t\treturn False\n\t\n\t# check whether the tag is the only character in the tag\n\tif tag.count(tag[0]) == len(tag):\n\t\treturn False\n\t\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\tLEEGAL_TAG_CHARS = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-_.~'\n\tif tag[0] not in LEEGAL_TAG_CHARS:\n\t\treturn False\n\tfor c in tag:\n\t\tif c not in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\t\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\t\n\t# Creates a set with the characters that belong to a tag.\n\ttag_set = set(LEEGAL_TAG_CHARS)\n\t\n\t# Checks whether each character belongs to a tag.\n\treturn not any(map(lambda c: c not in tag_set, tag))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\t\"\"\"\n\tChecks whether each character in the LEEGAL_TAG_CHARS belongs to a tag. If any character belongs to a tag, the value False is returned. Otherwise, the value True is returned.\n\t\"\"\"\n\ttag_char_chars = [\n\t\tchar\n\t\tfor char in LEEGAL_TAG_CHARS\n\t]\n\treturn tag not in tag_char_chars\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tresult = {}\n\tfor child in node.getchildren():\n\t\tresult[child.tag] = child.text\n\treturn result\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.iter():\n\t\tchildren[child.tag] = child.text\n\treturn children\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren_dict = {}\n\tfor child in node.getchildren():\n\t\tchildren_dict[child.tag] = xml_children_as_dict(child)\n\treturn children_dict\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\tchildren[child.tag] = child\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tchildren[child.tag] = child.text\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {c.tag : c for c in node}\n\tif '{' in children['<xml>'].text:\n\t\tchildren['<xml>'].text = children['<xml>'].text.replace('{','').replace('}','').replace('\\n','')\n\treturn children\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tret = {}\n\tfor child in node:\n\t\tret[child.tag] = child.text\n\treturn ret\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node.iter():\n\t\td.setdefault(child.tag, []).append(child.text)\n\treturn d\n\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tchildren = list(node)\n\tif children:\n\t\tfor child in children:\n\t\t\td[child.tag] = child.text\n\treturn d\n", "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node.getchildren():\n\t\tif child.text:\n\t\t\td[child.tag] = child.text\n\t\telse:\n\t\t\td[child.tag] = xml_children_as_dict(child)\n\treturn d\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tresult = {}\n\tfor child in node.getchildren():\n\t\tresult[child.tag] = child.text\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.iter():\n\t\tchildren[child.tag] = child.text\n\treturn children\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren_dict = {}\n\tfor child in node.getchildren():\n\t\tchildren_dict[child.tag] = xml_children_as_dict(child)\n\treturn children_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node.getchildren():\n\t\tchildren[child.tag] = child\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {}\n\tfor child in node:\n\t\tchildren[child.tag] = child.text\n\treturn children\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tchildren = {c.tag : c for c in node}\n\tif '{' in children['<xml>'].text:\n\t\tchildren['<xml>'].text = children['<xml>'].text.replace('{','').replace('}','').replace('\\n','')\n\treturn children\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\tret = {}\n\tfor child in node:\n\t\tret[child.tag] = child.text\n\treturn ret\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node.iter():\n\t\td.setdefault(child.tag, []).append(child.text)\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tchildren = list(node)\n\tif children:\n\t\tfor child in children:\n\t\t\td[child.tag] = child.text\n\treturn d\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\t\"\"\"\n\tTurn the children of node <xml> into a dict, keyed by tag name.\n\t\"\"\"\n\td = {}\n\tfor child in node.getchildren():\n\t\tif child.text:\n\t\t\td[child.tag] = child.text\n\t\telse:\n\t\t\td[child.tag] = xml_children_as_dict(child)\n\treturn d\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\treturn sender_handle == entity_handle\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\treturn sender_handle == entity_handle", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tis_match = False\n\tif sende_handle == entity_handle:\n\t\tis_match = True\n\treturn is_match", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tprint('Checking sender handle and entity handle...')\n\tprint('Sender handle: ' + sender_handle)\n\tprint('Entity handle: ' + entity_handle)\n\tif sender_handle == entity_handle:\n\t\tprint('Sender handle matches entity handle.')\n\t\treturn True\n\telse:\n\t\tprint('Sender handle does not match entity handle.')\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\ttry:\n\t\tis_match = sender_handle == entity_handle\n\texcept TypeError:\n\t\tis_match = False\n\tif is_match:\n\t\treturn True\n\telse:\n\t\treturn False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\treturn sender_handle == entity_handle\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\treturn sender_handle == entity_handle", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tis_match = False\n\tif sende_handle == entity_handle:\n\t\tis_match = True\n\treturn is_match", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\tprint('Checking sender handle and entity handle...')\n\tprint('Sender handle: ' + sender_handle)\n\tprint('Entity handle: ' + entity_handle)\n\tif sender_handle == entity_handle:\n\t\tprint('Sender handle matches entity handle.')\n\t\treturn True\n\telse:\n\t\tprint('Sender handle does not match entity handle.')\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\t\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\t\"\"\"\n\tCheck whether sender_handle is the same as entity_handle. If yes, return True. Otherwise, return False.\n\t\"\"\"\n\ttry:\n\t\tis_match = sender_handle == entity_handle\n\texcept TypeError:\n\t\tis_match = False\n\tif is_match:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tresponse = requests.get(url).content\n\troot = json.loads(response)\n\tdocument_path = root.get('document_path') or document_path\n\tnodeinfo_url = root.get('nodeinfo_url')\n\treturn {\n\t\t'document_path': document_path,\n\t\t'nodeinfo_url': nodeinfo_url\n\t}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_url = url.split(':')[0]\n\turl_path = '/'.join(url.split(':')[1].split('/')[1:])\n\t\n\tnodeinfo_info = {}\n\tnodeinfo_info['url'] = nodeinfo_url\n\tnodeinfo_info['path'] = url_path\n\t\n\tif document_path:\n\t\tnodeinfo_info['document_path'] = document_path\n\telse:\n\t\tnodeinfo_info['document_path'] = url_path\n\t\n\treturn nodeinfo_info\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\twell_known_document = []\n\t\n\twell_known_document.append({'url': url})\n\tif document_path is not None:\n\t\twell_known_document.append({'document_path': document_path})\n\t\n\treturn well_known_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\ttry:\n\t\turl_info = urlparse(url)\n\texcept:\n\t\treturn None\n\t\n\tdocument_path = document_path or url_info.path\n\tpath = os.path.normpath(os.path.dirname(document_path))\n\t\n\tnodeinfo_dict = {\n\t\t\"document_path\": document_path,\n\t\t\"url\": url,\n\t\t\"path\": path,\n\t\t\"document_url\": url_info.geturl(),\n\t\t\"url_scheme\": url_info.scheme,\n\t\t\"url_host\": url_info.hostname,\n\t\t\"url_path\": url_info.path,\n\t\t\"url_query\": url_info.query,\n\t\t\"url_fragment\": url_info.fragment,\n\t}\n\t\n\tif url_info.scheme == \"http\":\n\t\tnodeinfo_dict[\"url_port\"] = url_info.port\n\t\tnodeinfo_dict[\"url_netloc\"] = url_info.netloc\n\telif url_info.scheme == \"https\":\n\t\tnodeinfo_dict[\"url_netloc\"] = url_info.netloc\n\t\tnodeinfo_dict[\"url_port\"] = url_info.port\n\t\n\treturn nodeinfo_dict", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tresponse = requests.get(url)\n\tif response.status_code == 200:\n\t\ttry:\n\t\t\tjson_response = response.json()\n\t\t\tnodeinfo_url = json_response['links']['self']\n\t\texcept:\n\t\t\tnodeinfo_url = url\n\t\tnodeinfo_dict = {'url': nodeinfo_url,\n\t\t\t'document': document_path}\n\t\treturn nodeinfo_dict\n\telse:\n\t\traise Exception(\"Request Status Code was not 200: %s\" % response.status_code)\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\twell_known_document = {}\n\twell_known_document['url'] = url\n\tif document_path:\n\t\twell_known_document['document_path'] = document_path\n\treturn well_known_document", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tdef get_nodeinfo_well_known_document_url(url):\n\t\treturn url.replace(\"http://\", \"https://\").replace(\"https://\", \"https://\") + \".well-known\"\n\n\tdef get_nodeinfo_well_known_document_path(document_path):\n\t\tdocument_path = document_path.replace(\"https://\", \"https://\").replace(\"http://\", \"https://\").replace(\"https://\", \"https://\")\n\t\treturn \"/.well-known/\" + document_path\n\n\tnodeinfo_well_known_document_url = get_nodeinfo_well_known_document_url(url)\n\n\tif document_path:\n\t\tnodeinfo_well_known_document_path = get_nodeinfo_well_known_document_path(document_path)\n\telse:\n\t\tnodeinfo_well_known_document_path = None\n\n\treturn {\n\t\t\"url\": nodeinfo_well_known_document_url,\n\t\t\"path\": nodeinfo_well_known_document_path\n\t}", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\tif document_path is None:\n\t\tdocument_path = os.path.basename(url)\n\t\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\ttry:\n\t\turl = url.rstrip(\"/\")\n\t\turl = url.split(\"?\", 1)[0]\n\t\turl = url.split(\"#\", 1)[0]\n\t\turl = url.split(\"//\", 1)[1]\n\texcept IndexError:\n\t\turl = url\n\ttry:\n\t\tdocument_path = document_path.rstrip(\"/\")\n\t\tdocument_path = document_path.split(\"?\", 1)[0]\n\t\tdocument_path = document_path.split(\"#\", 1)[0]\n\t\tdocument_path = document_path.split(\"//\", 1)[1]\n\texcept IndexError:\n\t\tdocument_path = url\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {}\n\tnodeinfo_well_known_document['url'] = url\n\tif document_path:\n\t\tnodeinfo_well_known_document['document_path'] = document_path\n\treturn nodeinfo_well_known_document\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tresponse = requests.get(url).content\n\troot = json.loads(response)\n\tdocument_path = root.get('document_path') or document_path\n\tnodeinfo_url = root.get('nodeinfo_url')\n\treturn {\n\t\t'document_path': document_path,\n\t\t'nodeinfo_url': nodeinfo_url\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_url = url.split(':')[0]\n\turl_path = '/'.join(url.split(':')[1].split('/')[1:])\n\t\n\tnodeinfo_info = {}\n\tnodeinfo_info['url'] = nodeinfo_url\n\tnodeinfo_info['path'] = url_path\n\t\n\tif document_path:\n\t\tnodeinfo_info['document_path'] = document_path\n\telse:\n\t\tnodeinfo_info['document_path'] = url_path\n\t\n\treturn nodeinfo_info\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\twell_known_document = []\n\t\n\twell_known_document.append({'url': url})\n\tif document_path is not None:\n\t\twell_known_document.append({'document_path': document_path})\n\t\n\treturn well_known_document", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\ttry:\n\t\turl_info = urlparse(url)\n\texcept:\n\t\treturn None\n\t\n\tdocument_path = document_path or url_info.path\n\tpath = os.path.normpath(os.path.dirname(document_path))\n\t\n\tnodeinfo_dict = {\n\t\t\"document_path\": document_path,\n\t\t\"url\": url,\n\t\t\"path\": path,\n\t\t\"document_url\": url_info.geturl(),\n\t\t\"url_scheme\": url_info.scheme,\n\t\t\"url_host\": url_info.hostname,\n\t\t\"url_path\": url_info.path,\n\t\t\"url_query\": url_info.query,\n\t\t\"url_fragment\": url_info.fragment,\n\t}\n\t\n\tif url_info.scheme == \"http\":\n\t\tnodeinfo_dict[\"url_port\"] = url_info.port\n\t\tnodeinfo_dict[\"url_netloc\"] = url_info.netloc\n\telif url_info.scheme == \"https\":\n\t\tnodeinfo_dict[\"url_netloc\"] = url_info.netloc\n\t\tnodeinfo_dict[\"url_port\"] = url_info.port\n\t\n\treturn nodeinfo_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tresponse = requests.get(url)\n\tif response.status_code == 200:\n\t\ttry:\n\t\t\tjson_response = response.json()\n\t\t\tnodeinfo_url = json_response['links']['self']\n\t\texcept:\n\t\t\tnodeinfo_url = url\n\t\tnodeinfo_dict = {'url': nodeinfo_url,\n\t\t\t'document': document_path}\n\t\treturn nodeinfo_dict\n\telse:\n\t\traise Exception(\"Request Status Code was not 200: %s\" % response.status_code)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\twell_known_document = {}\n\twell_known_document['url'] = url\n\tif document_path:\n\t\twell_known_document['document_path'] = document_path\n\treturn well_known_document", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tdef get_nodeinfo_well_known_document_url(url):\n\t\treturn url.replace(\"http://\", \"https://\").replace(\"https://\", \"https://\") + \".well-known\"\n\n\tdef get_nodeinfo_well_known_document_path(document_path):\n\t\tdocument_path = document_path.replace(\"https://\", \"https://\").replace(\"http://\", \"https://\").replace(\"https://\", \"https://\")\n\t\treturn \"/.well-known/\" + document_path\n\n\tnodeinfo_well_known_document_url = get_nodeinfo_well_known_document_url(url)\n\n\tif document_path:\n\t\tnodeinfo_well_known_document_path = get_nodeinfo_well_known_document_path(document_path)\n\telse:\n\t\tnodeinfo_well_known_document_path = None\n\n\treturn {\n\t\t\"url\": nodeinfo_well_known_document_url,\n\t\t\"path\": nodeinfo_well_known_document_path\n\t}", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\t\n\tif document_path is None:\n\t\tdocument_path = os.path.basename(url)\n\t\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\ttry:\n\t\turl = url.rstrip(\"/\")\n\t\turl = url.split(\"?\", 1)[0]\n\t\turl = url.split(\"#\", 1)[0]\n\t\turl = url.split(\"//\", 1)[1]\n\texcept IndexError:\n\t\turl = url\n\ttry:\n\t\tdocument_path = document_path.rstrip(\"/\")\n\t\tdocument_path = document_path.split(\"?\", 1)[0]\n\t\tdocument_path = document_path.split(\"#\", 1)[0]\n\t\tdocument_path = document_path.split(\"//\", 1)[1]\n\texcept IndexError:\n\t\tdocument_path = url\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\"\"\"\n\tReturns a formatted dictionary, including information such as url and document_path.\n\t\"\"\"\n\tnodeinfo_well_known_document = {}\n\tnodeinfo_well_known_document['url'] = url\n\tif document_path:\n\t\tnodeinfo_well_known_document['document_path'] = document_path\n\treturn nodeinfo_well_known_document\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom cryptography.hazmat.backends import default_backend\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives import hashes\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives import serialization\n\tfrom cryptography.hazmat.primitives.asymmetric import padding\n\tfrom cryptography.hazmat.primitives.asymmetric import utils\n\tfrom cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePublicKey\n\tfrom cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePrivateKey\n\tfrom cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePrivateKey\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives import hashes\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.prim", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom lxml import etree\n\tfrom lxml.etree import XMLSyntaxError\n\tfrom lxml.etree import XMLSchema\n\timport base64\n\t\n\t# Decode the signature, which is a Base64 encoded XML string.\n\tdecoded_signature = base64.decodestring(signature)\n\t\n\t# Create an XML schema object from the XML schema file.\n\tschema = XMLSchema(XMLSchema.XMLNS_ATTRIBUTE_NS_PREFIX)\n\tschema.assertValid(etree.fromstring(decoded_signature))\n\t\n\t# Parse the XML into an object.\n\ttry:\n\t\txml = etree.fromstring(doc)\n\texcept XMLSyntaxError:\n\t\t# The XML is malformed.\n\t\treturn False\n\t\n\t# Check that the signature is signed by the public key.\n\ttry:\n\t\txml.verify(public_key, schema)\n\texcept etree.DocumentInvalid:\n\t\t# The XML is signed by the wrong key.\n\t\treturn False\n\t\n\t# The signature was verified successfully.\n\treturn True\n\n#src/tests/test_verify_relayable_signature.py\nimport unittest\n\nfrom verify_relayable_signature import verify_relayable_signature\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport hashlib\n\timport base64\n\timport xml.etree.ElementTree as ET\n\timport os\n\tfrom base64 import b64decode\n\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA\n\n\tfile_name = 'test.xml'\n\t# file_name = 'test4.xml'\n\twith open(file_name, 'w') as f:\n\t\tET.dump(doc, f)\n\n\twith open(file_name, 'rb') as f:\n\t\tbody = f.read()\n\n\thash = hashlib.sha256()\n\thash.update(body)\n\tsignature_value = base64.b64decode(signature)\n\thash.update(signature_value)\n\thash_value = hash.digest()\n\n\tkey = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(key)\n\ttry:\n\t\tverifier.verify(hash_value, signature_value)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\ttry:\n\t\tfrom xmlsec import transforms\n\texcept ImportError:\n\t\traise RuntimeError('libxmlsec is required to use this function.')\n\n\t# Create an XML document from the signed XML\n\tdoc_from_xml = doc.documentElement\n\tdoc_from_xml.set('xmlns', 'http://www.w3.org/2000/09/xmldsig#')\n\tdoc_from_xml.set('xmlns:ds', 'http://www.w3.org/2000/09/xmldsig#')\n\tdoc_from_xml.set('xmlns:xades', 'http://uri.etsi.org/01903/v1.3.2#')\n\n\t# Create a document to hold the result\n\tdoc_result = doc.createElement('xades:Signature')\n\tdoc_result.setAttribute('xmlns', 'http://uri.etsi.org/01903/v1.3.2#')\n\tdoc_result.setAttribute('xmlns:xades', 'http://uri.etsi.org/01903/v1.3.2#')\n\n\t# Create a context for the document\n\tcontext = transforms.TransformContext(doc_from_xml, doc", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom lxml import etree\n\t\n\troot = etree.fromstring(doc)\n\t\n\txpath = '/ds:Signature/ds:SignedInfo/ds:CanonicalizationMethod'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No canonicalization method found.')\n\t\n\txpath = '/ds:Signature/ds:SignedInfo/ds:SignatureMethod'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No signature method found.')\n\t\n\txpath = '/ds:Signature/ds:KeyInfo/ds:X509Data/ds:X509Certificate'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No certificate found.')\n\t\n\txpath = '/ds:Signature/ds:SignatureValue'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No signature value found.')\n\t\n\tfrom cryptography.hazmat.primitives import serialization\n\t\n\tcert = serialization.load_der_x509_certificate(\n\t\tpublic_key.public_bytes(\n\t\t\tencoding=serialization.Encoding.DER,\n\t\t\tformat=serialization.PublicFormat.SubjectPublicKeyInfo\n\t", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom hashlib import sha256\n\tfrom xml.etree import ElementTree\n\tfrom xmlsec.keys import Key, KeyStore\n\tfrom xmlsec.signature import XMLSignature\n\tfrom xmlsec.utils import strip_namespace\n\tfrom xmlsec.errors import XMLSecError\n\n\tks = KeyStore()\n\tks.add_key(Key(public_key))\n\ttry:\n\t\tsignature_object = XMLSignature(doc)\n\t\tsignature_object.validate(ks=ks)\n\texcept XMLSecError:\n\t\treturn False\n\n\treturn True\n", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom lxml import etree\n\tfrom hashlib import sha1\n\n\ttry:\n\t\tpub_key = etree.XML(public_key)\n\texcept:\n\t\tlog.error(\"Invalid public key: %s\" % public_key)\n\t\treturn False\n\n\tdoc = etree.XML(doc)\n\tdoc.xpath('//key', namespaces={None: 'http://www.w3.org/2000/09/xmldsig#'})\n\ttry:\n\t\tpub_key.xpath('//key', namespaces={None: 'http://www.w3.org/2000/09/xmldsig#'})\n\texcept:\n\t\tlog.error(\"Public key does not contain a signing key - %s\" % public_key)\n\t\treturn False\n\n\tsigner = pub_key[0]\n\tif signer.text != pub_key[0].get('id'):\n\t\tlog.error(\"IDs don't match for signer, %s != %s\" % (signer.text, pub_key[0].get('id')))\n\t\treturn False\n\n\t# Get the signature\n\ttry:\n\t\tsignature = etree.XML(signature)\n\texcept:\n\t", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom hashlib import sha256\n\tfrom xml.etree import ElementTree\n\n\troot = ElementTree.fromstring(doc)\n\n\tfrom . import constants\n\tfrom . import util\n\n\tdef validate_digest_and_signature(root_element):\n\t\t# Verify message digest and signature\n\t\tif root_element.attrib['signature_type'] != 'sha256':\n\t\t\traise Exception('Unsupported signature type: %s' % root_element.attrib['signature_type'])\n\t\tdigest_hex = root_element.attrib['digest_hex']\n\t\tdigest = bytes.fromhex(digest_hex)\n\n\t\tsignature_hex = root_element.attrib['signature_hex']\n\t\tsignature = bytes.fromhex(signature_hex)\n\t\ttry:\n\t\t\tutil.verify_signature(public_key, digest, signature)\n\t\texcept Exception as e:\n\t\t\traise Exception('Could not verify signature for %s: %s' % (root_element.attrib['message_reference'], e))\n\n\tdef validate_content(root_element):\n\t\tmessage_reference = root_element.attrib['message_reference']\n\n\t\t# Verify the message reference\n\t\tif not util.verify_message_", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\n\tif not isinstance(public_key, str):\n\t\traise TypeError(\"public_key must be a string.\")\n\n\tif not isinstance(doc, str):\n\t\traise TypeError(\"doc must be a string.\")\n\n\tif not isinstance(signature, str):\n\t\traise TypeError(\"signature must be a string.\")\n\n\ttry:\n\t\tfrom cryptography.hazmat.backends import default_backend\n\t\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\t\tfrom cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey\n\t\tfrom cryptography.hazmat.primitives.asymmetric.x25519 import X25519PublicKey\n\n\t\tkey = X25519PrivateKey.from_pkcs8_private_key_bytes(\n\t\t\tbytes.fromhex(public_key),\n\t\t\tdefault_backend()\n\t\t)\n\texcept Exception as e:\n\t\traise ValueError(\n\t\t\t\"The public_key parameter must be a valid X.509-formatted EC public key.\"\n\t\t)\n\n\ttry:\n\t\tfrom lxml import etree\n\n\t\tparser = etree.XMLParser(rec", "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom pyotp import TOTP\n\n\ttotp = TOTP(secret=public_key)\n\ttotp_verifier = totp.now()\n\n\ttry:\n\t\tfrom pyotp import TOTP\n\t\tverify_totp = TOTP(secret=public_key, digits=6).verify(signature)\n\texcept ImportError:\n\t\tfrom pyotp.totp import TOTP\n\t\tverify_totp = TOTP(secret=public_key, digits=6).verify(signature)\n\n\tif verify_totp == \"\":\n\t\treturn False\n\n\tif verify_totp == \"1\":\n\t\treturn True\n\treturn False\n\n#src/pyotp/pyotp.py\n#!/usr/bin/env python\n\nfrom . import oath\nfrom . import base32\nfrom . import totp\nfrom . import hotp\n\n__version__ = '0.6.3'\n\n#src/pyotp/__init__.py\nfrom . import base32\nfrom . import totp\nfrom . import oath\nfrom . import hotp\n\n__version__ = '0.6.3'\n\n#tests/test_base32.py\n#!/usr/bin/env python\n#\n# Copyright"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom cryptography.hazmat.backends import default_backend\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives import hashes\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives import serialization\n\tfrom cryptography.hazmat.primitives.asymmetric import padding\n\tfrom cryptography.hazmat.primitives.asymmetric import utils\n\tfrom cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePublicKey\n\tfrom cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePrivateKey\n\tfrom cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePrivateKey\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives import hashes\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\tfrom cryptography.hazmat.prim", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom lxml import etree\n\tfrom lxml.etree import XMLSyntaxError\n\tfrom lxml.etree import XMLSchema\n\timport base64\n\t\n\t# Decode the signature, which is a Base64 encoded XML string.\n\tdecoded_signature = base64.decodestring(signature)\n\t\n\t# Create an XML schema object from the XML schema file.\n\tschema = XMLSchema(XMLSchema.XMLNS_ATTRIBUTE_NS_PREFIX)\n\tschema.assertValid(etree.fromstring(decoded_signature))\n\t\n\t# Parse the XML into an object.\n\ttry:\n\t\txml = etree.fromstring(doc)\n\texcept XMLSyntaxError:\n\t\t# The XML is malformed.\n\t\treturn False\n\t\n\t# Check that the signature is signed by the public key.\n\ttry:\n\t\txml.verify(public_key, schema)\n\texcept etree.DocumentInvalid:\n\t\t# The XML is signed by the wrong key.\n\t\treturn False\n\t\n\t# The signature was verified successfully.\n\treturn True\n\n#src/tests/test_verify_relayable_signature.py\nimport unittest\n\nfrom verify_relayable_signature import verify_relayable_signature\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\timport hashlib\n\timport base64\n\timport xml.etree.ElementTree as ET\n\timport os\n\tfrom base64 import b64decode\n\n\tfrom Crypto.PublicKey import RSA\n\tfrom Crypto.Signature import PKCS1_v1_5\n\tfrom Crypto.Hash import SHA\n\n\tfile_name = 'test.xml'\n\t# file_name = 'test4.xml'\n\twith open(file_name, 'w') as f:\n\t\tET.dump(doc, f)\n\n\twith open(file_name, 'rb') as f:\n\t\tbody = f.read()\n\n\thash = hashlib.sha256()\n\thash.update(body)\n\tsignature_value = base64.b64decode(signature)\n\thash.update(signature_value)\n\thash_value = hash.digest()\n\n\tkey = RSA.importKey(public_key)\n\tverifier = PKCS1_v1_5.new(key)\n\ttry:\n\t\tverifier.verify(hash_value, signature_value)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\ttry:\n\t\tfrom xmlsec import transforms\n\texcept ImportError:\n\t\traise RuntimeError('libxmlsec is required to use this function.')\n\n\t# Create an XML document from the signed XML\n\tdoc_from_xml = doc.documentElement\n\tdoc_from_xml.set('xmlns', 'http://www.w3.org/2000/09/xmldsig#')\n\tdoc_from_xml.set('xmlns:ds', 'http://www.w3.org/2000/09/xmldsig#')\n\tdoc_from_xml.set('xmlns:xades', 'http://uri.etsi.org/01903/v1.3.2#')\n\n\t# Create a document to hold the result\n\tdoc_result = doc.createElement('xades:Signature')\n\tdoc_result.setAttribute('xmlns', 'http://uri.etsi.org/01903/v1.3.2#')\n\tdoc_result.setAttribute('xmlns:xades', 'http://uri.etsi.org/01903/v1.3.2#')\n\n\t# Create a context for the document\n\tcontext = transforms.TransformContext(doc_from_xml, doc", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom lxml import etree\n\t\n\troot = etree.fromstring(doc)\n\t\n\txpath = '/ds:Signature/ds:SignedInfo/ds:CanonicalizationMethod'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No canonicalization method found.')\n\t\n\txpath = '/ds:Signature/ds:SignedInfo/ds:SignatureMethod'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No signature method found.')\n\t\n\txpath = '/ds:Signature/ds:KeyInfo/ds:X509Data/ds:X509Certificate'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No certificate found.')\n\t\n\txpath = '/ds:Signature/ds:SignatureValue'\n\t\n\tif not root.xpath(xpath):\n\t\traise ValueError('No signature value found.')\n\t\n\tfrom cryptography.hazmat.primitives import serialization\n\t\n\tcert = serialization.load_der_x509_certificate(\n\t\tpublic_key.public_bytes(\n\t\t\tencoding=serialization.Encoding.DER,\n\t\t\tformat=serialization.PublicFormat.SubjectPublicKeyInfo\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom hashlib import sha256\n\tfrom xml.etree import ElementTree\n\tfrom xmlsec.keys import Key, KeyStore\n\tfrom xmlsec.signature import XMLSignature\n\tfrom xmlsec.utils import strip_namespace\n\tfrom xmlsec.errors import XMLSecError\n\n\tks = KeyStore()\n\tks.add_key(Key(public_key))\n\ttry:\n\t\tsignature_object = XMLSignature(doc)\n\t\tsignature_object.validate(ks=ks)\n\texcept XMLSecError:\n\t\treturn False\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom lxml import etree\n\tfrom hashlib import sha1\n\n\ttry:\n\t\tpub_key = etree.XML(public_key)\n\texcept:\n\t\tlog.error(\"Invalid public key: %s\" % public_key)\n\t\treturn False\n\n\tdoc = etree.XML(doc)\n\tdoc.xpath('//key', namespaces={None: 'http://www.w3.org/2000/09/xmldsig#'})\n\ttry:\n\t\tpub_key.xpath('//key', namespaces={None: 'http://www.w3.org/2000/09/xmldsig#'})\n\texcept:\n\t\tlog.error(\"Public key does not contain a signing key - %s\" % public_key)\n\t\treturn False\n\n\tsigner = pub_key[0]\n\tif signer.text != pub_key[0].get('id'):\n\t\tlog.error(\"IDs don't match for signer, %s != %s\" % (signer.text, pub_key[0].get('id')))\n\t\treturn False\n\n\t# Get the signature\n\ttry:\n\t\tsignature = etree.XML(signature)\n\texcept:\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom hashlib import sha256\n\tfrom xml.etree import ElementTree\n\n\troot = ElementTree.fromstring(doc)\n\n\tfrom . import constants\n\tfrom . import util\n\n\tdef validate_digest_and_signature(root_element):\n\t\t# Verify message digest and signature\n\t\tif root_element.attrib['signature_type'] != 'sha256':\n\t\t\traise Exception('Unsupported signature type: %s' % root_element.attrib['signature_type'])\n\t\tdigest_hex = root_element.attrib['digest_hex']\n\t\tdigest = bytes.fromhex(digest_hex)\n\n\t\tsignature_hex = root_element.attrib['signature_hex']\n\t\tsignature = bytes.fromhex(signature_hex)\n\t\ttry:\n\t\t\tutil.verify_signature(public_key, digest, signature)\n\t\texcept Exception as e:\n\t\t\traise Exception('Could not verify signature for %s: %s' % (root_element.attrib['message_reference'], e))\n\n\tdef validate_content(root_element):\n\t\tmessage_reference = root_element.attrib['message_reference']\n\n\t\t# Verify the message reference\n\t\tif not util.verify_message_", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\t\n\tif not isinstance(public_key, str):\n\t\traise TypeError(\"public_key must be a string.\")\n\n\tif not isinstance(doc, str):\n\t\traise TypeError(\"doc must be a string.\")\n\n\tif not isinstance(signature, str):\n\t\traise TypeError(\"signature must be a string.\")\n\n\ttry:\n\t\tfrom cryptography.hazmat.backends import default_backend\n\t\tfrom cryptography.hazmat.primitives.asymmetric import ec\n\t\tfrom cryptography.hazmat.primitives.asymmetric.x25519 import X25519PrivateKey\n\t\tfrom cryptography.hazmat.primitives.asymmetric.x25519 import X25519PublicKey\n\n\t\tkey = X25519PrivateKey.from_pkcs8_private_key_bytes(\n\t\t\tbytes.fromhex(public_key),\n\t\t\tdefault_backend()\n\t\t)\n\texcept Exception as e:\n\t\traise ValueError(\n\t\t\t\"The public_key parameter must be a valid X.509-formatted EC public key.\"\n\t\t)\n\n\ttry:\n\t\tfrom lxml import etree\n\n\t\tparser = etree.XMLParser(rec", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t\"\"\"\n\tVerify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.\n\t\"\"\"\n\tfrom pyotp import TOTP\n\n\ttotp = TOTP(secret=public_key)\n\ttotp_verifier = totp.now()\n\n\ttry:\n\t\tfrom pyotp import TOTP\n\t\tverify_totp = TOTP(secret=public_key, digits=6).verify(signature)\n\texcept ImportError:\n\t\tfrom pyotp.totp import TOTP\n\t\tverify_totp = TOTP(secret=public_key, digits=6).verify(signature)\n\n\tif verify_totp == \"\":\n\t\treturn False\n\n\tif verify_totp == \"1\":\n\t\treturn True\n\treturn False\n\n#src/pyotp/pyotp.py\n#!/usr/bin/env python\n\nfrom . import oath\nfrom . import base32\nfrom . import totp\nfrom . import hotp\n\n__version__ = '0.6.3'\n\n#src/pyotp/__init__.py\nfrom . import base32\nfrom . import totp\nfrom . import oath\nfrom . import hotp\n\n__version__ = '0.6.3'\n\n#tests/test_base32.py\n#!/usr/bin/env python\n#\n# Copyright", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument_json = json.loads(document)\n\thcard_url = document_json[\"links\"][0][\"href\"]\n\thcard_json = requests.get(hcard_url).json()\n\treturn hcard_json", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\tdocument = json.loads(document)\n\texcept Exception as e:\n\t\traise Exception('Could not parse document')\n\tif 'hcard_url' not in document:\n\t\traise Exception('Could not find hcard_url in document')\n\treturn document['hcard_url']\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn webfinger['links'][0]['href']", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\treturn json.loads(document)\n\texcept json.JSONDecodeError as e:\n\t\traise ValueError(\"Invalid webfinger document\") from e\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\timport json\n\timport re\n\t\n\tdocument = re.sub(r'<[^>]+>', '', document)\n\tdocument = re.sub(r'(\\s*)@(.+?):(.+?)\\s*$', r'\\1\\2@\\3', document)\n\tdocument = json.loads(document)\n\t\n\treturn document['links'][0]['href']\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument = document.replace(\"'\", \"\\\"\")\n\tdocument = document.replace(\"href\", \"hcard_url\")\n\treturn json.loads(document)\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\thcard_url = None\n\tparsed = json.loads(document)\n\tfor link in parsed[\"links\"]:\n\t\tif link[\"rel\"] == \"hcard\":\n\t\t\thcard_url = link[\"href\"]\n\tif hcard_url is None:\n\t\treturn {}\n\treturn {\n\t\t\"username\": hcard_url.split(\"/\")[-1],\n\t\t\"displayname\": parsed[\"name\"],\n\t\t\"avatar\": parsed[\"photo\"][\"urls\"][\"original\"],\n\t}\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\tdocument = json.loads(document)\n\texcept Exception as e:\n\t\tprint(f\"Error parsing webfinger document: {e}\")\n\t\treturn None\n\n\tif \"type\" not in document or document[\"type\"] != \"h-card\":\n\t\tprint(\"Webfinger document is not a h-card\")\n\t\treturn None\n\n\ttry:\n\t\twebfinger_url = document[\"properties\"][\"hcard_url\"]\n\texcept Exception as e:\n\t\tprint(f\"Error parsing hcard_url: {e}\")\n\t\treturn None\n\n\treturn {\n\t\t\"username\": document[\"properties\"][\"preferred_username\"],\n\t\t\"webfinger_url\": webfinger_url,\n\t}\n\n#src/diaspora_webfinger_parser/__init__.py\n\"\"\"\ndiaspora_webfinger_parser\n\nA Python library for parsing webfinger documents.\n\"\"\"\n\n__version__ = \"0.0.1\"\n\nfrom .diaspora_webfinger_parser import parse_diaspora_webfinger", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdoc = json.loads(document)\n\thcard_url = doc['links']['hcard_url']\n\thcard_url = hcard_url.replace('https://', '')\n\thcard_url = hcard_url.replace('http://', '')\n\thcard_url = hcard_url.replace('www.', '')\n\thcard_url = hcard_url.replace('.', '')\n\thcard_url = hcard_url.replace(':80', '')\n\thcard_url = hcard_url.replace(':443', '')\n\thcard_url = hcard_url.replace('@', '')\n\treturn {'hcard_url': hcard_url}", "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument = document.replace(\"'\", '\"')\n\tdocument = json.loads(document)\n\thcard_url = document.get('hcard_url')\n\tlink = document.get('links')\n\tif link is None:\n\t\treturn None\n\tlinks = link[0]\n\thref = links.get('href')\n\tif href is None:\n\t\treturn None\n\tsplit_href = href.split(\":\")\n\tscheme = split_href[0]\n\thost = split_href[1]\n\tpath = split_href[2]\n\tpath = path.replace(\"/@\", \"\")\n\tif scheme != \"https\" or host != \"diaspora.social\":\n\t\treturn None\n\tsplit_path = path.split(\"/\")\n\tusername = split_path[0]\n\tprofile_id = split_path[1]\n\tif profile_id == \"me\":\n\t\tprofile_id = None\n\treturn {\"username\": username, \"profile_id\": profile_id}"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument_json = json.loads(document)\n\thcard_url = document_json[\"links\"][0][\"href\"]\n\thcard_json = requests.get(hcard_url).json()\n\treturn hcard_json", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\tdocument = json.loads(document)\n\texcept Exception as e:\n\t\traise Exception('Could not parse document')\n\tif 'hcard_url' not in document:\n\t\traise Exception('Could not find hcard_url in document')\n\treturn document['hcard_url']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\twebfinger = json.loads(document)\n\treturn webfinger['links'][0]['href']", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\treturn json.loads(document)\n\texcept json.JSONDecodeError as e:\n\t\traise ValueError(\"Invalid webfinger document\") from e\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\timport json\n\timport re\n\t\n\tdocument = re.sub(r'<[^>]+>', '', document)\n\tdocument = re.sub(r'(\\s*)@(.+?):(.+?)\\s*$', r'\\1\\2@\\3', document)\n\tdocument = json.loads(document)\n\t\n\treturn document['links'][0]['href']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument = document.replace(\"'\", \"\\\"\")\n\tdocument = document.replace(\"href\", \"hcard_url\")\n\treturn json.loads(document)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\thcard_url = None\n\tparsed = json.loads(document)\n\tfor link in parsed[\"links\"]:\n\t\tif link[\"rel\"] == \"hcard\":\n\t\t\thcard_url = link[\"href\"]\n\tif hcard_url is None:\n\t\treturn {}\n\treturn {\n\t\t\"username\": hcard_url.split(\"/\")[-1],\n\t\t\"displayname\": parsed[\"name\"],\n\t\t\"avatar\": parsed[\"photo\"][\"urls\"][\"original\"],\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\ttry:\n\t\tdocument = json.loads(document)\n\texcept Exception as e:\n\t\tprint(f\"Error parsing webfinger document: {e}\")\n\t\treturn None\n\n\tif \"type\" not in document or document[\"type\"] != \"h-card\":\n\t\tprint(\"Webfinger document is not a h-card\")\n\t\treturn None\n\n\ttry:\n\t\twebfinger_url = document[\"properties\"][\"hcard_url\"]\n\texcept Exception as e:\n\t\tprint(f\"Error parsing hcard_url: {e}\")\n\t\treturn None\n\n\treturn {\n\t\t\"username\": document[\"properties\"][\"preferred_username\"],\n\t\t\"webfinger_url\": webfinger_url,\n\t}\n\n#src/diaspora_webfinger_parser/__init__.py\n\"\"\"\ndiaspora_webfinger_parser\n\nA Python library for parsing webfinger documents.\n\"\"\"\n\n__version__ = \"0.0.1\"\n\nfrom .diaspora_webfinger_parser import parse_diaspora_webfinger", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdoc = json.loads(document)\n\thcard_url = doc['links']['hcard_url']\n\thcard_url = hcard_url.replace('https://', '')\n\thcard_url = hcard_url.replace('http://', '')\n\thcard_url = hcard_url.replace('www.', '')\n\thcard_url = hcard_url.replace('.', '')\n\thcard_url = hcard_url.replace(':80', '')\n\thcard_url = hcard_url.replace(':443', '')\n\thcard_url = hcard_url.replace('@', '')\n\treturn {'hcard_url': hcard_url}", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\t\"\"\"\n\tThe webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.\n\t\"\"\"\n\tdocument = document.replace(\"'\", '\"')\n\tdocument = json.loads(document)\n\thcard_url = document.get('hcard_url')\n\tlink = document.get('links')\n\tif link is None:\n\t\treturn None\n\tlinks = link[0]\n\thref = links.get('href')\n\tif href is None:\n\t\treturn None\n\tsplit_href = href.split(\":\")\n\tscheme = split_href[0]\n\thost = split_href[1]\n\tpath = split_href[2]\n\tpath = path.replace(\"/@\", \"\")\n\tif scheme != \"https\" or host != \"diaspora.social\":\n\t\treturn None\n\tsplit_path = path.split(\"/\")\n\tusername = split_path[0]\n\tprofile_id = split_path[1]\n\tif profile_id == \"me\":\n\t\tprofile_id = None\n\treturn {\"username\": username, \"profile_id\": profile_id}", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tresponse = requests.get(f\"https://activitypub.rocks/api/users/{handle}\", timeout=5)\n\tif response.status_code == 404:\n\t\treturn None\n\telif response.status_code != 200:\n\t\traise Exception(f\"Received unexpected status code {response.status_code}.\")\n\tjson = response.json()\n\treturn json[\"webfinger\"][\"document\"]\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept UnknownHandleException:\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tdocument = requests.get(\n\t\t\turl='https://' + handle + '/.well-known/webfinger',\n\t\t\tparams={'resource': 'acct:' + handle},\n\t\t\ttimeout=10,\n\t\t).text\n\t\treturn document\n\texcept:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tfrom .webfinger_client import get_webfinger_document\n\n\ttry:\n\t\treturn get_webfinger_document(handle)\n\texcept Exception as e:\n\t\tlog.debug(\"Failed to retrieve webfinger document: %s\", e)\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn requests.get(\"https://\" + handle + \"@webfinger.net/\", timeout=3).text\n\texcept:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = f\"https://{handle}.com/webfinger?resource={handle}\"\n\tr = requests.get(url)\n\treturn r.text if r.status_code == 200 else None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = f'https://{handle}.com/.well-known/webfinger'\n\ttry:\n\t\tresponse = requests.get(url)\n\texcept requests.exceptions.HTTPError as e:\n\t\tif e.response.status_code == 404:\n\t\t\traise ResourceNotFoundException()\n\t\treturn None\n\texcept Exception as e:\n\t\traise e\n\treturn response.text\n\n#src/webfinger/webfinger.py\n\"\"\"\nRFC7033 Webfinger\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Optional\n\nfrom webfinger.exceptions import ResourceNotFoundException\nfrom webfinger.utils import try_retrieve_webfinger_document\n\nif TYPE_CHECKING:\n\tfrom webfinger.types import WebfingerDocument\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\twebfinger_document = get_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\treturn webfinger_document\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\twith get(f\"https://webfinger.net/api/v1/rel/webfinger?resource={handle}\") as response:\n\t\tif response.status_code == 200:\n\t\t\treturn response.text\n\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept webfinger_error.WebfingerError as e:\n\t\tif e.errno == webfinger_error.ERRNO_NOTFOUND:\n\t\t\treturn None\n\t\traise\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tresponse = requests.get(f\"https://activitypub.rocks/api/users/{handle}\", timeout=5)\n\tif response.status_code == 404:\n\t\treturn None\n\telif response.status_code != 200:\n\t\traise Exception(f\"Received unexpected status code {response.status_code}.\")\n\tjson = response.json()\n\treturn json[\"webfinger\"][\"document\"]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept UnknownHandleException:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\tdocument = requests.get(\n\t\t\turl='https://' + handle + '/.well-known/webfinger',\n\t\t\tparams={'resource': 'acct:' + handle},\n\t\t\ttimeout=10,\n\t\t).text\n\t\treturn document\n\texcept:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\tfrom .webfinger_client import get_webfinger_document\n\n\ttry:\n\t\treturn get_webfinger_document(handle)\n\texcept Exception as e:\n\t\tlog.debug(\"Failed to retrieve webfinger document: %s\", e)\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn requests.get(\"https://\" + handle + \"@webfinger.net/\", timeout=3).text\n\texcept:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = f\"https://{handle}.com/webfinger?resource={handle}\"\n\tr = requests.get(url)\n\treturn r.text if r.status_code == 200 else None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\turl = f'https://{handle}.com/.well-known/webfinger'\n\ttry:\n\t\tresponse = requests.get(url)\n\texcept requests.exceptions.HTTPError as e:\n\t\tif e.response.status_code == 404:\n\t\t\traise ResourceNotFoundException()\n\t\treturn None\n\texcept Exception as e:\n\t\traise e\n\treturn response.text\n\n#src/webfinger/webfinger.py\n\"\"\"\nRFC7033 Webfinger\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING, Optional\n\nfrom webfinger.exceptions import ResourceNotFoundException\nfrom webfinger.utils import try_retrieve_webfinger_document\n\nif TYPE_CHECKING:\n\tfrom webfinger.types import WebfingerDocument\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\twebfinger_document = get_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\treturn webfinger_document\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\twith get(f\"https://webfinger.net/api/v1/rel/webfinger?resource={handle}\") as response:\n\t\tif response.status_code == 200:\n\t\t\treturn response.text\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t\"\"\"\n\tTry to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n\t\"\"\"\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept webfinger_error.WebfingerError as e:\n\t\tif e.errno == webfinger_error.ERRNO_NOTFOUND:\n\t\t\treturn None\n\t\traise\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'https://%s@%s/webfinger?resource=acct:%s' % (settings.DIASPORA_WEBFINGER_USERNAME,\n\t\tsettings.DIASPORA_WEBFINGER_HOST,\n\t\thandle)\n\ttry:\n\t\tresponse = urllib2.urlopen(url, timeout=5)\n\texcept urllib2.HTTPError, e:\n\t\treturn None\n\n\tdata = response.read()\n\ttry:\n\t\tdata_dict = json.loads(data)\n\texcept ValueError:\n\t\treturn None\n\n\tdata_dict = data_dict['links'][0]\n\n\tif data_dict['rel'] != ['http://webfinger.net/rel/profile-page']:\n\t\treturn None\n\n\treturn data_dict\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\ttry:\n\t\tresponse = urllib2.urlopen('http://' + handle + '.diaspora.org/profile/')\n\texcept urllib2.HTTPError, e:\n\t\treturn None\n\texcept Exception, e:\n\t\tlogging.error('Error while retrieving remote webfinger: %s', e)\n\t\treturn None\n\n\ttry:\n\t\tresponse_body = response.read()\n\t\tresponse_body = json.loads(response_body)\n\texcept Exception, e:\n\t\tlogging.error('Error while parsing remote webfinger: %s', e)\n\t\treturn None\n\n\treturn response_body\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = \"https://%s@%s/api/v1/accounts/%s\" % (handle, settings.Diaspora_host, handle)\n\tr = requests.get(url)\n\tif r.status_code == 404:\n\t\treturn None\n\tr = r.json()\n\treturn r\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tresponse = requests.get(\n\t\t\"http://#{handle}@diaspora.com/api/v1/entities/#{handle}\".format(\n\t\t\thandle=handle))\n\n\ttry:\n\t\tresponse.raise_for_status()\n\texcept Exception as exc:\n\t\tprint(response.text)\n\t\traise exc\n\n\treturn response.json()\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tresponse = requests.get(\n\t\t'https://#{handle}@#{BASE_URL}/#{handle}'\n\t)\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\treturn response.json()\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = retrieve_diaspora_webfinger(handle)\n\tif webfinger:\n\t\twebfinger = json.loads(webfinger)\n\t\tif 'subject' in webfinger:\n\t\t\twebfinger = webfinger['subject']\n\t\t\twebfinger = webfinger.replace('acct:', '')\n\t\twebfinger = webfinger.split('@')\n\t\tprint('webfinger: %s' % webfinger)\n\t\treturn {'handle': webfinger[0], 'host': webfinger[1]}\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tdiaspora_webfinger_response = get_diaspora_webfinger(handle)\n\tdiaspora_webfinger_data = diaspora_webfinger_response.json()\n\tdiaspora_webfinger_data['handle'] = handle\n\treturn diaspora_webfinger_data\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\trequest_uri = 'http://' + handle + '.diaspora.software/profile.json'\n\trequest = urllib2.Request(request_uri)\n\tresponse = urllib2.urlopen(request)\n\tdata = json.loads(response.read())\n\n\treturn data\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = \"https://{handle}.diaspora.software/users/show.json\".format(handle=handle)\n\ttry:\n\t\tresponse = requests.get(url)\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\n\tif response.status_code == 200:\n\t\tdata = response.json()\n\t\tif data.has_key(\"status\") and data[\"status\"] == \"ok\":\n\t\t\treturn data[\"user\"]\n\t\telse:\n\t\t\treturn None\n\telse:\n\t\treturn None\n\n#diaspora_utils/__init__.py\nimport requests\n\nfrom .diaspora_utils import retrieve_and_parse_diaspora_webfinger\nfrom .diaspora_utils import retrieve_and_parse_diaspora_posts\n\n__version__ = \"0.0.2\"\n\n__all__ = [\n    \"retrieve_and_parse_diaspora_webfinger\",\n    \"retrieve_and_parse_diaspora_posts\",\n]", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'https://diaspora.org/users/{handle}.atom'.format(handle=handle)\n\tr = requests.get(url)\n\tif r.status_code != 200:\n\t\traise Exception(\"Webfinger not found\")\n\tj = r.json()\n\tif j['items'][0]['type'] != 'person':\n\t\traise Exception(\"Webfinger not found\")\n\tj = j['items'][0]['properties']\n\treturn j\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'https://%s@%s/webfinger?resource=acct:%s' % (settings.DIASPORA_WEBFINGER_USERNAME,\n\t\tsettings.DIASPORA_WEBFINGER_HOST,\n\t\thandle)\n\ttry:\n\t\tresponse = urllib2.urlopen(url, timeout=5)\n\texcept urllib2.HTTPError, e:\n\t\treturn None\n\n\tdata = response.read()\n\ttry:\n\t\tdata_dict = json.loads(data)\n\texcept ValueError:\n\t\treturn None\n\n\tdata_dict = data_dict['links'][0]\n\n\tif data_dict['rel'] != ['http://webfinger.net/rel/profile-page']:\n\t\treturn None\n\n\treturn data_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\ttry:\n\t\tresponse = urllib2.urlopen('http://' + handle + '.diaspora.org/profile/')\n\texcept urllib2.HTTPError, e:\n\t\treturn None\n\texcept Exception, e:\n\t\tlogging.error('Error while retrieving remote webfinger: %s', e)\n\t\treturn None\n\n\ttry:\n\t\tresponse_body = response.read()\n\t\tresponse_body = json.loads(response_body)\n\texcept Exception, e:\n\t\tlogging.error('Error while parsing remote webfinger: %s', e)\n\t\treturn None\n\n\treturn response_body\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = \"https://%s@%s/api/v1/accounts/%s\" % (handle, settings.Diaspora_host, handle)\n\tr = requests.get(url)\n\tif r.status_code == 404:\n\t\treturn None\n\tr = r.json()\n\treturn r\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tresponse = requests.get(\n\t\t\"http://#{handle}@diaspora.com/api/v1/entities/#{handle}\".format(\n\t\t\thandle=handle))\n\n\ttry:\n\t\tresponse.raise_for_status()\n\texcept Exception as exc:\n\t\tprint(response.text)\n\t\traise exc\n\n\treturn response.json()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tresponse = requests.get(\n\t\t'https://#{handle}@#{BASE_URL}/#{handle}'\n\t)\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\treturn response.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\twebfinger = retrieve_diaspora_webfinger(handle)\n\tif webfinger:\n\t\twebfinger = json.loads(webfinger)\n\t\tif 'subject' in webfinger:\n\t\t\twebfinger = webfinger['subject']\n\t\t\twebfinger = webfinger.replace('acct:', '')\n\t\twebfinger = webfinger.split('@')\n\t\tprint('webfinger: %s' % webfinger)\n\t\treturn {'handle': webfinger[0], 'host': webfinger[1]}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\tdiaspora_webfinger_response = get_diaspora_webfinger(handle)\n\tdiaspora_webfinger_data = diaspora_webfinger_response.json()\n\tdiaspora_webfinger_data['handle'] = handle\n\treturn diaspora_webfinger_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\trequest_uri = 'http://' + handle + '.diaspora.software/profile.json'\n\trequest = urllib2.Request(request_uri)\n\tresponse = urllib2.urlopen(request)\n\tdata = json.loads(response.read())\n\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = \"https://{handle}.diaspora.software/users/show.json\".format(handle=handle)\n\ttry:\n\t\tresponse = requests.get(url)\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\n\tif response.status_code == 200:\n\t\tdata = response.json()\n\t\tif data.has_key(\"status\") and data[\"status\"] == \"ok\":\n\t\t\treturn data[\"user\"]\n\t\telse:\n\t\t\treturn None\n\telse:\n\t\treturn None\n\n#diaspora_utils/__init__.py\nimport requests\n\nfrom .diaspora_utils import retrieve_and_parse_diaspora_webfinger\nfrom .diaspora_utils import retrieve_and_parse_diaspora_posts\n\n__version__ = \"0.0.2\"\n\n__all__ = [\n    \"retrieve_and_parse_diaspora_webfinger\",\n    \"retrieve_and_parse_diaspora_posts\",\n]", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t\"\"\"\n\tRetrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict\n\t\"\"\"\n\turl = 'https://diaspora.org/users/{handle}.atom'.format(handle=handle)\n\tr = requests.get(url)\n\tif r.status_code != 200:\n\t\traise Exception(\"Webfinger not found\")\n\tj = r.json()\n\tif j['items'][0]['type'] != 'person':\n\t\traise Exception(\"Webfinger not found\")\n\tj = j['items'][0]['properties']\n\treturn j\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrdlib.network.http import HTTP\n\tfrom xrdlib.utils import json_loads, json_dumps\n\tfrom xrdlib.network.http import HTTPError\n\tfrom xrdlib.network.http import HTTPResponse\n\tfrom xrdlib.network.http import HTTPNotFound\n\tfrom xrdlib.network.http import HTTPRedirect\n\tfrom xrdlib.network.http import HTTPUnauthorized\n\tfrom xrdlib.network.http import HTTPForbidden\n\tfrom xrdlib.network.http import HTTPMethodNotAllowed\n\tfrom xrdlib.network.http import HTTPUnprocessableEntity\n\tfrom xrdlib.network.http import HTTPRequestTimeout\n\tfrom xrdlib.network.http import HTTPRequestEntityTooLarge\n\tfrom xrdlib.network.http import HTTPRequestTooMany\n\tfrom xrdlib.network.http import HTTPRequestFailed\n\tfrom xrdlib.network.http import HTTPRequestNotImplemented\n\tfrom xrdlib.network.http import HTTPRequestBadRequest\n\tfrom xrdlib.network.http import HTTPRequestInternalServerError\n\tfrom xrdlib.network.http import HTTPRequestServiceUnavailable\n\tfrom xrdlib.network.http import HTTPRequestGatewayTimeout\n\tfrom xrdlib.network.http import HTTPRequestHTTPVersion", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treq = requests.get(\n\t\tsettings.DIASPORA_HOST_META_URL.format(host=host),\n\t\theaders={\n\t\t\t'Authorization': 'Token token=\"%s\"' % settings.DIASPORA_TOKEN,\n\t\t},\n\t)\n\treturn req.json()\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = host.get_xrd_from_uri(\n\t\t\"%s/host-meta/%s\" % (host.uri, host.id)\n\t)\n\treturn host_meta\n\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\txrd = XRD(host)\n\txrd.get('/host-meta')\n\treturn xrd", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\txrd = XRD(host=host)\n\txrd.get()\n\treturn xrd\n\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\txd = XRD(host)\n\txd.read()\n\treturn xd\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom pyxd import xrd\n\txrd.get(host, '/host-meta')\n\treturn xrd.doc\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = host.get_host_meta()\n\thost_meta['diaspora_host'] = host\n\treturn host_meta\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = get_remote_document(host, 'host-meta')\n\treturn host_meta\n", "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport pydiaspora as xrd\n\txrd = xrd.XRD(host)\n\txrd.retrieve_host_meta()\n\treturn xrd\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom xrdlib.network.http import HTTP\n\tfrom xrdlib.utils import json_loads, json_dumps\n\tfrom xrdlib.network.http import HTTPError\n\tfrom xrdlib.network.http import HTTPResponse\n\tfrom xrdlib.network.http import HTTPNotFound\n\tfrom xrdlib.network.http import HTTPRedirect\n\tfrom xrdlib.network.http import HTTPUnauthorized\n\tfrom xrdlib.network.http import HTTPForbidden\n\tfrom xrdlib.network.http import HTTPMethodNotAllowed\n\tfrom xrdlib.network.http import HTTPUnprocessableEntity\n\tfrom xrdlib.network.http import HTTPRequestTimeout\n\tfrom xrdlib.network.http import HTTPRequestEntityTooLarge\n\tfrom xrdlib.network.http import HTTPRequestTooMany\n\tfrom xrdlib.network.http import HTTPRequestFailed\n\tfrom xrdlib.network.http import HTTPRequestNotImplemented\n\tfrom xrdlib.network.http import HTTPRequestBadRequest\n\tfrom xrdlib.network.http import HTTPRequestInternalServerError\n\tfrom xrdlib.network.http import HTTPRequestServiceUnavailable\n\tfrom xrdlib.network.http import HTTPRequestGatewayTimeout\n\tfrom xrdlib.network.http import HTTPRequestHTTPVersion", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\treq = requests.get(\n\t\tsettings.DIASPORA_HOST_META_URL.format(host=host),\n\t\theaders={\n\t\t\t'Authorization': 'Token token=\"%s\"' % settings.DIASPORA_TOKEN,\n\t\t},\n\t)\n\treturn req.json()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = host.get_xrd_from_uri(\n\t\t\"%s/host-meta/%s\" % (host.uri, host.id)\n\t)\n\treturn host_meta\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\txrd = XRD(host)\n\txrd.get('/host-meta')\n\treturn xrd", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\txrd = XRD(host=host)\n\txrd.get()\n\treturn xrd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\txd = XRD(host)\n\txd.read()\n\treturn xd\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\tfrom pyxd import xrd\n\txrd.get(host, '/host-meta')\n\treturn xrd.doc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = host.get_host_meta()\n\thost_meta['diaspora_host'] = host\n\treturn host_meta\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\thost_meta = get_remote_document(host, 'host-meta')\n\treturn host_meta\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\t\"\"\"\n\tRetrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance\n\t\"\"\"\n\timport pydiaspora as xrd\n\txrd = xrd.XRD(host)\n\txrd.retrieve_host_meta()\n\treturn xrd\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tresponse = requests.post(url, data=data, timeout=timeout, method=method, *args, **kwargs)\n\treturn response\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\timport urllib2\n\n\tif method == \"post\":\n\t\tdata = urllib2.urlencode(data)\n\telse:\n\t\tdata = data\n\n\treq = urllib2.Request(url, data)\n\tresponse = urllib2.urlopen(req, timeout=timeout)\n\n\treturn response.read()\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\theaders = {\"Content-Type\": \"application/octet-stream\"}\n\ttry:\n\t\tresponse = requests.request(method, url, headers=headers, data=data, *args, **kwargs)\n\t\tresponse.raise_for_status()\n\texcept Exception as e:\n\t\traise e\n\treturn response\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.post(url, data=data, timeout=timeout, method=method, *args, **kwargs)\n\texcept Exception as e:\n\t\treturn None\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tdata[\"method\"] = method\n\tdata[\"url\"] = url\n\tdata[\"timeout\"] = timeout\n\ttry:\n\t\tr = requests.post(url, data=data, *args, **kwargs)\n\texcept Exception as e:\n\t\traise e\n\treturn r\n\n#api.py\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function, absolute_import, division, unicode_literals\n\nimport os\nimport logging\nimport json\nfrom os import path\n\nfrom flask import Flask, request, Response\nfrom flask_cors import CORS\nfrom flask_sslify import SSLify\n\nfrom libs.database import Database\nfrom libs.mail import send_document\n\n\n# Setup logging\nlogfile = \"logs/api.log\"\nlog_path = path.dirname(path.abspath(__file__))\nif not os.path.exists(log_path):\n\tos.makedirs(log_path)\nlogging.basicConfig(\n\tfilename=logfile,\n\tlevel=logging.DEBUG,\n\tformat=\"%(asctime)s %(levelname)s %(name)s %(message)s\"\n)\n\n# Setup Flask\napp = Flask(__name__)\nC", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport httplib\n\timport urllib\n\t\n\theaders = {\n\t\t\"Content-type\": \"application/json\",\n\t\t\"Content-Length\": str(len(data)),\n\t}\n\t\n\tconn = httplib.HTTPConnection(url, timeout=timeout)\n\tconn.request(method, url, data, headers)\n\tresponse = conn.getresponse()\n\t\n\tif response.status != 200:\n\t\traise Exception(\"Failed to send post request: \" + response.status)\n\n\treturn response.read()\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tdata = urllib.urlencode(data)\n\theaders = {\n\t\t'Content-Type': 'application/x-www-form-urlencoded',\n\t\t'Content-Length': len(data)\n\t}\n\ttry:\n\t\trequest = urllib2.Request(url, data, headers)\n\t\tresponse = urllib2.urlopen(request, timeout=timeout if timeout else None)\n\texcept urllib2.HTTPError, e:\n\t\traise e\n\treturn response.read()\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\theaders = {\n\t\t\"Content-Type\": \"application/json\"\n\t}\n\t\n\treturn send_request(url, data, headers, method, timeout, *args, **kwargs)\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport urllib2\n\timport cookielib\n\timport json\n\timport socket\n\t\n\theaders = {\n\t\t'Content-Type': 'application/json',\n\t\t'Accept': 'application/json'\n\t}\n\t\n\ttry:\n\t\tdata = json.dumps(data)\n\texcept Exception as e:\n\t\traise e\n\t\n\trequest = urllib2.Request(url, data, headers)\n\t\n\ttimeout = int(timeout)\n\t\n\ttry:\n\t\tresponse = urllib2.urlopen(request, timeout=timeout)\n\texcept Exception as e:\n\t\traise e\n\t\n\tif response.code != 200:\n\t\traise Exception(\"Request failed with code %s\" % response.code)\n\t\n\ttry:\n\t\tdata = response.read()\n\texcept Exception as e:\n\t\traise e\n\t\n\ttry:\n\t\tdata = json.loads(data)\n\texcept Exception as e:\n\t\traise e\n\t\n\treturn data\n\n#misc/post_data.py\nimport requests\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.post(url, data, timeout=timeout, method=method, *args, **kwargs)\n\texcept Exception as e:\n\t\traise e\n\n#app/tasks.py\nfrom celery.decorators import task\nfrom app.models import *\nimport requests\nfrom .utils import send_document\n\n\n@task(name=\"send_document\")"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tresponse = requests.post(url, data=data, timeout=timeout, method=method, *args, **kwargs)\n\treturn response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\timport urllib2\n\n\tif method == \"post\":\n\t\tdata = urllib2.urlencode(data)\n\telse:\n\t\tdata = data\n\n\treq = urllib2.Request(url, data)\n\tresponse = urllib2.urlopen(req, timeout=timeout)\n\n\treturn response.read()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\theaders = {\"Content-Type\": \"application/octet-stream\"}\n\ttry:\n\t\tresponse = requests.request(method, url, headers=headers, data=data, *args, **kwargs)\n\t\tresponse.raise_for_status()\n\texcept Exception as e:\n\t\traise e\n\treturn response\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.post(url, data=data, timeout=timeout, method=method, *args, **kwargs)\n\texcept Exception as e:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tdata[\"method\"] = method\n\tdata[\"url\"] = url\n\tdata[\"timeout\"] = timeout\n\ttry:\n\t\tr = requests.post(url, data=data, *args, **kwargs)\n\texcept Exception as e:\n\t\traise e\n\treturn r\n\n#api.py\n# -*- coding: utf-8 -*-\nfrom __future__ import print_function, absolute_import, division, unicode_literals\n\nimport os\nimport logging\nimport json\nfrom os import path\n\nfrom flask import Flask, request, Response\nfrom flask_cors import CORS\nfrom flask_sslify import SSLify\n\nfrom libs.database import Database\nfrom libs.mail import send_document\n\n\n# Setup logging\nlogfile = \"logs/api.log\"\nlog_path = path.dirname(path.abspath(__file__))\nif not os.path.exists(log_path):\n\tos.makedirs(log_path)\nlogging.basicConfig(\n\tfilename=logfile,\n\tlevel=logging.DEBUG,\n\tformat=\"%(asctime)s %(levelname)s %(name)s %(message)s\"\n)\n\n# Setup Flask\napp = Flask(__name__)\nC", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport httplib\n\timport urllib\n\t\n\theaders = {\n\t\t\"Content-type\": \"application/json\",\n\t\t\"Content-Length\": str(len(data)),\n\t}\n\t\n\tconn = httplib.HTTPConnection(url, timeout=timeout)\n\tconn.request(method, url, data, headers)\n\tresponse = conn.getresponse()\n\t\n\tif response.status != 200:\n\t\traise Exception(\"Failed to send post request: \" + response.status)\n\n\treturn response.read()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\tdata = urllib.urlencode(data)\n\theaders = {\n\t\t'Content-Type': 'application/x-www-form-urlencoded',\n\t\t'Content-Length': len(data)\n\t}\n\ttry:\n\t\trequest = urllib2.Request(url, data, headers)\n\t\tresponse = urllib2.urlopen(request, timeout=timeout if timeout else None)\n\texcept urllib2.HTTPError, e:\n\t\traise e\n\treturn response.read()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\t\n\theaders = {\n\t\t\"Content-Type\": \"application/json\"\n\t}\n\t\n\treturn send_request(url, data, headers, method, timeout, *args, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\timport urllib2\n\timport cookielib\n\timport json\n\timport socket\n\t\n\theaders = {\n\t\t'Content-Type': 'application/json',\n\t\t'Accept': 'application/json'\n\t}\n\t\n\ttry:\n\t\tdata = json.dumps(data)\n\texcept Exception as e:\n\t\traise e\n\t\n\trequest = urllib2.Request(url, data, headers)\n\t\n\ttimeout = int(timeout)\n\t\n\ttry:\n\t\tresponse = urllib2.urlopen(request, timeout=timeout)\n\texcept Exception as e:\n\t\traise e\n\t\n\tif response.code != 200:\n\t\traise Exception(\"Request failed with code %s\" % response.code)\n\t\n\ttry:\n\t\tdata = response.read()\n\texcept Exception as e:\n\t\traise e\n\t\n\ttry:\n\t\tdata = json.loads(data)\n\texcept Exception as e:\n\t\traise e\n\t\n\treturn data\n\n#misc/post_data.py\nimport requests\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\t\"\"\"\n\tSend a response containing data through the POST method.\n\t\"\"\"\n\ttry:\n\t\treturn requests.post(url, data, timeout=timeout, method=method, *args, **kwargs)\n\texcept Exception as e:\n\t\traise e\n\n#app/tasks.py\nfrom celery.decorators import task\nfrom app.models import *\nimport requests\nfrom .utils import send_document\n\n\n@task(name=\"send_document\")", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkey = key.split('.')\n\ttry:\n\t\tsub_dic = dic[key.pop(0)]\n\texcept KeyError:\n\t\tsub_dic = {}\n\t\tif not keys:\n\t\t\tsub_dic[key.pop(0)] = val\n\t\telse:\n\t\t\tsub_dic = dict_insert(sub_dic, val, key.pop(0), *keys)\n\treturn dict_insert(sub_dic, val, key.pop(0), *keys)\n\n#src/pysolr/solr/schema.py\nfrom collections import namedtuple\nfrom .config import DEFAULT_FIELD_TYPES\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttry:\n\t\tdic = dic[key]\n\texcept KeyError:\n\t\tdic[key] = val\n\telse:\n\t\tfor k in keys:\n\t\t\tdic = dic[k]\n\t\t\ttry:\n\t\t\t\tdic = dic[key]\n\t\t\texcept KeyError:\n\t\t\t\tdic[key] = val\n\t\t\telse:\n\t\t\t\tbreak\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tlast_key = keys[-1]\n\n\tif len(keys) == 1:\n\t\tdic[key] = val\n\telse:\n\t\tval = dict_insert(dic.get(key, {}), val, *keys[:-1])\n\t\tdic[key] = val\n\n\treturn dic\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkey_list = list(keys)\n\tkey_list.append(key)\n\tkey_path = '.'.join(key_list)\n\n\tif key_path not in dic:\n\t\tdic[key_path] = val\n\telse:\n\t\tdic[key_path] = val\n\n\treturn dic\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkeys = list(keys)\n\tkeys.append(key)\n\tkeys.reverse()\n\tif len(keys) == 1:\n\t\tdic[keys[0]] = val\n\telse:\n\t\tif keys[0] not in dic:\n\t\t\tdic[keys[0]] = {}\n\t\tdict_insert(dic[keys[0]], val, *keys[1:])\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tsub_keys = keys[:-1]\n\tsub_key = keys[-1]\n\tif sub_keys:\n\t\tfor sub_key in sub_keys:\n\t\t\tif sub_key not in dic:\n\t\t\t\tdic[sub_key] = {}\n\t\t\tdic = dic[sub_key]\n\tif key not in dic:\n\t\tdic[key] = val\n\telse:\n\t\tdic[key] = [dic[key], val]\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\td = dic\n\tfor k in keys[:-1]:\n\t\tif k not in d:\n\t\t\td = d[k] = {}\n\tk = keys[-1]\n\td[k] = val\n\treturn dic\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tcur_key = key\n\tnext_key = ''\n\n\tfor key in keys[:-1]:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tcur_key = key\n\t\tnext_key = keys[keys.index(key) + 1]\n\n\tdic[cur_key][next_key] = val\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkey_list = key.split('.')\n\ttry:\n\t\tval = dic[key]\n\texcept KeyError:\n\t\tdic[key] = {}\n\tfor key in key_list[:-1]:\n\t\tdic = dic[key]\n\tdic[key_list[-1]] = val\n\n\n", "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttemp = dic\n\tfor k in keys[:-1]:\n\t\tif k in temp:\n\t\t\ttemp = temp[k]\n\t\telse:\n\t\t\ttemp[k] = {}\n\t\t\ttemp = temp[k]\n\ttemp[keys[-1]] = val\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkey = key.split('.')\n\ttry:\n\t\tsub_dic = dic[key.pop(0)]\n\texcept KeyError:\n\t\tsub_dic = {}\n\t\tif not keys:\n\t\t\tsub_dic[key.pop(0)] = val\n\t\telse:\n\t\t\tsub_dic = dict_insert(sub_dic, val, key.pop(0), *keys)\n\treturn dict_insert(sub_dic, val, key.pop(0), *keys)\n\n#src/pysolr/solr/schema.py\nfrom collections import namedtuple\nfrom .config import DEFAULT_FIELD_TYPES\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttry:\n\t\tdic = dic[key]\n\texcept KeyError:\n\t\tdic[key] = val\n\telse:\n\t\tfor k in keys:\n\t\t\tdic = dic[k]\n\t\t\ttry:\n\t\t\t\tdic = dic[key]\n\t\t\texcept KeyError:\n\t\t\t\tdic[key] = val\n\t\t\telse:\n\t\t\t\tbreak\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tlast_key = keys[-1]\n\n\tif len(keys) == 1:\n\t\tdic[key] = val\n\telse:\n\t\tval = dict_insert(dic.get(key, {}), val, *keys[:-1])\n\t\tdic[key] = val\n\n\treturn dic\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkey_list = list(keys)\n\tkey_list.append(key)\n\tkey_path = '.'.join(key_list)\n\n\tif key_path not in dic:\n\t\tdic[key_path] = val\n\telse:\n\t\tdic[key_path] = val\n\n\treturn dic\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkeys = list(keys)\n\tkeys.append(key)\n\tkeys.reverse()\n\tif len(keys) == 1:\n\t\tdic[keys[0]] = val\n\telse:\n\t\tif keys[0] not in dic:\n\t\t\tdic[keys[0]] = {}\n\t\tdict_insert(dic[keys[0]], val, *keys[1:])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tsub_keys = keys[:-1]\n\tsub_key = keys[-1]\n\tif sub_keys:\n\t\tfor sub_key in sub_keys:\n\t\t\tif sub_key not in dic:\n\t\t\t\tdic[sub_key] = {}\n\t\t\tdic = dic[sub_key]\n\tif key not in dic:\n\t\tdic[key] = val\n\telse:\n\t\tdic[key] = [dic[key], val]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\td = dic\n\tfor k in keys[:-1]:\n\t\tif k not in d:\n\t\t\td = d[k] = {}\n\tk = keys[-1]\n\td[k] = val\n\treturn dic\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tcur_key = key\n\tnext_key = ''\n\n\tfor key in keys[:-1]:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tcur_key = key\n\t\tnext_key = keys[keys.index(key) + 1]\n\n\tdic[cur_key][next_key] = val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\tkey_list = key.split('.')\n\ttry:\n\t\tval = dic[key]\n\texcept KeyError:\n\t\tdic[key] = {}\n\tfor key in key_list[:-1]:\n\t\tdic = dic[key]\n\tdic[key_list[-1]] = val\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\t\"\"\"\n\tinsert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain\n\t\"\"\"\n\ttemp = dic\n\tfor k in keys[:-1]:\n\t\tif k in temp:\n\t\t\ttemp = temp[k]\n\t\telse:\n\t\t\ttemp[k] = {}\n\t\t\ttemp = temp[k]\n\ttemp[keys[-1]] = val\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\timport sys\n\tfrom IniType import IniType\n\n\tprint \"Calling cli.ListOfFileNames() ...\"\n\tif spec_option.spec_file:\n\t\tprint \"spec file is: \" + spec_option.spec_file\n\t\tspec_file = open(spec_option.spec_file, 'r')\n\t\tspec_file_content = spec_file.read()\n\t\tspec_file.close()\n\t\tspec_ini = IniType.create_from_string(spec_file_content)\n\telse:\n\t\tspec_ini = None\n\n\tparser = IniType.create(settings_dirs)\n\tparser.parse_from_cli(spec_option)\n\n\tif spec_ini:\n\t\tparser.override_with(spec_ini)\n\n\tlist_of_file_names = parser.ListOfFileNames()\n\treturn list_of_file_names\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom collections import OrderedDict\n\tfrom cli.IniFile import IniFile\n\tfrom cli.IniType import IniType\n\tfrom cli.ListOfFileNames import ListOfFileNames\n\tfrom cli.ListOfString import ListOfString\n\tfrom cli.Specification import Specification\n\tfrom cli.SpecificationOption import SpecificationOption\n\tfrom cli.String import String\n\tfrom cli.Type import Type\n\t\n\tfile_name_options = OrderedDict()\n\tfile_name_options['FileNames'] = ListOfString()\n\tfile_name_options['NoFileNames'] = Type()\n\t\n\tfile_name_options['FileNames'].value_type = String()\n\tfile_name_options['NoFileNames'].value_type = Type()\n\t\n\tspec = Specification(settings_dirs, spec_option)\n\tspec.add_option(file_name_options)\n\t\n\treturn ListOfFileNames(IniFile.load(spec.get_spec_file()))", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\tfile_names = cli.ListOfFileNames()\n\n\tif settings_dirs is not None:\n\t\tfor settings_dir in settings_dirs:\n\t\t\tfile_names.add_settings_directory(settings_dir)\n\n\tif spec_option is not None:\n\t\tfile_names.spec_option = spec_option\n\n\treturn file_names", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli.IniType import IniType\n\tfrom cli.IniTypeList import IniTypeList\n\tfrom cli.ini import cli_parse_ini_file\n\tfrom cli.util import check_spec_option\n\timport os\n\n\t# make sure we have an existing settings directory\n\tsettings_dirs = [os.path.expanduser(x) for x in settings_dirs]\n\tsettings_dirs = [os.path.abspath(x) for x in settings_dirs]\n\tfor settings_dir in settings_dirs:\n\t\tif not os.path.isdir(settings_dir):\n\t\t\traise Exception('settings_dir %s does not exist' % settings_dir)\n\n\t# check for valid spec_option\n\tcheck_spec_option(spec_option)\n\n\t# parse our ini file\n\tini_dict = cli_parse_ini_file(spec_option)\n\n\t# build the new IniTypeList object\n\tini_type_list = IniTypeList()\n\tini_type_list.set_spec(spec_option)\n\n\t# add each of our settings_dirs to the new IniTypeList object\n\tfor settings_dir in settings_dirs:\n\t\tini_type = IniType()\n\t\tini", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli.IniType import IniType\n\n\tfrom cli.ListOfFileNames import ListOfFileNames\n\n\tfrom cli.ListOfFileNames import ListOfFileNames\n\n\tlst_file_names = ListOfFileNames()\n\n\tlst_file_names.settings_dirs = settings_dirs\n\tlst_file_names.spec_option = spec_option\n\n\treturn lst_file_names\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\tlist_of_file_names = ListOfFileNames()\n\tlist_of_file_names.SetSettingsDirs(settings_dirs)\n\tlist_of_file_names.SetSpecOption(spec_option)\n\n\treturn list_of_file_names\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\tfrom cli import IniType\n\tfrom cli.type_list import ListOfFileNamesType\n\t\n\tclass ListOfFileNames(ListOfFileNamesType):\n\t\t\"\"\"\n\t\tA list of file names.\n\t\t\"\"\"\n\t\t\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tsuper(ListOfFileNames, self).__init__(*args, **kwargs)\n\t\t\tself.spec = spec_option\n\t\t\n\tdef create_list(self, settings_dirs):\n\t\t\"\"\"\n\t\tCreate and return a new ListOfFileNames object.\n\t\t\"\"\"\n\t\t\n\t\tfrom cli import IniType\n\t\tfrom cli.type_list import ListOfFileNamesType\n\t\t\n\t\tclass ListOfFileNames(ListOfFileNamesType):\n\t\t\t\"\"\"\n\t\t\tA list of file names.\n\t\t\t\"\"\"\n\t\t\t\n\t\t\tdef __init__(self, *args, **kwargs):\n\t\t\t\tsuper(ListOfFileNames, self).__init__(*args, **kwargs)\n\t\t\t\tself.spec = spec_option\n\t\n\t\tnew_list = ListOfFileNames()\n\t\t\n\t\tfrom cli import IniType\n\t\t\n\t\tfor settings_dir in settings_dirs:\n\t\t\tfrom cli import", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli.list_of_file_names import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tretval = []\n\t\n\tfor settings_dir in settings_dirs:\n\t\tretval.append(\n\t\t\tcli.ListOfFileNames(\n\t\t\t\tFileNames = settings_dir,\n\t\t\t\tSpec = spec_option\n\t\t\t)\n\t\t)\n\t\n\treturn retval\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\tlist_of_file_names = INIType.ListOfFileNames(spec_option)\n\tfor settings_dir in settings_dirs:\n\t\tlist_of_file_names.Add(settings_dir)\n\t\n\treturn list_of_file_names\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\timport sys\n\tfrom IniType import IniType\n\n\tprint \"Calling cli.ListOfFileNames() ...\"\n\tif spec_option.spec_file:\n\t\tprint \"spec file is: \" + spec_option.spec_file\n\t\tspec_file = open(spec_option.spec_file, 'r')\n\t\tspec_file_content = spec_file.read()\n\t\tspec_file.close()\n\t\tspec_ini = IniType.create_from_string(spec_file_content)\n\telse:\n\t\tspec_ini = None\n\n\tparser = IniType.create(settings_dirs)\n\tparser.parse_from_cli(spec_option)\n\n\tif spec_ini:\n\t\tparser.override_with(spec_ini)\n\n\tlist_of_file_names = parser.ListOfFileNames()\n\treturn list_of_file_names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom collections import OrderedDict\n\tfrom cli.IniFile import IniFile\n\tfrom cli.IniType import IniType\n\tfrom cli.ListOfFileNames import ListOfFileNames\n\tfrom cli.ListOfString import ListOfString\n\tfrom cli.Specification import Specification\n\tfrom cli.SpecificationOption import SpecificationOption\n\tfrom cli.String import String\n\tfrom cli.Type import Type\n\t\n\tfile_name_options = OrderedDict()\n\tfile_name_options['FileNames'] = ListOfString()\n\tfile_name_options['NoFileNames'] = Type()\n\t\n\tfile_name_options['FileNames'].value_type = String()\n\tfile_name_options['NoFileNames'].value_type = Type()\n\t\n\tspec = Specification(settings_dirs, spec_option)\n\tspec.add_option(file_name_options)\n\t\n\treturn ListOfFileNames(IniFile.load(spec.get_spec_file()))", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\tfile_names = cli.ListOfFileNames()\n\n\tif settings_dirs is not None:\n\t\tfor settings_dir in settings_dirs:\n\t\t\tfile_names.add_settings_directory(settings_dir)\n\n\tif spec_option is not None:\n\t\tfile_names.spec_option = spec_option\n\n\treturn file_names", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli.IniType import IniType\n\tfrom cli.IniTypeList import IniTypeList\n\tfrom cli.ini import cli_parse_ini_file\n\tfrom cli.util import check_spec_option\n\timport os\n\n\t# make sure we have an existing settings directory\n\tsettings_dirs = [os.path.expanduser(x) for x in settings_dirs]\n\tsettings_dirs = [os.path.abspath(x) for x in settings_dirs]\n\tfor settings_dir in settings_dirs:\n\t\tif not os.path.isdir(settings_dir):\n\t\t\traise Exception('settings_dir %s does not exist' % settings_dir)\n\n\t# check for valid spec_option\n\tcheck_spec_option(spec_option)\n\n\t# parse our ini file\n\tini_dict = cli_parse_ini_file(spec_option)\n\n\t# build the new IniTypeList object\n\tini_type_list = IniTypeList()\n\tini_type_list.set_spec(spec_option)\n\n\t# add each of our settings_dirs to the new IniTypeList object\n\tfor settings_dir in settings_dirs:\n\t\tini_type = IniType()\n\t\tini", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli.IniType import IniType\n\n\tfrom cli.ListOfFileNames import ListOfFileNames\n\n\tfrom cli.ListOfFileNames import ListOfFileNames\n\n\tlst_file_names = ListOfFileNames()\n\n\tlst_file_names.settings_dirs = settings_dirs\n\tlst_file_names.spec_option = spec_option\n\n\treturn lst_file_names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli import ListOfFileNames\n\tlist_of_file_names = ListOfFileNames()\n\tlist_of_file_names.SetSettingsDirs(settings_dirs)\n\tlist_of_file_names.SetSpecOption(spec_option)\n\n\treturn list_of_file_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\tfrom cli import IniType\n\tfrom cli.type_list import ListOfFileNamesType\n\t\n\tclass ListOfFileNames(ListOfFileNamesType):\n\t\t\"\"\"\n\t\tA list of file names.\n\t\t\"\"\"\n\t\t\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tsuper(ListOfFileNames, self).__init__(*args, **kwargs)\n\t\t\tself.spec = spec_option\n\t\t\n\tdef create_list(self, settings_dirs):\n\t\t\"\"\"\n\t\tCreate and return a new ListOfFileNames object.\n\t\t\"\"\"\n\t\t\n\t\tfrom cli import IniType\n\t\tfrom cli.type_list import ListOfFileNamesType\n\t\t\n\t\tclass ListOfFileNames(ListOfFileNamesType):\n\t\t\t\"\"\"\n\t\t\tA list of file names.\n\t\t\t\"\"\"\n\t\t\t\n\t\t\tdef __init__(self, *args, **kwargs):\n\t\t\t\tsuper(ListOfFileNames, self).__init__(*args, **kwargs)\n\t\t\t\tself.spec = spec_option\n\t\n\t\tnew_list = ListOfFileNames()\n\t\t\n\t\tfrom cli import IniType\n\t\t\n\t\tfor settings_dir in settings_dirs:\n\t\t\tfrom cli import", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tfrom cli.list_of_file_names import ListOfFileNames\n\treturn ListOfFileNames(settings_dirs, spec_option)", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\tretval = []\n\t\n\tfor settings_dir in settings_dirs:\n\t\tretval.append(\n\t\t\tcli.ListOfFileNames(\n\t\t\t\tFileNames = settings_dir,\n\t\t\t\tSpec = spec_option\n\t\t\t)\n\t\t)\n\t\n\treturn retval\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\"\"\"\n\tCreate and return a new IniType complex type via cli.ListOfFileNames()\n\t\"\"\"\n\t\n\tlist_of_file_names = INIType.ListOfFileNames(spec_option)\n\tfor settings_dir in settings_dirs:\n\t\tlist_of_file_names.Add(settings_dir)\n\t\n\treturn list_of_file_names\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tconfig_manager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\tif not config_manager:\n\t\traise RuntimeError('No Ansible Config Manager Service found')\n\treturn config_manager\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.ANSIBLE_CONFIG_MANAGER\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tmanager = cls._get_service('ANSIBLE_CONFIG_MANAGER')\n\tif manager is None:\n\t\traise Exception(\"No ansible config manager is available\")\n\telse:\n\t\treturn manager\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry: \n\t\treturn cls._get_service().get(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\texcept Exception as e:\n\t\traise Exception(f\"Couldn't get ansible config manager. Error: {e}\")\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.get(ANSIBLE_CONFIG_MANAGER)", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tansible_config_manager = cls._get_service().get('ANSIBLE_CONFIG_MANAGER')\n\tif ansible_config_manager:\n\t\treturn ansible_config_manager\n\traise Exception('Could not find ansible_config_manager')", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service:\n\t\treturn service.get(\"ANSIBLE_CONFIG_MANAGER\")\n\telse:\n\t\treturn None", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif not service:\n\t\treturn None\n\treturn service.get(service.ANSIBLE_CONFIG_MANAGER)\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service:\n\t\treturn service.get('ANSIBLE_CONFIG_MANAGER')\n\treturn ''\n\n", "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tansible_config_manager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\tif not ansible_config_manager:\n\t\traise ServiceNotFoundError(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\treturn ansible_config_manager"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tconfig_manager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\tif not config_manager:\n\t\traise RuntimeError('No Ansible Config Manager Service found')\n\treturn config_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.ANSIBLE_CONFIG_MANAGER\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tmanager = cls._get_service('ANSIBLE_CONFIG_MANAGER')\n\tif manager is None:\n\t\traise Exception(\"No ansible config manager is available\")\n\telse:\n\t\treturn manager\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry: \n\t\treturn cls._get_service().get(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\texcept Exception as e:\n\t\traise Exception(f\"Couldn't get ansible config manager. Error: {e}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.get(ANSIBLE_CONFIG_MANAGER)", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tansible_config_manager = cls._get_service().get('ANSIBLE_CONFIG_MANAGER')\n\tif ansible_config_manager:\n\t\treturn ansible_config_manager\n\traise Exception('Could not find ansible_config_manager')", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service:\n\t\treturn service.get(\"ANSIBLE_CONFIG_MANAGER\")\n\telse:\n\t\treturn None", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif not service:\n\t\treturn None\n\treturn service.get(service.ANSIBLE_CONFIG_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\tif service:\n\t\treturn service.get('ANSIBLE_CONFIG_MANAGER')\n\treturn ''\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_config_manager(cls):\n\t\"\"\"\n\tGets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()\n\t\"\"\"\n\tansible_config_manager = cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\tif not ansible_config_manager:\n\t\traise ServiceNotFoundError(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\treturn ansible_config_manager", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tdef get_workspace_manager(self):\n\t\twm = self._get_service(ServiceName.WORKSPACE_MANAGER)\n\t\treturn wm.workspace_manager\n\n\tcls.workspace_manager = property(get_workspace_manager)\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom service_names import ServiceName\n\treturn getattr(cls._get_service(ServiceName.WORKSPACE_MANAGER), 'workspace_manager')\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls._get_service().WORKSPACE_MANAGER\n\texcept:\n\t\tpass", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.WORKSPACE_MANAGER if service else None\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\twm = cls._get_service().WORKSPACE_MANAGER\n\treturn wm\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.get_service(service.WORKSPACE_MANAGER)\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.WORKSPACE_MANAGER\n\n", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\tmanager = getattr(cls._get_service(), 'WORKSPACE_MANAGER', None)\n\texcept AttributeError:\n\t\treturn None\n\telse:\n\t\treturn manager", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn getattr(service.WORKSPACE_MANAGER, cls.__name__.upper(), None)\n\n#src/py/service/__init__.py\n\nfrom .utils import workspace_manager\n\nfrom . import AUTH_SERVICE\nfrom . import FILE_SERVICE\nfrom . import PROJECT_SERVICE\nfrom . import WORKSPACE_SERVICE\nfrom . import WORKSPACE_MANAGER\n\n\n__all__ = [\n\t'AUTH_SERVICE',\n\t'FILE_SERVICE',\n\t'PROJECT_SERVICE',\n\t'WORKSPACE_SERVICE',\n\t'WORKSPACE_MANAGER'\n]\n\n#src/py/service/project/__init__.py\n\nfrom .utils import project_manager\n\nfrom . import PROJECT_MANAGER\n\n\n__all__ = [\n\t'PROJECT_MANAGER'\n]\n\n#src/py/service/workspace/__init__.py\n\nfrom .utils import workspace_manager\n\nfrom . import WORKSPACE_MANAGER\n\n\n__all__ = [\n\t'WORKSPACE_MANAGER'\n]\n\n#src/py/service/workspace/manager/__init__.py\n\nfrom .", "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tmanager = cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\treturn manager\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tdef get_workspace_manager(self):\n\t\twm = self._get_service(ServiceName.WORKSPACE_MANAGER)\n\t\treturn wm.workspace_manager\n\n\tcls.workspace_manager = property(get_workspace_manager)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom service_names import ServiceName\n\treturn getattr(cls._get_service(ServiceName.WORKSPACE_MANAGER), 'workspace_manager')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls._get_service().WORKSPACE_MANAGER\n\texcept:\n\t\tpass", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.WORKSPACE_MANAGER if service else None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\twm = cls._get_service().WORKSPACE_MANAGER\n\treturn wm\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.get_service(service.WORKSPACE_MANAGER)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.WORKSPACE_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\tmanager = getattr(cls._get_service(), 'WORKSPACE_MANAGER', None)\n\texcept AttributeError:\n\t\treturn None\n\telse:\n\t\treturn manager", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn getattr(service.WORKSPACE_MANAGER, cls.__name__.upper(), None)\n\n#src/py/service/__init__.py\n\nfrom .utils import workspace_manager\n\nfrom . import AUTH_SERVICE\nfrom . import FILE_SERVICE\nfrom . import PROJECT_SERVICE\nfrom . import WORKSPACE_SERVICE\nfrom . import WORKSPACE_MANAGER\n\n\n__all__ = [\n\t'AUTH_SERVICE',\n\t'FILE_SERVICE',\n\t'PROJECT_SERVICE',\n\t'WORKSPACE_SERVICE',\n\t'WORKSPACE_MANAGER'\n]\n\n#src/py/service/project/__init__.py\n\nfrom .utils import project_manager\n\nfrom . import PROJECT_MANAGER\n\n\n__all__ = [\n\t'PROJECT_MANAGER'\n]\n\n#src/py/service/workspace/__init__.py\n\nfrom .utils import workspace_manager\n\nfrom . import WORKSPACE_MANAGER\n\n\n__all__ = [\n\t'WORKSPACE_MANAGER'\n]\n\n#src/py/service/workspace/manager/__init__.py\n\nfrom .", "is_pass": false, "return_code": 1}, {"generate_code": "def workspace_manager(cls):\n\t\"\"\"\n\tGets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()\n\t\"\"\"\n\tmanager = cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\treturn manager\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls._get_service().PLUGINS_MANAGER\n\texcept AttributeError:\n\t\treturn None\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tmanager = cls._get_service(cls.PLUGINS_MANAGER)\n\tif not manager:\n\t\traise ValueError(f\"No plugins manager in {cls.__name__}\")\n\treturn manager\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tdef plugin_manager(self):\n\t\treturn self._get_service(ServiceName.PLUGINS_MANAGER)\n\n\tcls.plugin_manager = plugin_manager\n\n\treturn cls", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tresult = cls._get_service(ServiceName.PLUGINS_MANAGER)\n\tif result is None:\n\t\traise NameError(\"plugins_manager not found in %s\" % cls.__name__)\n\treturn result", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls.get_service(cls._get_service().PLUGINS_MANAGER)\n\texcept AttributeError:\n\t\treturn None\n\texcept KeyError:\n\t\treturn None\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom . import plugin_manager as pm\n\treturn pm.PluginManager(cls._get_service(pm.ServiceName.PLUGINS_MANAGER))\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom . import service\n\tplugin_manager = getattr(cls._get_service(), service.ServiceName.PLUGINS_MANAGER)\n\tcls._set_service(plugin_manager)\n\treturn cls\n\n", "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls._get_service( ServiceName.PLUGINS_MANAGER )\n\texcept:\n\t\traise Exception( \"plugins_manager() failed\" )\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls._get_service().PLUGINS_MANAGER\n\texcept AttributeError:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tmanager = cls._get_service(cls.PLUGINS_MANAGER)\n\tif not manager:\n\t\traise ValueError(f\"No plugins manager in {cls.__name__}\")\n\treturn manager\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tdef plugin_manager(self):\n\t\treturn self._get_service(ServiceName.PLUGINS_MANAGER)\n\n\tcls.plugin_manager = plugin_manager\n\n\treturn cls", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tresult = cls._get_service(ServiceName.PLUGINS_MANAGER)\n\tif result is None:\n\t\traise NameError(\"plugins_manager not found in %s\" % cls.__name__)\n\treturn result", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls.get_service(cls._get_service().PLUGINS_MANAGER)\n\texcept AttributeError:\n\t\treturn None\n\texcept KeyError:\n\t\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom . import plugin_manager as pm\n\treturn pm.PluginManager(cls._get_service(pm.ServiceName.PLUGINS_MANAGER))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tservice = cls._get_service()\n\treturn service.PLUGINS_MANAGER\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\tfrom . import service\n\tplugin_manager = getattr(cls._get_service(), service.ServiceName.PLUGINS_MANAGER)\n\tcls._set_service(plugin_manager)\n\treturn cls\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plugins_manager(cls):\n\t\"\"\"\n\tGets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()\n\t\"\"\"\n\ttry:\n\t\treturn cls._get_service( ServiceName.PLUGINS_MANAGER )\n\texcept:\n\t\traise Exception( \"plugins_manager() failed\" )\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException('Failed to parse YAML: ' + spec_content) from exc\n\texcept Exception as exc:\n\t\traise IRValidatorException('Failed to load spec data: ' + spec_content) from exc\n\n\trequired_fields = ['spec', 'version', 'services']\n\tfor field in required_fields:\n\t\tif field not in spec_data:\n\t\t\traise IRValidatorException('Missing required field: %s' % field)\n\n\treturn spec_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = {}\n\n\tif spec_content:\n\t\ttry:\n\t\t\tdata = yaml.safe_load(spec_content)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"Could not parse YAML content: %s\" % exc)\n\n\tmandatory_fields = ('name', 'version', 'authors', 'maintainers', 'description',\n\t\t'dependencies', 'categories')\n\n\tfor field in mandatory_fields:\n\t\tif field not in data:\n\t\t\traise IRValidatorException(\"Mandatory field '%s' is missing in spec file\" % field)\n\n\treturn data\n\n#src/ir/validator/validators/__init__.py\nfrom .from_content import validate_from_content\n\n__all__ = [\n\t'validate_from_content'\n]", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = yaml.safe_load(spec_content)\n\n\trequired_fields = cls.get_required_fields()\n\n\tmissing_mandatory_fields = []\n\tfor field in required_fields:\n\t\tif field not in spec.keys():\n\t\t\tmissing_mandatory_fields.append(field)\n\n\tif missing_mandatory_fields:\n\t\traise IRValidatorException(\n\t\t\tmessage=\"mandatory fields missing in spec file: %s\" % \", \".join(missing_mandatory_fields),\n\t\t\tdata=missing_mandatory_fields\n\t\t)\n\n\treturn spec\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = yaml.load(spec_content)\n\texcept ValueError:\n\t\traise IRValidatorException('Invalid YAML content')\n\n\trequired_keys = ['version', 'spec_version', 'name', 'project', 'description', 'type', 'language']\n\tfor key in required_keys:\n\t\tif key not in spec_data:\n\t\t\traise IRValidatorException('Required key {} is missing in {}'.format(key, spec_data))\n\n\treturn spec_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = {}\n\trequired_data = cls.get_required_data()\n\n\tif spec_content is not None:\n\t\ttry:\n\t\t\tspec_data = yaml.safe_load(spec_content)\n\t\texcept yaml.YAMLError as err:\n\t\t\traise IRValidatorException(f'{err}')\n\n\tfor key in required_data:\n\t\tif key not in spec_data:\n\t\t\traise IRValidatorException(f'Field {key} is missing in spec file')\n\n\treturn spec_data", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = yaml.safe_load(spec_content)\n\ttry:\n\t\tspec['id']\n\texcept KeyError:\n\t\traise IRValidatorException(\"Spec file must contain an 'id' field\")\n\n\ttry:\n\t\tspec['version']\n\texcept KeyError:\n\t\traise IRValidatorException(\"Spec file must contain a 'version' field\")\n\n\ttry:\n\t\tspec['data']\n\texcept KeyError:\n\t\traise IRValidatorException(\"Spec file must contain a 'data' field\")\n\n\ttry:\n\t\tspec['data']['name']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'name' field\")\n\n\ttry:\n\t\tspec['data']['type']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'type' field\")\n\n\ttry:\n\t\tspec['data']['description']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'description' field\")\n\n\ttry:\n\t\tspec['data']['properties']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'properties' field\")\n\n\treturn spec\n\n#src/ir_validator/validators/validators/", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = cls.load_spec_from_content(spec_content)\n\tcls._validate_spec_data(spec_data)\n\treturn spec_data\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = {}\n\n\tif spec_content:\n\t\tspec_data = yaml.load(spec_content)\n\n\tspec_data[\"name\"] = spec_data.get(\"name\", \"\")\n\tspec_data[\"type\"] = spec_data.get(\"type\", \"\")\n\tspec_data[\"description\"] = spec_data.get(\"description\", \"\")\n\n\tspec_data[\"data_format\"] = spec_data.get(\"data_format\", \"\")\n\tspec_data[\"data_schema\"] = spec_data.get(\"data_schema\", {})\n\tspec_data[\"data_format_options\"] = spec_data.get(\"data_format_options\", {})\n\tspec_data[\"output_format\"] = spec_data.get(\"output_format\", \"\")\n\tspec_data[\"output_format_options\"] = spec_data.get(\"output_format_options\", {})\n\tspec_data[\"output_format_options\"] = spec_data.get(\"output_format_options\", {})\n\n\treturn spec_data\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_spec = yaml.safe_load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Error loading spec from content\")\n\n\t# check mandatory fields\n\tif not \"spec\" in spec_spec:\n\t\traise IRValidatorException(\"spec field is missing in spec file\")\n\tif not \"version\" in spec_spec:\n\t\traise IRValidatorException(\"version field is missing in spec file\")\n\tif not \"resources\" in spec_spec:\n\t\traise IRValidatorException(\"resources field is missing in spec file\")\n\tif not \"parameters\" in spec_spec:\n\t\traise IRValidatorException(\"parameters field is missing in spec file\")\n\tif not \"outputs\" in spec_spec:\n\t\traise IRValidatorException(\"outputs field is missing in spec file\")\n\tif not \"resources\" in spec_spec:\n\t\traise IRValidatorException(\"resources field is missing in spec file\")\n\n\treturn spec_spec\n\n", "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = {}\n\n\tif spec_content is None:\n\t\traise IRValidatorException(\"spec_content is None\")\n\n\tdata = parse_yaml_string(spec_content)\n\n\t# validate each spec (YAML) entry\n\tfor entry_name, entry_data in data.items():\n\t\tvalidate_entry(entry_name, entry_data)\n\n\treturn data\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException('Failed to parse YAML: ' + spec_content) from exc\n\texcept Exception as exc:\n\t\traise IRValidatorException('Failed to load spec data: ' + spec_content) from exc\n\n\trequired_fields = ['spec', 'version', 'services']\n\tfor field in required_fields:\n\t\tif field not in spec_data:\n\t\t\traise IRValidatorException('Missing required field: %s' % field)\n\n\treturn spec_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = {}\n\n\tif spec_content:\n\t\ttry:\n\t\t\tdata = yaml.safe_load(spec_content)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"Could not parse YAML content: %s\" % exc)\n\n\tmandatory_fields = ('name', 'version', 'authors', 'maintainers', 'description',\n\t\t'dependencies', 'categories')\n\n\tfor field in mandatory_fields:\n\t\tif field not in data:\n\t\t\traise IRValidatorException(\"Mandatory field '%s' is missing in spec file\" % field)\n\n\treturn data\n\n#src/ir/validator/validators/__init__.py\nfrom .from_content import validate_from_content\n\n__all__ = [\n\t'validate_from_content'\n]", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = yaml.safe_load(spec_content)\n\n\trequired_fields = cls.get_required_fields()\n\n\tmissing_mandatory_fields = []\n\tfor field in required_fields:\n\t\tif field not in spec.keys():\n\t\t\tmissing_mandatory_fields.append(field)\n\n\tif missing_mandatory_fields:\n\t\traise IRValidatorException(\n\t\t\tmessage=\"mandatory fields missing in spec file: %s\" % \", \".join(missing_mandatory_fields),\n\t\t\tdata=missing_mandatory_fields\n\t\t)\n\n\treturn spec\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_data = yaml.load(spec_content)\n\texcept ValueError:\n\t\traise IRValidatorException('Invalid YAML content')\n\n\trequired_keys = ['version', 'spec_version', 'name', 'project', 'description', 'type', 'language']\n\tfor key in required_keys:\n\t\tif key not in spec_data:\n\t\t\traise IRValidatorException('Required key {} is missing in {}'.format(key, spec_data))\n\n\treturn spec_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = {}\n\trequired_data = cls.get_required_data()\n\n\tif spec_content is not None:\n\t\ttry:\n\t\t\tspec_data = yaml.safe_load(spec_content)\n\t\texcept yaml.YAMLError as err:\n\t\t\traise IRValidatorException(f'{err}')\n\n\tfor key in required_data:\n\t\tif key not in spec_data:\n\t\t\traise IRValidatorException(f'Field {key} is missing in spec file')\n\n\treturn spec_data", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec = yaml.safe_load(spec_content)\n\ttry:\n\t\tspec['id']\n\texcept KeyError:\n\t\traise IRValidatorException(\"Spec file must contain an 'id' field\")\n\n\ttry:\n\t\tspec['version']\n\texcept KeyError:\n\t\traise IRValidatorException(\"Spec file must contain a 'version' field\")\n\n\ttry:\n\t\tspec['data']\n\texcept KeyError:\n\t\traise IRValidatorException(\"Spec file must contain a 'data' field\")\n\n\ttry:\n\t\tspec['data']['name']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'name' field\")\n\n\ttry:\n\t\tspec['data']['type']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'type' field\")\n\n\ttry:\n\t\tspec['data']['description']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'description' field\")\n\n\ttry:\n\t\tspec['data']['properties']\n\texcept KeyError:\n\t\traise IRValidatorException(\"'data' field must contain a 'properties' field\")\n\n\treturn spec\n\n#src/ir_validator/validators/validators/", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = cls.load_spec_from_content(spec_content)\n\tcls._validate_spec_data(spec_data)\n\treturn spec_data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tspec_data = {}\n\n\tif spec_content:\n\t\tspec_data = yaml.load(spec_content)\n\n\tspec_data[\"name\"] = spec_data.get(\"name\", \"\")\n\tspec_data[\"type\"] = spec_data.get(\"type\", \"\")\n\tspec_data[\"description\"] = spec_data.get(\"description\", \"\")\n\n\tspec_data[\"data_format\"] = spec_data.get(\"data_format\", \"\")\n\tspec_data[\"data_schema\"] = spec_data.get(\"data_schema\", {})\n\tspec_data[\"data_format_options\"] = spec_data.get(\"data_format_options\", {})\n\tspec_data[\"output_format\"] = spec_data.get(\"output_format\", \"\")\n\tspec_data[\"output_format_options\"] = spec_data.get(\"output_format_options\", {})\n\tspec_data[\"output_format_options\"] = spec_data.get(\"output_format_options\", {})\n\n\treturn spec_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\ttry:\n\t\tspec_spec = yaml.safe_load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Error loading spec from content\")\n\n\t# check mandatory fields\n\tif not \"spec\" in spec_spec:\n\t\traise IRValidatorException(\"spec field is missing in spec file\")\n\tif not \"version\" in spec_spec:\n\t\traise IRValidatorException(\"version field is missing in spec file\")\n\tif not \"resources\" in spec_spec:\n\t\traise IRValidatorException(\"resources field is missing in spec file\")\n\tif not \"parameters\" in spec_spec:\n\t\traise IRValidatorException(\"parameters field is missing in spec file\")\n\tif not \"outputs\" in spec_spec:\n\t\traise IRValidatorException(\"outputs field is missing in spec file\")\n\tif not \"resources\" in spec_spec:\n\t\traise IRValidatorException(\"resources field is missing in spec file\")\n\n\treturn spec_spec\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t\"\"\"\n\tvalidates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file\n\t\"\"\"\n\tdata = {}\n\n\tif spec_content is None:\n\t\traise IRValidatorException(\"spec_content is None\")\n\n\tdata = parse_yaml_string(spec_content)\n\n\t# validate each spec (YAML) entry\n\tfor entry_name, entry_data in data.items():\n\t\tvalidate_entry(entry_name, entry_data)\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tdata = load_yaml(yaml_file)\n\tvalidate_required(cls, data)\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tdata = load_yaml_file(yaml_file)\n\tif not data:\n\t\traise IRValidatorException(\"Mandatory data is missing in file\")\n\n\tif not cls.required:\n\t\traise IRValidatorException(\"Required data is missing in class\")\n\n\tfor key in cls.required:\n\t\tif not data.get(key):\n\t\t\traise IRValidatorException(\"Mandatory data is missing in file - %s\" % (key))\n\n\treturn data\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file, 'r') as fd:\n\t\tyaml_data = yaml.safe_load(fd)\n\n\treturn cls.validate_from_dict(yaml_data)\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\timport yaml\n\t\n\tif not yaml_file:\n\t\tyaml_file = cls.get_yaml_file()\n\t\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = yaml.load(f.read())\n\texcept IOError as e:\n\t\traise IRValidatorException(\"Could not open YAML file: \" + yaml_file)\n\texcept yaml.YAMLError as e:\n\t\traise IRValidatorException(\"Could not parse YAML file: \" + yaml_file)\n\t\n\tif not data:\n\t\traise IRValidatorException(\"Could not load YAML file: \" + yaml_file)\n\t\n\trequired_data = cls.get_required_data()\n\t\n\tfor r in required_data:\n\t\tif not r in data:\n\t\t\traise IRValidatorException(\"Missing data in YAML file: \" + yaml_file)\n\t\n\tif not data['__COMMENT__']:\n\t\traise IRValidatorException(\"Missing comment in YAML file: \" + yaml_file)\n\t\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\tyaml_file = get_file_from_path(yaml_file)\n\t\twith open(yaml_file, 'r') as yaml_file:\n\t\t\tdata = yaml.load(yaml_file)\n\texcept IOError as e:\n\t\traise IRValidatorException(\"Unable to read YAML file: %s\" % e)\n\n\ttry:\n\t\tassert_mandatory_fields(data)\n\texcept AssertionError as e:\n\t\traise IRValidatorException(\"Mandatory fields missing in YAML file: %s\" % e)\n\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = yaml.safe_load(f)\n\texcept Exception as e:\n\t\traise IRValidatorException(f\"{e}\")\n\n\trequired = cls.required_fields()\n\n\tmissing = [field for field in required if field not in data]\n\n\tif missing:\n\t\traise IRValidatorException(f\"Missing mandatory data in file {yaml_file}: {missing}\")\n\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tconfig = yaml.load(f)\n\texcept IOError as e:\n\t\traise IRValidatorException(\"Unable to load YAML file: {}\".format(e))\n\n\tif not config:\n\t\traise IRValidatorException(\"Empty YAML file\")\n\n\ttry:\n\t\tcls.validate(config)\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(\"YAML file is not valid: {}\".format(e))\n\n\treturn config", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tyaml_data = {}\n\tif not yaml_file:\n\t\tyaml_data = load_yaml(yaml_file)\n\telse:\n\t\tyaml_data = load_yaml(yaml_file)\n\tvalidate_data(yaml_data)\n\treturn yaml_data\n", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tdata = None\n\tif yaml_file is None:\n\t\tdata = load_yaml()\n\telse:\n\t\tdata = load_yaml_from_file(yaml_file)\n\n\tvalidator = cls.get_validator()\n\tvalidator.validate(data)\n\treturn data", "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tyaml_data = yaml.load(stream)\n\texcept Exception as e:\n\t\traise IRValidatorException('Invalid YAML file {0}'.format(yaml_file))\n\tcls.validate(yaml_data)\n\treturn yaml_data\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tdata = load_yaml(yaml_file)\n\tvalidate_required(cls, data)\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\tdata = load_yaml_file(yaml_file)\n\tif not data:\n\t\traise IRValidatorException(\"Mandatory data is missing in file\")\n\n\tif not cls.required:\n\t\traise IRValidatorException(\"Required data is missing in class\")\n\n\tfor key in cls.required:\n\t\tif not data.get(key):\n\t\t\traise IRValidatorException(\"Mandatory data is missing in file - %s\" % (key))\n\n\treturn data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\twith open(yaml_file, 'r') as fd:\n\t\tyaml_data = yaml.safe_load(fd)\n\n\treturn cls.validate_from_dict(yaml_data)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\t\n\timport yaml\n\t\n\tif not yaml_file:\n\t\tyaml_file = cls.get_yaml_file()\n\t\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = yaml.load(f.read())\n\texcept IOError as e:\n\t\traise IRValidatorException(\"Could not open YAML file: \" + yaml_file)\n\texcept yaml.YAMLError as e:\n\t\traise IRValidatorException(\"Could not parse YAML file: \" + yaml_file)\n\t\n\tif not data:\n\t\traise IRValidatorException(\"Could not load YAML file: \" + yaml_file)\n\t\n\trequired_data = cls.get_required_data()\n\t\n\tfor r in required_data:\n\t\tif not r in data:\n\t\t\traise IRValidatorException(\"Missing data in YAML file: \" + yaml_file)\n\t\n\tif not data['__COMMENT__']:\n\t\traise IRValidatorException(\"Missing comment in YAML file: \" + yaml_file)\n\t\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\tyaml_file = get_file_from_path(yaml_file)\n\t\twith open(yaml_file, 'r') as yaml_file:\n\t\t\tdata = yaml.load(yaml_file)\n\texcept IOError as e:\n\t\traise IRValidatorException(\"Unable to read YAML file: %s\" % e)\n\n\ttry:\n\t\tassert_mandatory_fields(data)\n\texcept AssertionError as e:\n\t\traise IRValidatorException(\"Mandatory fields missing in YAML file: %s\" % e)\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = yaml.safe_load(f)\n\texcept Exception as e:\n\t\traise IRValidatorException(f\"{e}\")\n\n\trequired = cls.required_fields()\n\n\tmissing = [field for field in required if field not in data]\n\n\tif missing:\n\t\traise IRValidatorException(f\"Missing mandatory data in file {yaml_file}: {missing}\")\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tconfig = yaml.load(f)\n\texcept IOError as e:\n\t\traise IRValidatorException(\"Unable to load YAML file: {}\".format(e))\n\n\tif not config:\n\t\traise IRValidatorException(\"Empty YAML file\")\n\n\ttry:\n\t\tcls.validate(config)\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(\"YAML file is not valid: {}\".format(e))\n\n\treturn config", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tyaml_data = {}\n\tif not yaml_file:\n\t\tyaml_data = load_yaml(yaml_file)\n\telse:\n\t\tyaml_data = load_yaml(yaml_file)\n\tvalidate_data(yaml_data)\n\treturn yaml_data\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\tdata = None\n\tif yaml_file is None:\n\t\tdata = load_yaml()\n\telse:\n\t\tdata = load_yaml_from_file(yaml_file)\n\n\tvalidator = cls.get_validator()\n\tvalidator.validate(data)\n\treturn data", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\t\"\"\"\n\tLoads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file\n\t\"\"\"\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tyaml_data = yaml.load(stream)\n\texcept Exception as e:\n\t\traise IRValidatorException('Invalid YAML file {0}'.format(yaml_file))\n\tcls.validate(yaml_data)\n\treturn yaml_data\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tdef _include_group(self, group_name):\n\t\t\"\"\"\n\t\tIncludes one group of a spec file.\n\t\t\"\"\"\n\t\tparser = parser_dict.get(group_name)\n\t\t\n\t\t# If the group has not been parsed, parse it.\n\t\tif not parser:\n\t\t\tparser = SpecParser(group_name, self.parser_dict)\n\t\t\tparser_dict[group_name] = parser\n\t\t\n\t\t# Parse the group and resolve the includes.\n\t\tparser.parse_spec_group()\n\t\tself._include_groups(parser_dict, parser.group_dict)\n\n\tdef _include_spec(self, spec_name):\n\t\t\"\"\"\n\t\tIncludes one spec.\n\t\t\"\"\"\n\t\tparser = self.parser_dict.get(spec_name)\n\t\tif not parser:\n\t\t\tparser = SpecParser(spec_name, self.parser_dict)\n\t\t\tself.parser_dict[spec_name] = parser\n\t\t\n\t\t# Parse the spec.\n\t\tparser.parse_spec()\n\t\t\n\t\t# Resolve the includes.\n\t\tself._include_groups(self.parser_dict, parser.group_dict)\n\t\n\tdef _resolve_includes(self):\n\t\t\"\"\"\n\t", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get(\"include_groups\", [])\n\tfor group in include_groups:\n\t\tself._include_group(group)\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict['include_groups'].split(',')\n\n\t# Read each include group.\n\tparser_dict['includes'] = []\n\tfor include_group in include_groups:\n\t\tinclude_group_parser = parser_dict.get('include_' + include_group, None)\n\t\tif include_group_parser is not None:\n\t\t\tparser_dict['includes'].extend(include_group_parser['includes'])\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get(\"include_groups\", None)\n\tif include_groups is not None:\n\t\tfor group in include_groups:\n\t\t\tself._include_group(group)\n\n\tspec_files = parser_dict.get(\"spec_files\", None)\n\tif spec_files is not None:\n\t\tfor filename in spec_files:\n\t\t\tself._include_file(filename)\n\n\tdef _include_group(self, group):\n\t\t\"\"\"\n\t\tIncludes a group of spec files.\n\t\t\"\"\"\n\t\tspec_files = group.get(\"spec_files\", None)\n\t\tif spec_files is not None:\n\t\t\tfor filename in spec_files:\n\t\t\t\tself._include_file(filename)\n\n\tdef _include_file(self, filename):\n\t\t\"\"\"\n\t\tIncludes a single spec file.\n\t\t\"\"\"\n\t\tbase, ext = os.path.splitext(filename)\n\t\tif ext != \".spec\":\n\t\t\tfilename += \".spec\"\n\t\tspec_file = SpecFile.from_filename(filename)\n\t\tself.add_spec_file(spec_file)\n\t\tself.spec_files.append(spec_file)\n\n\tdef _get_spec_files(self):\n\t\t\"\"\"", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfrom common import get_spec_file\n\tfrom spec_parser import SpecParser\n\tfrom spec_parser import SpecParserError\n\tspec_file = get_spec_file(parser_dict[\"specs\"])\n\tif spec_file:\n\t\tparser = SpecParser(spec_file)\n\t\ttry:\n\t\t\tparser.parse()\n\t\t\tparser_dict[\"includes\"] = parser.get_includes()\n\t\texcept SpecParserError as error:\n\t\t\tparser_dict[\"includes\"] = []\n\telse:\n\t\tparser_dict[\"includes\"] = []\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get('include', None)\n\tif include_groups is None:\n\t\tself.warn(f\"No include group defined in spec file {self.filename}\")\n\t\treturn\n\n\tself.resolved_include_groups = []\n\n\tfor group in include_groups:\n\t\tself.include_group(group)\n\n\tself.resolved_include_groups.sort()\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = self._get_include_groups(self.include_dict)\n\tparser_dict.update(include_groups)\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser_dict = _resolve_includes(self, parser_dict)\n\n\tparser_dict['input_groups'] = []\n\n\tfor group_name, group_dict in parser_dict['groups'].items():\n\t\tif group_dict['type'] == 'include':\n\t\t\tparser_dict['input_groups'].append(group_name)\n\n\treturn parser_dict\n\n", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tpath_list = []\n\tinclude_list = []\n\n\tdef _resolve_include(include_path):\n\t\t\"\"\"\n\t\tResolves the include path and appends it to the include list.\n\t\t\"\"\"\n\t\tinclude_path = os.path.abspath(include_path)\n\t\tinclude_path = os.path.relpath(include_path, os.path.dirname(self.spec_file))\n\t\tif include_path not in path_list:\n\t\t\tpath_list.append(include_path)\n\t\t\tinclude_list.append(include_path)\n\t\t\tself.include_paths.append(include_path)\n\n\tdef _resolve_includes(include_dict):\n\t\t\"\"\"\n\t\tRecursively resolves include paths.\n\t\t\"\"\"\n\t\tfor path in include_dict:\n\t\t\tif isinstance(include_dict[path], dict):\n\t\t\t\t_resolve_includes(include_dict[path])\n\t\t\telse:\n\t\t\t\t_resolve_include(include_dict[path])\n\n\t# Resolve the include path from the parser dict.\n\t_resolve_include(parser_dict['include'])\n\n\t# Recursively resolve all the included paths.\n\t_resolve_includes(parser_dict['includes'])\n\n\tself.include_paths_set = set", "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\tinclude = parser_dict[\"include\"]\n\t\n\t# If there are no includes, there is nothing to do.\n\tif include is None:\n\t\treturn\n\t\n\t# Include the specified files\n\tself.spec_files.extend(include)\n\t\n\t# If the include directive is a list, resolve it recursively\n\tif isinstance(include, list):\n\t\tfor include_file in include:\n\t\t\twith open(include_file, \"r\") as file:\n\t\t\t\tself.resolve_spec_file(parser_dict, file.read())\n\n#src/parser.py\nimport os\nimport re\n\nfrom .directives import _include_groups\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tdef _include_group(self, group_name):\n\t\t\"\"\"\n\t\tIncludes one group of a spec file.\n\t\t\"\"\"\n\t\tparser = parser_dict.get(group_name)\n\t\t\n\t\t# If the group has not been parsed, parse it.\n\t\tif not parser:\n\t\t\tparser = SpecParser(group_name, self.parser_dict)\n\t\t\tparser_dict[group_name] = parser\n\t\t\n\t\t# Parse the group and resolve the includes.\n\t\tparser.parse_spec_group()\n\t\tself._include_groups(parser_dict, parser.group_dict)\n\n\tdef _include_spec(self, spec_name):\n\t\t\"\"\"\n\t\tIncludes one spec.\n\t\t\"\"\"\n\t\tparser = self.parser_dict.get(spec_name)\n\t\tif not parser:\n\t\t\tparser = SpecParser(spec_name, self.parser_dict)\n\t\t\tself.parser_dict[spec_name] = parser\n\t\t\n\t\t# Parse the spec.\n\t\tparser.parse_spec()\n\t\t\n\t\t# Resolve the includes.\n\t\tself._include_groups(self.parser_dict, parser.group_dict)\n\t\n\tdef _resolve_includes(self):\n\t\t\"\"\"\n\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get(\"include_groups\", [])\n\tfor group in include_groups:\n\t\tself._include_group(group)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict['include_groups'].split(',')\n\n\t# Read each include group.\n\tparser_dict['includes'] = []\n\tfor include_group in include_groups:\n\t\tinclude_group_parser = parser_dict.get('include_' + include_group, None)\n\t\tif include_group_parser is not None:\n\t\t\tparser_dict['includes'].extend(include_group_parser['includes'])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get(\"include_groups\", None)\n\tif include_groups is not None:\n\t\tfor group in include_groups:\n\t\t\tself._include_group(group)\n\n\tspec_files = parser_dict.get(\"spec_files\", None)\n\tif spec_files is not None:\n\t\tfor filename in spec_files:\n\t\t\tself._include_file(filename)\n\n\tdef _include_group(self, group):\n\t\t\"\"\"\n\t\tIncludes a group of spec files.\n\t\t\"\"\"\n\t\tspec_files = group.get(\"spec_files\", None)\n\t\tif spec_files is not None:\n\t\t\tfor filename in spec_files:\n\t\t\t\tself._include_file(filename)\n\n\tdef _include_file(self, filename):\n\t\t\"\"\"\n\t\tIncludes a single spec file.\n\t\t\"\"\"\n\t\tbase, ext = os.path.splitext(filename)\n\t\tif ext != \".spec\":\n\t\t\tfilename += \".spec\"\n\t\tspec_file = SpecFile.from_filename(filename)\n\t\tself.add_spec_file(spec_file)\n\t\tself.spec_files.append(spec_file)\n\n\tdef _get_spec_files(self):\n\t\t\"\"\"", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tfrom common import get_spec_file\n\tfrom spec_parser import SpecParser\n\tfrom spec_parser import SpecParserError\n\tspec_file = get_spec_file(parser_dict[\"specs\"])\n\tif spec_file:\n\t\tparser = SpecParser(spec_file)\n\t\ttry:\n\t\t\tparser.parse()\n\t\t\tparser_dict[\"includes\"] = parser.get_includes()\n\t\texcept SpecParserError as error:\n\t\t\tparser_dict[\"includes\"] = []\n\telse:\n\t\tparser_dict[\"includes\"] = []\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = parser_dict.get('include', None)\n\tif include_groups is None:\n\t\tself.warn(f\"No include group defined in spec file {self.filename}\")\n\t\treturn\n\n\tself.resolved_include_groups = []\n\n\tfor group in include_groups:\n\t\tself.include_group(group)\n\n\tself.resolved_include_groups.sort()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tinclude_groups = self._get_include_groups(self.include_dict)\n\tparser_dict.update(include_groups)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tparser_dict = _resolve_includes(self, parser_dict)\n\n\tparser_dict['input_groups'] = []\n\n\tfor group_name, group_dict in parser_dict['groups'].items():\n\t\tif group_dict['type'] == 'include':\n\t\t\tparser_dict['input_groups'].append(group_name)\n\n\treturn parser_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\tpath_list = []\n\tinclude_list = []\n\n\tdef _resolve_include(include_path):\n\t\t\"\"\"\n\t\tResolves the include path and appends it to the include list.\n\t\t\"\"\"\n\t\tinclude_path = os.path.abspath(include_path)\n\t\tinclude_path = os.path.relpath(include_path, os.path.dirname(self.spec_file))\n\t\tif include_path not in path_list:\n\t\t\tpath_list.append(include_path)\n\t\t\tinclude_list.append(include_path)\n\t\t\tself.include_paths.append(include_path)\n\n\tdef _resolve_includes(include_dict):\n\t\t\"\"\"\n\t\tRecursively resolves include paths.\n\t\t\"\"\"\n\t\tfor path in include_dict:\n\t\t\tif isinstance(include_dict[path], dict):\n\t\t\t\t_resolve_includes(include_dict[path])\n\t\t\telse:\n\t\t\t\t_resolve_include(include_dict[path])\n\n\t# Resolve the include path from the parser dict.\n\t_resolve_include(parser_dict['include'])\n\n\t# Recursively resolve all the included paths.\n\t_resolve_includes(parser_dict['includes'])\n\n\tself.include_paths_set = set", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\"\"\"\n\tResolves the include dict directive in the spec files.\n\t\"\"\"\n\t\n\tinclude = parser_dict[\"include\"]\n\t\n\t# If there are no includes, there is nothing to do.\n\tif include is None:\n\t\treturn\n\t\n\t# Include the specified files\n\tself.spec_files.extend(include)\n\t\n\t# If the include directive is a list, resolve it recursively\n\tif isinstance(include, list):\n\t\tfor include_file in include:\n\t\t\twith open(include_file, \"r\") as file:\n\t\t\t\tself.resolve_spec_file(parser_dict, file.read())\n\n#src/parser.py\nimport os\nimport re\n\nfrom .directives import _include_groups\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = dict(self._get_defaults())\n\n\tfor source in self.spec.get('sources', []):\n\t\tdefaults.update(source.get('defaults', {}))\n\n\treturn defaults\n\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tdefaults = self._get_defaults()\n\n\tif not defaults:\n\t\treturn spec\n\n\tdefaults = self.resolve_defaults(spec, defaults)\n\treturn defaults\n\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\n\tfor name, val in self.defaults.items():\n\t\tif val is not None:\n\t\t\tdefaults[name] = self._get_defaults(name, val)\n\n\treturn defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\tif isinstance(defaults, dict):\n\t\tdefaults.update(self._get_defaults())\n\telse:\n\t\tdefaults = self._get_defaults()\n\treturn defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\n\tif self.spec:\n\t\tdefaults = self._get_spec_defaults(defaults)\n\n\tself.spec_defaults = defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec_defaults = self._get_defaults()\n\tdefaults = self.defaults()\n\n\t# Merge defaults and spec's settings\n\tdefaults.update(spec_defaults)\n\n\treturn defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\n\tdefaults = {}\n\tdefaults.update(self._get_defaults())\n\tdefaults.update(self._get_spec_defaults(spec))\n\n\tdefaults.update(self._get_env_defaults())\n\n\treturn defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec_defaults = self._get_defaults()\n\tspec_defaults = self._resolve_defaults(spec_defaults)\n\n\tconfig_defaults = self._get_config_defaults()\n\tconfig_defaults = self._resolve_defaults(config_defaults)\n\n\tdefaults = {**config_defaults, **spec_defaults}\n\n\treturn defaults\n", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\n\tdefaults = {\n\t\t'name': self._name_from_spec(),\n\t\t'version': self._version_from_spec(),\n\t\t'description': self._description_from_spec(),\n\t\t'author': self._author_from_spec(),\n\t\t'license': self._license_from_spec(),\n\t\t'maintainer': self._maintainer_from_spec(),\n\t\t'maintainer_email': self._maintainer_email_from_spec(),\n\t\t'url': self._url_from_spec(),\n\t\t'download_url': self._download_url_from_spec(),\n\t\t'platforms': self._platforms_from_spec(),\n\t\t'license_file': self._license_file_from_spec(),\n\t\t'package_data': self._package_data_from_spec(),\n\t\t'classifiers': self._classifiers_from_spec(),\n\t\t'long_description': self._long_description_from_spec(),\n\t\t'long_description_content_type': self._long_description_content_type_from_spec(),\n\t\t'project_urls': self._project_urls_from_spec(),\n\t\t'entry_points': self._entry_points_from_spec(),\n\t\t'python_requires': self._python_requires", "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\tdefaults.extend(self._get_from_spec())\n\treturn defaults\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = dict(self._get_defaults())\n\n\tfor source in self.spec.get('sources', []):\n\t\tdefaults.update(source.get('defaults', {}))\n\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\tdefaults = self._get_defaults()\n\n\tif not defaults:\n\t\treturn spec\n\n\tdefaults = self.resolve_defaults(spec, defaults)\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = {}\n\n\tfor name, val in self.defaults.items():\n\t\tif val is not None:\n\t\t\tdefaults[name] = self._get_defaults(name, val)\n\n\treturn defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\tif isinstance(defaults, dict):\n\t\tdefaults.update(self._get_defaults())\n\telse:\n\t\tdefaults = self._get_defaults()\n\treturn defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\n\tif self.spec:\n\t\tdefaults = self._get_spec_defaults(defaults)\n\n\tself.spec_defaults = defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec_defaults = self._get_defaults()\n\tdefaults = self.defaults()\n\n\t# Merge defaults and spec's settings\n\tdefaults.update(spec_defaults)\n\n\treturn defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\n\tdefaults = {}\n\tdefaults.update(self._get_defaults())\n\tdefaults.update(self._get_spec_defaults(spec))\n\n\tdefaults.update(self._get_env_defaults())\n\n\treturn defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec_defaults = self._get_defaults()\n\tspec_defaults = self._resolve_defaults(spec_defaults)\n\n\tconfig_defaults = self._get_config_defaults()\n\tconfig_defaults = self._resolve_defaults(config_defaults)\n\n\tdefaults = {**config_defaults, **spec_defaults}\n\n\treturn defaults\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tspec = self.spec\n\n\tdefaults = {\n\t\t'name': self._name_from_spec(),\n\t\t'version': self._version_from_spec(),\n\t\t'description': self._description_from_spec(),\n\t\t'author': self._author_from_spec(),\n\t\t'license': self._license_from_spec(),\n\t\t'maintainer': self._maintainer_from_spec(),\n\t\t'maintainer_email': self._maintainer_email_from_spec(),\n\t\t'url': self._url_from_spec(),\n\t\t'download_url': self._download_url_from_spec(),\n\t\t'platforms': self._platforms_from_spec(),\n\t\t'license_file': self._license_file_from_spec(),\n\t\t'package_data': self._package_data_from_spec(),\n\t\t'classifiers': self._classifiers_from_spec(),\n\t\t'long_description': self._long_description_from_spec(),\n\t\t'long_description_content_type': self._long_description_content_type_from_spec(),\n\t\t'project_urls': self._project_urls_from_spec(),\n\t\t'entry_points': self._entry_points_from_spec(),\n\t\t'python_requires': self._python_requires", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\t\"\"\"\n\tResolve arguments' values from spec and other sources with self._get_defaults()\n\t\"\"\"\n\tdefaults = self._get_defaults()\n\tdefaults.extend(self._get_from_spec())\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_options = {}\n\tfor arg_name in self.spec_helper.deprecated_options:\n\t\tdeprecated_options[arg_name] = getattr(self, arg_name)\n\treturn deprecated_options\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_options = {\n\t\t'--no-cache': '--no-cache option is deprecated and will be removed in future versions. Use --no-cache option with --no-deps.',\n\t\t'--no-deps': '--no-deps option is deprecated and will be removed in future versions. Use --no-cache option with --no-deps.',\n\t\t'--no-build-isolation': '--no-build-isolation option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache': '--no-build-use-cache option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache-for-tests': '--no-build-use-cache-for-tests option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache-for-all': '--no-build-use-cache-for-all option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache-for-tests-and-all': '--no-build", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option['deprecated']:\n\t\t\tdeprecated_args[option['name']] = self.__doc__.format(**option)\n\treturn deprecated_args\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tif self.spec_helper.deprecated_args:\n\t\tdeprecated_args['deprecated_args'] = self.spec_helper.deprecated_args\n\treturn deprecated_args\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\toptions = self.spec_helper.iterate_option_specs()\n\tdeprecated_options = dict((k, v) for k, v in options if k in self.deprecated_options)\n\treturn deprecated_options", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif spec['deprecated']:\n\t\t\tdeprecated_args[spec['name']] = spec['default']\n\treturn deprecated_args\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\targs_deprecation = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option.spec.deprecated:\n\t\t\targs_deprecation[option.name] = option.spec.args\n\treturn args_deprecation\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor key, value in self.spec_helper.iterate_option_specs():\n\t\tif self.spec_helper.is_deprecated(key):\n\t\t\tdeprecated_args[key] = value\n\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option['deprecated']:\n\t\t\tdeprecated_args[option['name']] = option['default']\n\treturn deprecated_args", "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.iterate_option_specs():\n\t\tif option.deprecated:\n\t\t\tdeprecated_args[option.name] = option.default\n\n\treturn deprecated_args"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_options = {}\n\tfor arg_name in self.spec_helper.deprecated_options:\n\t\tdeprecated_options[arg_name] = getattr(self, arg_name)\n\treturn deprecated_options\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_options = {\n\t\t'--no-cache': '--no-cache option is deprecated and will be removed in future versions. Use --no-cache option with --no-deps.',\n\t\t'--no-deps': '--no-deps option is deprecated and will be removed in future versions. Use --no-cache option with --no-deps.',\n\t\t'--no-build-isolation': '--no-build-isolation option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache': '--no-build-use-cache option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache-for-tests': '--no-build-use-cache-for-tests option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache-for-all': '--no-build-use-cache-for-all option is deprecated and will be removed in future versions. Use --no-deps option for dependencies.',\n\t\t'--no-build-use-cache-for-tests-and-all': '--no-build", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option['deprecated']:\n\t\t\tdeprecated_args[option['name']] = self.__doc__.format(**option)\n\treturn deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tif self.spec_helper.deprecated_args:\n\t\tdeprecated_args['deprecated_args'] = self.spec_helper.deprecated_args\n\treturn deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\toptions = self.spec_helper.iterate_option_specs()\n\tdeprecated_options = dict((k, v) for k, v in options if k in self.deprecated_options)\n\treturn deprecated_options", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif spec['deprecated']:\n\t\t\tdeprecated_args[spec['name']] = spec['default']\n\treturn deprecated_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\targs_deprecation = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option.spec.deprecated:\n\t\t\targs_deprecation[option.name] = option.spec.args\n\treturn args_deprecation\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor key, value in self.spec_helper.iterate_option_specs():\n\t\tif self.spec_helper.is_deprecated(key):\n\t\t\tdeprecated_args[key] = value\n\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option['deprecated']:\n\t\t\tdeprecated_args[option['name']] = option['default']\n\treturn deprecated_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\t\"\"\"\n\tReturning dict with options which deprecate others. options from self.spec_helper.iterate_option_specs()\n\t\"\"\"\n\tdeprecated_args = {}\n\tfor option in self.iterate_option_specs():\n\t\tif option.deprecated:\n\t\t\tdeprecated_args[option.name] = option.default\n\n\treturn deprecated_args", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"Warning: The following arguments are deprecated:\")\n\tdeprecation_list = sorted(set(cli_args.keys() + answer_file_args.keys()))\n\tfor key in deprecation_list:\n\t\tif key in cli_args.keys() and key in answer_file_args.keys():\n\t\t\tprint(\"-\", key, \":\", cli_args[key], \"is deprecated and will be removed in future version\")\n\t\tif key in cli_args.keys():\n\t\t\tprint(\"-\", key, \":\", cli_args[key], \"is deprecated and will be removed in future version\")\n\t\tif key in answer_file_args.keys():\n\t\t\tprint(\"-\", key, \":\", answer_file_args[key], \"is deprecated and will be removed in future version\")\n\t\tprint(\"\")\n\n#python/src/vcs_tool/cli/__init__.py\nfrom .validate_args import validate_arg_deprecation\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tvalidate_args = self.validate_args(cli_args, answer_file_args)\n\tself.print_deprecation_notices(validate_args)\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef validate_deprecated_arg(arg, arg_type):\n\t\tif arg in cli_args:\n\t\t\tif (arg in answer_file_args) and (answer_file_args[arg]):\n\t\t\t\tprint(\"Warning: argument \" + arg + \" is deprecated. Please use \" + arg_type + \" instead.\")\n\n\tvalidate_deprecated_arg(\"-p\", \"--path\")\n\tvalidate_deprecated_arg(\"-d\", \"--directory\")\n\tvalidate_deprecated_arg(\"-o\", \"--output\")\n\tvalidate_deprecated_arg(\"-n\", \"--name\")\n\tvalidate_deprecated_arg(\"-s\", \"--size\")\n\tvalidate_deprecated_arg(\"-q\", \"--quality\")\n\tvalidate_deprecated_arg(\"-f\", \"--format\")", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecation_message = ''\n\tdeprecation_message += 'DEPRECATED ARGUMENT MESSAGES\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'Arguments in the answer file are deprecated. Please update your answer file to use the new arguments.\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'For example, the following argument should be changed:\\n'\n\tdeprecation_message += '  - --timeout=600\\n'\n\tdeprecation_message += '  - --timeout=600\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'To use the new arguments, please update your answer file to:\\n'\n\tdeprecation_message += '  - --timeout 600\\n'\n\tdeprecation_message += '  - --timeout\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'Note that there are two different --timeout arguments.\\n'\n\tdeprecation_message += 'The first argument is a positional argument, and the second argument is a key-value argument.\\n'\n\tdeprecation_message += 'The value of the key-value argument is not used.", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"Validating arguments...\")\n\n\t# Check if the deprecated arguments are in the cli\n\tfor arg in self.DEPRECATION_ARGS:\n\t\tif arg in cli_args:\n\t\t\tprint(\"The deprecated argument:\", arg, \"is set to:\", cli_args[arg])\n\n\t# Check if the deprecated arguments are in the answer file\n\tfor arg in self.DEPRECATION_ARGS:\n\t\tif arg in answer_file_args:\n\t\t\tprint(\"The deprecated argument:\", arg, \"is set to:\", answer_file_args[arg])\n\n#validate_args.py\n\"\"\"\nValidates the input arguments provided by the user.\n\"\"\"\nimport argparse\nfrom deprecation_validation import validate_arg_deprecation\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"This program is deprecated. Please use the new command line interface.\")\n\tprint(\"To see the new command line interface, run:\")\n\tprint(\"python3 -m python_cli\")\n\tprint(\"To see the new command line interface's arguments, run:\")\n\tprint(\"python3 -m python_cli -h\")\n\tprint(\"To see the new command file's arguments, run:\")\n\tprint(\"python3 -m python_cli --help\")\n\tprint(\"To see the new command line interface's arguments with examples, run:\")\n\tprint(\"python3 -m python_cli -e\")\n\tprint(\"To see the new command file's arguments with examples, run:\")\n\tprint(\"python3 -m python_cli --help -e\")\n\n#python_cli/__init__.py\n# -*- coding: utf-8 -*-\n\n\"\"\"Top-level package for python-cli.\"\"\"\n\n__author__ = \"\"\"Cody Jones\"\"\"\n__email__ = 'cody@codyjones.net'\n__version__ = '0.1.0'\n\nfrom .cli import main\n\nif __name__ == \"__main__\":\n\tmain()\n\n#docs/conf.py\n# -*- coding: utf-8 -*-\n\nimport os\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef print_deprecation_warning(arg, old_name, new_name, deprecation_date):\n\t\tprint(\"Warning: The argument \" + old_name + \" is deprecated and will be removed in the future. Use \" + new_name + \" instead.\")\n\t\tprint(\"         \" + arg + \" will be removed on \" + deprecation_date)\n\n\tif (cli_args is not None) and (answer_file_args is not None):\n\t\t# check deprecation on cli arguments\n\t\tfor arg in cli_args:\n\t\t\tif arg in answer_file_args:\n\t\t\t\told_name = answer_file_args[arg]\n\t\t\t\tnew_name = cli_args[arg]\n\t\t\t\tdeprecation_date = answer_file_args.get('deprecation_date')\n\t\t\t\tprint_deprecation_warning(arg, old_name, new_name, deprecation_date)\n\n\t\t# check deprecation on answer file arguments\n\t\tfor arg in answer_file_args:\n\t\t\tif arg in cli_args:\n\t\t\t\told_name = cli_args[arg]\n\t\t\t\tnew_name = answer_file_args[arg]\n\t\t\t\tdeprecation_date = answer_file_args.get('deprecation", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef print_deprecation(arg, message):\n\t\tprint(f\"{arg} is deprecated and will be removed in future releases. {message}\")\n\n\tif cli_args.get('--version', False):\n\t\tprint_deprecation('--version', 'Use --version-json')\n\n\tif cli_args.get('--version-json', False):\n\t\tprint_deprecation('--version-json',\n\t\t                 'Use --version-json-path to specify the path to the version JSON file.')\n\n\tif answer_file_args.get('--version-json-path', False):\n\t\tprint_deprecation('--version-json-path',\n\t\t                 'Use --version-json-path-file to specify the path to the version JSON file.')\n\n\tif cli_args.get('--version-json-path', False):\n\t\tprint_deprecation('--version-json-path',\n\t\t                 'Use --version-json-path-file to specify the path to the version JSON file.')\n\n\tif answer_file_args.get('--version-json-path-file', False):\n\t\tprint_deprecation('--version-json-path-file', 'Use --version-json-path instead.')\n\n\tif cli_args", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"\")\n\tprint(\"### DEPRECATION MESSAGE ###\")\n\tprint(\"\")\n\tprint(\"The following arguments are deprecated and will be removed in the future. Please \"\n\t\t\"remove them from your configuration files.\")\n\tprint(\"\")\n\tprint(\"The following arguments were provided:\")\n\tprint(\" - \" + \", \".join(cli_args))\n\tprint(\"\")\n\tprint(\"The following arguments were provided in the answers file:\")\n\tprint(\" - \" + \", \".join(answer_file_args))\n\tprint(\"\")\n\tprint(\"### END DEPRECATION MESSAGE ###\")", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecation_args = {}\n\n\tfor arg in cli_args:\n\t\tif arg in answer_file_args:\n\t\t\tif answer_file_args[arg] == \"yes\":\n\t\t\t\tdeprecation_args[arg] = cli_args[arg]\n\t\telse:\n\t\t\tdeprecation_args[arg] = cli_args[arg]\n\n\tself.print_info(\n\t\t\"The following arguments are deprecated\\n\"\n\t\t\"please use the --<option> instead of <option>\\n\"\n\t\t\"{}\\n\".format(deprecation_args)\n\t)\n\n#ansible_cli/utils/validate_ansible_version.py\nimport argparse\n\nimport ansible_cli.utils.cli_utils as cli_utils\nimport ansible_cli.utils.colors as colors\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"Warning: The following arguments are deprecated:\")\n\tdeprecation_list = sorted(set(cli_args.keys() + answer_file_args.keys()))\n\tfor key in deprecation_list:\n\t\tif key in cli_args.keys() and key in answer_file_args.keys():\n\t\t\tprint(\"-\", key, \":\", cli_args[key], \"is deprecated and will be removed in future version\")\n\t\tif key in cli_args.keys():\n\t\t\tprint(\"-\", key, \":\", cli_args[key], \"is deprecated and will be removed in future version\")\n\t\tif key in answer_file_args.keys():\n\t\t\tprint(\"-\", key, \":\", answer_file_args[key], \"is deprecated and will be removed in future version\")\n\t\tprint(\"\")\n\n#python/src/vcs_tool/cli/__init__.py\nfrom .validate_args import validate_arg_deprecation\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tvalidate_args = self.validate_args(cli_args, answer_file_args)\n\tself.print_deprecation_notices(validate_args)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef validate_deprecated_arg(arg, arg_type):\n\t\tif arg in cli_args:\n\t\t\tif (arg in answer_file_args) and (answer_file_args[arg]):\n\t\t\t\tprint(\"Warning: argument \" + arg + \" is deprecated. Please use \" + arg_type + \" instead.\")\n\n\tvalidate_deprecated_arg(\"-p\", \"--path\")\n\tvalidate_deprecated_arg(\"-d\", \"--directory\")\n\tvalidate_deprecated_arg(\"-o\", \"--output\")\n\tvalidate_deprecated_arg(\"-n\", \"--name\")\n\tvalidate_deprecated_arg(\"-s\", \"--size\")\n\tvalidate_deprecated_arg(\"-q\", \"--quality\")\n\tvalidate_deprecated_arg(\"-f\", \"--format\")", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecation_message = ''\n\tdeprecation_message += 'DEPRECATED ARGUMENT MESSAGES\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'Arguments in the answer file are deprecated. Please update your answer file to use the new arguments.\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'For example, the following argument should be changed:\\n'\n\tdeprecation_message += '  - --timeout=600\\n'\n\tdeprecation_message += '  - --timeout=600\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'To use the new arguments, please update your answer file to:\\n'\n\tdeprecation_message += '  - --timeout 600\\n'\n\tdeprecation_message += '  - --timeout\\n'\n\tdeprecation_message += '\\n'\n\tdeprecation_message += 'Note that there are two different --timeout arguments.\\n'\n\tdeprecation_message += 'The first argument is a positional argument, and the second argument is a key-value argument.\\n'\n\tdeprecation_message += 'The value of the key-value argument is not used.", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"Validating arguments...\")\n\n\t# Check if the deprecated arguments are in the cli\n\tfor arg in self.DEPRECATION_ARGS:\n\t\tif arg in cli_args:\n\t\t\tprint(\"The deprecated argument:\", arg, \"is set to:\", cli_args[arg])\n\n\t# Check if the deprecated arguments are in the answer file\n\tfor arg in self.DEPRECATION_ARGS:\n\t\tif arg in answer_file_args:\n\t\t\tprint(\"The deprecated argument:\", arg, \"is set to:\", answer_file_args[arg])\n\n#validate_args.py\n\"\"\"\nValidates the input arguments provided by the user.\n\"\"\"\nimport argparse\nfrom deprecation_validation import validate_arg_deprecation\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"This program is deprecated. Please use the new command line interface.\")\n\tprint(\"To see the new command line interface, run:\")\n\tprint(\"python3 -m python_cli\")\n\tprint(\"To see the new command line interface's arguments, run:\")\n\tprint(\"python3 -m python_cli -h\")\n\tprint(\"To see the new command file's arguments, run:\")\n\tprint(\"python3 -m python_cli --help\")\n\tprint(\"To see the new command line interface's arguments with examples, run:\")\n\tprint(\"python3 -m python_cli -e\")\n\tprint(\"To see the new command file's arguments with examples, run:\")\n\tprint(\"python3 -m python_cli --help -e\")\n\n#python_cli/__init__.py\n# -*- coding: utf-8 -*-\n\n\"\"\"Top-level package for python-cli.\"\"\"\n\n__author__ = \"\"\"Cody Jones\"\"\"\n__email__ = 'cody@codyjones.net'\n__version__ = '0.1.0'\n\nfrom .cli import main\n\nif __name__ == \"__main__\":\n\tmain()\n\n#docs/conf.py\n# -*- coding: utf-8 -*-\n\nimport os\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef print_deprecation_warning(arg, old_name, new_name, deprecation_date):\n\t\tprint(\"Warning: The argument \" + old_name + \" is deprecated and will be removed in the future. Use \" + new_name + \" instead.\")\n\t\tprint(\"         \" + arg + \" will be removed on \" + deprecation_date)\n\n\tif (cli_args is not None) and (answer_file_args is not None):\n\t\t# check deprecation on cli arguments\n\t\tfor arg in cli_args:\n\t\t\tif arg in answer_file_args:\n\t\t\t\told_name = answer_file_args[arg]\n\t\t\t\tnew_name = cli_args[arg]\n\t\t\t\tdeprecation_date = answer_file_args.get('deprecation_date')\n\t\t\t\tprint_deprecation_warning(arg, old_name, new_name, deprecation_date)\n\n\t\t# check deprecation on answer file arguments\n\t\tfor arg in answer_file_args:\n\t\t\tif arg in cli_args:\n\t\t\t\told_name = cli_args[arg]\n\t\t\t\tnew_name = answer_file_args[arg]\n\t\t\t\tdeprecation_date = answer_file_args.get('deprecation", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdef print_deprecation(arg, message):\n\t\tprint(f\"{arg} is deprecated and will be removed in future releases. {message}\")\n\n\tif cli_args.get('--version', False):\n\t\tprint_deprecation('--version', 'Use --version-json')\n\n\tif cli_args.get('--version-json', False):\n\t\tprint_deprecation('--version-json',\n\t\t                 'Use --version-json-path to specify the path to the version JSON file.')\n\n\tif answer_file_args.get('--version-json-path', False):\n\t\tprint_deprecation('--version-json-path',\n\t\t                 'Use --version-json-path-file to specify the path to the version JSON file.')\n\n\tif cli_args.get('--version-json-path', False):\n\t\tprint_deprecation('--version-json-path',\n\t\t                 'Use --version-json-path-file to specify the path to the version JSON file.')\n\n\tif answer_file_args.get('--version-json-path-file', False):\n\t\tprint_deprecation('--version-json-path-file', 'Use --version-json-path instead.')\n\n\tif cli_args", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tprint(\"\")\n\tprint(\"### DEPRECATION MESSAGE ###\")\n\tprint(\"\")\n\tprint(\"The following arguments are deprecated and will be removed in the future. Please \"\n\t\t\"remove them from your configuration files.\")\n\tprint(\"\")\n\tprint(\"The following arguments were provided:\")\n\tprint(\" - \" + \", \".join(cli_args))\n\tprint(\"\")\n\tprint(\"The following arguments were provided in the answers file:\")\n\tprint(\" - \" + \", \".join(answer_file_args))\n\tprint(\"\")\n\tprint(\"### END DEPRECATION MESSAGE ###\")", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\"\"\"\n\tValidates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files\n\t\"\"\"\n\tdeprecation_args = {}\n\n\tfor arg in cli_args:\n\t\tif arg in answer_file_args:\n\t\t\tif answer_file_args[arg] == \"yes\":\n\t\t\t\tdeprecation_args[arg] = cli_args[arg]\n\t\telse:\n\t\t\tdeprecation_args[arg] = cli_args[arg]\n\n\tself.print_info(\n\t\t\"The following arguments are deprecated\\n\"\n\t\t\"please use the --<option> instead of <option>\\n\"\n\t\t\"{}\\n\".format(deprecation_args)\n\t)\n\n#ansible_cli/utils/validate_ansible_version.py\nimport argparse\n\nimport ansible_cli.utils.cli_utils as cli_utils\nimport ansible_cli.utils.colors as colors\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser(command_name)\n\toptions = parser.option_list\n\toptions.extend(parser.option_defaults)\n\toptions.extend(parser.add_option_group(parser.option_list[-1]))\n\treturn options\n\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = []\n\n\tif command_name == 'main':\n\t\tparser_option_specs = [\n\t\t\t('-h', '--hostname',\n\t\t\t 'hostname',\n\t\t\t {'help': 'hostname', 'required': True, 'choices': ['localhost', '127.0.0.1']}),\n\t\t\t('-p', '--port',\n\t\t\t 'port',\n\t\t\t {'help': 'port', 'default': '5000', 'choices': ['5000', '5001', '5002', '5003']}),\n\t\t\t('--debug',\n\t\t\t 'debug',\n\t\t\t {'help': 'debug mode', 'action': 'store_true', 'default': False}),\n\t\t\t('-v', '--verbose',\n\t\t\t 'verbose',\n\t\t\t {'help': 'verbose mode', 'action': 'store_true', 'default': False}),\n\t\t\t('-q', '--quiet',\n\t\t\t 'quiet',\n\t\t\t {'help': 'quiet mode', 'action': 'store_true', 'default': False})\n\t\t]\n\n\telif command_name == 'virsh':\n\t\tparser_option_specs = [\n\t\t\t('-h', '--hostname',\n\t\t\t 'host", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\toptions = []\n\tif command_name == 'virsh':\n\t\toptions = self.get_virsh_option_specs()\n\telif command_name == 'ospd':\n\t\toptions = self.get_ospd_option_specs()\n\telif command_name == 'main':\n\t\toptions = self.get_main_option_specs()\n\treturn options\n\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser(command_name)\n\toption_specs = []\n\tif parser.add_option:\n\t\toption_specs.append(parser.add_option)\n\tif parser.add_argument:\n\t\toption_specs.append(parser.add_argument)\n\treturn option_specs\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = self.parser_options_by_command_name.get(command_name, {})\n\tparser_option_specs.update(self.parser_options_by_command_name.get(self.DEFAULT_COMMAND_NAME, {}))\n\treturn parser_option_specs\n\n\n# get_parser_option_specs()\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.option_parser\n\toption_groups = parser._get_option_groups()\n\tfor group in option_groups:\n\t\tif group.title == command_name:\n\t\t\treturn group.get_option_list()\n\treturn []\n\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.option_parsers[command_name]\n\n\toption_specs = []\n\tfor option in parser._actions:\n\t\toption_specs.append(option)\n\n\treturn option_specs\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser_for_command(command_name)\n\treturn parser._actions", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_options = []\n\tfor option in self.options:\n\t\tif option.group_name == command_name:\n\t\t\tparser_options.append(option)\n\n\treturn parser_options\n", "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser()\n\tknown_options = parser.get_known_options()\n\tknown_args = parser.get_known_args()\n\tknown_args_without_options = [x for x in known_args if x not in known_options]\n\n\toption_specs = {}\n\tfor option in known_options:\n\t\toption_spec = option.get_option_spec()\n\t\toption_specs[option.name] = option_spec\n\n\toption_specs.update({\n\t\t'--verbose': {\n\t\t\t'help': 'Increase verbosity of output. Use multiple --verbose to increase verbosity.',\n\t\t\t'default': 1,\n\t\t\t'nargs': '?',\n\t\t\t'const': 2,\n\t\t\t'choices': [-1, 0, 1, 2],\n\t\t},\n\t\t'--log-file': {\n\t\t\t'help': 'Specify a log file to write to.',\n\t\t\t'nargs': '?',\n\t\t\t'const': 'STDOUT',\n\t\t\t'choices': ['STDOUT', 'STDERR', None],\n\t\t},\n\t\t'--log-level': {\n\t\t\t'help': 'Set the log level (ERROR, WARNING, INFO, DEBUG, TRACE)',\n\t\t"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser(command_name)\n\toptions = parser.option_list\n\toptions.extend(parser.option_defaults)\n\toptions.extend(parser.add_option_group(parser.option_list[-1]))\n\treturn options\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = []\n\n\tif command_name == 'main':\n\t\tparser_option_specs = [\n\t\t\t('-h', '--hostname',\n\t\t\t 'hostname',\n\t\t\t {'help': 'hostname', 'required': True, 'choices': ['localhost', '127.0.0.1']}),\n\t\t\t('-p', '--port',\n\t\t\t 'port',\n\t\t\t {'help': 'port', 'default': '5000', 'choices': ['5000', '5001', '5002', '5003']}),\n\t\t\t('--debug',\n\t\t\t 'debug',\n\t\t\t {'help': 'debug mode', 'action': 'store_true', 'default': False}),\n\t\t\t('-v', '--verbose',\n\t\t\t 'verbose',\n\t\t\t {'help': 'verbose mode', 'action': 'store_true', 'default': False}),\n\t\t\t('-q', '--quiet',\n\t\t\t 'quiet',\n\t\t\t {'help': 'quiet mode', 'action': 'store_true', 'default': False})\n\t\t]\n\n\telif command_name == 'virsh':\n\t\tparser_option_specs = [\n\t\t\t('-h', '--hostname',\n\t\t\t 'host", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\toptions = []\n\tif command_name == 'virsh':\n\t\toptions = self.get_virsh_option_specs()\n\telif command_name == 'ospd':\n\t\toptions = self.get_ospd_option_specs()\n\telif command_name == 'main':\n\t\toptions = self.get_main_option_specs()\n\treturn options\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser(command_name)\n\toption_specs = []\n\tif parser.add_option:\n\t\toption_specs.append(parser.add_option)\n\tif parser.add_argument:\n\t\toption_specs.append(parser.add_argument)\n\treturn option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_option_specs = self.parser_options_by_command_name.get(command_name, {})\n\tparser_option_specs.update(self.parser_options_by_command_name.get(self.DEFAULT_COMMAND_NAME, {}))\n\treturn parser_option_specs\n\n\n# get_parser_option_specs()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.option_parser\n\toption_groups = parser._get_option_groups()\n\tfor group in option_groups:\n\t\tif group.title == command_name:\n\t\t\treturn group.get_option_list()\n\treturn []\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.option_parsers[command_name]\n\n\toption_specs = []\n\tfor option in parser._actions:\n\t\toption_specs.append(option)\n\n\treturn option_specs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser_for_command(command_name)\n\treturn parser._actions", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser_options = []\n\tfor option in self.options:\n\t\tif option.group_name == command_name:\n\t\t\tparser_options.append(option)\n\n\treturn parser_options\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\"\"\"\n\tGets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options\n\t\"\"\"\n\tparser = self.get_parser()\n\tknown_options = parser.get_known_options()\n\tknown_args = parser.get_known_args()\n\tknown_args_without_options = [x for x in known_args if x not in known_options]\n\n\toption_specs = {}\n\tfor option in known_options:\n\t\toption_spec = option.get_option_spec()\n\t\toption_specs[option.name] = option_spec\n\n\toption_specs.update({\n\t\t'--verbose': {\n\t\t\t'help': 'Increase verbosity of output. Use multiple --verbose to increase verbosity.',\n\t\t\t'default': 1,\n\t\t\t'nargs': '?',\n\t\t\t'const': 2,\n\t\t\t'choices': [-1, 0, 1, 2],\n\t\t},\n\t\t'--log-file': {\n\t\t\t'help': 'Specify a log file to write to.',\n\t\t\t'nargs': '?',\n\t\t\t'const': 'STDOUT',\n\t\t\t'choices': ['STDOUT', 'STDERR', None],\n\t\t},\n\t\t'--log-level': {\n\t\t\t'help': 'Set the log level (ERROR, WARNING, INFO, DEBUG, TRACE)',\n\t\t", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toptions = getattr(self, \"options_specs\"+command_name)\n\toption_spec = {}\n\tif options:\n\t\tfor option in options:\n\t\t\tif option[\"name\"] == argument_name:\n\t\t\t\toption_spec = option\n\t\t\t\tbreak\n\treturn option_spec", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tparser_option_specs = self.get_parser_option_specs(command_name)\n\tfor option_spec in parser_option_specs:\n\t\tif option_spec['option_name'] == argument_name:\n\t\t\treturn option_spec\n\n\traise CLIError('option \"{0}\" is not a valid option for command \"{1}\"'.format(argument_name, command_name))\n\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toptions = self.get_parser_option_specs(command_name)\n\tif not options:\n\t\treturn None\n\n\tfor option in options:\n\t\tif option.name == argument_name:\n\t\t\treturn option\n\n\treturn None", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toption_specs = self.get_parser_option_specs()\n\toption_specs = option_specs[command_name]\n\tif not option_specs:\n\t\traise Exception('Command not found: %s'%command_name)\n\tif not argument_name in option_specs:\n\t\traise Exception('Option not found: %s.%s'%(command_name, argument_name))\n\treturn option_specs[argument_name]\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\ttry:\n\t\treturn self.get_parser_option_specs()[command_name][argument_name]\n\texcept KeyError:\n\t\traise Exception(\"No option specified for %s %s\" % (command_name, argument_name))", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tself.get_parser_option_specs()\n\treturn self.option_specs[command_name][argument_name]", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toptions = self.get_parser_option_specs(command_name)\n\n\tif argument_name in options:\n\t\treturn options[argument_name]\n\telse:\n\t\treturn None", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspecs = self.get_parser_option_specs(command_name)\n\tif argument_name in specs:\n\t\treturn specs[argument_name]\n\traise Exception('The specified option does not exist for the specified command.')\n", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspecs = self.get_parser_option_specs()\n\tif command_name not in specs:\n\t\traise Exception(\"Command %s is not in the parser options\" % command_name)\n\n\tcommand_spec = specs[command_name]\n\tif argument_name not in command_spec:\n\t\traise Exception(\"Argument %s is not in the spec for command %s\" % (argument_name, command_name))\n\n\treturn command_spec[argument_name]", "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tself.parser.print_usage()\n\tself.parser.print_help()\n\tparser_option_specs = self.get_parser_option_specs()\n\tif command_name not in parser_option_specs:\n\t\traise Exception(\"No such command: \" + command_name)\n\toption_specs = parser_option_specs[command_name]\n\tif argument_name not in option_specs:\n\t\traise Exception(\"No such argument: \" + argument_name)\n\treturn option_specs[argument_name]\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toptions = getattr(self, \"options_specs\"+command_name)\n\toption_spec = {}\n\tif options:\n\t\tfor option in options:\n\t\t\tif option[\"name\"] == argument_name:\n\t\t\t\toption_spec = option\n\t\t\t\tbreak\n\treturn option_spec", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tparser_option_specs = self.get_parser_option_specs(command_name)\n\tfor option_spec in parser_option_specs:\n\t\tif option_spec['option_name'] == argument_name:\n\t\t\treturn option_spec\n\n\traise CLIError('option \"{0}\" is not a valid option for command \"{1}\"'.format(argument_name, command_name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toptions = self.get_parser_option_specs(command_name)\n\tif not options:\n\t\treturn None\n\n\tfor option in options:\n\t\tif option.name == argument_name:\n\t\t\treturn option\n\n\treturn None", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toption_specs = self.get_parser_option_specs()\n\toption_specs = option_specs[command_name]\n\tif not option_specs:\n\t\traise Exception('Command not found: %s'%command_name)\n\tif not argument_name in option_specs:\n\t\traise Exception('Option not found: %s.%s'%(command_name, argument_name))\n\treturn option_specs[argument_name]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\ttry:\n\t\treturn self.get_parser_option_specs()[command_name][argument_name]\n\texcept KeyError:\n\t\traise Exception(\"No option specified for %s %s\" % (command_name, argument_name))", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tself.get_parser_option_specs()\n\treturn self.option_specs[command_name][argument_name]", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\toptions = self.get_parser_option_specs(command_name)\n\n\tif argument_name in options:\n\t\treturn options[argument_name]\n\telse:\n\t\treturn None", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspecs = self.get_parser_option_specs(command_name)\n\tif argument_name in specs:\n\t\treturn specs[argument_name]\n\traise Exception('The specified option does not exist for the specified command.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tspecs = self.get_parser_option_specs()\n\tif command_name not in specs:\n\t\traise Exception(\"Command %s is not in the parser options\" % command_name)\n\n\tcommand_spec = specs[command_name]\n\tif argument_name not in command_spec:\n\t\traise Exception(\"Argument %s is not in the spec for command %s\" % (argument_name, command_name))\n\n\treturn command_spec[argument_name]", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\"\"\"\n\tGets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\tself.parser.print_usage()\n\tself.parser.print_help()\n\tparser_option_specs = self.get_parser_option_specs()\n\tif command_name not in parser_option_specs:\n\t\traise Exception(\"No such command: \" + command_name)\n\toption_specs = parser_option_specs[command_name]\n\tif argument_name not in option_specs:\n\t\traise Exception(\"No such argument: \" + argument_name)\n\treturn option_specs[argument_name]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tfor arg in args:\n\t\tif arg.startswith(\"-\"):\n\t\t\tsilent.append(arg)\n\treturn silent\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_args = ['-h', '--help', '--version', '--quiet', '--verbose']\n\tif args.verbose:\n\t\tsilence_args.extend(['--debug', '--trace'])\n\treturn silence_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\tif self._args.silence_unrecognized:\n\t\tsilenced_args.extend([\"--ignore-unknown\", \"--ignore-unknown-options\"])\n\tif self._args.silence_unrecognized_options:\n\t\tsilenced_args.extend([\"--ignore-unknown-options\"])\n\n\tsilenced_args.extend([\"-x\", \"--experimental\"])\n\n\tif self._args.silence_unrecognized_args:\n\t\tsilenced_args.extend([\"--ignore-unknown-args\"])\n\n\treturn silenced_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_list = [\n\t\t'--log-level',\n\t\t'--log-file',\n\t\t'--log-format',\n\t\t'--log-date-format',\n\t\t'--log-date-format',\n\t\t'--log-request-id',\n\t\t'--log-request-id',\n\t\t'--log-request-id',\n\t\t'--log-request-id',\n\t]\n\tsilence_list.extend(args)\n\treturn list(set(silence_list))", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\targs_list = []\n\n\t# find silent argument\n\tfor arg in args:\n\t\tif arg.startswith('-'):\n\t\t\targs_list.append(arg)\n\n\treturn args_list\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\tif not args:\n\t\treturn silenced_args\n\n\ti = 1\n\twhile i < len(args):\n\t\tif args[i] == '-s':\n\t\t\ti += 1\n\t\t\tif i < len(args):\n\t\t\t\tsilenced_args.append(args[i])\n\t\ti += 1\n\treturn silenced_args", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tsilent.append(args.debug)\n\tsilent.append(args.silent)\n\tsilent.append(args.verbose)\n\tsilent.append(args.version)\n\tsilent.append(args.help)\n\tsilent.append(args.list_servers)\n\tsilent.append(args.list_servers_all)\n\tsilent.append(args.list_clusters)\n\tsilent.append(args.list_clusters_all)\n\tsilent.append(args.list_nodes)\n\tsilent.append(args.list_nodes_all)\n\tsilent.append(args.list_tasks)\n\tsilent.append(args.list_tasks_all)\n\tsilent.append(args.list_pods)\n\tsilent.append(args.list_pods_all)\n\treturn silent\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = ['-h', '--help']\n\tsilenced_args += ['-v', '--verbose', '-q', '--quiet', '-vv']\n\treturn list(filter(lambda x: x in args, silenced_args))\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_args = []\n\tsilence_args = [x.lower() for x in args]\n\n\treturn silence_args\n\n", "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_list = ['--help', '-h']\n\treturn [arg for arg in args if arg not in silence_list]\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tfor arg in args:\n\t\tif arg.startswith(\"-\"):\n\t\t\tsilent.append(arg)\n\treturn silent\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_args = ['-h', '--help', '--version', '--quiet', '--verbose']\n\tif args.verbose:\n\t\tsilence_args.extend(['--debug', '--trace'])\n\treturn silence_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\tif self._args.silence_unrecognized:\n\t\tsilenced_args.extend([\"--ignore-unknown\", \"--ignore-unknown-options\"])\n\tif self._args.silence_unrecognized_options:\n\t\tsilenced_args.extend([\"--ignore-unknown-options\"])\n\n\tsilenced_args.extend([\"-x\", \"--experimental\"])\n\n\tif self._args.silence_unrecognized_args:\n\t\tsilenced_args.extend([\"--ignore-unknown-args\"])\n\n\treturn silenced_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_list = [\n\t\t'--log-level',\n\t\t'--log-file',\n\t\t'--log-format',\n\t\t'--log-date-format',\n\t\t'--log-date-format',\n\t\t'--log-request-id',\n\t\t'--log-request-id',\n\t\t'--log-request-id',\n\t\t'--log-request-id',\n\t]\n\tsilence_list.extend(args)\n\treturn list(set(silence_list))", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\targs_list = []\n\n\t# find silent argument\n\tfor arg in args:\n\t\tif arg.startswith('-'):\n\t\t\targs_list.append(arg)\n\n\treturn args_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = []\n\tif not args:\n\t\treturn silenced_args\n\n\ti = 1\n\twhile i < len(args):\n\t\tif args[i] == '-s':\n\t\t\ti += 1\n\t\t\tif i < len(args):\n\t\t\t\tsilenced_args.append(args[i])\n\t\ti += 1\n\treturn silenced_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilent = []\n\tsilent.append(args.debug)\n\tsilent.append(args.silent)\n\tsilent.append(args.verbose)\n\tsilent.append(args.version)\n\tsilent.append(args.help)\n\tsilent.append(args.list_servers)\n\tsilent.append(args.list_servers_all)\n\tsilent.append(args.list_clusters)\n\tsilent.append(args.list_clusters_all)\n\tsilent.append(args.list_nodes)\n\tsilent.append(args.list_nodes_all)\n\tsilent.append(args.list_tasks)\n\tsilent.append(args.list_tasks_all)\n\tsilent.append(args.list_pods)\n\tsilent.append(args.list_pods_all)\n\treturn silent\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilenced_args = ['-h', '--help']\n\tsilenced_args += ['-v', '--verbose', '-q', '--quiet', '-vv']\n\treturn list(filter(lambda x: x in args, silenced_args))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_args = []\n\tsilence_args = [x.lower() for x in args]\n\n\treturn silence_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\"\"\"\n\tlist of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names\n\t\"\"\"\n\tsilence_list = ['--help', '-h']\n\treturn [arg for arg in args if arg not in silence_list]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.get_required_args()\n\tif not required_args:\n\t\treturn\n\n\tfor arg in required_args:\n\t\tif not getattr(args, arg, None):\n\t\t\traise Exception(\"{0} is required\".format(arg))\n\n#src/common/commands/base/__init__.py\nfrom .validate_requires_args import validate_requires_args\n\n__all__ = ['validate_requires_args']\n\n#src/common/commands/__init__.py\nfrom .base import validate_requires_args\n\n__all__ = ['validate_requires_args']\n\n#src/common/commands/base/test_validate_requires_args.py\nimport unittest\n\nfrom . import validate_requires_args\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.get_required_args()\n\tif not required_args:\n\t\treturn True\n\tmissing_args = [arg for arg in required_args if arg not in args]\n\tif missing_args:\n\t\tmissing_args_str = \", \".join(missing_args)\n\t\tmissing_args_str += \".\"\n\t\traise ValueError(\"Missing required arguments: {0}\".format(missing_args_str))\n\treturn True", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tself.required_args = []\n\tif hasattr(self, 'required_args'):\n\t\tself.required_args = self.required_args\n\n\tfor arg in self.required_args:\n\t\tif not getattr(args, arg):\n\t\t\traise Exception('The argument {} must be provided.'.format(arg))\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tself.required_args = self.required_args or []\n\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\traise Exception('Required argument \"%s\" not provided' % arg)\n\t\treturn True", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tself.required_args = self.get_required_args()\n\n\targs = self.remove_empty_values(args)\n\n\tfor arg in self.required_args:\n\t\tif not args.get(arg) or args.get(arg) == '':\n\t\t\traise argparse.ArgumentTypeError(\"Missing required argument: --%s\" % arg)\n\n\treturn args\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\ttry:\n\t\tself.validate_requires_args_fields(args)\n\texcept ValidationError as e:\n\t\traise CommandError(e.message)", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\terror_msg = ''\n\targs_list = self.required_args\n\tfor arg in args_list:\n\t\tif arg not in args:\n\t\t\terror_msg += 'Argument %s missing. ' % arg\n\n\tif error_msg:\n\t\traise RuntimeError(error_msg)", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = self.required_args\n\tif not args or len(required) == 0:\n\t\treturn True\n\n\tmissing = set()\n\tfor arg in required:\n\t\tif arg not in args:\n\t\t\tmissing.add(arg)\n\n\tif len(missing) > 0:\n\t\tmissing_str = ','.join(missing)\n\t\traise Exception('Missing required arguments: ' + missing_str)\n\n\treturn True", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.args\n\tif not all(arg in args for arg in required_args):\n\t\traise ValidationError(\n\t\t\t\"The following arguments are required: %s\" % \", \".join(required_args)\n\t\t)\n\n", "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_required = self.get_args_required()\n\tfor arg_name in args_required:\n\t\tif arg_name not in args:\n\t\t\traise Exception(\n\t\t\t\t\"You must provide {} argument\".format(\n\t\t\t\t\targ_name,\n\t\t\t\t)\n\t\t\t)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.get_required_args()\n\tif not required_args:\n\t\treturn\n\n\tfor arg in required_args:\n\t\tif not getattr(args, arg, None):\n\t\t\traise Exception(\"{0} is required\".format(arg))\n\n#src/common/commands/base/__init__.py\nfrom .validate_requires_args import validate_requires_args\n\n__all__ = ['validate_requires_args']\n\n#src/common/commands/__init__.py\nfrom .base import validate_requires_args\n\n__all__ = ['validate_requires_args']\n\n#src/common/commands/base/test_validate_requires_args.py\nimport unittest\n\nfrom . import validate_requires_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.get_required_args()\n\tif not required_args:\n\t\treturn True\n\tmissing_args = [arg for arg in required_args if arg not in args]\n\tif missing_args:\n\t\tmissing_args_str = \", \".join(missing_args)\n\t\tmissing_args_str += \".\"\n\t\traise ValueError(\"Missing required arguments: {0}\".format(missing_args_str))\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tself.required_args = []\n\tif hasattr(self, 'required_args'):\n\t\tself.required_args = self.required_args\n\n\tfor arg in self.required_args:\n\t\tif not getattr(args, arg):\n\t\t\traise Exception('The argument {} must be provided.'.format(arg))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tself.required_args = self.required_args or []\n\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\traise Exception('Required argument \"%s\" not provided' % arg)\n\t\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\tself.required_args = self.get_required_args()\n\n\targs = self.remove_empty_values(args)\n\n\tfor arg in self.required_args:\n\t\tif not args.get(arg) or args.get(arg) == '':\n\t\t\traise argparse.ArgumentTypeError(\"Missing required argument: --%s\" % arg)\n\n\treturn args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\ttry:\n\t\tself.validate_requires_args_fields(args)\n\texcept ValidationError as e:\n\t\traise CommandError(e.message)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\terror_msg = ''\n\targs_list = self.required_args\n\tfor arg in args_list:\n\t\tif arg not in args:\n\t\t\terror_msg += 'Argument %s missing. ' % arg\n\n\tif error_msg:\n\t\traise RuntimeError(error_msg)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired = self.required_args\n\tif not args or len(required) == 0:\n\t\treturn True\n\n\tmissing = set()\n\tfor arg in required:\n\t\tif arg not in args:\n\t\t\tmissing.add(arg)\n\n\tif len(missing) > 0:\n\t\tmissing_str = ','.join(missing)\n\t\traise Exception('Missing required arguments: ' + missing_str)\n\n\treturn True", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\trequired_args = self.args\n\tif not all(arg in args for arg in required_args):\n\t\traise ValidationError(\n\t\t\t\"The following arguments are required: %s\" % \", \".join(required_args)\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t\"\"\"\n\tCheck if all the required arguments have been provided.\n\t\"\"\"\n\targs_required = self.get_args_required()\n\tfor arg_name in args_required:\n\t\tif arg_name not in args:\n\t\t\traise Exception(\n\t\t\t\t\"You must provide {} argument\".format(\n\t\t\t\t\targ_name,\n\t\t\t\t)\n\t\t\t)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\n\tfor option in options_spec:\n\t\tif option['required_when'] is not None:\n\t\t\tif option['required_when'](args):\n\t\t\t\trequired_args.append(option['id'])\n\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg in options_spec:\n\t\tif arg.required and arg.name in args:\n\t\t\tif arg.required_when:\n\t\t\t\tif eval(arg.required_when):\n\t\t\t\t\trequired_args.append(arg.name)\n\t\t\telse:\n\t\t\t\trequired_args.append(arg.name)\n\treturn required_args\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif option.name in args:\n\t\t\tif option.required_when:\n\t\t\t\trequired_args.append(option.name)\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif option.get('required_when') is not None:\n\t\t\tif eval(option.get('required_when')):\n\t\t\t\trequired_args.append(option.get('name'))\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\n\tfor arg in args:\n\t\targ_name = arg.replace('--', '')\n\t\targ_spec = options_spec.get(arg_name, None)\n\t\tif arg_spec and arg_spec.get('required_when'):\n\t\t\tif arg_spec.get('required_when') == command_name:\n\t\t\t\tresult.append(arg_name)\n\treturn result\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\targs_with_conditions = []\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\targ_name = arg[2:]\n\t\t\targ_spec = options_spec.get(arg_name)\n\t\t\tif arg_spec and arg_spec.get('required_when'):\n\t\t\t\targs_with_conditions.append(arg_name)\n\treturn args_with_conditions\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired = []\n\tfor arg_name, arg in args.iteritems():\n\t\tif arg_name not in options_spec:\n\t\t\tcontinue\n\t\targ_spec = options_spec[arg_name]\n\t\trequired_when = arg_spec.get('required_when', None)\n\t\tif required_when is None:\n\t\t\tcontinue\n\t\tif not isinstance(required_when, list):\n\t\t\traise ValueError(\n\t\t\t\t\"RequiredWhen condition must be a list, \"\n\t\t\t\t\"got %s\" % type(required_when)\n\t\t\t)\n\n\t\tif not command_name in required_when:\n\t\t\tcontinue\n\n\t\trequired.append(arg_name)\n\treturn required\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\targs_with_required_when = []\n\tfor arg_name, opt in options_spec:\n\t\tif opt.required_when:\n\t\t\tif opt.required_when.test(args):\n\t\t\t\targs_with_required_when.append(arg_name)\n\t\tif '--' + arg_name in args:\n\t\t\tif not opt.required_when:\n\t\t\t\targs_with_required_when.append(arg_name)\n\treturn args_with_required_when\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_args = []\n\tfor arg_spec in options_spec:\n\t\tif arg_spec.required_when:\n\t\t\tif arg_spec.required_when(self, command_name):\n\t\t\t\trequired_when_args.append(arg_spec.name)\n\treturn required_when_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_args = []\n\tfor option in options_spec:\n\t\tif option.get('required_when'):\n\t\t\tif not option.get('required_when')(args):\n\t\t\t\tcontinue\n\t\trequired_when_args.append(option.get('name'))\n\treturn required_when_args"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\n\tfor option in options_spec:\n\t\tif option['required_when'] is not None:\n\t\t\tif option['required_when'](args):\n\t\t\t\trequired_args.append(option['id'])\n\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor arg in options_spec:\n\t\tif arg.required and arg.name in args:\n\t\t\tif arg.required_when:\n\t\t\t\tif eval(arg.required_when):\n\t\t\t\t\trequired_args.append(arg.name)\n\t\t\telse:\n\t\t\t\trequired_args.append(arg.name)\n\treturn required_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif option.name in args:\n\t\t\tif option.required_when:\n\t\t\t\trequired_args.append(option.name)\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif option.get('required_when') is not None:\n\t\t\tif eval(option.get('required_when')):\n\t\t\t\trequired_args.append(option.get('name'))\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\tresult = []\n\n\tfor arg in args:\n\t\targ_name = arg.replace('--', '')\n\t\targ_spec = options_spec.get(arg_name, None)\n\t\tif arg_spec and arg_spec.get('required_when'):\n\t\t\tif arg_spec.get('required_when') == command_name:\n\t\t\t\tresult.append(arg_name)\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\targs_with_conditions = []\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\targ_name = arg[2:]\n\t\t\targ_spec = options_spec.get(arg_name)\n\t\t\tif arg_spec and arg_spec.get('required_when'):\n\t\t\t\targs_with_conditions.append(arg_name)\n\treturn args_with_conditions\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired = []\n\tfor arg_name, arg in args.iteritems():\n\t\tif arg_name not in options_spec:\n\t\t\tcontinue\n\t\targ_spec = options_spec[arg_name]\n\t\trequired_when = arg_spec.get('required_when', None)\n\t\tif required_when is None:\n\t\t\tcontinue\n\t\tif not isinstance(required_when, list):\n\t\t\traise ValueError(\n\t\t\t\t\"RequiredWhen condition must be a list, \"\n\t\t\t\t\"got %s\" % type(required_when)\n\t\t\t)\n\n\t\tif not command_name in required_when:\n\t\t\tcontinue\n\n\t\trequired.append(arg_name)\n\treturn required\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\targs_with_required_when = []\n\tfor arg_name, opt in options_spec:\n\t\tif opt.required_when:\n\t\t\tif opt.required_when.test(args):\n\t\t\t\targs_with_required_when.append(arg_name)\n\t\tif '--' + arg_name in args:\n\t\t\tif not opt.required_when:\n\t\t\t\targs_with_required_when.append(arg_name)\n\treturn args_with_required_when\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_args = []\n\tfor arg_spec in options_spec:\n\t\tif arg_spec.required_when:\n\t\t\tif arg_spec.required_when(self, command_name):\n\t\t\t\trequired_when_args.append(arg_spec.name)\n\treturn required_when_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t\"\"\"\n\tList arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition\n\t\"\"\"\n\trequired_when_args = []\n\tfor option in options_spec:\n\t\tif option.get('required_when'):\n\t\t\tif not option.get('required_when')(args):\n\t\t\t\tcontinue\n\t\trequired_when_args.append(option.get('name'))\n\treturn required_when_args", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tdef _check_length(args):\n\t\t\"\"\"\n\t\tCheck if length of the given args is not longer than length specified.\n\t\t:param args: The received arguments.\n\t\t\"\"\"\n\t\targs = args.split(\" \")\n\t\targs_len = len(args)\n\t\targs_len_max = len(self.args_max)\n\t\tif args_len_max != 0:\n\t\t\tif args_len > args_len_max:\n\t\t\t\traise Exception(\"The length of the arguments is longer than allowed. \\nPlease try again.\")\n\t\treturn args\n\n\treturn _check_length(args)\n\n#src/utils/get_data.py\nimport pandas as pd\nimport os\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.add_error(\"Invalid length of arguments\", args)", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.get_length()\n\targs_length = len(args)\n\tif args_length > length:\n\t\treturn False\n\telse:\n\t\treturn True\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tpass\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tret = True\n\targs_dict = args.__dict__\n\targs_dict.pop('self')\n\n\tfor name in args_dict:\n\t\tret = ret and not (args_dict[name] is None or len(str(args_dict[name])) > self.length)\n\treturn ret\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\n\tdef check_length(args):\n\t\t\"\"\"\n\t\tCheck length of the received arguments. \\n:param args: The received arguments.\n\t\t:return: Boolean to indicate if the length of the received arguments is valid.\n\t\t\"\"\"\n\t\tdef length_val(argument):\n\t\t\t\"\"\"\n\t\t\tCheck length of the argument. \\n:param argument: The argument to be checked.\n\t\t\t\"\"\"\n\t\t\tif len(argument) > self.length:\n\t\t\t\traise ValueError(f\"The argument '{argument}' has a length of {len(argument)} which is longer than the maximum length of {self.length}.\")\n\n\t\tif isinstance(args, dict):\n\t\t\tfor key, value in args.items():\n\t\t\t\tlength_val(value)\n\t\telse:\n\t\t\tlength_val(args)\n\n\tcheck_length(args)\n\n#src/validation/validators/__init__.py\n\"\"\"\nThis module contains all the validators available to the user.\n\"\"\"\n\nfrom .length import validate_length\nfrom .length_args import validate_length_args\n\nvalidation_functions = {\n\t\"length\": validate_length,\n\t\"length_args\": validate_length_args\n}", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = args[1][0]\n\tmessage = args[1][2]\n\tif len(args[2]) > length:\n\t\traise ValueError(f\"{message} must be {length} characters or less.\")\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\ti = 0\n\tfor arg in args:\n\t\tif len(arg) > self.length:\n\t\t\traise Exception(\"Argument of index {} is longer than {} characters\".format(i, self.length))", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\n\tdef _validate_length(arg):\n\t\t\"\"\"\n\t\tCheck if value of the given arg is not longer than length specified. \\n:param arg: The received argument.\n\t\t\"\"\"\n\t\targ = str(arg)\n\t\tif len(arg) > self.length:\n\t\t\traise Exception(f\"The value of argument '{arg}' is longer than {self.length} characters\")\n\t\n\tfor arg in args:\n\t\t_validate_length(arg)\n\n", "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\terror = f\"The value of the argument '{args[0]}' is longer than allowed. It should have a length of {args[1]}.\"\n\tif len(args[0]) > args[1]:\n\t\traise Exception(error)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tdef _check_length(args):\n\t\t\"\"\"\n\t\tCheck if length of the given args is not longer than length specified.\n\t\t:param args: The received arguments.\n\t\t\"\"\"\n\t\targs = args.split(\" \")\n\t\targs_len = len(args)\n\t\targs_len_max = len(self.args_max)\n\t\tif args_len_max != 0:\n\t\t\tif args_len > args_len_max:\n\t\t\t\traise Exception(\"The length of the arguments is longer than allowed. \\nPlease try again.\")\n\t\treturn args\n\n\treturn _check_length(args)\n\n#src/utils/get_data.py\nimport pandas as pd\nimport os\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.add_error(\"Invalid length of arguments\", args)", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = self.get_length()\n\targs_length = len(args)\n\tif args_length > length:\n\t\treturn False\n\telse:\n\t\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tret = True\n\targs_dict = args.__dict__\n\targs_dict.pop('self')\n\n\tfor name in args_dict:\n\t\tret = ret and not (args_dict[name] is None or len(str(args_dict[name])) > self.length)\n\treturn ret\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\n\tdef check_length(args):\n\t\t\"\"\"\n\t\tCheck length of the received arguments. \\n:param args: The received arguments.\n\t\t:return: Boolean to indicate if the length of the received arguments is valid.\n\t\t\"\"\"\n\t\tdef length_val(argument):\n\t\t\t\"\"\"\n\t\t\tCheck length of the argument. \\n:param argument: The argument to be checked.\n\t\t\t\"\"\"\n\t\t\tif len(argument) > self.length:\n\t\t\t\traise ValueError(f\"The argument '{argument}' has a length of {len(argument)} which is longer than the maximum length of {self.length}.\")\n\n\t\tif isinstance(args, dict):\n\t\t\tfor key, value in args.items():\n\t\t\t\tlength_val(value)\n\t\telse:\n\t\t\tlength_val(args)\n\n\tcheck_length(args)\n\n#src/validation/validators/__init__.py\n\"\"\"\nThis module contains all the validators available to the user.\n\"\"\"\n\nfrom .length import validate_length\nfrom .length_args import validate_length_args\n\nvalidation_functions = {\n\t\"length\": validate_length,\n\t\"length_args\": validate_length_args\n}", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\tlength = args[1][0]\n\tmessage = args[1][2]\n\tif len(args[2]) > length:\n\t\traise ValueError(f\"{message} must be {length} characters or less.\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\ti = 0\n\tfor arg in args:\n\t\tif len(arg) > self.length:\n\t\t\traise Exception(\"Argument of index {} is longer than {} characters\".format(i, self.length))", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\t\n\tdef _validate_length(arg):\n\t\t\"\"\"\n\t\tCheck if value of the given arg is not longer than length specified. \\n:param arg: The received argument.\n\t\t\"\"\"\n\t\targ = str(arg)\n\t\tif len(arg) > self.length:\n\t\t\traise Exception(f\"The value of argument '{arg}' is longer than {self.length} characters\")\n\t\n\tfor arg in args:\n\t\t_validate_length(arg)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is not longer than length specified. \\n:param args: The received arguments.\n\t\"\"\"\n\terror = f\"The value of the argument '{args[0]}' is longer than allowed. It should have a length of {args[1]}.\"\n\tif len(args[0]) > args[1]:\n\t\traise Exception(error)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = getattr(self, 'choices', None)\n\n\tif choices is None:\n\t\traise NotImplementedError(\n\t\t\t'The model %s does not have a defined choices tuple. It is not possible to determine if the given argument is valid.' % self.__class__.__name__)\n\n\tif not isinstance(args, (list, tuple)):\n\t\traise TypeError('The given argument must be a list or a tuple.')\n\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\traise ValueError('The given argument %s is not a valid choice.' % arg)\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tfor choice in choices:\n\t\tif choice[0] == args:\n\t\t\treturn True\n\treturn False\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\treturn args[0] in self.args", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tvalid_args = []\n\tfor dict_value in self.choices:\n\t\tvalid_args.append(dict_value.values()[0])\n\tif args in valid_args:\n\t\treturn True\n\telse:\n\t\traise TypeError('The given arguments is not a valid choice.')\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tvalid_values = []\n\tfor choice in self.choices:\n\t\tvalid_values.append(choice[0])\n\tif args not in valid_values:\n\t\traise ValueError('Invalid value for argument %s: %s. Valid values are: %s' % (args, args, str(valid_values)))\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.log.debug('validate_choices_args()')\n\tself.log.debug('args: %s', args)\n\n\t# If the value of the args is one of the available choices, return True.\n\tif args in self.choices:\n\t\tself.log.debug('The value of the args is one of the available choices.')\n\t\treturn True\n\n\t# If the value of the args is not one of the available choices, return False.\n\tself.log.warning('The value of the args is not one of the available choices.')\n\tself.log.debug('The value of the args is not one of the available choices.')\n\treturn False", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices_list = self.choices.split(\",\")\n\tfor choice in choices_list:\n\t\tif args == choice:\n\t\t\treturn True\n\treturn False\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tchoices_names = self.choices_names\n\tchoices_values = self.choices_values\n\tfor arg, value in args.items():\n\t\ttry:\n\t\t\tindex = choices_values.index(value)\n\t\t\tif choices_names[index] == 'choices_names':\n\t\t\t\tchoices_names = [choices_names[i] for i in range(len(choices_names)) if i != index]\n\t\t\tif choices_values[index] == 'choices_values':\n\t\t\t\tchoices_values = [choices_values[i] for i in range(len(choices_values)) if i != index]\n\t\t\treturn choices_names[index], choices_values[index]\n\t\texcept ValueError as e:\n\t\t\traise ValidationError(str(e))\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tvalue = args.get(\"value\")\n\tif type(value) is not str:\n\t\traise TypeError(f\"Value must be a string, got {type(value)}\")\n\tvalue = value.lower()\n\n\tif value in choices:\n\t\treturn value\n\telif value in [choice.lower() for choice in choices]:\n\t\treturn value\n\telif value in self.choices:\n\t\treturn value\n\telse:\n\t\traise ValueError(f\"{value} is not a valid choice\")\n\n#modules/utils/__init__.py\nfrom .check_choices_args import validate_choices_args\n\n__all__ = [\"validate_choices_args\"]\n\n#modules/utils/validators.py\nimport re\nfrom .check_choices_args import validate_choices_args\n\n", "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tis_valid = False\n\tchoices = self.choices\n\tfor choice in choices:\n\t\tif args in choice:\n\t\t\tis_valid = True\n\t\tif is_valid:\n\t\t\tbreak\n\tif not is_valid:\n\t\traise Exception(f\"Value {args} is not a valid choice. Choices are {choices}\")\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = getattr(self, 'choices', None)\n\n\tif choices is None:\n\t\traise NotImplementedError(\n\t\t\t'The model %s does not have a defined choices tuple. It is not possible to determine if the given argument is valid.' % self.__class__.__name__)\n\n\tif not isinstance(args, (list, tuple)):\n\t\traise TypeError('The given argument must be a list or a tuple.')\n\n\tfor arg in args:\n\t\tif arg not in choices:\n\t\t\traise ValueError('The given argument %s is not a valid choice.' % arg)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tfor choice in choices:\n\t\tif choice[0] == args:\n\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\ttry:\n\t\treturn args[0] in self.args", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tvalid_args = []\n\tfor dict_value in self.choices:\n\t\tvalid_args.append(dict_value.values()[0])\n\tif args in valid_args:\n\t\treturn True\n\telse:\n\t\traise TypeError('The given arguments is not a valid choice.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tvalid_values = []\n\tfor choice in self.choices:\n\t\tvalid_values.append(choice[0])\n\tif args not in valid_values:\n\t\traise ValueError('Invalid value for argument %s: %s. Valid values are: %s' % (args, args, str(valid_values)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tself.log.debug('validate_choices_args()')\n\tself.log.debug('args: %s', args)\n\n\t# If the value of the args is one of the available choices, return True.\n\tif args in self.choices:\n\t\tself.log.debug('The value of the args is one of the available choices.')\n\t\treturn True\n\n\t# If the value of the args is not one of the available choices, return False.\n\tself.log.warning('The value of the args is not one of the available choices.')\n\tself.log.debug('The value of the args is not one of the available choices.')\n\treturn False", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices_list = self.choices.split(\",\")\n\tfor choice in choices_list:\n\t\tif args == choice:\n\t\t\treturn True\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tchoices_names = self.choices_names\n\tchoices_values = self.choices_values\n\tfor arg, value in args.items():\n\t\ttry:\n\t\t\tindex = choices_values.index(value)\n\t\t\tif choices_names[index] == 'choices_names':\n\t\t\t\tchoices_names = [choices_names[i] for i in range(len(choices_names)) if i != index]\n\t\t\tif choices_values[index] == 'choices_values':\n\t\t\t\tchoices_values = [choices_values[i] for i in range(len(choices_values)) if i != index]\n\t\t\treturn choices_names[index], choices_values[index]\n\t\texcept ValueError as e:\n\t\t\traise ValidationError(str(e))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tchoices = self.choices\n\tvalue = args.get(\"value\")\n\tif type(value) is not str:\n\t\traise TypeError(f\"Value must be a string, got {type(value)}\")\n\tvalue = value.lower()\n\n\tif value in choices:\n\t\treturn value\n\telif value in [choice.lower() for choice in choices]:\n\t\treturn value\n\telif value in self.choices:\n\t\treturn value\n\telse:\n\t\traise ValueError(f\"{value} is not a valid choice\")\n\n#modules/utils/__init__.py\nfrom .check_choices_args import validate_choices_args\n\n__all__ = [\"validate_choices_args\"]\n\n#modules/utils/validators.py\nimport re\nfrom .check_choices_args import validate_choices_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is one of the available choices. \\n:param args: The received arguments.\n\t\"\"\"\n\tis_valid = False\n\tchoices = self.choices\n\tfor choice in choices:\n\t\tif args in choice:\n\t\t\tis_valid = True\n\t\tif is_valid:\n\t\t\tbreak\n\tif not is_valid:\n\t\traise Exception(f\"Value {args} is not a valid choice. Choices are {choices}\")\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value, max_value = args\n\tfor value in args:\n\t\tif value < min_value or value > max_value:\n\t\t\traise ValueError(f\"Value {value} is not between {min_value} and {max_value}\")\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\targs = int(args)\n\texcept ValueError:\n\t\targs = float(args)\n\n\tif args < self.min_value:\n\t\traise ValueError(\"Value must be greater than or equal to {0}\".format(self.min_value))\n\telif args > self.max_value:\n\t\traise ValueError(\"Value must be less than or equal to {0}\".format(self.max_value))\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = args[0]\n\tmax_value = args[1]\n\tvalue = args[2]\n\t\n\tif min_value > max_value:\n\t\traise ValueError(\"Maximum value must be greater than minimum value\")\n\t\n\tif value < min_value or value > max_value:\n\t\traise ValueError(\"Value must be between minimum and maximum values\")\n\n#src/scripts/util/validate_min_max_args.py\nfrom .validate_min_max_args import validate_min_max_args\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tvalue = args.get(self.param_name)\n\tif value is None:\n\t\treturn args\n\n\tminimum = self.min_value\n\tmaximum = self.max_value\n\n\tif minimum is not None and value < minimum:\n\t\traise CommandError('Parameter %s cannot be less than %s' % (self.param_name, minimum))\n\n\tif maximum is not None and value > maximum:\n\t\traise CommandError('Parameter %s cannot be greater than %s' % (self.param_name, maximum))\n\n\treturn args", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tvalid = True\n\tfor arg in args:\n\t\tif not (arg >= self.min and arg <= self.max):\n\t\t\tvalid = False\n\treturn valid\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\tif args['min'] and args['max']:\n\t\t\tif args['min'] > args['max']:\n\t\t\t\traise ValueError\n\texcept ValueError:\n\t\traise ValueError(\"The given min and max parameters must be between each other\")\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tdef check_min_max(arg, min_val, max_val):\n\t\tif not isinstance(arg, int):\n\t\t\traise ValueError(\"Invalid argument: {} must be an int\".format(arg))\n\t\tif not (min_val <= arg <= max_val):\n\t\t\traise ValueError(\"Invalid argument: {} must be between {} and {}\".format(arg, min_val, max_val))\n\n\tfor arg in args:\n\t\tcheck_min_max(arg, self.min_value, self.max_value)\n\n\treturn True\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = args[0]\n\tmax_value = args[1]\n\tif not min_value <= self.value <= max_value:\n\t\traise ValueError('Value of {0} is not between {1} and {2}'.format(\n\t\t\tself.name, min_value, max_value))\n\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\tif not (args.min <= args.max):\n\t\t\traise ValueError\n\texcept AttributeError:\n\t\traise AttributeError\n", "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\tmin_val = args[0]\n\tmax_val = args[1]\n\t\n\tif max_val < min_val:\n\t\traise Exception(\"Minimum value is greater than maximum value\")\n\t\n\tfor i in range(2, len(args)):\n\t\tval = args[i]\n\t\tif not (val >= min_val and val <= max_val):\n\t\t\traise Exception(\"Invalid value for argument: \" + str(args[i]))\n\n#src/utils.py\nimport os\nimport sys\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value, max_value = args\n\tfor value in args:\n\t\tif value < min_value or value > max_value:\n\t\t\traise ValueError(f\"Value {value} is not between {min_value} and {max_value}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\targs = int(args)\n\texcept ValueError:\n\t\targs = float(args)\n\n\tif args < self.min_value:\n\t\traise ValueError(\"Value must be greater than or equal to {0}\".format(self.min_value))\n\telif args > self.max_value:\n\t\traise ValueError(\"Value must be less than or equal to {0}\".format(self.max_value))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = args[0]\n\tmax_value = args[1]\n\tvalue = args[2]\n\t\n\tif min_value > max_value:\n\t\traise ValueError(\"Maximum value must be greater than minimum value\")\n\t\n\tif value < min_value or value > max_value:\n\t\traise ValueError(\"Value must be between minimum and maximum values\")\n\n#src/scripts/util/validate_min_max_args.py\nfrom .validate_min_max_args import validate_min_max_args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tvalue = args.get(self.param_name)\n\tif value is None:\n\t\treturn args\n\n\tminimum = self.min_value\n\tmaximum = self.max_value\n\n\tif minimum is not None and value < minimum:\n\t\traise CommandError('Parameter %s cannot be less than %s' % (self.param_name, minimum))\n\n\tif maximum is not None and value > maximum:\n\t\traise CommandError('Parameter %s cannot be greater than %s' % (self.param_name, maximum))\n\n\treturn args", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tvalid = True\n\tfor arg in args:\n\t\tif not (arg >= self.min and arg <= self.max):\n\t\t\tvalid = False\n\treturn valid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\tif args['min'] and args['max']:\n\t\t\tif args['min'] > args['max']:\n\t\t\t\traise ValueError\n\texcept ValueError:\n\t\traise ValueError(\"The given min and max parameters must be between each other\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tdef check_min_max(arg, min_val, max_val):\n\t\tif not isinstance(arg, int):\n\t\t\traise ValueError(\"Invalid argument: {} must be an int\".format(arg))\n\t\tif not (min_val <= arg <= max_val):\n\t\t\traise ValueError(\"Invalid argument: {} must be between {} and {}\".format(arg, min_val, max_val))\n\n\tfor arg in args:\n\t\tcheck_min_max(arg, self.min_value, self.max_value)\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\tmin_value = args[0]\n\tmax_value = args[1]\n\tif not min_value <= self.value <= max_value:\n\t\traise ValueError('Value of {0} is not between {1} and {2}'.format(\n\t\t\tself.name, min_value, max_value))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\ttry:\n\t\tif not (args.min <= args.max):\n\t\t\traise ValueError\n\texcept AttributeError:\n\t\traise AttributeError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\"\"\"\n\tCheck if value of the given args is between minimum and maximum values\n\t\"\"\"\n\t\n\tmin_val = args[0]\n\tmax_val = args[1]\n\t\n\tif max_val < min_val:\n\t\traise Exception(\"Minimum value is greater than maximum value\")\n\t\n\tfor i in range(2, len(args)):\n\t\tval = args[i]\n\t\tif not (val >= min_val and val <= max_val):\n\t\t\traise Exception(\"Invalid value for argument: \" + str(args[i]))\n\n#src/utils.py\nimport os\nimport sys\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\treturn COMPLEX_TYPES[type_name][option_name](self, subcommand, spec_option)\n\texcept KeyError as e:\n\t\traise CommandError(\"No such argument type: {}\".format(option_name))\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tfunc = COMPLEX_TYPES.get(type_name)\n\tif func is None:\n\t\treturn None\n\treturn func(self, subcommand, option_name, spec_option)", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttype_func = COMPLEX_TYPES[type_name]\n\tfunc_name = 'create_' + type_name + '_argumet_type'\n\n\tdef func(self, subcommand, option_name, spec_option):\n\t\t\"\"\"\n\t\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\t\"\"\"\n\t\tresult = type_func(self, subcommand, option_name, spec_option)\n\t\tself.action(result, subcommand, spec_option)\n\t\treturn result\n\n\tfunc.__name__ = func_name\n\tsetattr(self, func_name, func)\n\treturn func_name\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tmethod = getattr(self, type_name.upper())\n\treturn method(self.vars, self.defaults, self.plugin_path, subcommand, spec_option)\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\tfn = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise ArgumentTypeError('Unknown complex argument type %s' % type_name)\n\treturn fn(self, subcommand, option_name, spec_option)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_args = getattr(self, 'complex_args', None)\n\tif complex_args is not None and type_name in complex_args:\n\t\treturn complex_args[type_name].get(option_name, None)\n\n\n@register_option_type", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\treturn COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n\texcept KeyError:\n\t\traise InvalidOption(\n\t\t\t'Unsupported type: {0}'.format(type_name))\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n\t#The names of the functions in COMPLEX_TYPES are of the form COMPLEX_<type_name>_<option_name>_<subcommand>\n\t\n\tdef COMPLEX_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER(self, subcommand, spec_option):\n\t\t\"\"\"\n\t\tReturns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\t\"\"\"\n\t\t#The result is a tuple containing the result of the complex_action, and the result of the get_default_value function.\n\t\t#The result of the get_default_value function is None if spec_option", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tdef complex_action(self, option_value, option_spec):\n\t\t# check the spec is correct\n\t\tself.check_spec(option_spec, subcommand, spec_option, option_name)\n\t\t# extract the arguments\n\t\targuments = self.get_arguments(option_value, option_spec)\n\t\t# create the dictionary containing the arguments\n\t\targuments_dict = self.create_arguments_dict(arguments, option_name)\n\t\t# create the dictionary containing the option\n\t\toption_dict = self.create_option_dict(option_value, option_spec, option_name)\n\t\t# create the dictionary containing the spec_option\n\t\tspec_option_dict = self.create_spec_option_dict(spec_option, option_name)\n\t\t# create the arguments_dict containing the action\n\t\taction_dict = self.create_action_dict(subcommand, option_name)\n\t\t# return the arguments_dict\n\t\treturn arguments_dict, option_dict, spec_option_dict, action_dict\n\n\tdef get_arguments(self, option_value, option_spec):\n\t\t# check the spec is correct\n\t\tself.check_spec(option_spec, subcommand, spec_option, option_name)\n\t\t# extract the arguments\n\t\targuments = self.", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_type = self.complex_types.get(type_name)\n\tif complex_type is None:\n\t\traise TypeError('Complex type %s not defined' % type_name)\n\treturn complex_type(self, subcommand, option_name, spec_option)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\treturn COMPLEX_TYPES[type_name][option_name](self, subcommand, spec_option)\n\texcept KeyError as e:\n\t\traise CommandError(\"No such argument type: {}\".format(option_name))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tfunc = COMPLEX_TYPES.get(type_name)\n\tif func is None:\n\t\treturn None\n\treturn func(self, subcommand, option_name, spec_option)", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttype_func = COMPLEX_TYPES[type_name]\n\tfunc_name = 'create_' + type_name + '_argumet_type'\n\n\tdef func(self, subcommand, option_name, spec_option):\n\t\t\"\"\"\n\t\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\t\"\"\"\n\t\tresult = type_func(self, subcommand, option_name, spec_option)\n\t\tself.action(result, subcommand, spec_option)\n\t\treturn result\n\n\tfunc.__name__ = func_name\n\tsetattr(self, func_name, func)\n\treturn func_name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tmethod = getattr(self, type_name.upper())\n\treturn method(self.vars, self.defaults, self.plugin_path, subcommand, spec_option)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\tfn = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise ArgumentTypeError('Unknown complex argument type %s' % type_name)\n\treturn fn(self, subcommand, option_name, spec_option)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_args = getattr(self, 'complex_args', None)\n\tif complex_args is not None and type_name in complex_args:\n\t\treturn complex_args[type_name].get(option_name, None)\n\n\n@register_option_type", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\ttry:\n\t\treturn COMPLEX_TYPES[type_name](self, subcommand, option_name, spec_option)\n\texcept KeyError:\n\t\traise InvalidOption(\n\t\t\t'Unsupported type: {0}'.format(type_name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\t\n\t#The names of the functions in COMPLEX_TYPES are of the form COMPLEX_<type_name>_<option_name>_<subcommand>\n\t\n\tdef COMPLEX_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER_INTEGER(self, subcommand, spec_option):\n\t\t\"\"\"\n\t\tReturns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\t\"\"\"\n\t\t#The result is a tuple containing the result of the complex_action, and the result of the get_default_value function.\n\t\t#The result of the get_default_value function is None if spec_option", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tdef complex_action(self, option_value, option_spec):\n\t\t# check the spec is correct\n\t\tself.check_spec(option_spec, subcommand, spec_option, option_name)\n\t\t# extract the arguments\n\t\targuments = self.get_arguments(option_value, option_spec)\n\t\t# create the dictionary containing the arguments\n\t\targuments_dict = self.create_arguments_dict(arguments, option_name)\n\t\t# create the dictionary containing the option\n\t\toption_dict = self.create_option_dict(option_value, option_spec, option_name)\n\t\t# create the dictionary containing the spec_option\n\t\tspec_option_dict = self.create_spec_option_dict(spec_option, option_name)\n\t\t# create the arguments_dict containing the action\n\t\taction_dict = self.create_action_dict(subcommand, option_name)\n\t\t# return the arguments_dict\n\t\treturn arguments_dict, option_dict, spec_option_dict, action_dict\n\n\tdef get_arguments(self, option_value, option_spec):\n\t\t# check the spec is correct\n\t\tself.check_spec(option_spec, subcommand, spec_option, option_name)\n\t\t# extract the arguments\n\t\targuments = self.", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t\"\"\"\n\tReturns the corresponding function in COMPLEX_TYPES based on type_name and returns the result obtained by complex_action after option_name,(self.vars, self.defaults, self.plugin_path), subcommand, spec_option) is input.\n\t\"\"\"\n\tcomplex_type = self.complex_types.get(type_name)\n\tif complex_type is None:\n\t\traise TypeError('Complex type %s not defined' % type_name)\n\treturn complex_type(self, subcommand, option_name, spec_option)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('control_'):\n\t\t\tcontrol_args[arg.replace('control_', '')] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcontrol_arg_keys = ['ip', 'ansible_host', 'ansible_port', 'ansible_user', 'ansible_become']\n\tnested_arg_keys = ['ansible_ssh_pass', 'ansible_ssh_private_key_file']\n\n\tfor arg in args:\n\t\tif arg in control_arg_keys:\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg in nested_arg_keys:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn (control_args, nested_args)", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--control-\"):\n\t\t\tcontrol_args[arg.split(\"--control-\")[1]] = \"True\"\n\t\telse:\n\t\t\tnested_args[arg.split(\"--nested-\")[1]] = \"True\"\n\treturn control_args, nested_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\targ_type = arg.split(\"=\")[0]\n\t\tif arg_type == \"control\":\n\t\t\tcontrol_args[arg] = arg.split(\"=\")[1]\n\t\telse:\n\t\t\tnested_args[arg] = arg.split(\"=\")[1]\n\n\treturn (control_args, nested_args)\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = self.control_args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = self.nested_args[arg]\n\treturn control_args, nested_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg, val in args.items():\n\t\tif arg.startswith('control_'):\n\t\t\tcontrol_args[arg] = val\n\t\telif arg.startswith('nested_'):\n\t\t\tnested_args[arg] = val\n\t\telse:\n\t\t\traise RuntimeError('Unknown argument: {}'.format(arg))\n\treturn control_args, nested_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"ansible\"):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = dict()\n\tnested_args = dict()\n\tfor arg in args:\n\t\tif arg.startswith(('-', '--', '--')):\n\t\t\tcontrol_args[arg] = None\n\t\telse:\n\t\t\tnested_args[arg] = None\n\n\treturn control_args, nested_args", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = args\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif 'ansible_' in arg:\n\t\t\t# It is a custom argument\n\t\t\tcustom_args[arg.split('=', 1)[0]] = arg.split('=', 1)[1]\n\t\telse:\n\t\t\t# It is a nested argument\n\t\t\tnested_args = nested_args[arg]\n\n\treturn (control_args, nested_args), custom_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {'ansible_user': 'root', 'ansible_become': True}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--') and arg.split('=', 1)[0] not in control_args:\n\t\t\tnested_args.update({arg.split('=', 1)[0]: arg.split('=', 1)[1]})\n\n\treturn nested_args, control_args\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('control_'):\n\t\t\tcontrol_args[arg.replace('control_', '')] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tcontrol_arg_keys = ['ip', 'ansible_host', 'ansible_port', 'ansible_user', 'ansible_become']\n\tnested_arg_keys = ['ansible_ssh_pass', 'ansible_ssh_private_key_file']\n\n\tfor arg in args:\n\t\tif arg in control_arg_keys:\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg in nested_arg_keys:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn (control_args, nested_args)", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--control-\"):\n\t\t\tcontrol_args[arg.split(\"--control-\")[1]] = \"True\"\n\t\telse:\n\t\t\tnested_args[arg.split(\"--nested-\")[1]] = \"True\"\n\treturn control_args, nested_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\targ_type = arg.split(\"=\")[0]\n\t\tif arg_type == \"control\":\n\t\t\tcontrol_args[arg] = arg.split(\"=\")[1]\n\t\telse:\n\t\t\tnested_args[arg] = arg.split(\"=\")[1]\n\n\treturn (control_args, nested_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg in self.control_args:\n\t\t\tcontrol_args[arg] = self.control_args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = self.nested_args[arg]\n\treturn control_args, nested_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg, val in args.items():\n\t\tif arg.startswith('control_'):\n\t\t\tcontrol_args[arg] = val\n\t\telif arg.startswith('nested_'):\n\t\t\tnested_args[arg] = val\n\t\telse:\n\t\t\traise RuntimeError('Unknown argument: {}'.format(arg))\n\treturn control_args, nested_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"ansible\"):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = dict()\n\tnested_args = dict()\n\tfor arg in args:\n\t\tif arg.startswith(('-', '--', '--')):\n\t\t\tcontrol_args[arg] = None\n\t\telse:\n\t\t\tnested_args[arg] = None\n\n\treturn control_args, nested_args", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {}\n\tnested_args = args\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif 'ansible_' in arg:\n\t\t\t# It is a custom argument\n\t\t\tcustom_args[arg.split('=', 1)[0]] = arg.split('=', 1)[1]\n\t\telse:\n\t\t\t# It is a nested argument\n\t\t\tnested_args = nested_args[arg]\n\n\treturn (control_args, nested_args), custom_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\t\"\"\"\n\tSplit input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)\n\t\"\"\"\n\tcontrol_args = {'ansible_user': 'root', 'ansible_become': True}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--') and arg.split('=', 1)[0] not in control_args:\n\t\t\tnested_args.update({arg.split('=', 1)[0]: arg.split('=', 1)[1]})\n\n\treturn nested_args, control_args\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\timport os\n\textra_vars = extra_vars or []\n\tvars_dict.update(os.environ)\n\tfor var_name in extra_vars:\n\t\tvalue = os.environ.get(var_name)\n\t\tif value:\n\t\t\tvars_dict[var_name] = value\n\n#yum_plugins/__init__.py\n# -*- coding: utf-8 -*-\n\"\"\"\nYum plugins for Ansible.\n\n:copyright: (c) 2014 by Ansible, Inc.\n:license: BSD, see LICENSE for more details.\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\n__version__ = '1.1.0'\n\n__all__ = [\n    '__version__',\n]\n\n#yum_plugins/plugins/callback.py\n# -*- coding: utf-8 -*-\n# vim:fileencoding=utf-8\n\n# Copyright (c) 2013 Red Hat, Inc.\n#\n# This software is licensed to you under the GNU General Public License,\n# version 3 (GPLv3). There is NO WARRANTY for this software, express or\n# implied, including the implied warranties of MERCHANTABILITY or", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tvars_dict.update({ \"extra-vars\": extra_vars })", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor var in extra_vars:\n\t\tname, value = var.split('=')\n\t\tvars_dict[name] = value\n\n\treturn vars_dict", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars:\n\t\tfor extra in extra_vars:\n\t\t\tvars_dict[extra] = extra_vars[extra]\n\n\treturn vars_dict", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or {}\n\tfor extra_var in extra_vars:\n\t\tvars_dict[extra_var] = extra_vars[extra_var]\n\n\treturn vars_dict", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor var in extra_vars:\n\t\tif var.startswith('extra-vars'):\n\t\t\tvars_dict.update(yaml.safe_load(var))\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor extra_var in extra_vars:\n\t\tvars_dict[extra_var] = extra_var\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tvars_dict.update({\"extra-vars\": extra_vars})\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor var in extra_vars:\n\t\tvar_name, var_value = var.split('=')\n\t\tvars_dict.setdefault(var_name, []).append(var_value)\n\n\treturn vars_dict\n\n#\n# \tVariables\n#\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif not extra_vars:\n\t\treturn vars_dict\n\n\tvars_dict['extra-vars'] = extra_vars\n\n\treturn vars_dict"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\timport os\n\textra_vars = extra_vars or []\n\tvars_dict.update(os.environ)\n\tfor var_name in extra_vars:\n\t\tvalue = os.environ.get(var_name)\n\t\tif value:\n\t\t\tvars_dict[var_name] = value\n\n#yum_plugins/__init__.py\n# -*- coding: utf-8 -*-\n\"\"\"\nYum plugins for Ansible.\n\n:copyright: (c) 2014 by Ansible, Inc.\n:license: BSD, see LICENSE for more details.\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function\n\n__version__ = '1.1.0'\n\n__all__ = [\n    '__version__',\n]\n\n#yum_plugins/plugins/callback.py\n# -*- coding: utf-8 -*-\n# vim:fileencoding=utf-8\n\n# Copyright (c) 2013 Red Hat, Inc.\n#\n# This software is licensed to you under the GNU General Public License,\n# version 3 (GPLv3). There is NO WARRANTY for this software, express or\n# implied, including the implied warranties of MERCHANTABILITY or", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tvars_dict.update({ \"extra-vars\": extra_vars })", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor var in extra_vars:\n\t\tname, value = var.split('=')\n\t\tvars_dict[name] = value\n\n\treturn vars_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif extra_vars:\n\t\tfor extra in extra_vars:\n\t\t\tvars_dict[extra] = extra_vars[extra]\n\n\treturn vars_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or {}\n\tfor extra_var in extra_vars:\n\t\tvars_dict[extra_var] = extra_vars[extra_var]\n\n\treturn vars_dict", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor var in extra_vars:\n\t\tif var.startswith('extra-vars'):\n\t\t\tvars_dict.update(yaml.safe_load(var))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor extra_var in extra_vars:\n\t\tvars_dict[extra_var] = extra_var\n\treturn vars_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tvars_dict.update({\"extra-vars\": extra_vars})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\textra_vars = extra_vars or []\n\tfor var in extra_vars:\n\t\tvar_name, var_value = var.split('=')\n\t\tvars_dict.setdefault(var_name, []).append(var_value)\n\n\treturn vars_dict\n\n#\n# \tVariables\n#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\t\"\"\"\n\tExtend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars\n\t\"\"\"\n\t\n\tif not extra_vars:\n\t\treturn vars_dict\n\n\tvars_dict['extra-vars'] = extra_vars\n\n\treturn vars_dict", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\n\tir_workspace_dir = ir_workspace.dir\n\tir_plugin_dir = os.path.join(ir_workspace_dir, ir_plugin.name)\n\n\tif not os.path.exists(ir_plugin_dir):\n\t\tir_plugin.execute_subcommand(\n\t\t\tname='setup',\n\t\t\targs=[ir_workspace_dir, ir_plugin.name],\n\t\t\tdescription='Set up the plugin.',\n\t\t\traise_on_fail=True)\n\n\tif not os.path.exists(playbook_path):\n\t\tir_plugin.execute_subcommand(\n\t\t\tname='setup',\n\t\t\targs=[ir_workspace_dir, ir_plugin.name],\n\t\t\tdescription='Set up the plugin.',\n\t\t\traise_on_fail=True)\n\n\tplaybook_path = os.path.abspath(playbook_path)\n\tplaybook_dir = os.path.dirname(playbook_path)\n\n\tplaybook_name = os.path.basename(playbook_path)\n\n\tansible_args = ansible_args or {}\n\tansible_args['-i'] = os.path.join(ir_works", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\n\t# Make sure to use the right Ansible version\n\tansible_path = os.path.join(ir_workspace.ansible_plugins_dir, ir_plugin.ansible_plugin_name, 'bin', 'ansible-playbook')\n\tansible_args = ansible_args or {}\n\tansible_args['-i'] = '%s/inventory' % ir_workspace.ir_home_dir\n\tansible_args['-e'] = 'ansible_ssh_user=%s' % ir_plugin.ansible_ssh_user\n\tansible_args['-e'] = 'ansible_ssh_private_key_file=%s' % ir_plugin.ansible_ssh_key_file\n\tansible_args['-e'] = 'ansible_ssh_port=%s' % ir_plugin.ansible_ssh_port\n\tif verbose:\n\t\tansible_args['-vv'] = ''\n\tansible_args['-e'] = 'ansible_ssh_common_args=%s' % ir_plugin.ansible_ssh_common_args\n\tansible_args['-e'] = 'ansible_ssh_extra_args=%s' % ir_plugin.ansible", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport shutil\n\timport subprocess\n\tfrom infrared.util.ansible_util import get_ansible_version\n\n\tansible_path = shutil.which('ansible-playbook')\n\tif not os.path.exists(ansible_path):\n\t\traise Exception('ansible-playbook not found in PATH')\n\n\tansible_version = get_ansible_version()\n\tif ansible_version < '1.6.0':\n\t\traise Exception('Unsupported Ansible version %s - must be 1.6.0 or later' % ansible_version)\n\n\textra_vars = extra_vars or {}\n\t\n\tansible_args = ansible_args or {}\n\tansible_args.update({\n\t\t'-i': os.path.join(ir_workspace.get_dir(), 'inventory'),\n\t\t'-c': ir_plugin.get_ansible_config_dir(),\n\t\t'-vvvvvv': '' if verbose is None else verbose,\n\t\t})\n\n\tplaybook_args = [ansible_path, '-e', '@%s' % os.path.join(ir_workspace.get_dir(), 'playbook.yml'), playbook_path]\n\tplay", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tir_plugin.log(\"Ansible Playbook Invocation: %s\" % playbook_path)\n\t#TODO: Use a better way of passing these args\n\targs = [\n\t\t'-i', ir_plugin.inventory_path,\n\t\t'-l', ir_plugin.inventory_hostname,\n\t\t'-k',\n\t\tplaybook_path,\n\t]\n\tif verbose is not None:\n\t\targs.append('--verbosity=%s' % verbose)\n\tif extra_vars is not None:\n\t\targs.extend(['--extra-vars', '%s' % extra_vars])\n\tif ansible_args is not None:\n\t\targs.extend(ansible_args)\n\n\tir_plugin.log(\"Ansible CLI Args: %s\" % args)\n\tir_plugin.log(\"Ansible CLI Args: %s\" % args)\n\t# TODO: Use a proper Cmd object\n\tproc = ir_plugin.run_cmd(args)\n\tstdout, stderr = proc.communicate()\n\tir_plugin.log(\"Ansible CLI Stdout: %s\" % stdout)\n\tir_plugin.log(\"Ansible CLI Stderr: %s\" % stderr)\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\n\tansible_args = ansible_args or {}\n\n\tplaybook_command = [\n\t\t'ansible-playbook',\n\t\t'-vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\n\t# InfraredPlugin.ansible_args should be a dict of arguments, with\n\t# the keys being the argument names and the values being the\n\t# argument values.\n\tansible_args.update(ir_plugin.ansible_args)\n\n\targs = [\n\t\t'--inventory',\n\t\t'%s' % ir_workspace.inventory_path,\n\t\t'--extra-vars',\n\t\t'%s' % extra_vars\n\t]\n\n\tif verbose is not None:\n\t\targs.extend(['--verbose', verbose])\n\targs.extend(ir_plugin.ansible_extra_args)\n\targs.extend([playbook_path])\n\n\tcommand = [\n\t\t'ansible-playbook',\n\t] + args\n\n\tir_plugin.run(command)\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\n\textra_vars = extra_vars or {}\n\textra_vars.update({\n\t\t\"ir_workspace\": ir_workspace,\n\t\t\"ir_plugin\": ir_plugin,\n\t})\n\n\toptions = {\n\t\t\"playbook\": playbook_path,\n\t}\n\n\tif verbose is not None:\n\t\toptions.update({\n\t\t\t\"verbosity\": verbose,\n\t\t})\n\n\toptions.update(extra_vars)\n\n\targs = [\n\t\t\"ansible-playbook\",\n\t]\n\n\targs.extend(ansible_args.items())\n\targs.append(options)\n\n\tresult = ir_plugin.execute(args)\n\n\tif result.returncode != 0:\n\t\traise Exception(result.stderr)\n\n\treturn result.stdout\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\targs = [\n\t\t'ansible-playbook',\n\t\t'--inventory', '%s' % ir_workspace.inventory_file,\n\t\t'--extra-vars', '%s' % extra_vars,\n\t\t'--verbosity', verbose or ir_plugin.verbosity,\n\t]\n\n\tif ansible_args:\n\t\targs.extend(ansible_args)\n\n\targs.append(playbook_path)\n\tir_plugin.run(args)", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tplugin_name = ir_plugin.plugin_name\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tcmd = [\n\t\t'ansible-playbook',\n\t\t'-i', get_inventory_file(ir_workspace),\n\t\t'--extra-vars', json.dumps(extra_vars),\n\t\tplaybook_path\n\t]\n\tif ansible_args:\n\t\tcmd = cmd + ['--' + key + '=' + val for key, val in ansible_args.items()]\n\tif verbose is not None:\n\t\tcmd = cmd + ['-vvvv']\n\tcmd = cmd + ['--connection', 'local']\n\treturn run_cmd(cmd, plugin_name)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport subprocess\n\timport os\n\t\n\tdef get_ansible_command(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\t                        extra_vars=None, ansible_args=None):\n\t\t\"\"\"\n\t\tWraps the 'ansible-playbook' CLI\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\t\"\"\"\n\n\t\tcmd = [\"ansible-playbook\"]\n\n\t\tif verbose != None:\n\t\t\tcmd.append(\"-v\")\n\n\t\tif extra_vars != None:\n\t\t\tcmd.append(\"-e\")\n\t\t\tfor key, value in extra_vars.items():\n\t\t\t\tcmd.append(\"{}={}\".format(key, value))\n\n\t\tif ir_plugin != None:\n\t\t\tcmd.extend([\"--inventory\", ir"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\n\tir_workspace_dir = ir_workspace.dir\n\tir_plugin_dir = os.path.join(ir_workspace_dir, ir_plugin.name)\n\n\tif not os.path.exists(ir_plugin_dir):\n\t\tir_plugin.execute_subcommand(\n\t\t\tname='setup',\n\t\t\targs=[ir_workspace_dir, ir_plugin.name],\n\t\t\tdescription='Set up the plugin.',\n\t\t\traise_on_fail=True)\n\n\tif not os.path.exists(playbook_path):\n\t\tir_plugin.execute_subcommand(\n\t\t\tname='setup',\n\t\t\targs=[ir_workspace_dir, ir_plugin.name],\n\t\t\tdescription='Set up the plugin.',\n\t\t\traise_on_fail=True)\n\n\tplaybook_path = os.path.abspath(playbook_path)\n\tplaybook_dir = os.path.dirname(playbook_path)\n\n\tplaybook_name = os.path.basename(playbook_path)\n\n\tansible_args = ansible_args or {}\n\tansible_args['-i'] = os.path.join(ir_works", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\n\t# Make sure to use the right Ansible version\n\tansible_path = os.path.join(ir_workspace.ansible_plugins_dir, ir_plugin.ansible_plugin_name, 'bin', 'ansible-playbook')\n\tansible_args = ansible_args or {}\n\tansible_args['-i'] = '%s/inventory' % ir_workspace.ir_home_dir\n\tansible_args['-e'] = 'ansible_ssh_user=%s' % ir_plugin.ansible_ssh_user\n\tansible_args['-e'] = 'ansible_ssh_private_key_file=%s' % ir_plugin.ansible_ssh_key_file\n\tansible_args['-e'] = 'ansible_ssh_port=%s' % ir_plugin.ansible_ssh_port\n\tif verbose:\n\t\tansible_args['-vv'] = ''\n\tansible_args['-e'] = 'ansible_ssh_common_args=%s' % ir_plugin.ansible_ssh_common_args\n\tansible_args['-e'] = 'ansible_ssh_extra_args=%s' % ir_plugin.ansible", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport shutil\n\timport subprocess\n\tfrom infrared.util.ansible_util import get_ansible_version\n\n\tansible_path = shutil.which('ansible-playbook')\n\tif not os.path.exists(ansible_path):\n\t\traise Exception('ansible-playbook not found in PATH')\n\n\tansible_version = get_ansible_version()\n\tif ansible_version < '1.6.0':\n\t\traise Exception('Unsupported Ansible version %s - must be 1.6.0 or later' % ansible_version)\n\n\textra_vars = extra_vars or {}\n\t\n\tansible_args = ansible_args or {}\n\tansible_args.update({\n\t\t'-i': os.path.join(ir_workspace.get_dir(), 'inventory'),\n\t\t'-c': ir_plugin.get_ansible_config_dir(),\n\t\t'-vvvvvv': '' if verbose is None else verbose,\n\t\t})\n\n\tplaybook_args = [ansible_path, '-e', '@%s' % os.path.join(ir_workspace.get_dir(), 'playbook.yml'), playbook_path]\n\tplay", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tir_plugin.log(\"Ansible Playbook Invocation: %s\" % playbook_path)\n\t#TODO: Use a better way of passing these args\n\targs = [\n\t\t'-i', ir_plugin.inventory_path,\n\t\t'-l', ir_plugin.inventory_hostname,\n\t\t'-k',\n\t\tplaybook_path,\n\t]\n\tif verbose is not None:\n\t\targs.append('--verbosity=%s' % verbose)\n\tif extra_vars is not None:\n\t\targs.extend(['--extra-vars', '%s' % extra_vars])\n\tif ansible_args is not None:\n\t\targs.extend(ansible_args)\n\n\tir_plugin.log(\"Ansible CLI Args: %s\" % args)\n\tir_plugin.log(\"Ansible CLI Args: %s\" % args)\n\t# TODO: Use a proper Cmd object\n\tproc = ir_plugin.run_cmd(args)\n\tstdout, stderr = proc.communicate()\n\tir_plugin.log(\"Ansible CLI Stdout: %s\" % stdout)\n\tir_plugin.log(\"Ansible CLI Stderr: %s\" % stderr)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport os\n\timport subprocess\n\n\tansible_args = ansible_args or {}\n\n\tplaybook_command = [\n\t\t'ansible-playbook',\n\t\t'-vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\n\t# InfraredPlugin.ansible_args should be a dict of arguments, with\n\t# the keys being the argument names and the values being the\n\t# argument values.\n\tansible_args.update(ir_plugin.ansible_args)\n\n\targs = [\n\t\t'--inventory',\n\t\t'%s' % ir_workspace.inventory_path,\n\t\t'--extra-vars',\n\t\t'%s' % extra_vars\n\t]\n\n\tif verbose is not None:\n\t\targs.extend(['--verbose', verbose])\n\targs.extend(ir_plugin.ansible_extra_args)\n\targs.extend([playbook_path])\n\n\tcommand = [\n\t\t'ansible-playbook',\n\t] + args\n\n\tir_plugin.run(command)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tansible_args = ansible_args or {}\n\n\textra_vars = extra_vars or {}\n\textra_vars.update({\n\t\t\"ir_workspace\": ir_workspace,\n\t\t\"ir_plugin\": ir_plugin,\n\t})\n\n\toptions = {\n\t\t\"playbook\": playbook_path,\n\t}\n\n\tif verbose is not None:\n\t\toptions.update({\n\t\t\t\"verbosity\": verbose,\n\t\t})\n\n\toptions.update(extra_vars)\n\n\targs = [\n\t\t\"ansible-playbook\",\n\t]\n\n\targs.extend(ansible_args.items())\n\targs.append(options)\n\n\tresult = ir_plugin.execute(args)\n\n\tif result.returncode != 0:\n\t\traise Exception(result.stderr)\n\n\treturn result.stdout\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\targs = [\n\t\t'ansible-playbook',\n\t\t'--inventory', '%s' % ir_workspace.inventory_file,\n\t\t'--extra-vars', '%s' % extra_vars,\n\t\t'--verbosity', verbose or ir_plugin.verbosity,\n\t]\n\n\tif ansible_args:\n\t\targs.extend(ansible_args)\n\n\targs.append(playbook_path)\n\tir_plugin.run(args)", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\tplugin_name = ir_plugin.plugin_name\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tcmd = [\n\t\t'ansible-playbook',\n\t\t'-i', get_inventory_file(ir_workspace),\n\t\t'--extra-vars', json.dumps(extra_vars),\n\t\tplaybook_path\n\t]\n\tif ansible_args:\n\t\tcmd = cmd + ['--' + key + '=' + val for key, val in ansible_args.items()]\n\tif verbose is not None:\n\t\tcmd = cmd + ['-vvvv']\n\tcmd = cmd + ['--connection', 'local']\n\treturn run_cmd(cmd, plugin_name)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t\"\"\"\n\tWraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\"\"\"\n\timport subprocess\n\timport os\n\t\n\tdef get_ansible_command(ir_workspace, ir_plugin, playbook_path, verbose=None,\n\t                        extra_vars=None, ansible_args=None):\n\t\t\"\"\"\n\t\tWraps the 'ansible-playbook' CLI\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.\n\t\t\"\"\"\n\n\t\tcmd = [\"ansible-playbook\"]\n\n\t\tif verbose != None:\n\t\t\tcmd.append(\"-v\")\n\n\t\tif extra_vars != None:\n\t\t\tcmd.append(\"-e\")\n\t\t\tfor key, value in extra_vars.items():\n\t\t\t\tcmd.append(\"{}={}\".format(key, value))\n\n\t\tif ir_plugin != None:\n\t\t\tcmd.extend([\"--inventory\", ir", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom infrared.plugins.ansible.playbooks import playbook_runner as runner\n\n\tresult = runner.run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin)\n\tprint result\n\n\treturn result\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tplaybook_name = cli_args[1]\n\tplaybook_path = cli_args[2]\n\trunner = Runner(ir_plugin, ir_workspace)\n\trunner.load_vars_dict(vars_dict)\n\trunner.load_playbook(playbook_path, playbook_name)\n\trunner.run_playbook()\n\treturn runner.results\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom infrared.core.plugins.ansible import cli\n\tfrom infrared.core.plugins.ansible.cli import AnsibleCli\n\n\tansible_cli = AnsibleCli(ir_workspace=ir_workspace, plugin=ir_plugin)\n\tcli_args = ansible_cli.create_cli_args(cli_args)\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlMaster=auto -o ControlPersist=60s\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlPath=/tmp/ansible-ssh-%u\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ForwardAgent=yes\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlPersist=60s\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlPersist=10m\")\n\tcli_args.append(\"-e\")\n\tcli_", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible.runner\n\timport ansible.inventory\n\timport ansible.playbook\n\timport ansible.runner.task_include_filter\n\timport ansible.runner.task_queue_manager\n\n\tansible_inventory = ansible.inventory.Inventory(\n\t\tir_workspace.inventory_file)\n\n\trunner = ansible.runner.Runner(\n\t\tmodule_name='ping',\n\t\tremote_user=ir_workspace.user,\n\t\tmodule_args=cli_args,\n\t\tansible_inventory=ansible_inventory,\n\t\tsudo=ir_workspace.sudo,\n\t\tcheck=True,\n\t\tdeprecation=None,\n\t\tverbosity=0,\n\t\textravars=vars_dict,\n\t\tremote_tmp=ir_workspace.tmp_dir,\n\t\tremote_port=ir_workspace.port,\n\t\tssh_common_args=ir_workspace.ssh_common_args,\n\t\tssh_extra_args=ir_workspace.ssh_extra_args,\n\t\tssh_private_key_file=ir_workspace.ssh_private_key_file,\n\t\tssh_extra_", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tplaybook = cli_args[0]\n\n\tvars_dict['ir_workspace'] = ir_workspace\n\tvars_dict['ir_plugin'] = ir_plugin\n\n\tansible_cfg = ir_workspace.get_infrared_cfg(ir_plugin)\n\tif ansible_cfg:\n\t\tvars_dict['ansible_config'] = ansible_cfg\n\n\tansible_playbook = ir_workspace.get_infrared_playbook(ir_plugin, playbook)\n\tif ansible_playbook:\n\t\tvars_dict['ansible_playbook'] = ansible_playbook\n\n\tvars_dict['ir_workspace_dir'] = ir_workspace.get_path()\n\tvars_dict['ir_plugin_dir'] = ir_plugin.get_path()\n\n\tresult = ir_workspace.run_ansible(vars_dict, cli_args)\n\tresult.update(vars_dict)\n\treturn result\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\ttry:\n\t\tresult = ir_plugin.run_playbook(cli_args, vars_dict, ir_workspace)\n\t\treturn result\n\texcept Exception as e:\n\t\tir_plugin.log.error(\"Failed to run playbook: %s\" % e)\n\t\traise\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tansible_cmd = [\n\t\tir_plugin.ansible_path,\n\t\t'-i', ir_workspace.server_id,\n\t\t'-b',\n\t\t'-e', '@{0}'.format(vars_dict),\n\t\t'all',\n\t]\n\tansible_cmd.extend(cli_args)\n\tprint('[Ansible] Command: {0}'.format(' '.join(ansible_cmd)))\n\ttry:\n\t\tansible_results = subprocess.check_output(ansible_cmd, stderr=subprocess.STDOUT)\n\t\treturn ansible_results\n\texcept subprocess.CalledProcessError as e:\n\t\tprint('[Ansible] Command Error: {0}'.format(e.output))\n\t\treturn None", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tlog.debug(\"Variables passed to the playbook are: \" + str(vars_dict))\n\tlog.debug(\"Running playbook with command: \" + str(cli_args))\n\tcli_args.extend(['--extra-vars', str(vars_dict)])\n\tcli_args.append(ir_workspace.get_playbook_file())\n\tcli_args.append('--connection=local')\n\tcli_args.append('--timeout=10')\n\tcli_args.append('--inventory=localhost')\n\tcli_args.append('--user=root')\n\n\tcli_args.append('--become-user=root')\n\tcli_args.append('--become-method=sudo')\n\tcli_args.append('--become-extra-args=--prompt=\"%s\"')\n\tcli_args.append('--check')\n\tcli_args.append('--syntax-check')\n\tcli_args.append('--list-tasks')\n\tcli_args.append('--list-hosts')\n\tcli_args.append('--list-tags')\n\tcli_args.append('--list-facts')\n\tcli_args.append('--verbose')\n\n\tlog.debug(\"Running inventory role: \" + str", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible\n\timport ansible.inventory\n\timport ansible.runner\n\n\ttry:\n\t\tansible_vars = {}\n\t\tfor k, v in vars_dict.iteritems():\n\t\t\tif v is not None and v != \"\":\n\t\t\t\tansible_vars[k] = v\n\t\tinventory = ansible.inventory.Inventory(ir_workspace.inventory_path)\n\t\trunner = ansible.runner.Runner(\n\t\t\tinventory=inventory,\n\t\t\tmodule_name=ir_plugin.module_name,\n\t\t\tmodule_args=ansible_vars,\n\t\t\tremote_user=ir_workspace.remote_user,\n\t\t\tremote_host=ir_workspace.remote_host,\n\t\t\tremote_port=ir_workspace.remote_port,\n\t\t\tremote_path=ir_workspace.remote_path,\n\t\t\tcheck=False,\n\t\t\tsudo=ir_workspace.remote_sudo,\n\t\t\tsudo_user=ir_workspace.remote_sudo_user,\n\t\t\tsudo_pass=ir_workspace.remote_sudo_pass,\n\t\t\tssh_common_args=ir_workspace.ssh_common_args,\n\t\t\t", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tir_logger.debug(\"running ansible cli\")\n\tcli_args.append('--extra-vars')\n\tcli_args.append(vars_dict)\n\tcli_args.append('--connection')\n\tcli_args.append(ir_plugin.connection_type)\n\tir_logger.debug(\"cli args: %s\" % cli_args)\n\tresult = ir_plugin.run_cli(cli_args)\n\treturn result\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom infrared.plugins.ansible.playbooks import playbook_runner as runner\n\n\tresult = runner.run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin)\n\tprint result\n\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tplaybook_name = cli_args[1]\n\tplaybook_path = cli_args[2]\n\trunner = Runner(ir_plugin, ir_workspace)\n\trunner.load_vars_dict(vars_dict)\n\trunner.load_playbook(playbook_path, playbook_name)\n\trunner.run_playbook()\n\treturn runner.results\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tfrom infrared.core.plugins.ansible import cli\n\tfrom infrared.core.plugins.ansible.cli import AnsibleCli\n\n\tansible_cli = AnsibleCli(ir_workspace=ir_workspace, plugin=ir_plugin)\n\tcli_args = ansible_cli.create_cli_args(cli_args)\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlMaster=auto -o ControlPersist=60s\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlPath=/tmp/ansible-ssh-%u\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ForwardAgent=yes\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlPersist=60s\")\n\tcli_args.append(\"-e\")\n\tcli_args.append(\"ansible_ssh_extra_args=-o ControlPersist=10m\")\n\tcli_args.append(\"-e\")\n\tcli_", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible.runner\n\timport ansible.inventory\n\timport ansible.playbook\n\timport ansible.runner.task_include_filter\n\timport ansible.runner.task_queue_manager\n\n\tansible_inventory = ansible.inventory.Inventory(\n\t\tir_workspace.inventory_file)\n\n\trunner = ansible.runner.Runner(\n\t\tmodule_name='ping',\n\t\tremote_user=ir_workspace.user,\n\t\tmodule_args=cli_args,\n\t\tansible_inventory=ansible_inventory,\n\t\tsudo=ir_workspace.sudo,\n\t\tcheck=True,\n\t\tdeprecation=None,\n\t\tverbosity=0,\n\t\textravars=vars_dict,\n\t\tremote_tmp=ir_workspace.tmp_dir,\n\t\tremote_port=ir_workspace.port,\n\t\tssh_common_args=ir_workspace.ssh_common_args,\n\t\tssh_extra_args=ir_workspace.ssh_extra_args,\n\t\tssh_private_key_file=ir_workspace.ssh_private_key_file,\n\t\tssh_extra_", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tplaybook = cli_args[0]\n\n\tvars_dict['ir_workspace'] = ir_workspace\n\tvars_dict['ir_plugin'] = ir_plugin\n\n\tansible_cfg = ir_workspace.get_infrared_cfg(ir_plugin)\n\tif ansible_cfg:\n\t\tvars_dict['ansible_config'] = ansible_cfg\n\n\tansible_playbook = ir_workspace.get_infrared_playbook(ir_plugin, playbook)\n\tif ansible_playbook:\n\t\tvars_dict['ansible_playbook'] = ansible_playbook\n\n\tvars_dict['ir_workspace_dir'] = ir_workspace.get_path()\n\tvars_dict['ir_plugin_dir'] = ir_plugin.get_path()\n\n\tresult = ir_workspace.run_ansible(vars_dict, cli_args)\n\tresult.update(vars_dict)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\ttry:\n\t\tresult = ir_plugin.run_playbook(cli_args, vars_dict, ir_workspace)\n\t\treturn result\n\texcept Exception as e:\n\t\tir_plugin.log.error(\"Failed to run playbook: %s\" % e)\n\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tansible_cmd = [\n\t\tir_plugin.ansible_path,\n\t\t'-i', ir_workspace.server_id,\n\t\t'-b',\n\t\t'-e', '@{0}'.format(vars_dict),\n\t\t'all',\n\t]\n\tansible_cmd.extend(cli_args)\n\tprint('[Ansible] Command: {0}'.format(' '.join(ansible_cmd)))\n\ttry:\n\t\tansible_results = subprocess.check_output(ansible_cmd, stderr=subprocess.STDOUT)\n\t\treturn ansible_results\n\texcept subprocess.CalledProcessError as e:\n\t\tprint('[Ansible] Command Error: {0}'.format(e.output))\n\t\treturn None", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tlog.debug(\"Variables passed to the playbook are: \" + str(vars_dict))\n\tlog.debug(\"Running playbook with command: \" + str(cli_args))\n\tcli_args.extend(['--extra-vars', str(vars_dict)])\n\tcli_args.append(ir_workspace.get_playbook_file())\n\tcli_args.append('--connection=local')\n\tcli_args.append('--timeout=10')\n\tcli_args.append('--inventory=localhost')\n\tcli_args.append('--user=root')\n\n\tcli_args.append('--become-user=root')\n\tcli_args.append('--become-method=sudo')\n\tcli_args.append('--become-extra-args=--prompt=\"%s\"')\n\tcli_args.append('--check')\n\tcli_args.append('--syntax-check')\n\tcli_args.append('--list-tasks')\n\tcli_args.append('--list-hosts')\n\tcli_args.append('--list-tags')\n\tcli_args.append('--list-facts')\n\tcli_args.append('--verbose')\n\n\tlog.debug(\"Running inventory role: \" + str", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\timport ansible\n\timport ansible.inventory\n\timport ansible.runner\n\n\ttry:\n\t\tansible_vars = {}\n\t\tfor k, v in vars_dict.iteritems():\n\t\t\tif v is not None and v != \"\":\n\t\t\t\tansible_vars[k] = v\n\t\tinventory = ansible.inventory.Inventory(ir_workspace.inventory_path)\n\t\trunner = ansible.runner.Runner(\n\t\t\tinventory=inventory,\n\t\t\tmodule_name=ir_plugin.module_name,\n\t\t\tmodule_args=ansible_vars,\n\t\t\tremote_user=ir_workspace.remote_user,\n\t\t\tremote_host=ir_workspace.remote_host,\n\t\t\tremote_port=ir_workspace.remote_port,\n\t\t\tremote_path=ir_workspace.remote_path,\n\t\t\tcheck=False,\n\t\t\tsudo=ir_workspace.remote_sudo,\n\t\t\tsudo_user=ir_workspace.remote_sudo_user,\n\t\t\tsudo_pass=ir_workspace.remote_sudo_pass,\n\t\t\tssh_common_args=ir_workspace.ssh_common_args,\n\t\t\t", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t\"\"\"\n\tRuns ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results\n\t\"\"\"\n\tir_logger.debug(\"running ansible cli\")\n\tcli_args.append('--extra-vars')\n\tcli_args.append(vars_dict)\n\tcli_args.append('--connection')\n\tcli_args.append(ir_plugin.connection_type)\n\tir_logger.debug(\"cli args: %s\" % cli_args)\n\tresult = ir_plugin.run_cli(cli_args)\n\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tnon_cli_args = self.get_non_cli_args()\n\tfor key, value in values_dict.items():\n\t\tif key in non_cli_args:\n\t\t\tvalues_dict[key] = type(value)(value)\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\targuments = self.parser.parse_args(values_dict)\n\targuments = vars(arguments)\n\n\tif arguments:\n\t\tfor name, value in arguments.items():\n\t\t\tif not value:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tvalues_dict[name] = type(value)(value)\n\t\t\texcept (ValueError, TypeError):\n\t\t\t\traise CommandError(\n\t\t\t\t\t\"Error converting %s argument '%s' to type %s\" %\n\t\t\t\t\t(parser_name, name, str(type(value))))\n\n#src/libvirt_cli/__init__.py\n\"\"\"\nlibvirt_cli\n~~~~~~~~~~\n\nA pure python command line interface for libvirt\n\"\"\"\n\nfrom .command_base import Command\nfrom .command_base import CommandError\nfrom .command_base import CommandLine\n\n__version__ = '0.1.0'\n__author__ = 'Alexander Pyatkin <alexander@pyatkin.com>'\n__all__ = ['Command', 'CommandError', 'CommandLine']\n\n#tests/test_command_base.py\nimport pytest\n\nfrom libvirt_cli import Command\nfrom libvirt_cli import CommandError\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tsupported_types = {\n\t\t'boolean': bool,\n\t\t'int': int,\n\t\t'float': float,\n\t\t'string': str,\n\t}\n\n\tfor key, value in values_dict.items():\n\t\ttype_name = supported_types.get(key, str)\n\t\ttry:\n\t\t\tvalues_dict[key] = type_name(value)\n\t\texcept ValueError:\n\t\t\tvalues_dict[key] = None\n\n#src/ospd/__main__.py\nimport sys\nimport argparse\nimport ospd\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser = self.get_parser(parser_name)\n\tfor k, v in values_dict.items():\n\t\tif not v:\n\t\t\tcontinue\n\t\tif k in parser._actions:\n\t\t\taction = parser._actions[k]\n\t\t\tif action.default is not None:\n\t\t\t\tvalues_dict[k] = action.default\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tvalues_dict[k] = action.type(v)\n\t\t\texcept ValueError as e:\n\t\t\t\traise CommandError('Could not convert argument {}: {}'.format(k, e))\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\tif parser_name == \"main\":\n\t\tif \"--help\" in values_dict:\n\t\t\toptions.print_help()\n\t\t\tsys.exit(0)\n\n\t\telif \"--version\" in values_dict:\n\t\t\tprint(version)\n\t\t\tsys.exit(0)\n\n\telif parser_name in [\"virsh\", \"ospd\"]:\n\t\tif \"--help\" in values_dict:\n\t\t\toptions.print_help()\n\t\t\tsys.exit(0)\n\t\telif \"--version\" in values_dict:\n\t\t\tprint(version)\n\t\t\tsys.exit(0)\n\n\telse:\n\t\tvalues_dict[\"--help\"] = False\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\targs = values_dict.get('args', [])\n\targs_dict = {}\n\targs_dict.update(values_dict)\n\tvalues_dict = args_dict\n\n\tfor arg in args:\n\t\tvalue = values_dict[arg]\n\t\tif value:\n\t\t\ttry:\n\t\t\t\tvalues_dict[arg] = int(value)\n\t\t\texcept Exception:\n\t\t\t\tpass\n\n\nCommand.init_subcommand = _init_subcommand\nCommand.init_subparser = _init_subparser\nCommand.init_subcommand_parser = _init_subcommand_parser\nCommand.convert_non_cli_args = _convert_non_cli_args", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tself.arguments_converted = True\n\tself.logger.debug(\"Converting non-cli arguments\")\n\tself.check_parser_name(parser_name)\n\tself.logger.debug(\"Converting arguments for %s\", parser_name)\n\tfor key, value in values_dict.items():\n\t\tself.logger.debug(\"Converting %s = %s\", key, value)\n\t\tself.convert_argument(key, value)\n\n\tself.logger.debug(\"Done converting for %s\", parser_name)\n\n\tself.logger.debug(\"Converting arguments for %s\", parser_name)\n\tfor key, value in values_dict.items():\n\t\tself.logger.debug(\"Converting %s = %s\", key, value)\n\t\tself.convert_argument(key, value)\n\n\tself.logger.debug(\"Done converting for %s\", parser_name)\n\n\tif parser_name == \"osmgr\":\n\t\tself.convert_argument(\"--osmgr-log-level\", \"ERROR\")\n\t\tself.convert_argument(\"--osmgr-log-file\", \"osmgr.log\")\n\t\tself.convert_argument(\"--osmgr-data-dir\", \".\")\n\t\tself.convert_argument(\"--osmgr-pid-", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\ttry:\n\t\tvalues_dict['--version'] = str(values_dict['--version'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--debug'] = str(values_dict['--debug'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--debug-console'] = str(values_dict['--debug-console'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-file'] = str(values_dict['--log-file'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-level'] = str(values_dict['--log-level'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-file'] = str(values_dict['--log-file'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-level']", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(parser_name)\n\tfor key, value in values_dict.items():\n\t\tif value is not None and not isinstance(value, bool):\n\t\t\ttry:\n\t\t\t\tvalues_dict[key] = int(value)\n\t\t\texcept ValueError:\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[key] = float(value)\n\t\t\t\texcept ValueError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tvalues_dict[key] = bool(value)\n\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\tvalues_dict[key] = value\n\n#python/lib/cli/cli_lib.py"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tnon_cli_args = self.get_non_cli_args()\n\tfor key, value in values_dict.items():\n\t\tif key in non_cli_args:\n\t\t\tvalues_dict[key] = type(value)(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\targuments = self.parser.parse_args(values_dict)\n\targuments = vars(arguments)\n\n\tif arguments:\n\t\tfor name, value in arguments.items():\n\t\t\tif not value:\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tvalues_dict[name] = type(value)(value)\n\t\t\texcept (ValueError, TypeError):\n\t\t\t\traise CommandError(\n\t\t\t\t\t\"Error converting %s argument '%s' to type %s\" %\n\t\t\t\t\t(parser_name, name, str(type(value))))\n\n#src/libvirt_cli/__init__.py\n\"\"\"\nlibvirt_cli\n~~~~~~~~~~\n\nA pure python command line interface for libvirt\n\"\"\"\n\nfrom .command_base import Command\nfrom .command_base import CommandError\nfrom .command_base import CommandLine\n\n__version__ = '0.1.0'\n__author__ = 'Alexander Pyatkin <alexander@pyatkin.com>'\n__all__ = ['Command', 'CommandError', 'CommandLine']\n\n#tests/test_command_base.py\nimport pytest\n\nfrom libvirt_cli import Command\nfrom libvirt_cli import CommandError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tsupported_types = {\n\t\t'boolean': bool,\n\t\t'int': int,\n\t\t'float': float,\n\t\t'string': str,\n\t}\n\n\tfor key, value in values_dict.items():\n\t\ttype_name = supported_types.get(key, str)\n\t\ttry:\n\t\t\tvalues_dict[key] = type_name(value)\n\t\texcept ValueError:\n\t\t\tvalues_dict[key] = None\n\n#src/ospd/__main__.py\nimport sys\nimport argparse\nimport ospd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tparser = self.get_parser(parser_name)\n\tfor k, v in values_dict.items():\n\t\tif not v:\n\t\t\tcontinue\n\t\tif k in parser._actions:\n\t\t\taction = parser._actions[k]\n\t\t\tif action.default is not None:\n\t\t\t\tvalues_dict[k] = action.default\n\t\t\t\tcontinue\n\t\t\ttry:\n\t\t\t\tvalues_dict[k] = action.type(v)\n\t\t\texcept ValueError as e:\n\t\t\t\traise CommandError('Could not convert argument {}: {}'.format(k, e))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\t\n\tif parser_name == \"main\":\n\t\tif \"--help\" in values_dict:\n\t\t\toptions.print_help()\n\t\t\tsys.exit(0)\n\n\t\telif \"--version\" in values_dict:\n\t\t\tprint(version)\n\t\t\tsys.exit(0)\n\n\telif parser_name in [\"virsh\", \"ospd\"]:\n\t\tif \"--help\" in values_dict:\n\t\t\toptions.print_help()\n\t\t\tsys.exit(0)\n\t\telif \"--version\" in values_dict:\n\t\t\tprint(version)\n\t\t\tsys.exit(0)\n\n\telse:\n\t\tvalues_dict[\"--help\"] = False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\targs = values_dict.get('args', [])\n\targs_dict = {}\n\targs_dict.update(values_dict)\n\tvalues_dict = args_dict\n\n\tfor arg in args:\n\t\tvalue = values_dict[arg]\n\t\tif value:\n\t\t\ttry:\n\t\t\t\tvalues_dict[arg] = int(value)\n\t\t\texcept Exception:\n\t\t\t\tpass\n\n\nCommand.init_subcommand = _init_subcommand\nCommand.init_subparser = _init_subparser\nCommand.init_subcommand_parser = _init_subcommand_parser\nCommand.convert_non_cli_args = _convert_non_cli_args", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\tself.arguments_converted = True\n\tself.logger.debug(\"Converting non-cli arguments\")\n\tself.check_parser_name(parser_name)\n\tself.logger.debug(\"Converting arguments for %s\", parser_name)\n\tfor key, value in values_dict.items():\n\t\tself.logger.debug(\"Converting %s = %s\", key, value)\n\t\tself.convert_argument(key, value)\n\n\tself.logger.debug(\"Done converting for %s\", parser_name)\n\n\tself.logger.debug(\"Converting arguments for %s\", parser_name)\n\tfor key, value in values_dict.items():\n\t\tself.logger.debug(\"Converting %s = %s\", key, value)\n\t\tself.convert_argument(key, value)\n\n\tself.logger.debug(\"Done converting for %s\", parser_name)\n\n\tif parser_name == \"osmgr\":\n\t\tself.convert_argument(\"--osmgr-log-level\", \"ERROR\")\n\t\tself.convert_argument(\"--osmgr-log-file\", \"osmgr.log\")\n\t\tself.convert_argument(\"--osmgr-data-dir\", \".\")\n\t\tself.convert_argument(\"--osmgr-pid-", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\ttry:\n\t\tvalues_dict['--version'] = str(values_dict['--version'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--debug'] = str(values_dict['--debug'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--debug-console'] = str(values_dict['--debug-console'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-file'] = str(values_dict['--log-file'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-level'] = str(values_dict['--log-level'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-file'] = str(values_dict['--log-file'])\n\texcept KeyError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\n\ttry:\n\t\tvalues_dict['--log-level']", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t\"\"\"\n\tCasts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments\n\t\"\"\"\n\timport argparse\n\tparser = argparse.ArgumentParser(parser_name)\n\tfor key, value in values_dict.items():\n\t\tif value is not None and not isinstance(value, bool):\n\t\t\ttry:\n\t\t\t\tvalues_dict[key] = int(value)\n\t\t\texcept ValueError:\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[key] = float(value)\n\t\t\t\texcept ValueError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tvalues_dict[key] = bool(value)\n\t\t\t\t\texcept ValueError:\n\t\t\t\t\t\tvalues_dict[key] = value\n\n#python/lib/cli/cli_lib.py", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tfrom os import listdir\n\tfrom os.path import isfile, join\n\tfrom yaml import load\n\tfrom collections import defaultdict\n\tfrom itertools import chain\n\n\tspec_flatten_dict = defaultdict(list)\n\n\tspec_flatten_dict['name'] = 'plugin'\n\n\tfiles = [f for f in listdir(plugin_dir) if isfile(join(plugin_dir, f)) and f.endswith('.yaml')]\n\n\tfor f in files:\n\t\twith open(join(plugin_dir, f)) as fd:\n\t\t\tplugin_spec = load(fd)\n\t\t\tspec_flatten_dict['plugins'].append(plugin_spec)\n\n\tspec_flatten_dict['plugins'] = list(chain.from_iterable(plugin_spec['plugins'] for plugin_spec in spec_flatten_dict['plugins']))\n\n\treturn spec_flatten_dict\n\n#src/plugins/plugin_registry.py\nfrom .plugin_spec_flatten_dict import get_plugin_spec_flatten_dict\nfrom .plugin_manager import get_plugin_manager\nfrom .plugin_manager_factory import get_plugin_manager_factory\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport os\n\timport yaml\n\n\twith open(os.path.join(plugin_dir, 'plugin.spec')) as f:\n\t\tplugin_spec = yaml.load(f)\n\n\tplugin_flatten = {}\n\n\tif plugin_spec['name']:\n\t\tplugin_flatten['name'] = plugin_spec['name']\n\tif plugin_spec['description']:\n\t\tplugin_flatten['description'] = plugin_spec['description']\n\tif plugin_spec['entry_points']:\n\t\tplugin_flatten['entry_points'] = plugin_spec['entry_points']\n\tif plugin_spec['version']:\n\t\tplugin_flatten['version'] = plugin_spec['version']\n\tif plugin_spec['author']:\n\t\tplugin_flatten['author'] = plugin_spec['author']\n\tif plugin_spec['author_email']:\n\t\tplugin_flatten['author_email'] = plugin_spec['author_email']\n\tif plugin_spec['license']:\n\t\tplugin_flatten['license'] = plugin_spec['license']\n\tif plugin_spec['url']:\n\t\tplugin_flatten['url'] = plugin_spec['url']\n\tif plugin_spec['download_url']:\n\t\tplugin_flatten['download_url'] = plugin", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec = {}\n\twith open(os.path.join(plugin_dir, 'plugin.yaml')) as f:\n\t\tspec = yaml.load(f)\n\tif 'version' not in spec:\n\t\traise ValueError('Missing version in plugin.yaml')\n\treturn spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_dir_spec = {}\n\ttry:\n\t\twith open(os.path.join(plugin_dir, 'plugin_spec.yml')) as f:\n\t\t\tplugin_dir_spec = yaml.load(f)\n\texcept (FileNotFoundError, yaml.YAMLError) as e:\n\t\tlogging.warning(\"plugin_spec.yml not found in %s\", plugin_dir)\n\n\ttry:\n\t\twith open(os.path.join(plugin_dir, 'plugin_spec.yaml')) as f:\n\t\t\tplugin_dir_spec = yaml.load(f)\n\texcept (FileNotFoundError, yaml.YAMLError) as e:\n\t\tlogging.warning(\"plugin_spec.yaml not found in %s\", plugin_dir)\n\n\treturn plugin_dir_spec\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_dict = {}\n\tplugin_file = os.path.join(plugin_dir, 'plugin.yaml')\n\tif os.path.exists(plugin_file):\n\t\twith open(plugin_file, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tplugin_dict = yaml.safe_load(stream)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\treturn plugin_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\twith open(os.path.join(plugin_dir, \"plugin.yaml\")) as f:\n\t\tplugin_spec = yaml.load(f)\n\tplugin_spec = plugin_spec[\"plugin\"]\n\tplugin_spec[\"plugin_dir\"] = plugin_dir\n\treturn plugin_spec", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tyaml_file = plugin_dir + \"/spec.yaml\"\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.safe_load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tprint(exc)\n\treturn data\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\tplugin_spec_path = plugin_dir + \"/plugin_spec.yaml\"\n\twith open(plugin_spec_path, 'r') as f:\n\t\tplugin_spec = yaml.safe_load(f)\n\treturn plugin_spec\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\tspec_path = os.path.join(plugin_dir, 'spec.yml')\n\ttry:\n\t\twith open(spec_path, 'r') as spec_file:\n\t\t\tspec = yaml.safe_load(spec_file)\n\t\t\treturn spec\n\texcept IOError as e:\n\t\traise Exception(\"Spec file not found in plugin directory: %s\" % (spec_path))\n\texcept yaml.YAMLError as e:\n\t\traise Exception(\"Spec file not valid in plugin directory: %s\" % (spec_path))\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_file = os.path.join(plugin_dir, \"spec.yaml\")\n\twith open(spec_file, 'r') as stream:\n\t\ttry:\n\t\t\tspec = yaml.safe_load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tlog.error(\"Failed to read spec file. Error: %s\" % exc)\n\t\t\treturn None\n\treturn spec\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tfrom os import listdir\n\tfrom os.path import isfile, join\n\tfrom yaml import load\n\tfrom collections import defaultdict\n\tfrom itertools import chain\n\n\tspec_flatten_dict = defaultdict(list)\n\n\tspec_flatten_dict['name'] = 'plugin'\n\n\tfiles = [f for f in listdir(plugin_dir) if isfile(join(plugin_dir, f)) and f.endswith('.yaml')]\n\n\tfor f in files:\n\t\twith open(join(plugin_dir, f)) as fd:\n\t\t\tplugin_spec = load(fd)\n\t\t\tspec_flatten_dict['plugins'].append(plugin_spec)\n\n\tspec_flatten_dict['plugins'] = list(chain.from_iterable(plugin_spec['plugins'] for plugin_spec in spec_flatten_dict['plugins']))\n\n\treturn spec_flatten_dict\n\n#src/plugins/plugin_registry.py\nfrom .plugin_spec_flatten_dict import get_plugin_spec_flatten_dict\nfrom .plugin_manager import get_plugin_manager\nfrom .plugin_manager_factory import get_plugin_manager_factory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport os\n\timport yaml\n\n\twith open(os.path.join(plugin_dir, 'plugin.spec')) as f:\n\t\tplugin_spec = yaml.load(f)\n\n\tplugin_flatten = {}\n\n\tif plugin_spec['name']:\n\t\tplugin_flatten['name'] = plugin_spec['name']\n\tif plugin_spec['description']:\n\t\tplugin_flatten['description'] = plugin_spec['description']\n\tif plugin_spec['entry_points']:\n\t\tplugin_flatten['entry_points'] = plugin_spec['entry_points']\n\tif plugin_spec['version']:\n\t\tplugin_flatten['version'] = plugin_spec['version']\n\tif plugin_spec['author']:\n\t\tplugin_flatten['author'] = plugin_spec['author']\n\tif plugin_spec['author_email']:\n\t\tplugin_flatten['author_email'] = plugin_spec['author_email']\n\tif plugin_spec['license']:\n\t\tplugin_flatten['license'] = plugin_spec['license']\n\tif plugin_spec['url']:\n\t\tplugin_flatten['url'] = plugin_spec['url']\n\tif plugin_spec['download_url']:\n\t\tplugin_flatten['download_url'] = plugin", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec = {}\n\twith open(os.path.join(plugin_dir, 'plugin.yaml')) as f:\n\t\tspec = yaml.load(f)\n\tif 'version' not in spec:\n\t\traise ValueError('Missing version in plugin.yaml')\n\treturn spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_dir_spec = {}\n\ttry:\n\t\twith open(os.path.join(plugin_dir, 'plugin_spec.yml')) as f:\n\t\t\tplugin_dir_spec = yaml.load(f)\n\texcept (FileNotFoundError, yaml.YAMLError) as e:\n\t\tlogging.warning(\"plugin_spec.yml not found in %s\", plugin_dir)\n\n\ttry:\n\t\twith open(os.path.join(plugin_dir, 'plugin_spec.yaml')) as f:\n\t\t\tplugin_dir_spec = yaml.load(f)\n\texcept (FileNotFoundError, yaml.YAMLError) as e:\n\t\tlogging.warning(\"plugin_spec.yaml not found in %s\", plugin_dir)\n\n\treturn plugin_dir_spec\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tplugin_dict = {}\n\tplugin_file = os.path.join(plugin_dir, 'plugin.yaml')\n\tif os.path.exists(plugin_file):\n\t\twith open(plugin_file, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tplugin_dict = yaml.safe_load(stream)\n\t\t\texcept yaml.YAMLError as exc:\n\t\t\t\tprint(exc)\n\treturn plugin_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\twith open(os.path.join(plugin_dir, \"plugin.yaml\")) as f:\n\t\tplugin_spec = yaml.load(f)\n\tplugin_spec = plugin_spec[\"plugin\"]\n\tplugin_spec[\"plugin_dir\"] = plugin_dir\n\treturn plugin_spec", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tyaml_file = plugin_dir + \"/spec.yaml\"\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.safe_load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tprint(exc)\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\tplugin_spec_path = plugin_dir + \"/plugin_spec.yaml\"\n\twith open(plugin_spec_path, 'r') as f:\n\t\tplugin_spec = yaml.safe_load(f)\n\treturn plugin_spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\timport yaml\n\tspec_path = os.path.join(plugin_dir, 'spec.yml')\n\ttry:\n\t\twith open(spec_path, 'r') as spec_file:\n\t\t\tspec = yaml.safe_load(spec_file)\n\t\t\treturn spec\n\texcept IOError as e:\n\t\traise Exception(\"Spec file not found in plugin directory: %s\" % (spec_path))\n\texcept yaml.YAMLError as e:\n\t\traise Exception(\"Spec file not valid in plugin directory: %s\" % (spec_path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\t\"\"\"\n\tUse YAML to read various information in plugin_dir and return the information in dictionary form.\n\t\"\"\"\n\tspec_file = os.path.join(plugin_dir, \"spec.yaml\")\n\twith open(spec_file, 'r') as stream:\n\t\ttry:\n\t\t\tspec = yaml.safe_load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tlog.error(\"Failed to read spec file. Error: %s\" % exc)\n\t\t\treturn None\n\treturn spec\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\t# if the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path\n\tif not os.environ.get('ANSIBLE_CONFIG'):\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tfrom os import environ\n\n\tif 'ANSIBLE_CONFIG' not in environ:\n\t\tenviron['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n#src/ansible_collections/community/azure/plugins/modules/azure_rm_resource_group.py\nfrom ansible.module_utils.basic import AnsibleModule\n\nfrom ansible_collections.community.azure.plugins.utils.common import (\n    get_azure_client,\n    get_azure_resource_group,\n)\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tansible_config = os.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n# setup.py\nfrom setuptools import setup, find_packages\nimport os\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tself.ansible_config_path = os.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tself.ansible_config_path = self.config_path\n\n#src/pytest_ansible/inventory.py\nimport os\n\nfrom pytest_ansible.inventory_loader import InventoryLoader\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\tansible_config_path = self.ansible_config_path\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = ansible_config_path\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config_path = None\n\tself.ansible_config = os.getenv('ANSIBLE_CONFIG')\n\tif self.ansible_config is None:\n\t\tself.ansible_config_path = os.environ.get('ANSIBLE_CONFIG')\n\t\tif self.ansible_config_path is not None:\n\t\t\tself.ansible_config = self.ansible_config_path\n\n#src/ansible_runner/__init__.py\nfrom ansible_runner.ansible_runner import AnsibleRunner\n\n__all__ = ('AnsibleRunner',)\n\n#src/ansible_runner/ansible_runner.py\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport json\nimport subprocess\n\nfrom ansible_runner.base_runner import BaseRunner\nfrom ansible_runner.errors import AnsibleRunnerError\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config_path = os.environ.get('ANSIBLE_CONFIG') or os.path.expanduser('~/.ansible.cfg')\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tself.ansible_config_path = os.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tself.ansible_config_path = self.ansible_config_path\n\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tif not os.environ['ANSIBLE_CONFIG']:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\texcept KeyError:\n\t\tpass\n", "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config_path = os.getenv('ANSIBLE_CONFIG', '')\n\tif not self.ansible_config_path:\n\t\traise Exception(\"No Ansible config found\")\n\tself.set_config_path(self.ansible_config_path)\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\t# if the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path\n\tif not os.environ.get('ANSIBLE_CONFIG'):\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tfrom os import environ\n\n\tif 'ANSIBLE_CONFIG' not in environ:\n\t\tenviron['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n#src/ansible_collections/community/azure/plugins/modules/azure_rm_resource_group.py\nfrom ansible.module_utils.basic import AnsibleModule\n\nfrom ansible_collections.community.azure.plugins.utils.common import (\n    get_azure_client,\n    get_azure_resource_group,\n)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tansible_config = os.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n# setup.py\nfrom setuptools import setup, find_packages\nimport os\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tself.ansible_config_path = os.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tself.ansible_config_path = self.config_path\n\n#src/pytest_ansible/inventory.py\nimport os\n\nfrom pytest_ansible.inventory_loader import InventoryLoader\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\t\n\tansible_config_path = self.ansible_config_path\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = ansible_config_path\n", "is_pass": true, "return_code": 0}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config_path = None\n\tself.ansible_config = os.getenv('ANSIBLE_CONFIG')\n\tif self.ansible_config is None:\n\t\tself.ansible_config_path = os.environ.get('ANSIBLE_CONFIG')\n\t\tif self.ansible_config_path is not None:\n\t\t\tself.ansible_config = self.ansible_config_path\n\n#src/ansible_runner/__init__.py\nfrom ansible_runner.ansible_runner import AnsibleRunner\n\n__all__ = ('AnsibleRunner',)\n\n#src/ansible_runner/ansible_runner.py\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport json\nimport subprocess\n\nfrom ansible_runner.base_runner import BaseRunner\nfrom ansible_runner.errors import AnsibleRunnerError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config_path = os.environ.get('ANSIBLE_CONFIG') or os.path.expanduser('~/.ansible.cfg')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tself.ansible_config_path = os.environ['ANSIBLE_CONFIG']\n\texcept KeyError:\n\t\tself.ansible_config_path = self.ansible_config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\ttry:\n\t\tif not os.environ['ANSIBLE_CONFIG']:\n\t\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\texcept KeyError:\n\t\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\t\"\"\"\n\tIf the ANSIBLE_CONFIG property does not exist in os.environ, set it to self.ansible_config_path.\n\t\"\"\"\n\tself.ansible_config_path = os.getenv('ANSIBLE_CONFIG', '')\n\tif not self.ansible_config_path:\n\t\traise Exception(\"No Ansible config found\")\n\tself.set_config_path(self.ansible_config_path)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp='Generate spec files for the application.',\n\t)\n\tsubparser.set_defaults(func=self.spec)\n\n\tsubparser = subparser.add_subparsers()\n\tsubparser.add_parser('validate', help='Validate the spec file.')\n\tsubparser.add_parser('pretty', help='Pretty print the spec file.')", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_subparser = root_subparsers.add_parser('spec', help='Specify a spec file')\n\tspec_subparser.add_argument('spec_file', help='The spec file to use')\n\tspec_subparser.set_defaults(func=self.spec_file)", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = root_subparsers.add_parser(\"spec\",\n\t\tdescription=\"Manage spec files\",\n\t\thelp=\"Manage spec files\")\n\tsubparsers.add_argument(\n\t\t\"--spec-file\",\n\t\thelp=\"The spec file to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-version\",\n\t\thelp=\"The spec version to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-format\",\n\t\thelp=\"The spec format to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-name\",\n\t\thelp=\"The spec name to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-version\",\n\t\thelp=\"The spec version to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-url\",\n\t\thelp=\"The spec url to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-version\",\n\t\thelp=\"The spec version to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-tag\",\n\t\thelp=\"The spec tag to use\")\n\tsubparsers.add_argument(\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='spec commands.')\n\tsubparser.set_defaults(command='spec')\n\n\tspec_subparsers = subparser.add_subparsers(help='subcommands')\n\n\tspec_subparsers.add_parser('create', help='create a spec.')", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.add_argument(\"-N\", \"--num_steps\", type=int, default=1,\n\t\t\t\t\t  help=\"The number of steps to take.\")\n\tself.add_argument(\"-T\", \"--temperature\", type=float, default=1.0,\n\t\t\t\t\t  help=\"The temperature for the sampling.\")\n\tself.add_argument(\"-l\", \"--log_interval\", type=int, default=100,\n\t\t\t\t\t  help=\"The interval of logging.\")\n\tself.add_argument(\"-p\", \"--print_interval\", type=int, default=100,\n\t\t\t\t\t  help=\"The interval of logging.\")\n\tself.add_argument(\"-s\", \"--seed\", type=int, default=0,\n\t\t\t\t\t  help=\"The random seed.\")\n\tself.add_argument(\"-i\", \"--iterations\", type=int, default=100000,\n\t\t\t\t\t  help=\"The number of iterations to run the algorithm.\")\n\tself.add_argument(\"-d\", \"--debug\", type=int, default=0,\n\t\t\t\t\t  help=\"The debug level.\")\n\tself.add_argument(\"-b\", \"--burnin\", type=int, default=0,\n\t\t\t\t\t  help=\"The burnin iterations.\")\n\tself.add_", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='get the spec for a given service')\n\tsubparser.add_argument('service', help='the service to get the spec for')\n\tsubparser.set_defaults(func=self.get_spec)\n\n#python/pyservice/cli/__init__.py\nfrom .spec import *\nfrom .publish import *\nfrom .subscribe import *\n\n#python/pyservice/cli/cli.py\nimport argparse\n\nfrom .spec import extend_cli\nfrom .publish import extend_cli as extend_publish_cli\nfrom .subscribe import extend_cli as extend_subscribe_cli\n", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', description='Specify a spec file to use for the tests.')\n\tsubparser.add_argument('--spec', type=str, help='Specify a spec file to use for the tests.')\n\tsubparser.add_argument('--no-spec', action='store_true', help='Do not use a spec file.')\n\tsubparser.add_argument('--load-from-file', type=str, help='Load spec from file.')\n\tsubparser.add_argument('--skip-tests', action='store_true', help='Skip all tests.')\n\tsubparser.add_argument('--skip-test-files', action='store_true', help='Skip all test files.')\n\tsubparser.add_argument('--rerun', action='store_true', help='Rerun all tests.')\n\tsubparser.add_argument('--rerun-test-files', action='store_true', help='Rerun all test files.')\n\tsubparser.add_argument('--rerun-only', type=str, help='Rerun the tests that match the given regex.')\n\tsubparser.add_argument('--rerun-skipped', action", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.subparser = root_subparsers.add_parser(\n\t\t'spec', help='manage spec',\n\t\tdescription='manage the spec',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\n\tself.subparser.add_argument(\n\t\t'--spec', help='spec to deploy',\n\t\tdefault='default',\n\t)", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser(\n\t\t\"spec\",\n\t\tdescription=\"Generate a spec file for a project.\",\n\t)\n\n\t# Add the common arguments.\n\tcommon_args = SpecCommonArgs(subparser, self)\n\n\t# Add the spec file arguments.\n\tspec_args = SpecArgs(subparser, self)\n\n\t# Add the file argument.\n\tspec_args.add_argument(\n\t\t\"--file\",\n\t\tnargs=\"+\",\n\t\thelp=\"The file to generate a spec for.\",\n\t)\n\n\tcommands = {}\n\n\t# Add the project command.\n\tcommands[\"project\"] = ProjectSpec(subparser, self)\n\n\t# Add the file command.\n\tcommands[\"file\"] = FileSpec(subparser, self)\n\n\t# Add the add command.\n\tcommands[\"add\"] = AddSpec(subparser, self)\n\n\t# Add the remove command.\n\tcommands[\"remove\"] = RemoveSpec(subparser, self)\n\n\t# Add the test command.\n\tcommands[\"test\"] = TestSpec(subparser, self)\n\n\t# Add the spec command.\n\tcommands[\"spec\"] = SpecSpec(subparser, self)\n\n\tsubparser.add_argument(\n\t\t\"--verbose", "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.add_argument('--api-version',\n\t\t\t\t\t  help='The api version to use.',\n\t\t\t\t\t  default='v1')\n\n\tself.add_argument('--api-server',\n\t\t\t\t\t  help='The api server to use.',\n\t\t\t\t\t  default='https://api.datastore.googleapis.com')\n\n\tself.add_argument('--project',\n\t\t\t\t\t  help='The project to use.',\n\t\t\t\t\t  default='my-project')\n\n\tself.add_argument('--region',\n\t\t\t\t\t  help='The region to use.')\n\n\tself.add_argument('--credentials-file',\n\t\t\t\t\t  help='The path to the credentials file.',\n\t\t\t\t\t  required=True)\n\n\tself.add_argument('--credentials-path',\n\t\t\t\t\t  help='The path to store the credentials.',\n\t\t\t\t\t  required=True)\n\n\tself.add_argument('--scopes',\n\t\t\t\t\t  help='The scopes to use.',\n\t\t\t\t\t  default='https://www.googleapis.com/auth/datastore')\n\n\tself.add_argument('--service-account-email',\n\t\t\t\t\t  help='The service account email to use.',\n\t\t\t\t\t  required=True)\n\n#datastore/__init"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp='Generate spec files for the application.',\n\t)\n\tsubparser.set_defaults(func=self.spec)\n\n\tsubparser = subparser.add_subparsers()\n\tsubparser.add_parser('validate', help='Validate the spec file.')\n\tsubparser.add_parser('pretty', help='Pretty print the spec file.')", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tspec_subparser = root_subparsers.add_parser('spec', help='Specify a spec file')\n\tspec_subparser.add_argument('spec_file', help='The spec file to use')\n\tspec_subparser.set_defaults(func=self.spec_file)", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparsers = root_subparsers.add_parser(\"spec\",\n\t\tdescription=\"Manage spec files\",\n\t\thelp=\"Manage spec files\")\n\tsubparsers.add_argument(\n\t\t\"--spec-file\",\n\t\thelp=\"The spec file to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-version\",\n\t\thelp=\"The spec version to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-format\",\n\t\thelp=\"The spec format to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-name\",\n\t\thelp=\"The spec name to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-version\",\n\t\thelp=\"The spec version to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-url\",\n\t\thelp=\"The spec url to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-version\",\n\t\thelp=\"The spec version to use\")\n\tsubparsers.add_argument(\n\t\t\"--spec-tag\",\n\t\thelp=\"The spec tag to use\")\n\tsubparsers.add_argument(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='spec commands.')\n\tsubparser.set_defaults(command='spec')\n\n\tspec_subparsers = subparser.add_subparsers(help='subcommands')\n\n\tspec_subparsers.add_parser('create', help='create a spec.')", "is_pass": true, "return_code": 0}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.add_argument(\"-N\", \"--num_steps\", type=int, default=1,\n\t\t\t\t\t  help=\"The number of steps to take.\")\n\tself.add_argument(\"-T\", \"--temperature\", type=float, default=1.0,\n\t\t\t\t\t  help=\"The temperature for the sampling.\")\n\tself.add_argument(\"-l\", \"--log_interval\", type=int, default=100,\n\t\t\t\t\t  help=\"The interval of logging.\")\n\tself.add_argument(\"-p\", \"--print_interval\", type=int, default=100,\n\t\t\t\t\t  help=\"The interval of logging.\")\n\tself.add_argument(\"-s\", \"--seed\", type=int, default=0,\n\t\t\t\t\t  help=\"The random seed.\")\n\tself.add_argument(\"-i\", \"--iterations\", type=int, default=100000,\n\t\t\t\t\t  help=\"The number of iterations to run the algorithm.\")\n\tself.add_argument(\"-d\", \"--debug\", type=int, default=0,\n\t\t\t\t\t  help=\"The debug level.\")\n\tself.add_argument(\"-b\", \"--burnin\", type=int, default=0,\n\t\t\t\t\t  help=\"The burnin iterations.\")\n\tself.add_", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', help='get the spec for a given service')\n\tsubparser.add_argument('service', help='the service to get the spec for')\n\tsubparser.set_defaults(func=self.get_spec)\n\n#python/pyservice/cli/__init__.py\nfrom .spec import *\nfrom .publish import *\nfrom .subscribe import *\n\n#python/pyservice/cli/cli.py\nimport argparse\n\nfrom .spec import extend_cli\nfrom .publish import extend_cli as extend_publish_cli\nfrom .subscribe import extend_cli as extend_subscribe_cli\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser('spec', description='Specify a spec file to use for the tests.')\n\tsubparser.add_argument('--spec', type=str, help='Specify a spec file to use for the tests.')\n\tsubparser.add_argument('--no-spec', action='store_true', help='Do not use a spec file.')\n\tsubparser.add_argument('--load-from-file', type=str, help='Load spec from file.')\n\tsubparser.add_argument('--skip-tests', action='store_true', help='Skip all tests.')\n\tsubparser.add_argument('--skip-test-files', action='store_true', help='Skip all test files.')\n\tsubparser.add_argument('--rerun', action='store_true', help='Rerun all tests.')\n\tsubparser.add_argument('--rerun-test-files', action='store_true', help='Rerun all test files.')\n\tsubparser.add_argument('--rerun-only', type=str, help='Rerun the tests that match the given regex.')\n\tsubparser.add_argument('--rerun-skipped', action", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.subparser = root_subparsers.add_parser(\n\t\t'spec', help='manage spec',\n\t\tdescription='manage the spec',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\n\tself.subparser.add_argument(\n\t\t'--spec', help='spec to deploy',\n\t\tdefault='default',\n\t)", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tsubparser = root_subparsers.add_parser(\n\t\t\"spec\",\n\t\tdescription=\"Generate a spec file for a project.\",\n\t)\n\n\t# Add the common arguments.\n\tcommon_args = SpecCommonArgs(subparser, self)\n\n\t# Add the spec file arguments.\n\tspec_args = SpecArgs(subparser, self)\n\n\t# Add the file argument.\n\tspec_args.add_argument(\n\t\t\"--file\",\n\t\tnargs=\"+\",\n\t\thelp=\"The file to generate a spec for.\",\n\t)\n\n\tcommands = {}\n\n\t# Add the project command.\n\tcommands[\"project\"] = ProjectSpec(subparser, self)\n\n\t# Add the file command.\n\tcommands[\"file\"] = FileSpec(subparser, self)\n\n\t# Add the add command.\n\tcommands[\"add\"] = AddSpec(subparser, self)\n\n\t# Add the remove command.\n\tcommands[\"remove\"] = RemoveSpec(subparser, self)\n\n\t# Add the test command.\n\tcommands[\"test\"] = TestSpec(subparser, self)\n\n\t# Add the spec command.\n\tcommands[\"spec\"] = SpecSpec(subparser, self)\n\n\tsubparser.add_argument(\n\t\t\"--verbose", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\t\"\"\"\n\tAdds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.\n\t\"\"\"\n\tself.add_argument('--api-version',\n\t\t\t\t\t  help='The api version to use.',\n\t\t\t\t\t  default='v1')\n\n\tself.add_argument('--api-server',\n\t\t\t\t\t  help='The api server to use.',\n\t\t\t\t\t  default='https://api.datastore.googleapis.com')\n\n\tself.add_argument('--project',\n\t\t\t\t\t  help='The project to use.',\n\t\t\t\t\t  default='my-project')\n\n\tself.add_argument('--region',\n\t\t\t\t\t  help='The region to use.')\n\n\tself.add_argument('--credentials-file',\n\t\t\t\t\t  help='The path to the credentials file.',\n\t\t\t\t\t  required=True)\n\n\tself.add_argument('--credentials-path',\n\t\t\t\t\t  help='The path to store the credentials.',\n\t\t\t\t\t  required=True)\n\n\tself.add_argument('--scopes',\n\t\t\t\t\t  help='The scopes to use.',\n\t\t\t\t\t  default='https://www.googleapis.com/auth/datastore')\n\n\tself.add_argument('--service-account-email',\n\t\t\t\t\t  help='The service account email to use.',\n\t\t\t\t\t  required=True)\n\n#datastore/__init", "is_pass": false, "return_code": 1}]}
