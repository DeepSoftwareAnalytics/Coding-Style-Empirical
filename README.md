# Beyond Functional Correctness: Examining Inconsistencies in Coding Styles of LLM-based Code Generation

## RQ1
- [Code samples generated by LLMs](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/tree/main/RQ1_temp/code_samples_generated_by_LLMs) For each of the 230 Python code generaion tasks from CoderEval, we prompt the five LLMs(CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, DeepSeekCoder-6.7B and GPT-4) to perform code generation using the same prompting template. For each task, we instruct each model to generate 10 results, resulting in an initial total of 2,300 code samples for each model. To ensure the correctness of the collected code samples, we further filter out code samples that fail to pass any of the associated unit tests for the task, leading to 456, 189, 365, 497 and 570 results that pass all tests for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, DeepSeekCoder-6.7B, and GPT-4, respectively.

- [1179 unique code samples](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/tree/main/RQ1_temp) We further merge identical code samples to reduce analysis effort, resulting in 1,557 unique samples. We only annotate 1,557 unique code samples to ensure that the annotation results for the same code sample generated by different models are consistent, thereby avoiding the situation where the same code sample generated by different LLMs is annotated with different results. We filter out results that exhibit no inconsistency in coding style. As a result, 73 code samples are filtered out in this way. We filter out results that implement the task incorrectly despite passing the unit tests. The functional correctness of the generated result is verified by comparing it to the ground truth and the task descriptions. Previous work has shown that existing benchmarks suffer from test sufficiency issues, meaning that even if a generated result passes all tests, there is still a chance it could be incorrect. As a result, 286 code samples are filtered out in this way. We filter out results that contain extra code that does not contribute to fulfilling the function’s implementation requirements (e.g., two exactly the same loops). As a result, 19 code samples are filtered out in this way. As a result, we obtain 1,179 unique code samples for the study. The 1179_unique_samples file has been split into five tables: (i) 1179_unique_samples_part1.xlsx, (ii) 1179_unique_samples_part2.xlsx,  (iii)1179_unique_samples_part3.xlsx, (iv) 1179_unique_samples_part4.xlsx, and (v) 1179_unique_samples_part5.xlsx.

## RQ2 
- [models_code_samples](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/tree/main/RQ2_temp)  Here are the code samples used in section 4.2.2-4.2.3 generated by five models. These code samples are functionally correct, without extra code, exhibiting differences with ground truths in code style. The code samples have not undergone deduplication.
  


## RQ3
- [Coding style comparison](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/tree/main/RQ3_temp) We compare the code samples generated by LLMs from RQ1 with their corresponding ground truths and evaluate each of the three aspects. For each analysis, we determine whether the generated code is better than the ground truth ("model better"), whether they are comparable ("tie"), or whether the ground truth is better ("human better"). The scoring for the three aspects follows these principles:

(i) **Readability**: If code sample A conveys the intent and logical structure more clearly than code sample B, it is superior in readability.

(ii) **Conciseness**: If code sample A achieves the same functionality as code sample B while eliminating unnecessary redundancies, it is considered superior in terms of conciseness. This involves reducing the number of intermediate variables, optimizing loops, etc.

(iii) **Robustness**: If code sample A effectively implements error handling and fault tolerance mechanisms, whereas code sample B lacks such mechanisms, the former is deemed superior in robustness. This includes the presence of input validation, exception handling, etc.

In the table, "**-1**" represents "**model better**", "**0**" represents "**tie**", and "**1**" represents "**human better**".

## RQ4
We conduct experiments with DeepSeekCoder-6.7B on 20 sampled Python tasks from CoderEval. We choose DeepSeekCoder-6.7B to conduct the experiment. These tasks are randomly selected from those that DeepSeekCoder-6.7B can complete, meaning DeepSeekCoder-6.7B can generate code samples that pass all corresponding test cases. We design four types of enhanced prompts for this study, aiming to instruct the model to generate code with better coding style using explicit style guidelines. The design of these prompts investigates the impact of the placement and detail level of style guidelines. In prompt names, “-head” or “-end” specifies whether the style guidelines are placed before the function signature and docstring, similar to a directive, or appended at the end of the original docstring, simulating a normal docstring style. “-concise” and “-detailed” indicate the level of detail in the style guidelines. The detailed version includes three specific principles related to code readability, conciseness, and robustness, in addition to the concise information. According to the scoring principles outlined in Section 4.3, we evaluated the code samples generated using the basic prompt and four enhanced prompts for readability, conciseness, and robustness. 

- [The scoring results of basic prompt](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/blob/main/RQ4_temp/scoring_results_basic_prompt.xlsx). The results from the two annotators mentioned in Section 4.3 were based on the scoring principles outlined in the same section, where they evaluated the code samples generated from **basic prompt** for readability, conciseness, and robustness.

- The scoring results of four enhanced prompts](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/blob/main/RQ4_temp/scoring_results_enhanced_prompts.xlsx) The results from the two annotators mentioned in Section 4.3 were based on the scoring principles outlined in the same section, where they evaluated the code samples generated from **four enhanced prompts** for readability, conciseness, and robustness. The results for Prompt-1, Prompt-2, Prompt-3, and Prompt-4 correspond to the scoring outcomes of Prompt-head-concise, Prompt-head-detailed, Prompt-end-concise, and Prompt-end-detailed, respectively.
![Prompt templates](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/blob/main/images/prompts-4.png)
