# Beyond Functional Correctness: Examining Inconsistencies in Coding Styles of LLM-based Code Generation

## Empirical Study
- [Code samples generated by CodeLLMs](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/tree/main/code_samples_generated_by_codeLLMs) For each of the 230 Python code generaion tasks from CoderEval, we prompt the four Code LLMs to perform code generation using the same prompting template. For each task, we instruct each model to generate 10 results, resulting in an initial total of 2,300 code samples for each model. To ensure the correctness of the collected code samples, we further filter out code samples that fail to pass any of the associated unit tests for the task, leading to 456, 189, 365, 497 results that pass all tests for CodeLlama-7B, StarCoder2-7B, DeepSeekCoder-1.3B, and DeepSeekCoder-6.7B, respectively.

- [820 unique code samples](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/blob/main/820-unique-samples.csv) We further merge identical code samples to reduce analysis effort, resulting in 1,159 unique samples. We only annotate 1159 unique code samples to ensure that the annotation results for the same code sample generated by different models are consistent, thereby avoiding the situation where the same code sample generated by different Code LLMs is annotated with different results. We filter out results that exhibit no inconsistency in coding style. As a result, 56 code samples are filtered out in this way. We filter out results that implement the task incorrectly despite passing the unit tests. The functional correctness of the generated result is verified by comparing it to the ground truth and the task descriptions. Previous work has shown that existing benchmarks suffer from test sufficiency issues, meaning that even if a generated result passes all tests, there is still a chance it could be incorrect. As a result, 264 code samples are filtered out in this way. We filter out results that contain extra code that does not contribute to fulfilling the function’s implementation requirements (e.g., two exactly the same loops). As a result, 19 code samples are filtered out in this way. As a result, we obtain 820 unique code samples for the study.
  
- [Coding style comparison](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/tree/main/coding_style_comparison) In addition to the analysis of coding style inconsistency between Code LLMs and human programmers, we further investigate
which coding style is better. To this end, we annotate the code generated by Code LLMs by comparing it with the ground truth from three aspects: readability, conciseness, and robustness. Based on the code samples generated by Code LLMs collected
in RQ1, we compare them with the ground truth and score each of the three aspects according to the following criteria: model better (generated code is better than ground truth), tie (generated code is comparable to ground truth), and human better (the ground truth
is better than the generated code). The annotation is conducted independently by two of the authors. Any conflicts are resolved through discussions to reach a consensus.
Readability: If code sample A, compared to code sample B, more clearly conveys the intent of the code and exhibits a clearer logical structure, it is considered superior in terms of readability. 
Conciseness: If code sample A accomplishes the same functionality as code sample B with fewer lines of code, it is considered superior in conciseness. This involves reducing the number of intermediate variables while maintaining clarity, etc.
Robustness: If code sample A effectively implements error handling and fault tolerance mechanisms, whereas code sample B lacks such mechanisms, the former is deemed superior in robustness. This includes the presence of input validation, exception handling, etc.


## Style Improvement by Prompting Techniques
-[Prompting Techniques](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/tree/main/prompts). We conduct experiments with DeepSeekCoder-6.7B on 20 sampled Python tasks from CoderEval. We choose DeepSeekCoder-6.7B to conduct the experiment with type a because it achieves the best functional correctness in generating functions among the four models.These tasks are randomly selected from those that DeepSeekCoder-6.7B can complete, meaning DeepSeekCoder-6.7B can generate code samples that pass all corresponding test cases. We design four types of enhanced prompts for this study, aiming to instruct the model to generate code with better coding style using explicit style guidelines. The design of these prompts investigates the impact of the placement and detail level of style guidelines. In prompt names, “-head” or “-end” specifies whether the style guidelines are placed before the function signature and docstring, similar to a directive, or appended at the end of the original docstring, simulating a normal docstring style. “-concise” and “-detailed” indicate the level of detail in the style guidelines. The detailed version includes three specific principles related to code readability, conciseness, and robustness, in addition to the concise information. According to the scoring principles outlined in Section 4.3, we evaluated the code samples generated using the basic prompt and four enhanced prompts for readability, conciseness, and robustness. 

![Prompt templates](https://github.com/DeepSoftwareAnalytics/Coding-Style-Empirical/blob/main/images/prompts-4.png)
